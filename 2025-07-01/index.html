<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-07-01 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-07-01
            
            
        </div>
        <span class="post-date">
            Jul 1, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 69]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 46]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 3]</li>
<li><a href="#quant-ph">quant-ph</a> [Total: 1]</li>
<li><a href="#physics.optics">physics.optics</a> [Total: 1]</li>
<li><a href="#stat.ML">stat.ML</a> [Total: 1]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 6]</li>
<li><a href="#stat.ME">stat.ME</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 6]</li>
<li><a href="#cs.MM">cs.MM</a> [Total: 1]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 1]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 4]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Fine-Grained-Preference-Optimization-Improves-Spatial-Reasoning-in-VLMs-cs-CV-cs-CLPDF"><a href="#1-Fine-Grained-Preference-Optimization-Improves-Spatial-Reasoning-in-VLMs-cs-CV-cs-CLPDF" class="headerlink" title="[1] Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs cs.CV | cs.CLPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21656">Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21656" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifan Shen, Yuanzhe Liu, Jingyuan Zhu, Xu Cao, Xiaofeng Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为SpatialReasoner-R1的视觉语言推理模型，通过创新的M3CTS方法和fDPO优化技术，显著提升了细粒度空间推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉语言模型（VLM）在多步逻辑和精确空间对齐方面表现不佳，需要一种新的方法来提升细粒度空间推理能力。</p>
<p><strong>Result:</strong> fDPO在空间质量任务中平均提升4.1%，在空间数量任务中提升9.0%；SpatialReasoner-R1在SPATIALRGPT-Bench上超越基线9.8%。</p>
<p><strong>Insight:</strong> 通过结合蒙特卡洛树搜索和细粒度偏好优化，能够显著提升视觉语言模型在复杂空间推理任务中的表现。</p>
<p><strong>Abstract:</strong> Current Vision-Language Models (VLMs) struggle with fine-grained spatial reasoning, particularly when multi-step logic and precise spatial alignment are required. In this work, we introduce SpatialReasoner-R1, a vision-language reasoning model designed to address these limitations. To construct high-quality supervision for spatial reasoning, we design a Multi-Model Monte Carlo Tree Search (M3CTS) method that generates diverse, logically consistent Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose fine-grained Direct Preference Optimization (fDPO), which introduces segment-specific preference granularity for descriptive grounding and logical reasoning, guided by a spatial reward mechanism that evaluates candidate responses based on visual consistency, spatial grounding, and logical coherence. Experimental results demonstrate that fDPO achieves an average improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0% gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in average accuracy, while maintaining competitive performance on general vision-language tasks.</p>
  </div>
</details>

<hr>
<h3 id="2-TanDiT-Tangent-Plane-Diffusion-Transformer-for-High-Quality-360°-Panorama-Generation-cs-CV-cs-LGPDF"><a href="#2-TanDiT-Tangent-Plane-Diffusion-Transformer-for-High-Quality-360°-Panorama-Generation-cs-CV-cs-LGPDF" class="headerlink" title="[2] TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation cs.CV | cs.LGPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21681">TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21681" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hakan Çapuk, Andrew Bond, Muhammed Burak Kızıl, Emir Göçen, Erkut Erdem</span></p>
<p><strong>TL;DR:</strong> TanDiT提出了一种基于切平面扩散Transformer的方法，用于高质量360°全景图像生成，解决了传统方法在几何畸变和环状一致性方面的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的图像生成模型在合成全景图像时面临几何畸变和环状一致性的问题，限制了生成质量。</p>
<p><strong>Result:</strong> 实验表明TanDiT能有效泛化到训练数据之外，处理复杂文本提示，并与其他生成模型无缝集成。</p>
<p><strong>Insight:</strong> 通过切平面设计和全局一致性后处理，TanDiT解决了全景生成的几何和一致性挑战，为未来研究提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in image generation have led to remarkable improvements in synthesizing perspective images. However, these models still struggle with panoramic image generation due to unique challenges, including varying levels of geometric distortion and the requirement for seamless loop-consistency. To address these issues while leveraging the strengths of the existing models, we introduce TanDiT, a method that synthesizes panoramic scenes by generating grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike previous methods relying on multiple diffusion branches, TanDiT utilizes a unified diffusion model trained to produce these tangent-plane images simultaneously within a single denoising iteration. Furthermore, we propose a model-agnostic post-processing step specifically designed to enhance global coherence across the generated panoramas. To accurately assess panoramic image quality, we also present two specialized metrics, TangentIS and TangentFID, and provide a comprehensive benchmark comprising captioned panoramic datasets and standardized evaluation scripts. Extensive experiments demonstrate that our method generalizes effectively beyond its training data, robustly interprets detailed and complex text prompts, and seamlessly integrates with various generative models to yield high-quality, diverse panoramic images.</p>
  </div>
</details>

<hr>
<h3 id="3-FOCUS-Internal-MLLM-Representations-for-Efficient-Fine-Grained-Visual-Question-Answering-cs-CVPDF"><a href="#3-FOCUS-Internal-MLLM-Representations-for-Efficient-Fine-Grained-Visual-Question-Answering-cs-CVPDF" class="headerlink" title="[3] FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering cs.CVPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21710">FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21710" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为FOCUS的训练自由视觉裁剪方法，通过利用MLLM内部表示引导搜索最相关的图像区域，提升了细粒度视觉问答（VQA）的性能和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大型语言模型（MLLM）在细粒度VQA任务中表现不佳，尤其是涉及微小图像细节时。现有视觉裁剪方法存在需要任务特定微调、效率低下或与高效注意力机制不兼容等问题。</p>
<p><strong>Result:</strong> 在四个细粒度VQA数据集和两种MLLM上表现优异，准确率和效率均超过三种流行视觉裁剪方法，计算量减少3-6.5倍。</p>
<p><strong>Insight:</strong> MLLM内部表示（如KV缓存）可用于高效定位细粒度视觉信息，无需额外训练即可提升VQA性能。</p>
<p><strong>Abstract:</strong> While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.</p>
  </div>
</details>

<hr>
<h3 id="4-CAST-Cross-Attentive-Spatio-Temporal-feature-fusion-for-Deepfake-detection-cs-CVPDF"><a href="#4-CAST-Cross-Attentive-Spatio-Temporal-feature-fusion-for-Deepfake-detection-cs-CVPDF" class="headerlink" title="[4] CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection cs.CVPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21711">CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21711" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aryan Thakre, Omkar Nagwekar, Vedang Talekar, Aparna Santra Biswas</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种统一模型CAST，通过跨注意机制融合时空特征，提升了深度伪造视频检测的性能和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有CNN-Transformer模型独立处理时空特征，融合方式简单，限制了时空交互的深度。论文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在多个数据集（FaceForensics++、Celeb-DF、DeepfakeDetection）上表现优异，AUC和准确率显著提升。</p>
<p><strong>Insight:</strong> 跨注意机制能更有效地捕捉时空特征的动态关联，为深度伪造检测开辟了新方向。</p>
<p><strong>Abstract:</strong> Deepfakes have emerged as a significant threat to digital media authenticity, increasing the need for advanced detection techniques that can identify subtle and time-dependent manipulations. CNNs are effective at capturing spatial artifacts, and Transformers excel at modeling temporal inconsistencies. However, many existing CNN-Transformer models process spatial and temporal features independently. In particular, attention-based methods often use separate attention mechanisms for spatial and temporal features and combine them using naive approaches like averaging, addition, or concatenation, which limits the depth of spatio-temporal interaction. To address this challenge, we propose a unified CAST model that leverages cross-attention to effectively fuse spatial and temporal features in a more integrated manner. Our approach allows temporal features to dynamically attend to relevant spatial regions, enhancing the model’s ability to detect fine-grained, time-evolving artifacts such as flickering eyes or warped lips. This design enables more precise localization and deeper contextual understanding, leading to improved performance across diverse and challenging scenarios. We evaluate the performance of our model using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both intra- and cross-dataset settings to affirm the superiority of our approach. Our model achieves strong performance with an AUC of 99.49 percent and an accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset testing, it demonstrates impressive generalization by achieving a 93.31 percent AUC on the unseen DeepfakeDetection dataset. These results highlight the effectiveness of cross-attention-based feature fusion in enhancing the robustness of deepfake video detection.</p>
  </div>
</details>

<hr>
<h3 id="5-Asymmetric-Dual-Self-Distillation-for-3D-Self-Supervised-Representation-Learning-cs-CVPDF"><a href="#5-Asymmetric-Dual-Self-Distillation-for-3D-Self-Supervised-Representation-Learning-cs-CVPDF" class="headerlink" title="[5] Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning cs.CVPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21724">Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21724" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Remco F. Leijenaar, Hamidreza Kasaei</span></p>
<p><strong>TL;DR:</strong> 论文提出了AsymDSD，一种非对称双自蒸馏框架，用于3D自监督表示学习，通过潜在空间预测统一掩码建模和不变性学习，在ScanObjectNN上取得了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的基于重构的掩码点建模（MPM）在捕捉高级语义时表现有限，因此需要一种新的方法来解决这一问题。</p>
<p><strong>Result:</strong> 在ScanObjectNN上实现了90.53%的精度，预训练后进一步提升至93.72%。</p>
<p><strong>Insight:</strong> 潜在空间预测比输入空间重构更适用于捕捉高级语义，非对称设计和多掩码采样能够有效提升表示学习的效果。</p>
<p><strong>Abstract:</strong> Learning semantically meaningful representations from unstructured 3D point clouds remains a central challenge in computer vision, especially in the absence of large-scale labeled datasets. While masked point modeling (MPM) is widely used in self-supervised 3D learning, its reconstruction-based objective can limit its ability to capture high-level semantics. We propose AsymDSD, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space. AsymDSD builds on a joint embedding architecture and introduces several key design choices: an efficient asymmetric setup, disabling attention between masked queries to prevent shape leakage, multi-mask sampling, and a point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k shapes, surpassing prior methods.</p>
  </div>
</details>

<hr>
<h3 id="6-Exploring-Image-Generation-via-Mutually-Exclusive-Probability-Spaces-and-Local-Correlation-Hypothesis-cs-CV-cs-AIPDF"><a href="#6-Exploring-Image-Generation-via-Mutually-Exclusive-Probability-Spaces-and-Local-Correlation-Hypothesis-cs-CV-cs-AIPDF" class="headerlink" title="[6] Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis cs.CV | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21731">Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21731" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenqiu Zhao, Anup Basu</span></p>
<p><strong>TL;DR:</strong> 论文提出了两个理论框架MESP和LCH，揭示了概率生成模型可能存在的局限性，即全局分布学习导致记忆而非生成行为。通过改进VAE和提出BL-AE、ARVM，在标准数据集上取得竞争性FID分数，但发现这反映的是记忆而非生成能力，进而提出LCH假设。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究发现概率生成模型在全局分布学习时易陷入记忆而非生成行为，导致模型无法真正具备生成能力，希望通过理论框架和实验解决这一问题。</p>
<p><strong>Result:</strong> ARVM在标准数据集上取得竞争性FID分数，但实验表明高分反映的是记忆行为而非生成能力。</p>
<p><strong>Insight:</strong> 生成模型的性能指标如FID可能无法真实反映生成能力，局部相关性（LCH）可能是真正驱动生成的关键因素。</p>
<p><strong>Abstract:</strong> We propose two theoretical frameworks, the Mutually Exclusive Probability Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential limitation in probabilistic generative models; namely that learning global distributions leads to memorization rather than generative behavior. MESP emerges from our rethinking of the Variational Autoencoder (VAE). We observe that latent variable distributions in VAE exhibit overlap, which leads to an optimization conflict between the reconstruction loss and KL-divergence loss. A lower bound based on the overlap coefficient is proposed. We refer to this phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary Latent Autoencoder (BL-AE) is proposed to encode images into binary latent representations. These binary latents are used as the input to our Autoregressive Random Variable Model (ARVM), a modified autoregressive model outputting histograms. Our ARVM achieves competitive FID scores, outperforming state-of-the-art methods on standard datasets. However, such scores reflect memorization rather than generation. To address this issue, we propose the Local Correlation Hypothesis (LCH), which posits that generative capability arising from local correlations among latent variables. Comprehensive experiments and discussions are conducted to validate our frameworks.</p>
  </div>
</details>

<hr>
<h3 id="7-Equitable-Federated-Learning-with-NCA-cs-CVPDF"><a href="#7-Equitable-Federated-Learning-with-NCA-cs-CVPDF" class="headerlink" title="[7] Equitable Federated Learning with NCA cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21735">Equitable Federated Learning with NCA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21735" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nick Lemke, Mirko Konstantin, Henry John Krumb, John Kalkhof, Jonathan Stieber</span></p>
<p><strong>TL;DR:</strong> 本文提出了适用于医疗图像分割任务的轻量级联邦学习系统FedNCA，解决了资源受限地区（如低收入和中等收入国家）在联邦学习中面临的高计算资源需求和网络不可靠问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在资源受限地区，医疗专业人员有限，联邦学习（FL）可以促进跨机构的协作模型训练，但现有的FL系统对高性能计算和稳定网络的需求限制了其在低收入和中等收入国家（LMICs）的应用。</p>
<p><strong>Result:</strong> FedNCA实现了在资源受限环境下的高效、轻量级和安全的医疗图像分割任务，推动了这些地区的医疗公平。</p>
<p><strong>Insight:</strong> 通过轻量化和通信优化，FedNCA为资源受限地区提供了可行的联邦学习解决方案，为医疗AI的普及奠定了基础。</p>
<p><strong>Abstract:</strong> Federated Learning (FL) is enabling collaborative model training across institutions without sharing sensitive patient data. This approach is particularly valuable in low- and middle-income countries (LMICs), where access to trained medical professionals is limited. However, FL adoption in LMICs faces significant barriers, including limited high-performance computing resources and unreliable internet connectivity. To address these challenges, we introduce FedNCA, a novel FL system tailored for medical image segmentation tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training on low-cost edge devices, such as widely available smartphones, while minimizing communication costs. Additionally, our encryption-ready FedNCA proves to be suitable for compromised network communication. By overcoming infrastructural and security challenges, FedNCA paves the way for inclusive, efficient, lightweight, and encryption-ready medical imaging solutions, fostering equitable healthcare advancements in resource-constrained regions.</p>
  </div>
</details>

<hr>
<h3 id="8-ImplicitQA-Going-beyond-frames-towards-Implicit-Video-Reasoning-cs-CVPDF"><a href="#8-ImplicitQA-Going-beyond-frames-towards-Implicit-Video-Reasoning-cs-CVPDF" class="headerlink" title="[8] ImplicitQA: Going beyond frames towards Implicit Video Reasoning cs.CVPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21742">ImplicitQA: Going beyond frames towards Implicit Video Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21742" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sirnam Swetha, Rohit Gupta, Parth Parag Kulkarni, David G Shatwell, Jeffrey A Chan Santiago</span></p>
<p><strong>TL;DR:</strong> ImplicitQA是一个新的视频问答（VideoQA）基准测试，专注于测试模型在需要隐式推理（如动机、因果关系等）的任务上的表现，填补了当前视频问答系统在人类类似理解能力上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频问答基准主要关注可以通过明确视觉内容回答的问题，而忽略了人类在观看创意和叙事视频时所需的隐式推理能力。本文旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验表明当前VideoQA模型在隐式推理任务上表现显著下降，显示出对表层视觉线索的依赖和对隐式推理能力的不足。</p>
<p><strong>Insight:</strong> 隐式推理是视频理解的难点，需要模型具备跨时间和上下文的信息整合能力；当前模型在这一挑战中仍有很大改进空间。</p>
<p><strong>Abstract:</strong> Video QA has made significant strides by leveraging multimodal learning to align visual and textual modalities. However, current benchmarks overwhelmingly focus on questions answerable through explicit visual content - actions, objects &amp; events directly observable within individual frames or short clips. In contrast, creative and cinematic videos - such as movies, TV shows, and narrative-driven content - employ storytelling techniques that deliberately omit certain depictions, requiring viewers to infer motives, causality, and relationships across discontinuous frames. Humans naturally excel at such implicit reasoning, seamlessly integrating information across time and context to construct coherent narratives. Current VideoQA systems and benchmarks fail to capture this essential dimension of human-like understanding. To bridge this gap, we present ImplicitQA, a novel benchmark specifically designed to test models on implicit reasoning. It comprises 1K meticulously annotated QA pairs derived from 320+ high-quality creative video clips, systematically categorized into key reasoning dimensions: lateral and vertical spatial reasoning, depth and proximity, viewpoint and visibility, motion and trajectory, causal and motivational reasoning, social interactions, physical context, and inferred counting. These annotations are deliberately challenging, crafted by authors ensuring high-quality. Our extensive evaluations on leading VideoQA models reveals performance degradation, underscoring their reliance on surface-level visual cues and highlighting the difficulty of implicit reasoning. Performance variations across models further illustrate the complexity and diversity of the challenges presented by ImplicitQA. By releasing both the dataset and our data collection framework, we aim to stimulate further research and development in the community. <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/ucf-crcv/ImplicitQA">https://huggingface.co/datasets/ucf-crcv/ImplicitQA</a>.</p>
  </div>
</details>

<hr>
<h3 id="9-Early-Glaucoma-Detection-using-Deep-Learning-with-Multiple-Datasets-of-Fundus-Images-cs-CV-cs-LGPDF"><a href="#9-Early-Glaucoma-Detection-using-Deep-Learning-with-Multiple-Datasets-of-Fundus-Images-cs-CV-cs-LGPDF" class="headerlink" title="[9] Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images cs.CV | cs.LGPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21770">Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21770" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rishiraj Paul Chowdhury, Nirmit Shekar Karkera</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于EfficientNet-B0架构的深度学习流程，通过多数据集（ACRIMA、ORIGA、RIM-ONE）的顺序训练和微调，实现视网膜眼底图像的青光眼早期检测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 青光眼是导致不可逆失明的主要原因，早期检测可显著改善治疗效果。传统诊断方法通常具有侵入性且需专业设备，亟需非侵入性的高效解决方案。</p>
<p><strong>Result:</strong> 模型在AUC-ROC指标上表现优异，且在跨数据集测试中展示了强泛化能力。</p>
<p><strong>Insight:</strong> 1. 多数据集训练可提升模型泛化性；2. 简单预处理足以满足需求，避免复杂操作；3. 该方法具有临床实用潜力。</p>
<p><strong>Abstract:</strong> Glaucoma is a leading cause of irreversible blindness, but early detection can significantly improve treatment outcomes. Traditional diagnostic methods are often invasive and require specialized equipment. In this work, we present a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma detection from retinal fundus images. Unlike prior studies that rely on single datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA, and RIM-ONE datasets to enhance generalization. Our experiments show that minimal preprocessing yields higher AUC-ROC compared to more complex enhancements, and our model demonstrates strong discriminative performance on unseen datasets. The proposed pipeline offers a reproducible and scalable approach to early glaucoma detection, supporting its potential clinical utility.</p>
  </div>
</details>

<hr>
<h3 id="10-Comparing-Learning-Paradigms-for-Egocentric-Video-Summarization-cs-CV-cs-AIPDF"><a href="#10-Comparing-Learning-Paradigms-for-Egocentric-Video-Summarization-cs-CV-cs-AIPDF" class="headerlink" title="[10] Comparing Learning Paradigms for Egocentric Video Summarization cs.CV | cs.AIPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21785">Comparing Learning Paradigms for Egocentric Video Summarization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21785" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Daniel Wen</span></p>
<p><strong>TL;DR:</strong> 该研究比较了监督学习、无监督学习和提示调优（prompt fine-tuning）三种范式在自中心视频摘要任务中的表现，发现通用模型GPT-4o在调整后优于专用模型，突显了当前方法对第一视角视频的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自中心视频（egocentric video）的独特视角带来了处理上的特殊挑战，现有模型对其理解能力有限。研究旨在探索不同学习范式在这一任务中的潜力。</p>
<p><strong>Result:</strong> GPT-4o表现最佳，但所有模型在第一视角视频中的表现均劣于第三视角视频。</p>
<p><strong>Insight:</strong> 通用模型通过适当调优可能在特殊任务中超越专用模型；自中心视频领域仍需进一步研究。</p>
<p><strong>Abstract:</strong> In this study, we investigate various computer vision paradigms - supervised learning, unsupervised learning, and prompt fine-tuning - by assessing their ability to understand and interpret egocentric video data. Specifically, we examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM (state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned pre-trained model), evaluating their effectiveness in video summarization. Our results demonstrate that current state-of-the-art models perform less effectively on first-person videos compared to third-person videos, highlighting the need for further advancements in the egocentric video domain. Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these specialized models, emphasizing the limitations of existing approaches in adapting to the unique challenges of first-person perspectives. Although our evaluation is conducted on a small subset of egocentric videos from the Ego-Exo4D dataset due to resource constraints, the primary objective of this research is to provide a comprehensive proof-of-concept analysis aimed at advancing the application of computer vision techniques to first-person videos. By exploring novel methodologies and evaluating their potential, we aim to contribute to the ongoing development of models capable of effectively processing and interpreting egocentric perspectives.</p>
  </div>
</details>

<hr>
<h3 id="11-CAT-SG-A-Large-Dynamic-Scene-Graph-Dataset-for-Fine-Grained-Understanding-of-Cataract-Surgery-cs-CV-cs-AI-cs-LGPDF"><a href="#11-CAT-SG-A-Large-Dynamic-Scene-Graph-Dataset-for-Fine-Grained-Understanding-of-Cataract-Surgery-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[11] CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery cs.CV | cs.AI | cs.LGPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21813">CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21813" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Felix Holm, Gözde Ünver, Ghazal Ghazaei, Nassir Navab</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了CAT-SG数据集，首个针对白内障手术的动态场景图数据集，通过捕捉手术工具与组织间的语义关系，支持手术流程的细粒度理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系和时间依赖性的综合表征。因此，需要一种更全面的数据集来支持复杂手术流程的建模。</p>
<p><strong>Result:</strong> CatSGG模型在生成结构化手术表征方面表现优于现有方法，CAT-SG数据集支持更准确的手术阶段和技术识别。</p>
<p><strong>Insight:</strong> 该研究通过动态场景图建模手术流程，为AI驱动的手术培训、实时决策支持和流程分析提供了更智能、上下文感知的基础。</p>
<p><strong>Abstract:</strong> Understanding the intricate workflows of cataract surgery requires modeling complex interactions between surgical tools, anatomical structures, and procedural techniques. Existing datasets primarily address isolated aspects of surgical analysis, such as tool detection or phase segmentation, but lack comprehensive representations that capture the semantic relationships between entities over time. This paper introduces the Cataract Surgery Scene Graph (CAT-SG) dataset, the first to provide structured annotations of tool-tissue interactions, procedural variations, and temporal dependencies. By incorporating detailed semantic relations, CAT-SG offers a holistic view of surgical workflows, enabling more accurate recognition of surgical phases and techniques. Additionally, we present a novel scene graph generation model, CatSGG, which outperforms current methods in generating structured surgical representations. The CAT-SG dataset is designed to enhance AI-driven surgical training, real-time decision support, and workflow analysis, paving the way for more intelligent, context-aware systems in clinical practice.</p>
  </div>
</details>

<hr>
<h3 id="12-Few-Shot-Segmentation-of-Historical-Maps-via-Linear-Probing-of-Vision-Foundation-Models-cs-CV-cs-AI-cs-LGPDF"><a href="#12-Few-Shot-Segmentation-of-Historical-Maps-via-Linear-Probing-of-Vision-Foundation-Models-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[12] Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models cs.CV | cs.AI | cs.LGPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21826">Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21826" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rafael Sterzinger, Marco Peer, Robert Sablatnig</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种高效的小样本历史地图分割方法，利用视觉基础模型的语义嵌入和参数高效微调，在数据稀缺情况下显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 历史地图的多样性标注数据稀缺，传统方法难以自动化处理，需要一种能在小样本情况下高效分割的方法。</p>
<p><strong>Result:</strong> 在Siegfried数据集上，mIoU相对提升5%至20%；在ICDAR 2021数据集上，PQ达到67.3%，展示了强泛化能力。</p>
<p><strong>Insight:</strong> 小样本分割任务可以通过预训练视觉基础模型的迁移学习显著提升性能，同时减少标注和计算成本。</p>
<p><strong>Abstract:</strong> As rich sources of history, maps provide crucial insights into historical changes, yet their diverse visual representations and limited annotated data pose significant challenges for automated processing. We propose a simple yet effective approach for few-shot segmentation of historical maps, leveraging the rich semantic embeddings of large vision foundation models combined with parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on the Siegfried benchmark dataset in vineyard and railway segmentation, achieving +5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20% in the more challenging 5-shot setting. Additionally, it demonstrates strong performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3% for building block segmentation, despite not being optimized for this shape-sensitive metric, underscoring its generalizability. Notably, our approach maintains high performance even in extremely low-data regimes (10- &amp; 5-shot), while requiring only 689k trainable parameters - just 0.21% of the total model size. Our approach enables precise segmentation of diverse historical maps while drastically reducing the need for manual annotations, advancing automated processing and analysis in the field. Our implementation is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/RafaelSterzinger/few-shot-map-segmentation">https://github.com/RafaelSterzinger/few-shot-map-segmentation</a>.</p>
  </div>
</details>

<hr>
<h3 id="13-TaleForge-Interactive-Multimodal-System-for-Personalized-Story-Creation-cs-CVPDF"><a href="#13-TaleForge-Interactive-Multimodal-System-for-Personalized-Story-Creation-cs-CVPDF" class="headerlink" title="[13] TaleForge: Interactive Multimodal System for Personalized Story Creation cs.CVPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21832">TaleForge: Interactive Multimodal System for Personalized Story Creation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21832" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minh-Loi Nguyen, Quang-Khai Le, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</span></p>
<p><strong>TL;DR:</strong> TaleForge是一个交互式多模态系统，通过结合大语言模型（LLMs）和文本到图像扩散技术，将用户的面部图像嵌入故事叙述和插图中，实现个性化故事创作。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通常将用户视为被动消费者，提供通用情节且缺乏个性化，限制了用户的参与感和沉浸感。TaleForge旨在通过个性化故事创作提升用户的参与度和沉浸感。</p>
<p><strong>Result:</strong> 用户研究表明，当用户作为故事主角时，参与感和归属感显著提升。用户赞赏系统的实时预览和直观控制，但也提出了对更精细叙事编辑工具的需求。</p>
<p><strong>Insight:</strong> 多模态技术（如LLMs和文本到图像扩散）的结合可以显著提升个性化故事创作的沉浸感和用户体验，但需要进一步优化编辑功能以满足用户需求。</p>
<p><strong>Abstract:</strong> Storytelling is a deeply personal and creative process, yet existing methods often treat users as passive consumers, offering generic plots with limited personalization. This undermines engagement and immersion, especially where individual style or appearance is crucial. We introduce TaleForge, a personalized story-generation system that integrates large language models (LLMs) and text-to-image diffusion to embed users’ facial images within both narratives and illustrations. TaleForge features three interconnected modules: Story Generation, where LLMs create narratives and character descriptions from user prompts; Personalized Image Generation, merging users’ faces and outfit choices into character illustrations; and Background Generation, creating scene backdrops that incorporate personalized characters. A user study demonstrated heightened engagement and ownership when individuals appeared as protagonists. Participants praised the system’s real-time previews and intuitive controls, though they requested finer narrative editing tools. TaleForge advances multimodal storytelling by aligning personalized text and imagery to create immersive, user-centric experiences.</p>
  </div>
</details>

<hr>
<h3 id="14-GenEscape-Hierarchical-Multi-Agent-Generation-of-Escape-Room-Puzzles-cs-CV-cs-CLPDF"><a href="#14-GenEscape-Hierarchical-Multi-Agent-Generation-of-Escape-Room-Puzzles-cs-CV-cs-CLPDF" class="headerlink" title="[14] GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles cs.CV | cs.CLPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21839">GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21839" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengyi Shan, Brian Curless, Ira Kemelmacher-Shlizerman, Steve Seitz</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种层次化多智能体框架GenEscape，用于生成视觉吸引、逻辑严密且富有挑战性的密室逃脱谜题图片，通过功能设计、符号场景图推理、布局合成和局部图像编辑的分阶段协作提升生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的文本到图像模型在空间关系理解和功能推理能力上表现不足，难以生成复杂且功能合理的密室逃脱谜题图像，因此需要一种更结构化的方法来分解和优化这一生成过程。</p>
<p><strong>Result:</strong> 实验表明，智能体协作显著提高了谜题的逻辑严密性（如避免捷径）和功能清晰性，同时保持了视觉质量。</p>
<p><strong>Insight:</strong> 通过任务分解和协作反馈，可以弥补单一模型在复杂生成任务中的局限性，为多模态生成任务提供新思路。</p>
<p><strong>Abstract:</strong> We challenge text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning, we propose a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable. Experiments show that agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.</p>
  </div>
</details>

<hr>
<h3 id="15-Periodic-MAE-Periodic-Video-Masked-Autoencoder-for-rPPG-Estimation-cs-CVPDF"><a href="#15-Periodic-MAE-Periodic-Video-Masked-Autoencoder-for-rPPG-Estimation-cs-CVPDF" class="headerlink" title="[15] Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation cs.CVPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21855">Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21855" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiho Choi, Sang Jun Lee</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Periodic-MAE的自监督学习方法，通过视频掩码自编码器从无标记的面部视频中学习周期性信号的通用表示，并将其应用于远程光电容积描记（rPPG）的估计任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 远程光电容积描记（rPPG）是一种无需接触就能测量生理信号（如心率）的技术，但现有方法在复杂场景下的泛化能力有限。论文旨在通过自监督学习提取更鲁棒的周期性信号表示，提升rPPG的性能。</p>
<p><strong>Result:</strong> 实验结果表明，所提方法在多个数据集上显著优于现有方法，特别是在跨数据集场景中表现突出。</p>
<p><strong>Insight:</strong> 1. 自监督预训练能有效捕捉面部视频中的周期性信号；2. 生理带宽约束是提升模型性能的关键因素；3. 方法在复杂场景下具有较强的泛化能力。</p>
<p><strong>Abstract:</strong> In this paper, we propose a method that learns a general representation of periodic signals from unlabeled facial videos by capturing subtle changes in skin tone over time. The proposed framework employs the video masked autoencoder to learn a high-dimensional spatio-temporal representation of the facial region through self-supervised learning. Capturing quasi-periodic signals in the video is crucial for remote photoplethysmography (rPPG) estimation. To account for signal periodicity, we apply frame masking in terms of video sampling, which allows the model to capture resampled quasi-periodic signals during the pre-training stage. Moreover, the framework incorporates physiological bandlimit constraints, leveraging the property that physiological signals are sparse within their frequency bandwidth to provide pulse cues to the model. The pre-trained encoder is then transferred to the rPPG task, where it is used to extract physiological signals from facial videos. We evaluate the proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and V4V datasets. Our results demonstrate significant performance improvements, particularly in challenging cross-dataset evaluations. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ziiho08/Periodic-MAE">https://github.com/ziiho08/Periodic-MAE</a>.</p>
  </div>
</details>

<hr>
<h3 id="16-SPADE-Spatial-Transcriptomics-and-Pathology-Alignment-Using-a-Mixture-of-Data-Experts-for-an-Expressive-Latent-Space-cs-CV-cs-AI-cs-LGPDF"><a href="#16-SPADE-Spatial-Transcriptomics-and-Pathology-Alignment-Using-a-Mixture-of-Data-Experts-for-an-Expressive-Latent-Space-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[16] SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space cs.CV | cs.AI | cs.LGPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21857">SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21857" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ekaterina Redekop, Mara Pleasure, Zichen Wang, Kimberly Flores, Anthony Sisk</span></p>
<p><strong>TL;DR:</strong> SPADE是一种基于多模态数据（病理全切片图像和空间转录组）的基础模型，通过专家混合技术和对比学习，构建了一个统一的潜在空间，显著提升了few-shot性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 数字病理学和多模态数据的快速发展需要一种方法，能够整合病理图像和空间转录组数据，以捕捉更丰富的分子异质性。</p>
<p><strong>Result:</strong> SPADE在few-shot学习中表现显著优于基线模型，证明了其在形态学和分子信息整合方面的优势。</p>
<p><strong>Insight:</strong> 整合多模态数据（如病理图像和空间转录组）可以显著提升模型的表示能力，尤其是在数据稀缺的下游任务中。</p>
<p><strong>Abstract:</strong> The rapid growth of digital pathology and advances in self-supervised deep learning have enabled the development of foundational models for various pathology tasks across diverse diseases. While multimodal approaches integrating diverse data sources have emerged, a critical gap remains in the comprehensive integration of whole-slide images (WSIs) with spatial transcriptomics (ST), which is crucial for capturing critical molecular heterogeneity beyond standard hematoxylin &amp; eosin (H&amp;E) staining. We introduce SPADE, a foundation model that integrates histopathology with ST data to guide image representation learning within a unified framework, in effect creating an ST-informed latent space. SPADE leverages a mixture-of-data experts technique, where experts, created via two-stage feature-space clustering, use contrastive learning to learn representations of co-registered WSI patches and gene expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is evaluated on 14 downstream tasks, demonstrating significantly superior few-shot performance compared to baseline models, highlighting the benefits of integrating morphological and molecular information into one latent space.</p>
  </div>
</details>

<hr>
<h3 id="17-LLaVA-Scissor-Token-Compression-with-Semantic-Connected-Components-for-Video-LLMs-cs-CV-cs-AI-cs-HC-cs-MMPDF"><a href="#17-LLaVA-Scissor-Token-Compression-with-Semantic-Connected-Components-for-Video-LLMs-cs-CV-cs-AI-cs-HC-cs-MMPDF" class="headerlink" title="[17] LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs cs.CV | cs.AI | cs.HC | cs.MMPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21862">LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.HC | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21862" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Boyuan Sun, Jiaxing Zhao, Xihan Wei, Qibin Hou</span></p>
<p><strong>TL;DR:</strong> LLaVA-Scissor提出了一种无需训练的token压缩策略，通过语义连通分量（SCC）方法在视频多模态大语言模型中实现高效的token压缩，显著提升了视频理解任务的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的token压缩方法主要基于注意力分数，但无法全面捕捉语义区域且存在冗余。LLaVA-Scissor旨在通过语义连通分量方法更有效地压缩token，提高模型效率。</p>
<p><strong>Result:</strong> 在视频问答、长视频理解等任务中，LLaVA-Scissor在低token保留率下表现优于其他方法。</p>
<p><strong>Insight:</strong> 语义驱动的token压缩方法比基于注意力的方法更有效，尤其适用于视频多模态模型。</p>
<p><strong>Abstract:</strong> In this paper, we present LLaVA-Scissor, a training-free token compression strategy designed for video multimodal large language models. Previous methods mostly attempt to compress tokens based on attention scores, but fail to effectively capture all semantic regions and often lead to token redundancy. Differently, we propose to leverage the Semantic Connected Components (SCC) approach that assigns tokens to distinct semantic regions within the token set, ensuring comprehensive semantic coverage. The outcome is a two-step spatio-temporal token compression strategy that utilizes SCC in both spatial and temporal domains. This strategy can effectively compress tokens by representing the entire video with a set of non-overlapping semantic tokens. We conduct extensive evaluations of the token compression capabilities of LLaVA-Scissor across diverse video understanding benchmarks, including video question answering, long video understanding, and comprehensive multi-choices benchmarks. Experimental results show that the proposed LLaVA-Scissor outperforms other token compression methods, achieving superior performance in various video understanding benchmarks, particularly at low token retention ratios. Project page: <a target="_blank" rel="noopener" href="https://github.com/HumanMLLM/LLaVA-Scissor">https://github.com/HumanMLLM/LLaVA-Scissor</a>.</p>
  </div>
</details>

<hr>
<h3 id="18-Remote-Sensing-Large-Vision-Language-Model-Semantic-augmented-Multi-level-Alignment-and-Semantic-aware-Expert-Modeling-cs-CVPDF"><a href="#18-Remote-Sensing-Large-Vision-Language-Model-Semantic-augmented-Multi-level-Alignment-and-Semantic-aware-Expert-Modeling-cs-CVPDF" class="headerlink" title="[18] Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling cs.CVPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21863">Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21863" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sungjune Park, Yeongyun Kim, Se Yeon Kim, Yong Man Ro</span></p>
<p><strong>TL;DR:</strong> 该论文针对遥感图像中的视觉-语言任务提出了一种新型的大型视觉-语言模型框架，通过语义增强的多级对齐和语义感知的专家建模，解决了现有LVLMs在遥感领域的适配问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 遥感图像与自然图像在视觉外观、对象尺度和语义上存在显著差异，限制了现有LVLMs的直接适配。遥感场景包含从粗到细的多级语义信息，需要专门的模型来处理。</p>
<p><strong>Result:</strong> 在多个遥感任务（如场景分类和视觉问答）中表现出显著的性能提升，证明了框架在跨语义层级理解上的有效性。</p>
<p><strong>Insight:</strong> 通过多级语义对齐和专家分层处理，可以显著提升模型在遥感领域的跨模态理解能力，为未来遥感专用LVLMs的研究提供了新思路。</p>
<p><strong>Abstract:</strong> Large Vision and Language Models (LVLMs) have shown strong performance across various vision-language tasks in natural image domains. However, their application to remote sensing (RS) remains underexplored due to significant domain differences in visual appearances, object scales, and semantics. These discrepancies hider the effective understanding of RS scenes, which contain rich, multi-level semantic information spanning from coarse-to-fine levels. Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To address this gap, we propose a novel LVLM framework tailored for RS understanding, incorporating two core components: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. First, to align multi-level visual features, we introduce the retrieval-based Semantic Augmentation Module which enriches the visual features with relevant semantics across fine-to-coarse levels (e.g., object- and scene-level information). It is designed to retrieve relevant semantic cues from a RS semantic knowledge database, followed by aggregation of semantic cues with user query and multi-level visual features, resulting in semantically enriched representation across multiple levels. Second, for Semantic-aware Expert Modeling, we design semantic experts, where each expert is responsible for processing semantic representation at different levels separately. This enables hierarchical semantic understanding from coarse to fine levels. Evaluations across multiple RS tasks-including scene classification and VQA, etc.-demonstrate that the proposed framework achieves consistent improvements across multiple semantic levels. This highlights its capability and effectiveness in bridging the gap between general LVLMs and unique demands of RS-specific vision-language understanding.</p>
  </div>
</details>

<hr>
<h3 id="19-Dual-Perspective-United-Transformer-for-Object-Segmentation-in-Optical-Remote-Sensing-Images-cs-CVPDF"><a href="#19-Dual-Perspective-United-Transformer-for-Object-Segmentation-in-Optical-Remote-Sensing-Images-cs-CVPDF" class="headerlink" title="[19] Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images cs.CVPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21866">Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21866" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yanguang Sun, Jiexi Yan, Jianjun Qian, Chunyan Xu, Jian Yang</span></p>
<p><strong>TL;DR:</strong> DPU-Former提出了一种新型双视角统一Transformer，用于光学遥感图像中的目标分割，通过全局-局部混合注意力和傅里叶空间融合策略，解决了现有方法在特征异质性和模型复杂性上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法主要基于卷积或Transformer特征，各自优势尚未充分结合，且特征异质性和模型复杂性常被忽视，导致分割性能不佳。</p>
<p><strong>Result:</strong> 在多个数据集上超越现有最优方法，证明了模型的有效性。</p>
<p><strong>Insight:</strong> 结合卷积与Transformer特征的优势需解决特征异质性，而傅里叶空间融合策略为高效特征融合提供了新思路。</p>
<p><strong>Abstract:</strong> Automatically segmenting objects from optical remote sensing images (ORSIs) is an important task. Most existing models are primarily based on either convolutional or Transformer features, each offering distinct advantages. Exploiting both advantages is valuable research, but it presents several challenges, including the heterogeneity between the two types of features, high complexity, and large parameters of the model. However, these issues are often overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with a unique structure designed to simultaneously integrate long-range dependencies and spatial details. In particular, we design the global-local mixed attention, which captures diverse information through two perspectives and introduces a Fourier-space merging strategy to obviate deviations for efficient fusion. Furthermore, we present a gated linear feed-forward network to increase the expressive ability. Additionally, we construct a DPU-Former decoder to aggregate and strength features at different layers. Consequently, the DPU-Former model outperforms the state-of-the-art methods on multiple datasets. Code: <a target="_blank" rel="noopener" href="https://github.com/CSYSI/DPU-Former">https://github.com/CSYSI/DPU-Former</a>.</p>
  </div>
</details>

<hr>
<h3 id="20-Grounding-Aware-Token-Pruning-Recovering-from-Drastic-Performance-Drops-in-Visual-Grounding-Caused-by-Pruning-cs-CV-cs-AIPDF"><a href="#20-Grounding-Aware-Token-Pruning-Recovering-from-Drastic-Performance-Drops-in-Visual-Grounding-Caused-by-Pruning-cs-CV-cs-AIPDF" class="headerlink" title="[20] Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning cs.CV | cs.AIPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21873">Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21873" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tzu-Chun Chien, Chieh-Kai Lin, Shiang-Feng Tsai, Ruei-Chi Lai, Hung-Jen Chen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Grounding-Aware Token Pruning (GAP)方法，通过调整位置ID来恢复因视觉令牌修剪导致的视觉接地性能下降，无需额外训练或计算资源。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型(MLLMs)在视觉接地任务中表现出色，但处理大量视觉令牌带来的高计算成本促使了令牌修剪技术的发展。然而，修剪会导致模型接地能力显著下降，论文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在RefCOCO验证集上，LLaVA的接地准确率从修剪后的15.34%恢复到51.42%，接近原始性能的90%。</p>
<p><strong>Insight:</strong> 位置ID的顺序和值对视觉接地任务的性能至关重要，GAP提供了一种轻量级解决方案，适用于多种模型。</p>
<p><strong>Abstract:</strong> Recent Multimodal Large Language Models (MLLMs) have demonstrated strong performance in visual grounding, establishing themselves as a general interface for various vision-language applications. This progress has driven the development of token pruning methods to mitigate the high computational costs associated with processing numerous visual tokens. However, we observe that pruning significantly weakens the model’s grounding ability, leading to incorrect predictions and drastic performance degradation. In Referring Expression Comprehension (REC), for instance, pruning causes the accuracy of LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis identifies misaligned position IDs after pruning as the primary cause of this degradation, as both the order and value of these IDs are crucial for maintaining performance in grounding tasks. To address this issue, we propose Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to position IDs that recovers REC accuracy back to 51.42%, which is 90% of the original performance in the without pruning setting, all while requiring no additional training, memory, or computational overhead. Applied to models such as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves performance across various token pruning strategies.</p>
  </div>
</details>

<hr>
<h3 id="21-GRASP-PsONet-Gradient-based-Removal-of-Spurious-Patterns-for-PsOriasis-Severity-Classification-cs-CVPDF"><a href="#21-GRASP-PsONet-Gradient-based-Removal-of-Spurious-Patterns-for-PsOriasis-Severity-Classification-cs-CVPDF" class="headerlink" title="[21] GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification cs.CVPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21883">GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21883" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Basudha Pal, Sharif Amit Kamran, Brendon Lutnick, Molly Lucas, Chaitanya Parmar</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于梯度的框架GRASP-PsONet，用于自动识别并去除训练数据中引入虚假相关性的图像，从而提升银屑病（PsO）严重程度分类模型的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 银屑病严重程度评分的自动化受到图像质量不一致和标注变异的挑战，传统方法依赖多标注者和人工评审，成本高昂。</p>
<p><strong>Result:</strong> 移除8.2%的问题图像后，模型AUC-ROC提升了5%（从85%到90%），并能高效识别标注不一致的样本（覆盖90%的评分差异案例）。</p>
<p><strong>Insight:</strong> 梯度分析方法可有效检测数据中的虚假相关性，为远程医疗中的自动化评分提供了高鲁棒性的解决方案。</p>
<p><strong>Abstract:</strong> Psoriasis (PsO) severity scoring is important for clinical trials but is hindered by inter-rater variability and the burden of in person clinical evaluation. Remote imaging using patient captured mobile photos offers scalability but introduces challenges, such as variation in lighting, background, and device quality that are often imperceptible to humans but can impact model performance. These factors, along with inconsistencies in dermatologist annotations, reduce the reliability of automated severity scoring. We propose a framework to automatically flag problematic training images that introduce spurious correlations which degrade model generalization, using a gradient based interpretability approach. By tracing the gradients of misclassified validation images, we detect training samples where model errors align with inconsistently rated examples or are affected by subtle, nonclinical artifacts. We apply this method to a ConvNeXT based weakly supervised model designed to classify PsO severity from phone images. Removing 8.2% of flagged images improves model AUC-ROC by 5% (85% to 90%) on a held out test set. Commonly, multiple annotators and an adjudication process ensure annotation accuracy, which is expensive and time consuming. Our method detects training images with annotation inconsistencies, potentially removing the need for manual review. When applied to a subset of training data rated by two dermatologists, the method identifies over 90% of cases with inter-rater disagreement by reviewing only the top 30% of samples. This improves automated scoring for remote assessments, ensuring robustness despite data collection variability.</p>
  </div>
</details>

<hr>
<h3 id="22-Integrating-Multi-Modal-Sensors-A-Review-of-Fusion-Techniques-for-Intelligent-Vehicles-cs-CV-cs-MM-cs-ROPDF"><a href="#22-Integrating-Multi-Modal-Sensors-A-Review-of-Fusion-Techniques-for-Intelligent-Vehicles-cs-CV-cs-MM-cs-ROPDF" class="headerlink" title="[22] Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles cs.CV | cs.MM | cs.ROPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21885">Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MM | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21885" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chuheng Wei, Ziye Qin, Ziyan Zhang, Guoyuan Wu, Matthew J. Barth</span></p>
<p><strong>TL;DR:</strong> 这篇论文综述了多传感器融合在自动驾驶中的关键作用，归类了数据级、特征级和决策级融合策略，并系统回顾了基于深度学习的融合方法。同时探讨了多模态数据集的应用及新兴趋势，如视觉语言模型和大语言模型的集成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多传感器融合对于提升自动驾驶的环境感知能力至关重要，尤其在恶劣天气和复杂城市场景中。当前的方法需要系统化的总结和未来方向的展望。</p>
<p><strong>Result:</strong> 通过融合多种传感器数据，可以显著提升自动驾驶系统的环境感知能力和鲁棒性，尤其是在复杂场景中。</p>
<p><strong>Insight:</strong> 视觉语言模型和大语言模型的集成可能是未来多传感器融合的重要方向，有望进一步提升系统的自适应能力。</p>
<p><strong>Abstract:</strong> Multi-sensor fusion plays a critical role in enhancing perception for autonomous driving, overcoming individual sensor limitations, and enabling comprehensive environmental understanding. This paper first formalizes multi-sensor fusion strategies into data-level, feature-level, and decision-level categories and then provides a systematic review of deep learning-based methods corresponding to each strategy. We present key multi-modal datasets and discuss their applicability in addressing real-world challenges, particularly in adverse weather conditions and complex urban environments. Additionally, we explore emerging trends, including the integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and the role of sensor fusion in end-to-end autonomous driving, highlighting its potential to enhance system adaptability and robustness. Our work offers valuable insights into current methods and future directions for multi-sensor fusion in autonomous driving.</p>
  </div>
</details>

<hr>
<h3 id="23-DIVE-Deep-search-Iterative-Video-Exploration-A-Technical-Report-for-the-CVRR-Challenge-at-CVPR-2025-cs-CVPDF"><a href="#23-DIVE-Deep-search-Iterative-Video-Exploration-A-Technical-Report-for-the-CVRR-Challenge-at-CVPR-2025-cs-CVPDF" class="headerlink" title="[23] DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025 cs.CVPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21891">DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21891" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Umihiro Kamoto, Tatsuya Ishibashi, Noriyuki Kugo</span></p>
<p><strong>TL;DR:</strong> 这篇报告介绍了在CVPR 2025 CVRR挑战赛中夺冠的解决方案DIVE，通过迭代推理方法在复杂视频问答任务中实现了81.44%的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决复杂视频问答任务中的语义理解和推理问题，提升系统在多样化现实场景中的问答能力。</p>
<p><strong>Result:</strong> 在CVRR-ES测试集上达到81.44%准确率，排名第一。</p>
<p><strong>Insight:</strong> 迭代推理框架能有效处理复杂语义和上下文关系，适用于需要多层次推理的视频问答任务。</p>
<p><strong>Abstract:</strong> In this report, we present the winning solution that achieved the 1st place in the Complex Video Reasoning &amp; Robustness Evaluation Challenge 2025. This challenge evaluates the ability to generate accurate natural language answers to questions about diverse, real-world video clips. It uses the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists of 214 unique videos and 2,400 question-answer pairs spanning 11 categories. Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative reasoning approach, in which each input question is semantically decomposed and solved through stepwise reasoning and progressive inference. This enables our system to provide highly accurate and contextually appropriate answers to even the most complex queries. Applied to the CVRR-ES benchmark, our approach achieves 81.44% accuracy on the test set, securing the top position among all participants. This report details our methodology and provides a comprehensive analysis of the experimental results, demonstrating the effectiveness of our iterative reasoning framework in achieving robust video question answering. The code is available at <a target="_blank" rel="noopener" href="https://github.com/PanasonicConnect/DIVE">https://github.com/PanasonicConnect/DIVE</a></p>
  </div>
</details>

<hr>
<h3 id="24-SODA-Out-of-Distribution-Detection-in-Domain-Shifted-Point-Clouds-via-Neighborhood-Propagation-cs-CV-cs-AIPDF"><a href="#24-SODA-Out-of-Distribution-Detection-in-Domain-Shifted-Point-Clouds-via-Neighborhood-Propagation-cs-CV-cs-AIPDF" class="headerlink" title="[24] SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation cs.CV | cs.AIPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21892">SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21892" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Adam Goodge, Xun Xu, Bryan Hooi, Wee Siong Ng, Jingyi Liao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为SODA的新方法，通过基于邻域的分数传播方案，改进点云数据的OOD检测，解决了合成数据到真实数据域偏移的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 点云数据在多种应用中日益普及，但OOD检测问题尚未充分研究。由于预训练3D视觉语言模型的数据集较小且多为合成数据，导致在实际任务中域偏移严重，影响了OOD检测的性能。</p>
<p><strong>Result:</strong> 实验表明，SODA在合成到真实数据的域偏移场景中显著提升了OOD检测性能，优于现有方法。</p>
<p><strong>Insight:</strong> 点云OOD检测的挑战主要来自域偏移问题，利用邻域传播可以有效缓解这一问题，提升模型在实际任务中的可靠性。</p>
<p><strong>Abstract:</strong> As point cloud data increases in prevalence in a variety of applications, the ability to detect out-of-distribution (OOD) point cloud objects becomes critical for ensuring model safety and reliability. However, this problem remains under-explored in existing research. Inspired by success in the image domain, we propose to exploit advances in 3D vision-language models (3D VLMs) for OOD detection in point cloud objects. However, a major challenge is that point cloud datasets used to pre-train 3D VLMs are drastically smaller in size and object diversity than their image-based counterparts. Critically, they often contain exclusively computer-designed synthetic objects. This leads to a substantial domain shift when the model is transferred to practical tasks involving real objects scanned from the physical environment. In this paper, our empirical experiments show that synthetic-to-real domain shift significantly degrades the alignment of point cloud with their associated text embeddings in the 3D VLM latent space, hindering downstream performance. To address this, we propose a novel methodology called SODA which improves the detection of OOD point clouds through a neighborhood-based score propagation scheme. SODA is inference-based, requires no additional model training, and achieves state-of-the-art performance over existing approaches across datasets and problem settings.</p>
  </div>
</details>

<hr>
<h3 id="25-Exploring-Task-Solving-Paradigm-for-Generalized-Cross-Domain-Face-Anti-Spoofing-via-Reinforcement-Fine-Tuning-cs-CVPDF"><a href="#25-Exploring-Task-Solving-Paradigm-for-Generalized-Cross-Domain-Face-Anti-Spoofing-via-Reinforcement-Fine-Tuning-cs-CVPDF" class="headerlink" title="[25] Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning cs.CVPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21895">Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21895" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fangling Jiang, Qi Li, Weining Wang, Gang Wang, Bing Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于强化微调的人脸反欺骗方法，通过多模态大语言模型学习解决反欺骗任务的能力，而非依赖模式记忆，实现了跨域泛化性能和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的人脸反欺骗方法容易记忆训练数据的模式，导致对未知攻击类型的泛化能力差且缺乏可解释性。作者希望通过强化学习优化策略，引导模型从多角度探索推理策略，从而提升跨域泛化性能。</p>
<p><strong>Result:</strong> 实验表明，该方法在跨域泛化性能上达到SOTA，对未知攻击类型具有强适应能力，同时提供可解释的推理过程。</p>
<p><strong>Insight:</strong> 通过强化学习引导模型主动学习和推理，而非依赖数据模式记忆，可显著提升跨域任务的泛化能力和模型的可解释性。</p>
<p><strong>Abstract:</strong> Recently the emergence of novel presentation attacks has drawn increasing attention to face anti-spoofing. However, existing methods tend to memorize data patterns from the training set, resulting in poor generalization to unknown attack types across different scenarios and limited interpretability. To address these challenges, this paper presents a reinforcement fine-tuning-based face anti-spoofing method that stimulates the capabilities of multimodal large language models to think and learn how to solve the anti-spoofing task itself, rather than relying on the memorization of authenticity patterns. We design verifiable class consistent reward and reasoning consistent reward, and employ a GRPO-based optimization strategy to guide the model in exploring reasoning policies from multiple perspectives to maximize expected rewards. As a result, through iterative trial-and-error learning while retaining only high-reward trajectories, the model distills highly generalizable decision-making rules from the extensive solution space to effectively address cross-domain face anti-spoofing tasks. Extensive experimental results demonstrate that our method achieves state-of-the-art cross-domain generalization performance. It generalizes well to diverse unknown attack types in unseen target domains while providing interpretable reasoning for its authenticity decisions without requiring labor-intensive textual annotations for training.</p>
  </div>
</details>

<hr>
<h3 id="26-Visual-Content-Detection-in-Educational-Videos-with-Transfer-Learning-and-Dataset-Enrichment-cs-CVPDF"><a href="#26-Visual-Content-Detection-in-Educational-Videos-with-Transfer-Learning-and-Dataset-Enrichment-cs-CVPDF" class="headerlink" title="[26] Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment cs.CVPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21903">Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21903" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dipayan Biswas, Shishir Shah, Jaspal Subhlok</span></p>
<p><strong>TL;DR:</strong> 采用迁移学习和数据集增强技术，检测教育视频中的视觉内容（如图表、表格），优化YOLO模型并通过半监督自动标注策略提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 教育视频中的视觉元素（如图表、表格）对理解和检索至关重要，但由于其结构独特且缺乏标注数据，现有目标检测模型效果不佳，因此需要改进。</p>
<p><strong>Result:</strong> 优化的YOLO模型在教育视频视觉内容检测任务中表现优异，并开发了通用解决方案。</p>
<p><strong>Insight:</strong> 迁移学习和半监督策略能有效解决教育视频中视觉内容检测的数据稀缺和结构多样性问题。</p>
<p><strong>Abstract:</strong> Video is transforming education with online courses and recorded lectures supplementing and replacing classroom teaching. Recent research has focused on enhancing information retrieval for video lectures with advanced navigation, searchability, summarization, as well as question answering chatbots. Visual elements like tables, charts, and illustrations are central to comprehension, retention, and data presentation in lecture videos, yet their full potential for improving access to video content remains underutilized. A major factor is that accurate automatic detection of visual elements in a lecture video is challenging; reasons include i) most visual elements, such as charts, graphs, tables, and illustrations, are artificially created and lack any standard structure, and ii) coherent visual objects may lack clear boundaries and may be composed of connected text and visual components. Despite advancements in deep learning based object detection, current models do not yield satisfactory performance due to the unique nature of visual content in lectures and scarcity of annotated datasets. This paper reports on a transfer learning approach for detecting visual elements in lecture video frames. A suite of state of the art object detection models were evaluated for their performance on lecture video datasets. YOLO emerged as the most promising model for this task. Subsequently YOLO was optimized for lecture video object detection with training on multiple benchmark datasets and deploying a semi-supervised auto labeling strategy. Results evaluate the success of this approach, also in developing a general solution to the problem of object detection in lecture videos. Paper contributions include a publicly released benchmark of annotated lecture video frames, along with the source code to facilitate future research.</p>
  </div>
</details>

<hr>
<h3 id="27-RAUM-Net-Regional-Attention-and-Uncertainty-aware-Mamba-Network-cs-CVPDF"><a href="#27-RAUM-Net-Regional-Attention-and-Uncertainty-aware-Mamba-Network-cs-CVPDF" class="headerlink" title="[27] RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network cs.CVPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21905">RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21905" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mingquan Liu</span></p>
<p><strong>TL;DR:</strong> RAUM-Net是一种半监督方法，结合视觉Mamba、区域注意力和贝叶斯不确定性，针对细粒度视觉分类（FGVC）任务，在标注数据稀缺时表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> FGVC任务因类间差异细微且特征表示脆弱而极具挑战性，尤其当标注数据不足时，现有方法表现不佳。</p>
<p><strong>Result:</strong> 在FGVC基准测试中表现优异，尤其在标注数据有限且存在遮挡的情况下展现了强鲁棒性。</p>
<p><strong>Insight:</strong> 区域注意力与贝叶斯不确定性相结合的策略在提升半监督FGVC任务性能中发挥了重要作用。</p>
<p><strong>Abstract:</strong> Fine Grained Visual Categorization (FGVC) remains a challenging task in computer vision due to subtle inter class differences and fragile feature representations. Existing methods struggle in fine grained scenarios, especially when labeled data is scarce. We propose a semi supervised method combining Mamba based feature modeling, region attention, and Bayesian uncertainty. Our approach enhances local to global feature modeling while focusing on key areas during learning. Bayesian inference selects high quality pseudo labels for stability. Experiments show strong performance on FGVC benchmarks with occlusions, demonstrating robustness when labeled data is limited. Code is available at <a target="_blank" rel="noopener" href="https://github.com/wxqnl/RAUM">https://github.com/wxqnl/RAUM</a> Net.</p>
  </div>
</details>

<hr>
<h3 id="28-CERBERUS-Crack-Evaluation-Recognition-Benchmark-for-Engineering-Reliability-Urban-Stability-cs-CVPDF"><a href="#28-CERBERUS-Crack-Evaluation-Recognition-Benchmark-for-Engineering-Reliability-Urban-Stability-cs-CVPDF" class="headerlink" title="[28] CERBERUS: Crack Evaluation &amp; Recognition Benchmark for Engineering Reliability &amp; Urban Stability cs.CVPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21909">CERBERUS: Crack Evaluation &amp; Recognition Benchmark for Engineering Reliability &amp; Urban Stability</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21909" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Justin Reinman, Sunwoong Choi</span></p>
<p><strong>TL;DR:</strong> CERBERUS是一个用于训练和评估AI模型在基础设施裂缝检测中的合成基准，包含裂缝图像生成器和3D检测场景，测试表明合成与真实数据结合可提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基础设施裂缝检测在实际应用中面临数据稀缺和多样性不足的问题，需要一个灵活、可重复的基准来支持模型训练和评估。</p>
<p><strong>Result:</strong> 实验表明，合成与真实数据结合可显著提升模型在真实场景中的裂缝检测性能。</p>
<p><strong>Insight:</strong> 合成数据可作为真实数据的补充，提升模型泛化能力；复杂场景设计有助于评估模型的鲁棒性。</p>
<p><strong>Abstract:</strong> CERBERUS is a synthetic benchmark designed to help train and evaluate AI models for detecting cracks and other defects in infrastructure. It includes a crack image generator and realistic 3D inspection scenarios built in Unity. The benchmark features two types of setups: a simple Fly-By wall inspection and a more complex Underpass scene with lighting and geometry challenges. We tested a popular object detection model (YOLO) using different combinations of synthetic and real crack data. Results show that combining synthetic and real data improves performance on real-world images. CERBERUS provides a flexible, repeatable way to test defect detection systems and supports future research in automated infrastructure inspection. CERBERUS is publicly available at <a target="_blank" rel="noopener" href="https://github.com/justinreinman/Cerberus-Defect-Generator">https://github.com/justinreinman/Cerberus-Defect-Generator</a>.</p>
  </div>
</details>

<hr>
<h3 id="29-SPAZER-Spatial-Semantic-Progressive-Reasoning-Agent-for-Zero-shot-3D-Visual-Grounding-cs-CVPDF"><a href="#29-SPAZER-Spatial-Semantic-Progressive-Reasoning-Agent-for-Zero-shot-3D-Visual-Grounding-cs-CVPDF" class="headerlink" title="[29] SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding cs.CVPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21924">SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21924" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhao Jin, Rong-Cheng Tu, Jingyi Liao, Wenhao Sun, Xiao Luo</span></p>
<p><strong>TL;DR:</strong> SPAZER 是一个结合空间与语义推理的零样本 3D 视觉定位系统，无需 3D 标注数据即可实现高精度定位，显著超过现有零样本方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前零样本 3D 视觉定位方法过于依赖单一空间或语义模态，限制了复杂场景下的表现，因此需要一种能结合两者优势的推理框架。</p>
<p><strong>Result:</strong> 在 ScanRefer 和 Nr3D 基准测试中，分别以 9.0% 和 10.9% 的准确率优势超越现有零样本方法。</p>
<p><strong>Insight:</strong> 结合空间与语义模态的渐进式推理能显著提升零样本 3D 视觉定位性能，尤其在复杂场景下表现突出。</p>
<p><strong>Abstract:</strong> 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene based on natural language queries. To alleviate the reliance on costly 3D training data, recent studies have explored zero-shot 3DVG by leveraging the extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and VLMs. However, existing paradigms tend to emphasize either spatial (3D-based) or semantic (2D-based) understanding, limiting their effectiveness in complex real-world applications. In this work, we introduce SPAZER - a VLM-driven agent that combines both modalities in a progressive reasoning framework. It first holistically analyzes the scene and produces a 3D rendering from the optimal viewpoint. Based on this, anchor-guided candidate screening is conducted to perform a coarse-level localization of potential objects. Furthermore, leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is efficiently performed to determine the best-matching object. By bridging spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot grounding without training on 3D-labeled data. Extensive experiments on ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms previous state-of-the-art zero-shot methods, achieving notable gains of 9.0% and 10.9% in accuracy.</p>
  </div>
</details>

<hr>
<h3 id="30-TASeg-Text-aware-RGB-T-Semantic-Segmentation-based-on-Fine-tuning-Vision-Foundation-Models-cs-CVPDF"><a href="#30-TASeg-Text-aware-RGB-T-Semantic-Segmentation-based-on-Fine-tuning-Vision-Foundation-Models-cs-CVPDF" class="headerlink" title="[30] TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models cs.CVPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21975">TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21975" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meng Yu, Te Cui, Qitong Chu, Wenjie Song, Yi Yang</span></p>
<p><strong>TL;DR:</strong> TASeg提出了一种基于低秩适应（LoRA）微调视觉基础模型的文本感知RGB-T语义分割框架，解决了现有模型缺乏高层文本信息和模态异构性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有RGB-T语义分割模型依赖低层视觉特征，缺乏高层文本信息；SAM在多模态环境下应用受限，亟需一种高效集成视觉与文本信息的方法。</p>
<p><strong>Result:</strong> 实验证明TASeg在多样数据集上性能优越，尤其在视觉特征相似场景下表现突出。</p>
<p><strong>Insight:</strong> 高层文本信息与多模态特征融合可显著提升语义分割的准确性，同时LoRA微调保持了模型的高效性。</p>
<p><strong>Abstract:</strong> Reliable semantic segmentation of open environments is essential for intelligent systems, yet significant problems remain: 1) Existing RGB-T semantic segmentation models mainly rely on low-level visual features and lack high-level textual information, which struggle with accurate segmentation when categories share similar visual characteristics. 2) While SAM excels in instance-level segmentation, integrating it with thermal images and text is hindered by modality heterogeneity and computational inefficiency. To address these, we propose TASeg, a text-aware RGB-T segmentation framework by using Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the image encoder, which effectively merges features from multiple visual modalities while freezing SAM’s original transformer blocks. Additionally, we incorporate CLIP-generated text embeddings in the mask decoder to enable semantic alignment, which further rectifies the classification error and improves the semantic understanding accuracy. Experimental results across diverse datasets demonstrate that our method achieves superior performance in challenging scenarios with fewer trainable parameters.</p>
  </div>
</details>

<hr>
<h3 id="31-R1-Track-Direct-Application-of-MLLMs-to-Visual-Object-Tracking-via-Reinforcement-Learning-cs-CVPDF"><a href="#31-R1-Track-Direct-Application-of-MLLMs-to-Visual-Object-Tracking-via-Reinforcement-Learning-cs-CVPDF" class="headerlink" title="[31] R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning cs.CVPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21980">R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21980" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Biao Wang, Wenwen Li</span></p>
<p><strong>TL;DR:</strong> R1-Track将多模态大语言模型（MLLM）直接应用于视觉目标跟踪任务，通过强化学习微调Qwen2.5-VL模型，取得了显著性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统跟踪方法依赖大规模监督训练且缺乏灵活性，而MLLM在基础任务中表现出色，因此尝试将其直接应用于跟踪任务。</p>
<p><strong>Result:</strong> 在GOT-10k基准测试中表现优异，支持边界框或文本描述的初始化。</p>
<p><strong>Insight:</strong> MLLM的潜力可通过强化学习扩展到特定视觉任务，但需要进一步优化以适应更复杂场景。</p>
<p><strong>Abstract:</strong> Visual single object tracking aims to continuously localize and estimate the scale of a target in subsequent video frames, given only its initial state in the first frame. This task has traditionally been framed as a template matching problem, evolving through major phases including correlation filters, two-stream networks, and one-stream networks with significant progress achieved. However, these methods typically require explicit classification and regression modeling, depend on supervised training with large-scale datasets, and are limited to the single task of tracking, lacking flexibility. In recent years, multi-modal large language models (MLLMs) have advanced rapidly. Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational capabilities, demonstrate excellent performance in grounding tasks. This has spurred interest in applying such models directly to visual tracking. However, experiments reveal that Qwen2.5-VL struggles with template matching between image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement learning method on a small-scale dataset with a rule-based reward function. The resulting model, R1-Track, achieved notable performance on the GOT-10k benchmark. R1-Track supports flexible initialization via bounding boxes or text descriptions while retaining most of the original model’s general capabilities. And we further discuss potential improvements for R1-Track. This rough technical report summarizes our findings as of May 2025.</p>
  </div>
</details>

<hr>
<h3 id="32-RoboEnvision-A-Long-Horizon-Video-Generation-Model-for-Multi-Task-Robot-Manipulation-cs-CVPDF"><a href="#32-RoboEnvision-A-Long-Horizon-Video-Generation-Model-for-Multi-Task-Robot-Manipulation-cs-CVPDF" class="headerlink" title="[32] RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation cs.CVPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22007">RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22007" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liudi Yang, Yang Bai, George Eskandar, Fengyi Shen, Mohammad Altillawi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为RoboEnvision的新方法，用于生成长时间跨度的视频以支持多任务机器人操作。通过分解高层目标、语义保持注意力模块和轻量级策略模型，避免了自回归生成带来的误差累积。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的文本到视频扩散模型在机器人长时间任务中表现不佳，且自回归生成会导致误差累积。因此，需要一种新方法来提高长时间跨度视频的生成质量和任务一致性。</p>
<p><strong>Result:</strong> 在两个基准测试中实现了视频质量和一致性上的最先进表现，并在长时间任务中优于之前的策略模型。</p>
<p><strong>Insight:</strong> 通过分解任务和插值生成方法，可以有效避免自回归误差积累，同时语义保持注意力模块确保了视频的连贯性和一致性。</p>
<p><strong>Abstract:</strong> We address the problem of generating long-horizon videos for robotic manipulation tasks. Text-to-video diffusion models have made significant progress in photorealism, language understanding, and motion generation but struggle with long-horizon robotic tasks. Recent works use video diffusion models for high-quality simulation data and predictive rollouts in robot planning. However, these works predict short sequences of the robot achieving one task and employ an autoregressive paradigm to extend to the long horizon, leading to error accumulations in the generated video and in the execution. To overcome these limitations, we propose a novel pipeline that bypasses the need for autoregressive generation. We achieve this through a threefold contribution: 1) we first decompose the high-level goals into smaller atomic tasks and generate keyframes aligned with these instructions. A second diffusion model then interpolates between each of the two generated frames, achieving the long-horizon video. 2) We propose a semantics preserving attention module to maintain consistency between the keyframes. 3) We design a lightweight policy model to regress the robot joint states from generated videos. Our approach achieves state-of-the-art results on two benchmarks in video quality and consistency while outperforming previous policy models on long-horizon tasks.</p>
  </div>
</details>

<hr>
<h3 id="33-Towards-Universal-Efficient-Model-Compression-via-Exponential-Torque-Pruning-cs-CVPDF"><a href="#33-Towards-Universal-Efficient-Model-Compression-via-Exponential-Torque-Pruning-cs-CVPDF" class="headerlink" title="[33] Towards Universal &amp; Efficient Model Compression via Exponential Torque Pruning cs.CVPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22015">Towards Universal &amp; Efficient Model Compression via Exponential Torque Pruning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22015" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sarthak Ketanbhai Modi, Lim Zi Pong, Shourya Kuchhal, Yoshi Cao, Yupeng Cheng</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于指数扭矩剪枝（ETP）的高效模型压缩方法，解决了现有扭矩正则化方法在剪枝效果上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代深度神经网络（DNNs）的规模和复杂性快速增长，导致计算成本和内存使用问题日益突出。现有基于扭矩正则化的剪枝方法在剪枝后网络仍较密集，且精度下降较大，作者认为这是因为线性力的施加方式不合理。</p>
<p><strong>Result:</strong> 实验表明，ETP在多领域中均能实现更高的压缩率，且精度损失极小。</p>
<p><strong>Insight:</strong> 力的施加形式对模型压缩效果具有重要影响，指数形式的力比线性力更适合区分关键模块和冗余模块。</p>
<p><strong>Abstract:</strong> The rapid growth in complexity and size of modern deep neural networks (DNNs) has increased challenges related to computational costs and memory usage, spurring a growing interest in efficient model compression techniques. Previous state-of-the-art approach proposes using a Torque-inspired regularization which forces the weights of neural modules around a selected pivot point. Whereas, we observe that the pruning effect of this approach is far from perfect, as the post-trained network is still dense and also suffers from high accuracy drop. In this work, we attribute such ineffectiveness to the default linear force application scheme, which imposes inappropriate force on neural module of different distances. To efficiently prune the redundant and distant modules while retaining those that are close and necessary for effective inference, in this work, we propose Exponential Torque Pruning (ETP), which adopts an exponential force application scheme for regularization. Experimental results on a broad range of domains demonstrate that, though being extremely simple, ETP manages to achieve significantly higher compression rate than the previous state-of-the-art pruning strategies with negligible accuracy drop.</p>
  </div>
</details>

<hr>
<h3 id="34-Advancing-Facial-Stylization-through-Semantic-Preservation-Constraint-and-Pseudo-Paired-Supervision-cs-CVPDF"><a href="#34-Advancing-Facial-Stylization-through-Semantic-Preservation-Constraint-and-Pseudo-Paired-Supervision-cs-CVPDF" class="headerlink" title="[34] Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision cs.CVPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22022">Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22022" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhanyi Lu, Yue Zhou</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合语义保留约束和伪配对监督的面部风格化方法，解决了现有方法中因忽略生成器的语义偏移而导致的伪影和内容不一致问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于StyleGAN的面部风格化方法虽取得了进展，但生成的图像仍存在伪影或与源图像内容不一致的问题。作者认为这是由于在风格化过程中忽略了生成器的语义偏移所导致。</p>
<p><strong>Result:</strong> 实验结果表明，该方法生成的高保真面部风格化图像在质量和内容一致性上优于以往方法。</p>
<p><strong>Insight:</strong> 语义保留约束和伪配对监督的结合能够显著提升面部风格化任务的效果，尤其是在减少伪影和保持内容一致性方面。</p>
<p><strong>Abstract:</strong> Facial stylization aims to transform facial images into appealing, high-quality stylized portraits, with the critical challenge of accurately learning the target style while maintaining content consistency with the original image. Although previous StyleGAN-based methods have made significant advancements, the generated results still suffer from artifacts or insufficient fidelity to the source image. We argue that these issues stem from neglecting semantic shift of the generator during stylization. Therefore, we propose a facial stylization method that integrates semantic preservation constraint and pseudo-paired supervision to enhance the content correspondence and improve the stylization effect. Additionally, we develop a methodology for creating multi-level pseudo-paired datasets to implement supervisory constraint. Furthermore, building upon our facial stylization framework, we achieve more flexible multimodal and reference-guided stylization without complex network architecture designs or additional training. Experimental results demonstrate that our approach produces high-fidelity, aesthetically pleasing facial style transfer that surpasses previous methods.</p>
  </div>
</details>

<hr>
<h3 id="35-Cross-modal-Ship-Re-Identification-via-Optical-and-SAR-Imagery-A-Novel-Dataset-and-Method-cs-CVPDF"><a href="#35-Cross-modal-Ship-Re-Identification-via-Optical-and-SAR-Imagery-A-Novel-Dataset-and-Method-cs-CVPDF" class="headerlink" title="[35] Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method cs.CVPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22027">Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22027" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Han Wang, Shengyang Li, Jian Yang, Yuxuan Liu, Yixuan Lv</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新的数据集HOSS ReID，用于通过光学与SAR图像进行跨模态船舶重识别，并基于Vision Transformer提出了一种基准方法TransOSS。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有船舶跟踪方法依赖于低分辨率的地球同步卫星或拍摄时间短的视频卫星，无法满足全天候、高覆盖的实时跟踪需求。通过结合光学与SAR数据，提出了一种更高效的解决方案。</p>
<p><strong>Result:</strong> 所提出的数据集和方法为跨模态船舶重识别提供了新的基准，并公开了数据和代码。</p>
<p><strong>Insight:</strong> 结合光学与SAR数据的跨模态方法能够解决全天候跟踪问题，Vision Transformer的改进为跨模态任务提供了新思路。</p>
<p><strong>Abstract:</strong> Detecting and tracking ground objects using earth observation imagery remains a significant challenge in the field of remote sensing. Continuous maritime ship tracking is crucial for applications such as maritime search and rescue, law enforcement, and shipping analysis. However, most current ship tracking methods rely on geostationary satellites or video satellites. The former offer low resolution and are susceptible to weather conditions, while the latter have short filming durations and limited coverage areas, making them less suitable for the real-world requirements of ship tracking. To address these limitations, we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the effectiveness of ship tracking using low-Earth orbit constellations of optical and SAR sensors. This approach ensures shorter re-imaging cycles and enables all-weather tracking. HOSS ReID dataset includes images of the same ship captured over extended periods under diverse conditions, using different satellites of different modalities at varying times and angles. Furthermore, we propose a baseline method for cross-modal ship re-identification, TransOSS, which is built on the Vision Transformer architecture. It refines the patch embedding structure to better accommodate cross-modal tasks, incorporates additional embeddings to introduce more reference information, and employs contrastive learning to pre-train on large-scale optical-SAR image pairs, ensuring the model’s ability to extract modality-invariant features. Our dataset and baseline method are publicly available on <a target="_blank" rel="noopener" href="https://github.com/Alioth2000/Hoss-ReID">https://github.com/Alioth2000/Hoss-ReID</a>.</p>
  </div>
</details>

<hr>
<h3 id="36-Partial-CLIP-is-Enough-Chimera-Seg-for-Zero-shot-Semantic-Segmentation-cs-CVPDF"><a href="#36-Partial-CLIP-is-Enough-Chimera-Seg-for-Zero-shot-Semantic-Segmentation-cs-CVPDF" class="headerlink" title="[36] Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation cs.CVPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22032">Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22032" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jialei Chen, Xu Zheng, Danda Pani Paudel, Luc Van Gool, Hiroshi Murase</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Chimera-Seg方法，通过结合分割主干和CLIP语义头（CSH）解决了零样本语义分割中的视觉-语言对齐和局部特征与全局表示之间的语义鸿沟问题，提出了选择性全局蒸馏（SGD）和语义对齐模块（SAM），在两个基准测试中分别提升了0.9%和1.2%的hIoU。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 零样本语义分割（ZSS）的目标是利用已见类别的监督分割未见和已见类别。基于蒸馏的方法需要将视觉语言模型（如CLIP）的对齐能力迁移到分割模型中，但存在视觉特征与文本空间对齐困难以及局部特征与全局表示的语义鸿沟问题。</p>
<p><strong>Result:</strong> 在两个基准测试中，hIoU分别提升了0.9%和1.2%，验证了方法的有效性。</p>
<p><strong>Insight:</strong> 仅使用CLIP的部分模块（如固定子网络和投影层）结合轻量级可训练组件，可以在保留分割能力的同时实现高效的视觉-语言对齐。</p>
<p><strong>Abstract:</strong> Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen classes using supervision from only seen classes. Beyond adaptation-based methods, distillation-based approaches transfer vision-language alignment of vision-language model, e.g., CLIP, to segmentation models. However, such knowledge transfer remains challenging due to: (1) the difficulty of aligning vision-based features with the textual space, which requires combining spatial precision with vision-language alignment; and (2) the semantic gap between CLIP’s global representations and the local, fine-grained features of segmentation models. To address challenge (1), we propose Chimera-Seg, which integrates a segmentation backbone as the body and a CLIP-based semantic head as the head, like the Chimera in Greek mythology, combining spatial precision with vision-language alignment. Specifically, Chimera-Seg comprises a trainable segmentation model and a CLIP Semantic Head (CSH), which maps dense features into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed projection layers from the CLIP visual encoder, along with lightweight trainable components. The partial module from CLIP visual encoder, paired with the segmentation model, retains segmentation capability while easing the mapping to CLIP’s semantic space. To address challenge (2), we propose Selective Global Distillation (SGD), which distills knowledge from dense features exhibiting high similarity to the CLIP CLS token, while gradually reducing the number of features used for alignment as training progresses. Besides, we also use a Semantic Alignment Module (SAM) to further align dense visual features with semantic embeddings extracted from the frozen CLIP text encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in hIoU.</p>
  </div>
</details>

<hr>
<h3 id="37-Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field-cs-CVPDF"><a href="#37-Few-Shot-Identity-Adaptation-for-3D-Talking-Heads-via-Global-Gaussian-Field-cs-CVPDF" class="headerlink" title="[37] Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field cs.CVPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22044">Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22044" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hong Nie, Fuyuan Cao, Lu Chen, Fengxin Chen, Yuefeng Zou</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为FIAG的新框架，用于通过少量训练数据快速适应3D说话头合成的身份特征，利用全局高斯场和通用运动场实现高效的身份扩展和运动建模。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于重建和渲染的说话头合成方法需要为每个新身份从头训练，计算成本高且扩展性差。为了解决这一问题，作者提出了FIAG框架，实现高效的身份适应。</p>
<p><strong>Result:</strong> 实验表明，FIAG在多个基准上优于现有方法，验证了其有效性和泛化能力。</p>
<p><strong>Insight:</strong> 通过共享结构和运动信息，可以显著减少新身份适应所需的数据和计算成本，为说话头合成提供了一种更高效的解决方案。</p>
<p><strong>Abstract:</strong> Reconstruction and rendering-based talking head synthesis methods achieve high-quality results with strong identity preservation but are limited by their dependence on identity-specific models. Each new identity requires training from scratch, incurring high computational costs and reduced scalability compared to generative model-based approaches. To overcome this limitation, we propose FIAG, a novel 3D speaking head synthesis framework that enables efficient identity-specific adaptation using only a few training footage. FIAG incorporates Global Gaussian Field, which supports the representation of multiple identities within a shared field, and Universal Motion Field, which captures the common motion dynamics across diverse identities. Benefiting from the shared facial structure information encoded in the Global Gaussian Field and the general motion priors learned in the motion field, our framework enables rapid adaptation from canonical identity representations to specific ones with minimal data. Extensive comparative and ablation experiments demonstrate that our method outperforms existing state-of-the-art approaches, validating both the effectiveness and generalizability of the proposed framework. Code is available at: \textit{<a target="_blank" rel="noopener" href="https://github.com/gme-hong/FIAG%7D">https://github.com/gme-hong/FIAG}</a>.</p>
  </div>
</details>

<hr>
<h3 id="38-EnLVAM-Enhanced-Left-Ventricle-Linear-Measurements-Utilizing-Anatomical-Motion-Mode-cs-CVPDF"><a href="#38-EnLVAM-Enhanced-Left-Ventricle-Linear-Measurements-Utilizing-Anatomical-Motion-Mode-cs-CVPDF" class="headerlink" title="[38] EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode cs.CVPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22063">EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22063" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Durgesh K. Singh, Ahcene Boubekki, Qing Cao, Svein Arne Aase, Robert Jenssen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种增强左心室线性测量的新框架EnLVAM，通过强制直线约束和利用解剖M模式（AMM）图像，改进了B模式超声心动图中的测量精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统B模式超声心动图中手动标记左心室线性测量点耗时且易出错，现有深度学习方法常导致标记点对齐不准确。论文旨在通过强制直线约束和利用AMM图像解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提高了左心室线性测量的准确性，优于传统B模式方法，并在不同网络架构中表现良好。</p>
<p><strong>Insight:</strong> 结合AMM图像和人机交互可以有效解决标记点对齐问题，同时保持临床灵活性；半自动设计简化了操作，提升了实用性和精度。</p>
<p><strong>Abstract:</strong> Linear measurements of the left ventricle (LV) in the Parasternal Long Axis (PLAX) view using B-mode echocardiography are crucial for cardiac assessment. These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular to the LV axis near the mitral valve tips. Manual placement is time-consuming and error-prone, while existing deep learning methods often misalign landmarks, causing inaccurate measurements. We propose a novel framework that enhances LV measurement accuracy by enforcing straight-line constraints. A landmark detector is trained on Anatomical M-Mode (AMM) images, computed in real time from B-mode videos, then transformed back to B-mode space. This approach addresses misalignment and reduces measurement errors. Experiments show improved accuracy over standard B-mode methods, and the framework generalizes well across network architectures. Our semi-automatic design includes a human-in-the-loop step where the user only places the SL, simplifying interaction while preserving alignment flexibility and clinical relevance.</p>
  </div>
</details>

<hr>
<h3 id="39-MirrorMe-Towards-Realtime-and-High-Fidelity-Audio-Driven-Halfbody-Animation-cs-CVPDF"><a href="#39-MirrorMe-Towards-Realtime-and-High-Fidelity-Audio-Driven-Halfbody-Animation-cs-CVPDF" class="headerlink" title="[39] MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation cs.CVPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22065">MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22065" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dechao Meng, Steven Xiao, Xindi Zhang, Guangyuan Wang, Peng Zhang</span></p>
<p><strong>TL;DR:</strong> MirrorMe 是一个基于 LTX 视频模型的实时、可控框架，通过高效的空间和时间压缩，解决了音频驱动肖像动画生成的高延迟和时间一致性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有音频驱动肖像动画方法依赖逐帧 UNet 架构，导致高延迟和时间不一致，无法满足实时高保真生成的需求。</p>
<p><strong>Result:</strong> 在 EMTD 基准测试中，MirrorMe 在保真度、唇音同步准确性和时间稳定性方面达到最先进水平。</p>
<p><strong>Insight:</strong> 通过空间和时间压缩、身份注入和渐进训练，MirrorMe 有效平衡了实时性与生成质量，为音频驱动动画提供了新思路。</p>
<p><strong>Abstract:</strong> Audio-driven portrait animation, which synthesizes realistic videos from reference images using audio signals, faces significant challenges in real-time generation of high-fidelity, temporally coherent animations. While recent diffusion-based methods improve generation quality by integrating audio into denoising processes, their reliance on frame-by-frame UNet architectures introduces prohibitive latency and struggles with temporal consistency. This paper introduces MirrorMe, a real-time, controllable framework built on the LTX video model, a diffusion transformer that compresses video spatially and temporally for efficient latent space denoising. To address LTX’s trade-offs between compression and semantic fidelity, we propose three innovations: 1. A reference identity injection mechanism via VAE-encoded image concatenation and self-attention, ensuring identity consistency; 2. A causal audio encoder and adapter tailored to LTX’s temporal structure, enabling precise audio-expression synchronization; and 3. A progressive training strategy combining close-up facial training, half-body synthesis with facial masking, and hand pose integration for enhanced gesture control. Extensive experiments on the EMTD Benchmark demonstrate MirrorMe’s state-of-the-art performance in fidelity, lip-sync accuracy, and temporal stability.</p>
  </div>
</details>

<hr>
<h3 id="40-Single-Scanline-Relative-Pose-Estimation-for-Rolling-Shutter-Cameras-cs-CV-68T45-I-4-5PDF"><a href="#40-Single-Scanline-Relative-Pose-Estimation-for-Rolling-Shutter-Cameras-cs-CV-68T45-I-4-5PDF" class="headerlink" title="[40] Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras cs.CV | 68T45 | I.4.5PDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22069">Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | 68T45 | I.4.5</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22069" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Petr Hruby, Marc Pollefeys</span></p>
<p><strong>TL;DR:</strong> 提出了一种新方法，通过单扫描线投影线交点估计滚动快门相机间的相对位姿，无需显式建模相机运动，支持单视图相对位姿估计，并为滚动快门SfM提供了基础模块。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 滚动快门相机在动态场景中广泛使用，但其相对位姿估计通常需要复杂的运动模型。本文旨在提出一种无需运动模型的轻量级方法，简化位姿估计流程。</p>
<p><strong>Result:</strong> 在Fastec数据集上的实验验证了该方法在初始化滚动快门SfM中的可行性，展示了其实际应用潜力。</p>
<p><strong>Insight:</strong> 通过单扫描线投影实现位姿估计，方法轻量且灵活，为滚动快门相机的SfM提供了一种新的基础模块。</p>
<p><strong>Abstract:</strong> We propose a novel approach for estimating the relative pose between rolling shutter cameras using the intersections of line projections with a single scanline per image. This allows pose estimation without explicitly modeling camera motion. Alternatively, scanlines can be selected within a single image, enabling single-view relative pose estimation for scanlines of rolling shutter cameras. Our approach is designed as a foundational building block for rolling shutter structure-from-motion (SfM), where no motion model is required, and each scanline’s pose can be computed independently. % We classify minimal solvers for this problem in both generic and specialized settings, including cases with parallel lines and known gravity direction, assuming known intrinsics and no lens distortion. Furthermore, we develop minimal solvers for the parallel-lines scenario, both with and without gravity priors, by leveraging connections between this problem and the estimation of 2D structure from 1D cameras. % Experiments on rolling shutter images from the Fastec dataset demonstrate the feasibility of our approach for initializing rolling shutter SfM, highlighting its potential for further development. % The code will be made publicly available.</p>
  </div>
</details>

<hr>
<h3 id="41-Reasoning-in-machine-vision-learning-to-think-fast-and-slow-cs-CVPDF"><a href="#41-Reasoning-in-machine-vision-learning-to-think-fast-and-slow-cs-CVPDF" class="headerlink" title="[41] Reasoning in machine vision: learning to think fast and slow cs.CVPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22075">Reasoning in machine vision: learning to think fast and slow</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22075" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaheer U. Saeed, Yipei Wang, Veeru Kasivisvanathan, Brian R. Davidson, Matthew J. Clarkson</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新学习范式，使机器能在视觉任务中通过增加推理时间提升性能，模仿人类快慢思考系统。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有机器智能依赖训练数据，缺乏动态推理能力。非语言推理任务（如视觉感知、医学诊断）仍是开放挑战。</p>
<p><strong>Result:</strong> 在视觉任务和医学图像癌症定位中表现优于大规模监督学习、基础模型及人类专家。</p>
<p><strong>Insight:</strong> 融合人类认知理论，展示了在非语言领域机器推理的潜力，为数据稀缺场景提供了新思路。</p>
<p><strong>Abstract:</strong> Reasoning is a hallmark of human intelligence, enabling adaptive decision-making in complex and unfamiliar scenarios. In contrast, machine intelligence remains bound to training data, lacking the ability to dynamically refine solutions at inference time. While some recent advances have explored reasoning in machines, these efforts are largely limited to verbal domains such as mathematical problem-solving, where explicit rules govern step-by-step reasoning. Other critical real-world tasks - including visual perception, spatial reasoning, and radiological diagnosis - require non-verbal reasoning, which remains an open challenge. Here we present a novel learning paradigm that enables machine reasoning in vision by allowing performance improvement with increasing thinking time (inference-time compute), even under conditions where labelled data is very limited. Inspired by dual-process theories of human cognition in psychology, our approach integrates a fast-thinking System I module for familiar tasks, with a slow-thinking System II module that iteratively refines solutions using self-play reinforcement learning. This paradigm mimics human reasoning by proposing, competing over, and refining solutions in data-scarce scenarios. We demonstrate superior performance through extended thinking time, compared not only to large-scale supervised learning but also foundation models and even human experts, in real-world vision tasks. These tasks include computer-vision benchmarks and cancer localisation on medical images across five organs, showcasing transformative potential for non-verbal machine reasoning.</p>
  </div>
</details>

<hr>
<h3 id="42-Towards-Accurate-Heart-Rate-Measurement-from-Ultra-Short-Video-Clips-via-Periodicity-Guided-rPPG-Estimation-and-Signal-Reconstruction-cs-CVPDF"><a href="#42-Towards-Accurate-Heart-Rate-Measurement-from-Ultra-Short-Video-Clips-via-Periodicity-Guided-rPPG-Estimation-and-Signal-Reconstruction-cs-CVPDF" class="headerlink" title="[42] Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction cs.CVPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22078">Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22078" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pei-Kai Huanga, Ya-Ting Chan, Kuan-Wen Chen, Yen-Chun Chou, Shih-Yu Yang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种通过周期性引导的rPPG估计和信号重建方法，从2秒超短视频片段中准确测量心率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 以往远程心率测量方法多关注10秒左右的视频片段，忽视了超短视频片段的心率测量需求。论文针对超短视频片段中心跳周期少和频谱泄漏问题，提出新方法。</p>
<p><strong>Result:</strong> 在四个rPPG基准数据集上的实验表明，该方法不仅能准确测量超短视频的心率，还超越了现有技术的性能。</p>
<p><strong>Insight:</strong> 论文表明，通过周期性约束和信号重建，可以有效解决超短视频中心率测量的挑战，为未来远程健康监测提供了新思路。</p>
<p><strong>Abstract:</strong> Many remote Heart Rate (HR) measurement methods focus on estimating remote photoplethysmography (rPPG) signals from video clips lasting around 10 seconds but often overlook the need for HR estimation from ultra-short video clips. In this paper, we aim to accurately measure HR from ultra-short 2-second video clips by specifically addressing two key challenges. First, to overcome the limited number of heartbeat cycles in ultra-short video clips, we propose an effective periodicity-guided rPPG estimation method that enforces consistent periodicity between rPPG signals estimated from ultra-short clips and their much longer ground truth signals. Next, to mitigate estimation inaccuracies due to spectral leakage, we propose including a generator to reconstruct longer rPPG signals from ultra-short ones while preserving their periodic consistency to enable more accurate HR measurement. Extensive experiments on four rPPG estimation benchmark datasets demonstrate that our proposed method not only accurately measures HR from ultra-short video clips but also outperform previous rPPG estimation techniques to achieve state-of-the-art performance.</p>
  </div>
</details>

<hr>
<h3 id="43-BezierGS-Dynamic-Urban-Scene-Reconstruction-with-Bezier-Curve-Gaussian-Splatting-cs-CVPDF"><a href="#43-BezierGS-Dynamic-Urban-Scene-Reconstruction-with-Bezier-Curve-Gaussian-Splatting-cs-CVPDF" class="headerlink" title="[43] BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting cs.CVPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22099">BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22099" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zipei Ma, Junzhe Jiang, Yurui Chen, Li Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了BézierGS方法，通过Bézier曲线表示动态对象的运动轨迹，无需依赖高精度物体标注，实现了动态场景的高精度重建。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有动态场景重建方法依赖物体姿态标注，限制了大规模场景重建的扩展性。研究旨在提出一种无需标注的方法，更高效地重建动态城市街景。</p>
<p><strong>Result:</strong> 在Waymo Open Dataset和nuPlan基准测试中，BézierGS在动态和静态场景重建及新视角合成任务上优于现有方法。</p>
<p><strong>Insight:</strong> Bézier曲线的引入不仅降低了对外部标注的依赖，还能通过时间信息优化运动轨迹，为动态场景重建提供了新思路。</p>
<p><strong>Abstract:</strong> The realistic reconstruction of street scenes is critical for developing real-world simulators in autonomous driving. Most existing methods rely on object pose annotations, using these poses to reconstruct dynamic objects and move them during the rendering process. This dependence on high-precision object annotations limits large-scale and extensive scene reconstruction. To address this challenge, we propose B&#39;ezier curve Gaussian splatting (B&#39;ezierGS), which represents the motion trajectories of dynamic objects using learnable B&#39;ezier curves. This approach fully leverages the temporal information of dynamic objects and, through learnable curve modeling, automatically corrects pose errors. By introducing additional supervision on dynamic object rendering and inter-curve consistency constraints, we achieve reasonable and accurate separation and reconstruction of scene elements. Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark demonstrate that B&#39;ezierGS outperforms state-of-the-art alternatives in both dynamic and static scene components reconstruction and novel view synthesis.</p>
  </div>
</details>

<hr>
<h3 id="44-Tied-Prototype-Model-for-Few-Shot-Medical-Image-Segmentation-cs-CV-cs-LG-stat-MLPDF"><a href="#44-Tied-Prototype-Model-for-Few-Shot-Medical-Image-Segmentation-cs-CV-cs-LG-stat-MLPDF" class="headerlink" title="[44] Tied Prototype Model for Few-Shot Medical Image Segmentation cs.CV | cs.LG | stat.MLPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22101">Tied Prototype Model for Few-Shot Medical Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG | stat.ML</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22101" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyeongji Kim, Stine Hansen, Michael Kampffmeyer</span></p>
<p><strong>TL;DR:</strong> 本文提出了Tied Prototype Model (TPM)，用于改进医学图像小样本分割，通过绑定前景和背景的原型分布，解决了现有方法依赖单一原型、固定阈值等问题，并支持多原型和多类分割。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有原型方法ADNet在医学图像分割中存在局限性，如依赖单一原型、仅支持二分类、固定阈值无法适应不同患者和器官的变异性。TPM旨在解决这些问题。</p>
<p><strong>Result:</strong> TPM显著提升了分割精度，尤其在处理非典型背景特征时表现更好。</p>
<p><strong>Insight:</strong> TPM为基于原型的医学图像小样本分割提供了新视角，通过绑定分布和多原型设计，提升了模型的适应性和性能。</p>
<p><strong>Abstract:</strong> Common prototype-based medical image few-shot segmentation (FSS) methods model foreground and background classes using class-specific prototypes. However, given the high variability of the background, a more promising direction is to focus solely on foreground modeling, treating the background as an anomaly – an approach introduced by ADNet. Yet, ADNet faces three key limitations: dependence on a single prototype per class, a focus on binary classification, and fixed thresholds that fail to adapt to patient and organ variability. To address these shortcomings, we propose the Tied Prototype Model (TPM), a principled reformulation of ADNet with tied prototype locations for foreground and background distributions. Building on its probabilistic foundation, TPM naturally extends to multiple prototypes and multi-class segmentation while effectively separating non-typical background features. Notably, both extensions lead to improved segmentation accuracy. Finally, we leverage naturally occurring class priors to define an ideal target for adaptive thresholds, boosting segmentation performance. Taken together, TPM provides a fresh perspective on prototype-based FSS for medical image segmentation. The code can be found at <a target="_blank" rel="noopener" href="https://github.com/hjk92g/TPM-FSS">https://github.com/hjk92g/TPM-FSS</a>.</p>
  </div>
</details>

<hr>
<h3 id="45-Pedestrian-Intention-and-Trajectory-Prediction-in-Unstructured-Traffic-Using-IDD-PeD-cs-CV-cs-HCPDF"><a href="#45-Pedestrian-Intention-and-Trajectory-Prediction-in-Unstructured-Traffic-Using-IDD-PeD-cs-CV-cs-HCPDF" class="headerlink" title="[45] Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD cs.CV | cs.HCPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22111">Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22111" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruthvik Bokkasam, Shankar Gangisetty, A. H. Abdul Hafez, C. V. Jawahar</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个针对非结构化交通环境的印度驾驶行人数据集（IDD-PeD），用于提升行人意图和轨迹预测的准确性。研究发现现有方法在该数据集上性能显著下降，突显了非结构化环境的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着自动驾驶技术的发展，准确预测行人在复杂和非结构化交通环境中的行为对安全性至关重要。目前缺乏能够捕捉此类环境的全面数据集，因此需要新的数据支持模型开发。</p>
<p><strong>Result:</strong> 实验结果表明，现有意图预测方法的性能下降高达15%，轨迹预测方法的MSE增加高达1208，表明非结构化环境的挑战性。</p>
<p><strong>Insight:</strong> 非结构化交通环境（如照明变化、遮挡等）显著增加了行人行为预测的难度，现有方法需要进一步改进以适应此类场景。</p>
<p><strong>Abstract:</strong> With the rapid advancements in autonomous driving, accurately predicting pedestrian behavior has become essential for ensuring safety in complex and unpredictable traffic conditions. The growing interest in this challenge highlights the need for comprehensive datasets that capture unstructured environments, enabling the development of more robust prediction models to enhance pedestrian safety and vehicle navigation. In this paper, we introduce an Indian driving pedestrian dataset designed to address the complexities of modeling pedestrian behavior in unstructured environments, such as illumination changes, occlusion of pedestrians, unsignalized scene types and vehicle-pedestrian interactions. The dataset provides high-level and detailed low-level comprehensive annotations focused on pedestrians requiring the ego-vehicle’s attention. Evaluation of the state-of-the-art intention prediction methods on our dataset shows a significant performance drop of up to $\mathbf{15%}$, while trajectory prediction methods underperform with an increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets. Additionally, we present exhaustive quantitative and qualitative analysis of intention and trajectory baselines. We believe that our dataset will open new challenges for the pedestrian behavior research community to build robust models. Project Page: <a target="_blank" rel="noopener" href="https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped">https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped</a></p>
  </div>
</details>

<hr>
<h3 id="46-Pipe-Reconstruction-from-Point-Cloud-Data-cs-CVPDF"><a href="#46-Pipe-Reconstruction-from-Point-Cloud-Data-cs-CVPDF" class="headerlink" title="[46] Pipe Reconstruction from Point Cloud Data cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22118">Pipe Reconstruction from Point Cloud Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22118" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Antje Alex, Jannis Stoppe</span></p>
<p><strong>TL;DR:</strong> 提出了一种从点云数据自动重建管道的流程，包括骨架曲线估计、曲线延伸、滚动球技术和3D平滑，支持快速准确的数字孪生模型构建。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 工业资产（如船舶和海上平台）的数字孪生需要精确的管道网络重建，但手动建模耗时耗力，亟需自动化解决方案。</p>
<p><strong>Result:</strong> 方法能够从非完整扫描数据中重建管道，精确提取半径、长度和方向等属性。</p>
<p><strong>Insight:</strong> 自动化管道重建支持数字孪生的快速开发，降低建模成本，适用于复杂工业场景。</p>
<p><strong>Abstract:</strong> Accurate digital twins of industrial assets, such as ships and offshore platforms, rely on the precise reconstruction of complex pipe networks. However, manual modelling of pipes from laser scan data is a time-consuming and labor-intensive process. This paper presents a pipeline for automated pipe reconstruction from incomplete laser scan data. The approach estimates a skeleton curve using Laplacian-based contraction, followed by curve elongation. The skeleton axis is then recentred using a rolling sphere technique combined with 2D circle fitting, and refined with a 3D smoothing step. This enables the determination of pipe properties, including radius, length and orientation, and facilitates the creation of detailed 3D models of complex pipe networks. By automating pipe reconstruction, this approach supports the development of digital twins, allowing for rapid and accurate modeling while reducing costs.</p>
  </div>
</details>

<hr>
<h3 id="47-Low-Rank-Implicit-Neural-Representation-via-Schatten-p-Quasi-Norm-and-Jacobian-Regularization-cs-CVPDF"><a href="#47-Low-Rank-Implicit-Neural-Representation-via-Schatten-p-Quasi-Norm-and-Jacobian-Regularization-cs-CVPDF" class="headerlink" title="[47] Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization cs.CVPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22134">Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22134" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhengyun Cheng, Changhao Wang, Guanwen Zhang, Yi Xu, Wei Zhou</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于CP分解的低秩张量函数（CP-INR），结合Schatten-p拟范数和雅可比谱范数正则化，用于隐式神经表示，解决了现有方法的稀疏性和平滑性问题，并在多维数据恢复任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有低秩张量表示方法（如Tucker和CP分解）在灵活性和可解释性之间存在权衡，且稀疏解难以获得。论文旨在通过结合CP分解和神经网络的隐式表示能力，实现连续数据的低秩表示。</p>
<p><strong>Result:</strong> 在图像修复、去噪和点云上采样任务中表现优于现有方法，验证了方法的有效性和普适性。</p>
<p><strong>Insight:</strong> 通过结合张量分解的显式结构与神经网络的隐式能力，能够更灵活地处理多维数据，同时提供理论保证。</p>
<p><strong>Abstract:</strong> Higher-order tensors are well-suited for representing multi-dimensional data, such as color images and videos. Low-rank tensor representation has become essential in machine learning and computer vision, but existing methods like Tucker decomposition offer flexibility at the expense of interpretability. In contrast, while the CANDECOMP&#x2F;PARAFAC (CP) decomposition provides a more natural and interpretable tensor structure, obtaining sparse solutions remains challenging. Leveraging the rich properties of CP decomposition, we propose a CP-based low-rank tensor function parameterized by neural networks for implicit neural representation (CP-INR). This approach enables continuous data representation beyond structured grids, fully exploiting the non-linearity of tensor data with theoretical guarantees on excess risk bounds. To achieve a sparse CP decomposition, we introduce a variational form of the Schatten-p quasi-norm and prove its relationship to multilinear rank minimization. For smoothness, we propose a regularization term based on the spectral norm of the Jacobian and Hutchinson’s trace estimator. Our proposed smoothness regularization is SVD-free and avoids explicit chain rule derivations. It can serve as an alternative to Total Variation (TV) regularization in image denoising tasks and is naturally applicable to continuous data. Extensive experiments on multi-dimensional data recovery tasks, including image inpainting, denoising, and point cloud upsampling, demonstrate the superiority and versatility of our method compared to state-of-the-art approaches.</p>
  </div>
</details>

<hr>
<h3 id="48-Q-Frame-Query-aware-Frame-Selection-and-Multi-Resolution-Adaptation-for-Video-LLMs-cs-CVPDF"><a href="#48-Q-Frame-Query-aware-Frame-Selection-and-Multi-Resolution-Adaptation-for-Video-LLMs-cs-CVPDF" class="headerlink" title="[48] Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs cs.CVPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22139">Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22139" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaojie Zhang, Jiahui Yang, Jianqin Yin, Zhenbo Luo, Jian Luan</span></p>
<p><strong>TL;DR:</strong> 论文提出了Q-Frame，一种针对视频内容与查询的自适应帧选择与多分辨率调整方法，解决了现有Video-LLMs在处理视频理解任务时的效率与准确性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视频理解任务中，现有Video-LLMs由于采用均匀帧采样，难以有效捕捉与查询相关的关键时空线索，亟需一种更高效的帧选择方法。</p>
<p><strong>Result:</strong> 在MLVU、LongVideoBench和Video-MME等基准数据集上表现出色，优于现有方法。</p>
<p><strong>Insight:</strong> Q-Frame展示了通过动态调整帧选择与分辨率，可以显著提升Video-LLMs的视频理解能力，同时保持计算效率。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated significant success in visual understanding tasks. However, challenges persist in adapting these models for video comprehension due to the large volume of data and temporal complexity. Existing Video-LLMs using uniform frame sampling often struggle to capture the query-related crucial spatiotemporal clues of videos effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive frame selection and multi-resolution scaling tailored to the video’s content and the specific query. Q-Frame employs a training-free, plug-and-play strategy generated by a text-image matching network like CLIP, utilizing the Gumbel-Max trick for efficient frame selection. Q-Frame allows Video-LLMs to process more frames without exceeding computational limits, thereby preserving critical temporal and spatial information. We demonstrate Q-Frame’s effectiveness through extensive experiments on benchmark datasets, including MLVU, LongVideoBench, and Video-MME, illustrating its superiority over existing methods and its applicability across various video understanding tasks.</p>
  </div>
</details>

<hr>
<h3 id="49-Visual-Structures-Helps-Visual-Reasoning-Addressing-the-Binding-Problem-in-VLMs-cs-CV-cs-AI-cs-LGPDF"><a href="#49-Visual-Structures-Helps-Visual-Reasoning-Addressing-the-Binding-Problem-in-VLMs-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[49] Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs cs.CV | cs.AI | cs.LGPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22146">Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22146" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amirmohammad Izadi, Mohammad Ali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi</span></p>
<p><strong>TL;DR:</strong> 论文通过引入低层次空间结构（如水平线）增强视觉输入，并结合文本提示来改善视觉-语言模型（VLM）的绑定问题，显著提升了在计数、视觉搜索等任务中的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉-语言模型（VLM）在视觉推理中的绑定问题（即无法可靠地将感知特征与正确视觉对象关联）导致其在计数、空间关系理解等任务中表现不佳。这一问题的核心在于缺乏空间基础和序列注意力机制。</p>
<p><strong>Result:</strong> 在2D合成数据集上，GPT-4o的视觉搜索准确率提升了25.00%，计数准确率提升了26.83%，场景描述的编辑距离误差减少了0.32，空间关系任务的性能提升了9.50%。</p>
<p><strong>Insight:</strong> 视觉输入的设计比纯语言方法（如思维链提示）对解决绑定问题更为关键，低层次视觉结构化是提升组合视觉推理能力的有效方向。</p>
<p><strong>Abstract:</strong> Despite progress in Vision-Language Models (VLMs), their capacity for visual reasoning is often limited by the \textit{binding problem}: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current VLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces a simple yet effective intervention: augmenting visual inputs with low-level spatial structures (e.g., horizontal lines) and pairing this with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks. Specifically, our method improves GPT-4o visual search accuracy by 25.00%, increases counting accuracy by 26.83%, reduces edit distance error in scene description by 0.32, and enhances performance on spatial relationship tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. Our method enhances binding only with a single-query inference, underscoring the importance of visual input design over purely linguistically-based approaches. These findings suggest that low-level visual structuring is a powerful and underexplored direction for improving compositional visual reasoning and could serve as a general strategy for enhancing VLM performance on spatially grounded tasks.</p>
  </div>
</details>

<hr>
<h3 id="50-RetFiner-A-Vision-Language-Refinement-Scheme-for-Retinal-Foundation-Models-cs-CVPDF"><a href="#50-RetFiner-A-Vision-Language-Refinement-Scheme-for-Retinal-Foundation-Models-cs-CVPDF" class="headerlink" title="[50] RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models cs.CVPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22149">RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22149" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ronald Fecso, José Morano, Ursula Schmidt-Erfurth, Hrvoje Bogunović</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为RetFiner的自监督学习视觉语言细化方案，旨在通过利用文本数据中的监督信号来改进现有的视网膜基础模型，从而提升其在多种下游任务中的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视网膜基础模型仅依赖图像数据进行训练，缺乏对图像的全面和鲁棒的语义理解，导致其在下游任务（尤其是复杂任务）中表现不佳，且需要监督微调以适应特定应用和人群。</p>
<p><strong>Result:</strong> 在RETFound、UrFound和VisionFM上，RetFiner分别带来5.8、3.9和2.1个百分点的平均性能提升。</p>
<p><strong>Insight:</strong> 结合视觉与语言信号可以显著提升基础模型的语义理解能力，而无需依赖昂贵的标注数据或监督微调。</p>
<p><strong>Abstract:</strong> The rise of imaging techniques such as optical coherence tomography (OCT) and advances in deep learning (DL) have enabled clinicians and researchers to streamline retinal disease staging. A popular DL approach is self-supervised learning (SSL), where models learn from vast amounts of unlabeled data, avoiding costly annotation. SSL has allowed the development of foundation models (FMs), large models that can be used for a variety of downstream tasks. However, existing FMs for OCT, trained solely on image data, lack a comprehensive and robust semantic understanding of images, as evidenced by their downstream performance (especially for complex tasks), and thus require supervised fine-tuning (which may be unfeasible) to better adapt to specific applications and populations. To address this, we propose RetFiner, an SSL vision-language refinement scheme that improves the representations of existing FMs and enables their efficient and direct adaptation to specific populations for improved downstream performance. Our method uses a diverse set of training objectives which take advantage of the rich supervisory signal found in textual data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM, showing significant improvements in linear probing performance on seven highly diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1 percentage points over their baselines, respectively. Our code and model weights are publicly available at <a target="_blank" rel="noopener" href="https://github.com/ronnief1/RetFiner">https://github.com/ronnief1/RetFiner</a>.</p>
  </div>
</details>

<hr>
<h3 id="51-Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection-cs-CVPDF"><a href="#51-Attention-disentangled-Uniform-Orthogonal-Feature-Space-Optimization-for-Few-shot-Object-Detection-cs-CVPDF" class="headerlink" title="[51] Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection cs.CVPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22161">Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22161" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Taijin Zhao, Heqian Qiu, Yu Dai, Lanxiao Wang, Fanman Meng</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种用于少样本目标检测的统一正交特征空间（UOFS）优化框架，通过解耦特征空间中的目标性和分类任务，解决了现有方法中类特定目标性标准和样本不具代表性的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的少样本目标检测方法通常在共享特征空间中耦合目标性识别和前景分类任务，导致类特定的目标性标准和新类样本的不具代表性。</p>
<p><strong>Result:</strong> 实验表明，该方法显著优于基于耦合特征空间的现有方法。</p>
<p><strong>Insight:</strong> 正交特征空间解耦和注意力机制的结合能够有效提升少样本目标检测的性能，特别是在处理新类样本时。</p>
<p><strong>Abstract:</strong> Few-shot object detection (FSOD) aims to detect objects with limited samples for novel classes, while relying on abundant data for base classes. Existing FSOD approaches, predominantly built on the Faster R-CNN detector, entangle objectness recognition and foreground classification within shared feature spaces. This paradigm inherently establishes class-specific objectness criteria and suffers from unrepresentative novel class samples. To resolve this limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization framework. First, UOFS decouples the feature space into two orthogonal components, where magnitude encodes objectness and angle encodes classification. This decoupling enables transferring class-agnostic objectness knowledge from base classes to novel classes. Moreover, implementing the disentanglement requires careful attention to two challenges: (1) Base set images contain unlabeled foreground instances, causing confusion between potential novel class instances and backgrounds. (2) Angular optimization depends exclusively on base class foreground instances, inducing overfitting of angular distributions to base classes. To address these challenges, we propose a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure background base set by removing unlabeled instances in original images to provide unbiased magnitude-based objectness supervision. (2) Incorporating unlabeled foreground instances in the original base set into angular optimization to enhance distribution uniformity. Additionally, we propose a Spatial-wise Attention Disentanglement and Association (SADA) module to address task conflicts between class-agnostic and class-specific tasks. Experiments demonstrate that our method significantly outperforms existing approaches based on entangled feature spaces.</p>
  </div>
</details>

<hr>
<h3 id="52-Frequency-Semantic-Enhanced-Variational-Autoencoder-for-Zero-Shot-Skeleton-based-Action-Recognition-cs-CV-cs-AIPDF"><a href="#52-Frequency-Semantic-Enhanced-Variational-Autoencoder-for-Zero-Shot-Skeleton-based-Action-Recognition-cs-CV-cs-AIPDF" class="headerlink" title="[52] Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition cs.CV | cs.AIPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22179">Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22179" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenhan Wu, Zhishuai Guo, Chen Chen, Hongfei Xue, Aidong Lu</span></p>
<p><strong>TL;DR:</strong> 提出了一种基于频率-语义增强的变分自编码器（FS-VAE），用于零样本骨架动作识别，通过频率分解和语义对齐提升了模型的识别能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在零样本骨架动作识别中主要关注视觉和语义的对齐，但忽略了语义空间中细粒度动作模式的重要性，导致识别效果受限。</p>
<p><strong>Result:</strong> 在基准测试中验证了模型的优越性，频率增强的语义特征能有效区分视觉和语义相似的动作簇。</p>
<p><strong>Insight:</strong> 频率分解和语义多级对齐是解决零样本骨架动作识别中细粒度差异的关键，校准损失能显著提升特征对齐的鲁棒性。</p>
<p><strong>Abstract:</strong> Zero-shot skeleton-based action recognition aims to develop models capable of identifying actions beyond the categories encountered during training. Previous approaches have primarily focused on aligning visual and semantic representations but often overlooked the importance of fine-grained action patterns in the semantic space (e.g., the hand movements in drinking water and brushing teeth). To address these limitations, we propose a Frequency-Semantic Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic representation learning with frequency decomposition. FS-VAE consists of three key components: 1) a frequency-based enhancement module with high- and low-frequency adjustments to enrich the skeletal semantics learning and improve the robustness of zero-shot action recognition; 2) a semantic-based action description with multilevel alignment to capture both local details and global correspondence, effectively bridging the semantic gap and compensating for the inherent loss of information in skeleton sequences; 3) a calibrated cross-alignment loss that enables valid skeleton-text pairs to counterbalance ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text features, thereby ensuring robust alignment. Evaluations on the benchmarks demonstrate the effectiveness of our approach, validating that frequency-enhanced semantic features enable robust differentiation of visually and semantically similar action clusters, improving zero-shot action recognition.</p>
  </div>
</details>

<hr>
<h3 id="53-Robust-and-Accurate-Multi-view-2D-3D-Image-Registration-with-Differentiable-X-ray-Rendering-and-Dual-Cross-view-Constraints-cs-CV-cs-ROPDF"><a href="#53-Robust-and-Accurate-Multi-view-2D-3D-Image-Registration-with-Differentiable-X-ray-Rendering-and-Dual-Cross-view-Constraints-cs-CV-cs-ROPDF" class="headerlink" title="[53] Robust and Accurate Multi-view 2D&#x2F;3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints cs.CV | cs.ROPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22191">Robust and Accurate Multi-view 2D&#x2F;3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22191" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuxin Cui, Rui Song, Yibin Li, Max Q. -H. Meng, Zhe Min</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新颖的多视图2D&#x2F;3D刚性配准方法，通过设计联合损失函数和引入跨视图约束，显著提高了配准的鲁棒性和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 单视图术中图像视野有限，影响了配准的准确性，因此需要利用多视图图像以增强配准的鲁棒性。</p>
<p><strong>Result:</strong> 在DeepFluoro数据集上实现了0.79±2.17 mm的平均目标配准误差（mTRE），优于现有方法。</p>
<p><strong>Insight:</strong> 多视图投影位姿的相互约束可以显著提升配准的鲁棒性，测试时优化能进一步细化位姿估计。</p>
<p><strong>Abstract:</strong> Robust and accurate 2D&#x2F;3D registration, which aligns preoperative models with intraoperative images of the same anatomy, is crucial for successful interventional navigation. To mitigate the challenge of a limited field of view in single-image intraoperative scenarios, multi-view 2D&#x2F;3D registration is required by leveraging multiple intraoperative images. In this paper, we propose a novel multi-view 2D&#x2F;3D rigid registration approach comprising two stages. In the first stage, a combined loss function is designed, incorporating both the differences between predicted and ground-truth poses and the dissimilarities (e.g., normalized cross-correlation) between simulated and observed intraoperative images. More importantly, additional cross-view training loss terms are introduced for both pose and image losses to explicitly enforce cross-view constraints. In the second stage, test-time optimization is performed to refine the estimated poses from the coarse stage. Our method exploits the mutual constraints of multi-view projection poses to enhance the robustness of the registration process. The proposed framework achieves a mean target registration error (mTRE) of $0.79 \pm 2.17$ mm on six specimens from the DeepFluoro dataset, demonstrating superior performance compared to state-of-the-art registration algorithms.</p>
  </div>
</details>

<hr>
<h3 id="54-ReF-LLE-Personalized-Low-Light-Enhancement-via-Reference-Guided-Deep-Reinforcement-Learning-cs-CV-eess-IVPDF"><a href="#54-ReF-LLE-Personalized-Low-Light-Enhancement-via-Reference-Guided-Deep-Reinforcement-Learning-cs-CV-eess-IVPDF" class="headerlink" title="[54] ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning cs.CV | eess.IVPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22216">ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22216" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ming Zhao, Pingping Liu, Tongshun Zhang, Zhe Zhang</span></p>
<p><strong>TL;DR:</strong> ReF-LLE是一种基于参考图像和强化学习的个性化低光增强方法，首次将强化学习引入傅里叶频域，通过零参考评分策略和自适应迭代实现个性化效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 低光图像增强面临两个主要挑战：图像条件差异大且增强结果依赖主观偏好。现有方法难以同时解决这两个问题。</p>
<p><strong>Result:</strong> 在基准数据集上性能优于现有方法，展现更高的感知质量和个性化适应能力。</p>
<p><strong>Insight:</strong> 傅里叶域与强化学习的结合为低光增强提供了新思路，零频分量可作为光照分布的强有效指标。</p>
<p><strong>Abstract:</strong> Low-light image enhancement presents two primary challenges: 1) Significant variations in low-light images across different conditions, and 2) Enhancement levels influenced by subjective preferences and user intent. To address these issues, we propose ReF-LLE, a novel personalized low-light image enhancement method that operates in the Fourier frequency domain and incorporates deep reinforcement learning. ReF-LLE is the first to integrate deep reinforcement learning into this domain. During training, a zero-reference image evaluation strategy is introduced to score enhanced images, providing reward signals that guide the model to handle varying degrees of low-light conditions effectively. In the inference phase, ReF-LLE employs a personalized adaptive iterative strategy, guided by the zero-frequency component in the Fourier domain, which represents the overall illumination level. This strategy enables the model to adaptively adjust low-light images to align with the illumination distribution of a user-provided reference image, ensuring personalized enhancement results. Extensive experiments on benchmark datasets demonstrate that ReF-LLE outperforms state-of-the-art methods, achieving superior perceptual quality and adaptability in personalized low-light image enhancement.</p>
  </div>
</details>

<hr>
<h3 id="55-Boosting-Classification-with-Quantum-Inspired-Augmentations-cs-CV-cond-mat-dis-nn-cs-LG-quant-phPDF"><a href="#55-Boosting-Classification-with-Quantum-Inspired-Augmentations-cs-CV-cond-mat-dis-nn-cs-LG-quant-phPDF" class="headerlink" title="[55] Boosting Classification with Quantum-Inspired Augmentations cs.CV | cond-mat.dis-nn | cs.LG | quant-phPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22241">Boosting Classification with Quantum-Inspired Augmentations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cond-mat.dis-nn | cs.LG | quant-ph</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22241" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matthias Tschöpe, Vitor Fortes Rey, Sogo Pierre Sanon, Paul Lukowicz, Nikolaos Palaiodimopoulos</span></p>
<p><strong>TL;DR:</strong> 该论文研究了量子启发的数据增强技术，通过模拟量子扰动（如随机Bloch球旋转）提升图像分类性能，在经典机器学习中取得了显著的准确性提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 量子计算中的小扰动通常被视为不利因素，但作者发现这些扰动可以作为数据增强的来源，从而提升机器学习性能。此外，这些扰动可以在经典硬件上高效模拟，为经典方法提供量子启发的改进途径。</p>
<p><strong>Result:</strong> 在ImageNet上，该方法使Top-1准确率提升3%，Top-5准确率提升2.5%，F1分数从8%增至12%。但强酉变换未显示出差分隐私的潜力。</p>
<p><strong>Insight:</strong> 量子扰动可作为增强数据多样性的有效工具，无需复杂量子硬件即可实现性能提升；然而，其隐私应用仍需进一步研究。</p>
<p><strong>Abstract:</strong> Understanding the impact of small quantum gate perturbations, which are common in quantum digital devices but absent in classical computers, is crucial for identifying potential advantages in quantum machine learning. While these perturbations are typically seen as detrimental to quantum computation, they can actually enhance performance by serving as a natural source of data augmentation. Additionally, they can often be efficiently simulated on classical hardware, enabling quantum-inspired approaches to improve classical machine learning methods. In this paper, we investigate random Bloch sphere rotations, which are fundamental SU(2) transformations, as a simple yet effective quantum-inspired data augmentation technique. Unlike conventional augmentations such as flipping, rotating, or cropping, quantum transformations lack intuitive spatial interpretations, making their application to tasks like image classification less straightforward. While common quantum augmentation methods rely on applying quantum models or trainable quanvolutional layers to classical datasets, we focus on the direct application of small-angle Bloch rotations and their effect on classical data. Using the large-scale ImageNet dataset, we demonstrate that our quantum-inspired augmentation method improves image classification performance, increasing Top-1 accuracy by 3%, Top-5 accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard classical augmentation methods. Finally, we examine the use of stronger unitary augmentations. Although these transformations preserve information in principle, they result in visually unrecognizable images with potential applications for privacy computations. However, we show that our augmentation approach and simple SU(2) transformations do not enhance differential privacy and discuss the implications of this limitation.</p>
  </div>
</details>

<hr>
<h3 id="56-4D-VLA-Spatiotemporal-Vision-Language-Action-Pretraining-with-Cross-Scene-Calibration-cs-CVPDF"><a href="#56-4D-VLA-Spatiotemporal-Vision-Language-Action-Pretraining-with-Cross-Scene-Calibration-cs-CVPDF" class="headerlink" title="[56] 4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration cs.CVPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22242">4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22242" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou</span></p>
<p><strong>TL;DR:</strong> 4D-VLA通过在输入中整合4D信息（深度和时序信息），解决了机器人数据预训练中的坐标系混乱和状态混乱问题，同时引入了内存银行采样策略，显著提升了模型的时空推理能力和训练效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通常使用简单观察作为输入，导致条件动作分布分散（称为坐标系混乱和状态混乱），严重影响了预训练效率。为解决这一问题，研究者提出了4D-VLA。</p>
<p><strong>Result:</strong> 模型在仿真和真实实验中均显著优于OpenVLA，成功率和空间理解能力显著提升。MV-Bench上的测试结果也表明模型具有更强的适应性和空间感知能力。</p>
<p><strong>Insight:</strong> 整合4D信息和有效的帧采样策略可以显著提升机器人预训练模型的时空推理能力和训练效率，同时多视角基准测试有助于评估模型的泛化能力。</p>
<p><strong>Abstract:</strong> Leveraging diverse robotic data for pretraining remains a critical challenge. Existing methods typically model the dataset’s action distribution using simple observations as inputs. However, these inputs are often incomplete, resulting in a dispersed conditional action distribution-an issue we refer to as coordinate system chaos and state chaos. This inconsistency significantly hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel approach that effectively integrates 4D information into the input to mitigate these sources of chaos. Our model introduces depth and temporal information into visual features with sequential RGB-D inputs, aligning the coordinate systems of the robot and the scene. This alignment endows the model with strong spatiotemporal reasoning capabilities while minimizing training overhead. Additionally, we introduce memory bank sampling, a frame sampling strategy designed to extract informative frames from historical images, further improving effectiveness and efficiency. Experimental results demonstrate that our pretraining method and architectural components substantially enhance model performance. In both simulated and real-world experiments, our model achieves a significant increase in success rate over OpenVLA. To further assess spatial perception and generalization to novel views, we introduce MV-Bench, a multi-view simulation benchmark. Our model consistently outperforms existing methods, demonstrating stronger spatial understanding and adaptability.</p>
  </div>
</details>

<hr>
<h3 id="57-EAMamba-Efficient-All-Around-Vision-State-Space-Model-for-Image-Restoration-cs-CVPDF"><a href="#57-EAMamba-Efficient-All-Around-Vision-State-Space-Model-for-Image-Restoration-cs-CVPDF" class="headerlink" title="[57] EAMamba: Efficient All-Around Vision State Space Model for Image Restoration cs.CVPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22246">EAMamba: Efficient All-Around Vision State Space Model for Image Restoration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22246" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu-Cheng Lin, Yu-Syuan Xu, Hao-Wei Chen, Hsien-Kai Kuo, Chun-Yi Lee</span></p>
<p><strong>TL;DR:</strong> 论文提出了EAMamba（高效全方位视觉状态空间模型），用于解决Vision Mamba在图像复原任务中的计算复杂性和局部像素遗忘问题，通过多头选择性扫描模块和全方位扫描机制显著提升了效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Vision Mamba虽然在建模长程依赖方面表现出色，但在低层视觉任务中面临计算复杂性和局部像素遗忘的挑战，因此需要一种更高效的改进方法。</p>
<p><strong>Result:</strong> 实验表明，EAMamba在超分辨率、去噪、去模糊和去雾等任务中保持了良好性能的同时，显著降低了计算开销。</p>
<p><strong>Insight:</strong> EAMamba通过高效的多头扫描和全局信息捕捉机制，为图像复原任务提供了一种更高效的解决方案，同时为其他低层视觉任务提供了改进方向。</p>
<p><strong>Abstract:</strong> Image restoration is a key task in low-level computer vision that aims to reconstruct high-quality images from degraded inputs. The emergence of Vision Mamba, which draws inspiration from the advanced state space model Mamba, marks a significant advancement in this field. Vision Mamba demonstrates excellence in modeling long-range dependencies with linear complexity, a crucial advantage for image restoration tasks. Despite its strengths, Vision Mamba encounters challenges in low-level vision tasks, including computational complexity that scales with the number of scanning sequences and local pixel forgetting. To address these limitations, this study introduces Efficient All-Around Mamba (EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently aggregates multiple scanning sequences, which avoids increases in computational complexity and parameter count. The all-around scanning strategy implements multiple patterns to capture holistic information and resolves the local pixel forgetting issue. Our experimental evaluations validate these innovations across several restoration tasks, including super resolution, denoising, deblurring, and dehazing. The results validate that EAMamba achieves a significant 31-89% reduction in FLOPs while maintaining favorable performance compared to existing low-level Vision Mamba methods.</p>
  </div>
</details>

<hr>
<h3 id="58-COOCO-–-Common-Objects-Out-of-Context-–-Semantic-Violation-in-Scenes-Investigating-Multimodal-Context-in-Referential-Communication-cs-CV-cs-CLPDF"><a href="#58-COOCO-–-Common-Objects-Out-of-Context-–-Semantic-Violation-in-Scenes-Investigating-Multimodal-Context-in-Referential-Communication-cs-CV-cs-CLPDF" class="headerlink" title="[58] COOCO – Common Objects Out-of-Context – Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication cs.CV | cs.CLPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22274">COOCO – Common Objects Out-of-Context – Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22274" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Filippo Merlo, Ece Takmaz, Wenkai Chen, Albert Gatt</span></p>
<p><strong>TL;DR:</strong> 该论文提出了COOCO数据集，用于研究视觉-语言模型（VLMs）在场景-对象一致性不同时的参考生成行为，发现模型会动态平衡局部和上下文信息。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究VLMs是否依赖场景上下文生成对象参考，尤其是在场景-对象语义不一致或噪声干扰时。</p>
<p><strong>Result:</strong> 模型在高场景-对象一致性或对象退化时更依赖上下文，并通过中层注意力聚焦目标对象。</p>
<p><strong>Insight:</strong> VLMs能够动态平衡局部和全局信息，尤其在噪声环境下表现出适应性。</p>
<p><strong>Abstract:</strong> Natural scenes provide us with rich contexts for object recognition and reference. In particular, knowing what type of scene one is looking at generates expectations about which objects will occur, and what their spatial configuration should be. Do Vision-Language Models (VLMs) learn to rely on scene contexts in a similar way, when generating references to objects? To address this question, we introduce the \textit{Common Objects Out-of-Context (COOCO)} dataset and test to what extent VLMs rely on scene context to refer to objects under different degrees of scene-object congruency, and different perturbations. Our findings show that models leverage scene context adaptively, depending on both the semantic relatedness between object and scene and the level of noise. In particular, models rely more on context under high target-scene congruence or when objects are degraded. Attention analysis reveals that successful object categorisation involves increased focus on the target in mid-level layers, especially under moderate noise, suggesting that VLMs dynamically balance local and contextual information for reference generation. We make our dataset, code and models available at \href{<a target="_blank" rel="noopener" href="https://github.com/cs-nlp-uu/scenereg%7D%7Bhttps://github.com/cs-nlp-uu/scenereg%7D">https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}</a>.</p>
  </div>
</details>

<hr>
<h3 id="59-Rethinking-Visual-Token-Reduction-in-LVLMs-under-Cross-modal-Misalignment-cs-CVPDF"><a href="#59-Rethinking-Visual-Token-Reduction-in-LVLMs-under-Cross-modal-Misalignment-cs-CVPDF" class="headerlink" title="[59] Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment cs.CVPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22283">Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22283" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rui Xu, Yunke Wang, Yong Luo, Bo Du</span></p>
<p><strong>TL;DR:</strong> 该论文提出VisionDrop，一种无需训练、仅基于视觉信息的修剪框架，通过视觉-视觉注意力选择信息丰富的视觉令牌，解决了跨模态错位问题，显著提升了视觉令牌减少的效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉令牌减少方法依赖文本条件交互，假设文本令牌能可靠捕捉视觉令牌的重要性，但研究发现跨模态错位（因果、语义和空间）会削弱这种方法的有效性。</p>
<p><strong>Result:</strong> 在多个基准测试中，VisionDrop相比现有方法表现更优，保留了细粒度视觉信息的同时实现了高效推理。</p>
<p><strong>Insight:</strong> 视觉令牌减少应避免依赖文本信号，而是利用视觉模态内部的信息相关性；跨模态错位是影响现有方法的关键因素。</p>
<p><strong>Abstract:</strong> Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences of patch-level tokens to capture fine-grained semantics. These visual tokens often outnumber their textual counterparts by a large margin, leading to substantial computational overhead and limiting the scalability of LVLMs in practice. Previous efforts have explored visual token reduction either prior to or within the large language models (LLM). However, most in-LLM reduction approaches rely on text-conditioned interactions, implicitly assuming that textual tokens can reliably capture the importance of visual tokens. In this work, we revisit this assumption and reveal causal, semantic, and spatial forms of cross-modal misalignment. These misalignments undermine the effectiveness of text-guided visual token reduction. To address this, we introduce VisionDrop, a training-free, visual-only pruning framework that selects informative visual tokens based on intra-modal (visual-to-visual) attention, without relying on textual signals. To further suppress redundancy throughout the model hierarchy, we treat the visual encoder and the LLM as a unified system and design a progressive pruning pipeline. Our method performs dominant token selection and lightweight contextual merging at multiple stages, enabling fine-grained visual information to be retained even under aggressive token budgets. Extensive experiments across diverse benchmarks show that VisionDrop achieves consistent improvements over existing methods, despite requiring no additional training or complex modifications. Its simple yet effective design enables efficient inference while preserving strong performance across tasks.</p>
  </div>
</details>

<hr>
<h3 id="60-RoomCraft-Controllable-and-Complete-3D-Indoor-Scene-Generation-cs-CV-cs-AIPDF"><a href="#60-RoomCraft-Controllable-and-Complete-3D-Indoor-Scene-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[60] RoomCraft: Controllable and Complete 3D Indoor Scene Generation cs.CV | cs.AIPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22291">RoomCraft: Controllable and Complete 3D Indoor Scene Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22291" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengqi Zhou, Xipeng Wang, Yuxi Wang, Zhaoxiang Zhang</span></p>
<p><strong>TL;DR:</strong> RoomCraft是一个多阶段生成3D室内场景的框架，通过约束驱动优化和冲突感知策略解决现有方法在全局一致性、多约束场景和布局完整性上的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在生成3D室内场景时存在全局空间推理不足、多约束下对象冲突频繁等问题，RoomCraft旨在通过可控生成和优化策略解决这些局限性。</p>
<p><strong>Result:</strong> 实验表明，RoomCraft在多种输入模态下均能生成更真实、语义一致且视觉吸引的室内场景布局。</p>
<p><strong>Insight:</strong> RoomCraft通过结构化约束和动态优化策略，为复杂场景生成提供了新的解决方案。</p>
<p><strong>Abstract:</strong> Generating realistic 3D indoor scenes from user inputs remains a challenging problem in computer vision and graphics, requiring careful balance of geometric consistency, spatial relationships, and visual realism. While neural generation methods often produce repetitive elements due to limited global spatial reasoning, procedural approaches can leverage constraints for controllable generation but struggle with multi-constraint scenarios. When constraints become numerous, object collisions frequently occur, forcing the removal of furniture items and compromising layout completeness.   To address these limitations, we propose RoomCraft, a multi-stage pipeline that converts real images, sketches, or text descriptions into coherent 3D indoor scenes. Our approach combines a scene generation pipeline with a constraint-driven optimization framework. The pipeline first extracts high-level scene information from user inputs and organizes it into a structured format containing room type, furniture items, and spatial relations. It then constructs a spatial relationship network to represent furniture arrangements and generates an optimized placement sequence using a heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence. To handle complex multi-constraint scenarios, we introduce a unified constraint representation that processes both formal specifications and natural language inputs, enabling flexible constraint-oriented adjustments through a comprehensive action space design. Additionally, we propose a Conflict-Aware Positioning Strategy (CAPS) that dynamically adjusts placement weights to minimize furniture collisions and ensure layout completeness.   Extensive experiments demonstrate that RoomCraft significantly outperforms existing methods in generating realistic, semantically coherent, and visually appealing room layouts across diverse input modalities.</p>
  </div>
</details>

<hr>
<h3 id="61-OutDreamer-Video-Outpainting-with-a-Diffusion-Transformer-cs-CVPDF"><a href="#61-OutDreamer-Video-Outpainting-with-a-Diffusion-Transformer-cs-CVPDF" class="headerlink" title="[61] OutDreamer: Video Outpainting with a Diffusion Transformer cs.CVPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22298">OutDreamer: Video Outpainting with a Diffusion Transformer</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22298" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Linhao Zhong, Fan Li, Yi Huang, Jianzhuang Liu, Renjing Pei</span></p>
<p><strong>TL;DR:</strong> OutDreamer提出了一种基于扩散变换器（DiT）的视频外绘框架，通过高效视频控制分支和条件外绘分支，结合掩码驱动的自注意力层和潜在对齐损失，实现了高质量且时间一致的视频扩展。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视频外绘任务需要同时保证时间和空间一致性，现有基于U-Net的潜在扩散模型难以兼顾质量和适应性。扩散变换器（DiT）因其优异性能成为新选择。</p>
<p><strong>Result:</strong> OutDreamer在零样本设置下超越现有方法，基准测试表现优异。</p>
<p><strong>Insight:</strong> 扩散变换器在视频生成任务中潜力巨大，掩码驱动机制和长视频细化策略对提升时空一致性至关重要。</p>
<p><strong>Abstract:</strong> Video outpainting is a challenging task that generates new video content by extending beyond the boundaries of an original input video, requiring both temporal and spatial consistency. Many state-of-the-art methods utilize latent diffusion models with U-Net backbones but still struggle to achieve high quality and adaptability in generated content. Diffusion transformers (DiTs) have emerged as a promising alternative because of their superior performance. We introduce OutDreamer, a DiT-based video outpainting framework comprising two main components: an efficient video control branch and a conditional outpainting branch. The efficient video control branch effectively extracts masked video information, while the conditional outpainting branch generates missing content based on these extracted conditions. Additionally, we propose a mask-driven self-attention layer that dynamically integrates the given mask information, further enhancing the model’s adaptability to outpainting tasks. Furthermore, we introduce a latent alignment loss to maintain overall consistency both within and between frames. For long video outpainting, we employ a cross-video-clip refiner to iteratively generate missing content, ensuring temporal consistency across video clips. Extensive evaluations demonstrate that our zero-shot OutDreamer outperforms state-of-the-art zero-shot methods on widely recognized benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="62-A-Deep-Learning-framework-for-building-damage-assessment-using-VHR-SAR-and-geospatial-data-demonstration-on-the-2023-Turkiye-Earthquake-cs-CV-cs-AIPDF"><a href="#62-A-Deep-Learning-framework-for-building-damage-assessment-using-VHR-SAR-and-geospatial-data-demonstration-on-the-2023-Turkiye-Earthquake-cs-CV-cs-AIPDF" class="headerlink" title="[62] A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake cs.CV | cs.AIPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22338">A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22338" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Luigi Russo, Deodato Tapete, Silvia Liberata Ullo, Paolo Gamba</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种多模态深度学习框架，利用单日超高分辨率SAR图像和辅助地理空间数据，快速评估建筑物损毁情况，并展示了在2023年土耳其地震中的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的光学卫星图像在灾害后的建筑物损毁评估中受限于云层覆盖或缺乏灾前数据，亟需一种不依赖灾前图像的快速评估方法。</p>
<p><strong>Result:</strong> 结果表明，整合地理空间特征显著提升了检测性能和泛化能力，能够高效评估建筑物损毁情况。</p>
<p><strong>Insight:</strong> 该方法解决了灾后快速响应的关键问题，通过自动化和可扩展的数据处理，为灾害管理提供了实用工具。</p>
<p><strong>Abstract:</strong> Building damage identification shortly after a disaster is crucial for guiding emergency response and recovery efforts. Although optical satellite imagery is commonly used for disaster mapping, its effectiveness is often hampered by cloud cover or the absence of pre-event acquisitions. To overcome these challenges, we introduce a novel multimodal deep learning (DL) framework for detecting building damage using single-date very high resolution (VHR) Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI) COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data. Our method integrates SAR image patches, OpenStreetMap (OSM) building footprints, digital surface model (DSM) data, and structural and exposure attributes from the Global Earthquake Model (GEM) to improve detection accuracy and contextual interpretation. Unlike existing approaches that depend on pre and post event imagery, our model utilizes only post event data, facilitating rapid deployment in critical scenarios. The framework effectiveness is demonstrated using a new dataset from the 2023 earthquake in Turkey, covering multiple cities with diverse urban settings. Results highlight that incorporating geospatial features significantly enhances detection performance and generalizability to previously unseen areas. By combining SAR imagery with detailed vulnerability and exposure information, our approach provides reliable and rapid building damage assessments without the dependency from available pre-event data. Moreover, the automated and scalable data generation process ensures the framework’s applicability across diverse disaster-affected regions, underscoring its potential to support effective disaster management and recovery efforts. Code and data will be made available upon acceptance of the paper.</p>
  </div>
</details>

<hr>
<h3 id="63-From-Ground-to-Air-Noise-Robustness-in-Vision-Transformers-and-CNNs-for-Event-Based-Vehicle-Classification-with-Potential-UAV-Applications-cs-CV-cs-AI-cs-LGPDF"><a href="#63-From-Ground-to-Air-Noise-Robustness-in-Vision-Transformers-and-CNNs-for-Event-Based-Vehicle-Classification-with-Potential-UAV-Applications-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[63] From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications cs.CV | cs.AI | cs.LGPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22360">From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22360" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nouf Almesafri, Hector Figueiredo, Miguel Arana-Catania</span></p>
<p><strong>TL;DR:</strong> 本文研究了CNN（ResNet34）和ViT（ViT B16）基于事件的相机数据在车辆分类任务中的表现，比较了它们在干净和有噪声数据下的性能，并探讨了其在无人机应用中的潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 事件相机因其动态捕捉能力适用于无人机和自动驾驶等动态环境，但现有研究较少比较CNN和ViT在此领域的性能差异和噪声鲁棒性。</p>
<p><strong>Result:</strong> ResNet34和ViT B16在干净数据上分别达到88%和86%的准确率；ViT在噪声环境下表现更稳定。</p>
<p><strong>Insight:</strong> ViT在小数据集和噪声环境下的鲁棒性为其在无人机等动态场景中的应用提供了潜力。</p>
<p><strong>Abstract:</strong> This study investigates the performance of the two most relevant computer vision deep learning architectures, Convolutional Neural Network and Vision Transformer, for event-based cameras. These cameras capture scene changes, unlike traditional frame-based cameras with capture static images, and are particularly suited for dynamic environments such as UAVs and autonomous vehicles. The deep learning models studied in this work are ResNet34 and ViT B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and compares these models under both standard conditions and in the presence of simulated noise. Initial evaluations on the clean GEN1 dataset reveal that ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with ResNet34 showing a slight advantage in classification accuracy. However, the ViT B16 model demonstrates notable robustness, particularly given its pre-training on a smaller dataset. Although this study focuses on ground-based vehicle classification, the methodologies and findings hold significant promise for adaptation to UAV contexts, including aerial object classification and event-based vision systems for aviation-related tasks.</p>
  </div>
</details>

<hr>
<h3 id="64-Exploiting-Vision-Language-Model-for-Training-Free-3D-Point-Cloud-OOD-Detection-via-Graph-Score-Propagation-cs-CVPDF"><a href="#64-Exploiting-Vision-Language-Model-for-Training-Free-3D-Point-Cloud-OOD-Detection-via-Graph-Score-Propagation-cs-CVPDF" class="headerlink" title="[64] Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation cs.CVPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22375">Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22375" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tiankai Chen, Yushu Li, Adam Goodge, Fei Teng, Xulei Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无需训练的框架，利用视觉语言模型（VLM）进行3D点云的离群检测（OOD），通过图分数传播（GSP）方法结合提示聚类和自训练负提示，显著提升了OOD检测效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 3D点云数据中的离群检测（OOD）在实际应用中至关重要，但目前的方法主要针对2D图像，扩展到3D环境面临挑战。本文旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验表明，GSP在合成和真实3D点云数据集上的OOD检测效果显著优于现有方法。</p>
<p><strong>Insight:</strong> VLM在3D点云OOD检测中具有潜力，图结构可以有效利用数据流形信息；提示聚类和负提示技术能进一步提升性能。</p>
<p><strong>Abstract:</strong> Out-of-distribution (OOD) detection in 3D point cloud data remains a challenge, particularly in applications where safe and robust perception is critical. While existing OOD detection methods have shown progress for 2D image data, extending these to 3D environments involves unique obstacles. This paper introduces a training-free framework that leverages Vision-Language Models (VLMs) for effective OOD detection in 3D point clouds. By constructing a graph based on class prototypes and testing data, we exploit the data manifold structure to enhancing the effectiveness of VLMs for 3D OOD detection. We propose a novel Graph Score Propagation (GSP) method that incorporates prompt clustering and self-training negative prompting to improve OOD scoring with VLM. Our method is also adaptable to few-shot scenarios, providing options for practical applications. We demonstrate that GSP consistently outperforms state-of-the-art methods across synthetic and real-world datasets 3D point cloud OOD detection.</p>
  </div>
</details>

<hr>
<h3 id="65-Can-Video-Large-Multimodal-Models-Think-Like-Doubters-or-Double-Down-A-Study-on-Defeasible-Video-Entailment-cs-CV-cs-AI-cs-CLPDF"><a href="#65-Can-Video-Large-Multimodal-Models-Think-Like-Doubters-or-Double-Down-A-Study-on-Defeasible-Video-Entailment-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[65] Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment cs.CV | cs.AI | cs.CLPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22385">Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22385" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Zhang, Jilei Sun, Yunhui Guo, Vibhav Gogate</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为Defeasible Video Entailment (DVidE)的新任务，挑战视频大模型（VLMMs）动态更新推理能力，并提出了两种框架：基于反事实推理的分类任务框架和结合ASR与LLM的生成任务框架，同时引入新数据集和评估指标。实验表明该方法显著提升了VLMMs的动态推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频大模型（VLMMs）在抽象和自适应推理方面表现不足，无法根据新信息动态更新推理结果。DVidE任务旨在模拟现实世界中结论可被强化的场景，提升模型的动态推理能力。</p>
<p><strong>Result:</strong> 实验结果表明，所提方法显著提升了VLMMs在动态推理任务上的性能。</p>
<p><strong>Insight:</strong> 1. 反事实推理有助于减少推理偏差；2. 结合ASR与LLM能够生成更连贯的更新；3. 动态推理能力是VLMMs未来发展的关键方向。</p>
<p><strong>Abstract:</strong> Video Large Multimodal Models (VLMMs) have made impressive strides in understanding video content, but they often struggle with abstract and adaptive reasoning-the ability to revise their interpretations when new information emerges. In reality, conclusions are rarely set in stone; additional context can strengthen or weaken an initial inference. To address this, we introduce Defeasible Video Entailment (DVidE), a new task that challenges models to think like doubters, constantly updating their reasoning based on evolving evidence. In DVidE, given a video premise and a textual hypothesis, models must determine whether a new update strengthens or weakens the hypothesis (classification version) or generate a coherent update that modifies the entailment relationship (generation version). For solving the classification task, we propose the Chain of Counterfactual Thought framework, utilizing counterfactual reasoning, ASR-enhanced video content, and rationale refinement to reduce inference bias. For the generation task, we develop a framework that combines ASR output with a Large Language Model (LLM) to produce coherent, contextually relevant updates aligned with the intended strengthener or weakener goals. Additionally, we introduce a novel benchmark dataset, with strengthener&#x2F;weakener annotations and an LLM-based evaluation metric specifically designed for assessing generative performance. Experimental results demonstrate significant improvements, highlighting our proposed method in enhancing dynamic reasoning capabilities of VLMMs.</p>
  </div>
</details>

<hr>
<h3 id="66-Test-Time-Consistency-in-Vision-Language-Models-cs-CVPDF"><a href="#66-Test-Time-Consistency-in-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[66] Test-Time Consistency in Vision Language Models cs.CVPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22395">Test-Time Consistency in Vision Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22395" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shih-Han Chou, Shivam Chandhok, James J. Little, Leonid Sigal</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种无需监督重新训练的测试时一致性框架，通过两种互补的目标函数增强视觉语言模型（VLM）的语义一致性，显著提升了模型在MM-R3基准上的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视觉语言模型在语义等效输入下表现不一致，尽管平均准确率高，但影响了其可靠性和鲁棒性。论文旨在解决这一问题，而不依赖模型架构修改或大规模监督微调。</p>
<p><strong>Result:</strong> 在MM-R3基准上，该方法显著提升了现有最先进模型的语义一致性表现，为多模态学习的推断时适应提供了新方向。</p>
<p><strong>Insight:</strong> 测试时自适应方法可以显著提升模型的一致性，而无需修改模型架构或依赖于大规模监督数据，展示了在推断阶段优化的潜力。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) have achieved impressive performance across a wide range of multimodal tasks, yet they often exhibit inconsistent behavior when faced with semantically equivalent inputs, undermining their reliability and robustness. Recent benchmarks, such as MM-R3, highlight that even state-of-the-art VLMs can produce divergent predictions across semantically equivalent inputs, despite maintaining high average accuracy. Prior work addresses this issue by modifying model architectures or conducting large-scale fine-tuning on curated datasets. In contrast, we propose a simple and effective test-time consistency framework that enhances semantic consistency without supervised re-training. Our method is entirely post-hoc, model-agnostic, and applicable to any VLM with access to its weights. Given a single test point, we enforce consistent predictions via two complementary objectives: (i) a Cross-Entropy Agreement Loss that aligns predictive distributions across semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that draws outputs toward a self-averaged consensus. Our method is plug-and-play and leverages information from a single test input itself to improve consistency. Experiments on the MM-R3 benchmark show that our framework yields substantial gains in consistency across state-of-the-art models, establishing a new direction for inference-time adaptation in multimodal learning.</p>
  </div>
</details>

<hr>
<h3 id="67-Shape-for-Motion-Precise-and-Consistent-Video-Editing-with-3D-Proxy-cs-CVPDF"><a href="#67-Shape-for-Motion-Precise-and-Consistent-Video-Editing-with-3D-Proxy-cs-CVPDF" class="headerlink" title="[67] Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy cs.CVPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22432">Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22432" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuhao Liu, Tengfei Wang, Fang Liu, Zhenwei Wang, Rynson W. H. Lau</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Shape-for-Motion框架，通过将视频中的目标对象转换为时间一致的3D代理网格，实现了精确且一致的视频编辑。其关键创新包括双传播策略和3D到2D的投影机制，支持多种精确操作。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频生成方法难以精确满足用户的编辑意图，尤其是在复杂操作（如姿态、纹理修改）中保持一致性。本研究旨在通过引入3D代理网格，提供更可控的视频编辑工具。</p>
<p><strong>Result:</strong> 实验证明了该框架在姿态编辑、纹理修改等多种操作中的优越性，实现了高质量且一致的效果。</p>
<p><strong>Insight:</strong> 3D代理网格为视频编辑提供了更结构化的控制手段，结合扩散模型的生成能力，展示了未来可控内容创作的潜力。</p>
<p><strong>Abstract:</strong> Recent advances in deep generative modeling have unlocked unprecedented opportunities for video synthesis. In real-world applications, however, users often seek tools to faithfully realize their creative editing intentions with precise and consistent control. Despite the progress achieved by existing methods, ensuring fine-grained alignment with user intentions remains an open and challenging problem. In this work, we present Shape-for-Motion, a novel framework that incorporates a 3D proxy for precise and consistent video editing. Shape-for-Motion achieves this by converting the target object in the input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be performed directly on the proxy and then inferred back to the video frames. To simplify the editing process, we design a novel Dual-Propagation Strategy that allows users to perform edits on the 3D mesh of a single frame, and the edits are then automatically propagated to the 3D meshes of the other frames. The 3D meshes for different frames are further projected onto the 2D space to produce the edited geometry and texture renderings, which serve as inputs to a decoupled video diffusion model for generating edited results. Our framework supports various precise and physically-consistent manipulations across the video frames, including pose editing, rotation, scaling, translation, texture modification, and object composition. Our approach marks a key step toward high-quality, controllable video editing workflows. Extensive experiments demonstrate the superiority and effectiveness of our approach. Project page: <a target="_blank" rel="noopener" href="https://shapeformotion.github.io/">https://shapeformotion.github.io/</a></p>
  </div>
</details>

<hr>
<h3 id="68-WarpRF-Multi-View-Consistency-for-Training-Free-Uncertainty-Quantification-and-Applications-in-Radiance-Fields-cs-CVPDF"><a href="#68-WarpRF-Multi-View-Consistency-for-Training-Free-Uncertainty-Quantification-and-Applications-in-Radiance-Fields-cs-CVPDF" class="headerlink" title="[68] WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields cs.CVPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22433">WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22433" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sadra Safadoust, Fabio Tosi, Fatma Güney, Matteo Poggi</span></p>
<p><strong>TL;DR:</strong> WarpRF 是一个无需训练的多视角一致性框架，用于量化辐射场的不确定性，通过反向映射和一致性测量实现高效的不确定性评估，并在下游任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前辐射场方法在不确定性量化方面缺乏高效且通用的解决方案，WarpRF 通过多视角一致性假设填补了这一空白。</p>
<p><strong>Result:</strong> WarpRF 在不确定性量化和下游任务（如主动视角选择和主动建图）中优于现有方法。</p>
<p><strong>Insight:</strong> 多视角一致性是量化辐射场不确定性的有效指标，无需额外训练即可实现高效评估。</p>
<p><strong>Abstract:</strong> We introduce WarpRF, a training-free general-purpose framework for quantifying the uncertainty of radiance fields. Built upon the assumption that photometric and geometric consistency should hold among images rendered by an accurate model, WarpRF quantifies its underlying uncertainty from an unseen point of view by leveraging backward warping across viewpoints, projecting reliable renderings to the unseen viewpoint and measuring the consistency with images rendered there. WarpRF is simple and inexpensive, does not require any training, and can be applied to any radiance field implementation for free. WarpRF excels at both uncertainty quantification and downstream tasks, e.g., active view selection and active mapping, outperforming any existing method tailored to specific frameworks.</p>
  </div>
</details>

<hr>
<h3 id="69-MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning-cs-CVPDF"><a href="#69-MiCo-Multi-image-Contrast-for-Reinforcement-Visual-Reasoning-cs-CVPDF" class="headerlink" title="[69] MiCo: Multi-image Contrast for Reinforcement Visual Reasoning cs.CVPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22434">MiCo: Multi-image Contrast for Reinforcement Visual Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22434" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种多图像对比方法（MiCo），通过自监督学习构建图像三元组，结合规则化的强化学习训练模型进行跨图像推理，无需人工标注，显著提升了多图像推理任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法依赖人工标注的问题-答案对，难以处理细粒度视觉细节和跨图像的复杂逻辑推理。受自监督视觉表征学习启发，作者发现图像本身的内在约束可以作为监督信号。</p>
<p><strong>Result:</strong> 在未使用人工标注的情况下，方法在多图像推理任务中表现显著优于基线，并在一般视觉任务中展现了强大性能。</p>
<p><strong>Insight:</strong> 图像的内在约束可以作为自监督信号，通过强化学习训练模型进行复杂推理任务，减少对人工标注的依赖。</p>
<p><strong>Abstract:</strong> This work explores enabling Chain-of-Thought (CoT) reasoning to link visual cues across multiple images. A straightforward solution is to adapt rule-based reinforcement learning for Vision-Language Models (VLMs). However, such methods typically rely on manually curated question-answer pairs, which can be particularly challenging when dealing with fine grained visual details and complex logic across images. Inspired by self-supervised visual representation learning, we observe that images contain inherent constraints that can serve as supervision. Based on this insight, we construct image triplets comprising two augmented views of the same image and a third, similar but distinct image. During training, the model is prompted to generate a reasoning process to compare these images (i.e., determine same or different). Then we optimize the model with rule-based reinforcement learning. Due to the high visual similarity and the presence of augmentations, the model must attend to subtle visual changes and perform logical reasoning to succeed. Experiments show that, although trained solely on visual comparison tasks, the learned reasoning ability generalizes effectively to a wide range of questions. Without relying on any human-annotated question-answer pairs, our method achieves significant improvements on multi-image reasoning benchmarks and shows strong performance on general vision tasks.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="70-VAT-KG-Knowledge-Intensive-Multimodal-Knowledge-Graph-Dataset-for-Retrieval-Augmented-Generation-cs-CLPDF"><a href="#70-VAT-KG-Knowledge-Intensive-Multimodal-Knowledge-Graph-Dataset-for-Retrieval-Augmented-Generation-cs-CLPDF" class="headerlink" title="[70] VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation cs.CLPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21556">VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21556" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyeongcheol Park, MinHyuk Jang, Ha Dam Baek, Gyusam Chang, Jiyoung Seo</span></p>
<p><strong>TL;DR:</strong> 本文提出了VAT-KG，首个概念为中心、知识密集型的多模态知识图谱，覆盖视觉、音频和文本信息，支持跨模态知识对齐和检索增强生成（RAG），在多模态任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MMKGs覆盖范围有限，知识可能过时或不完整，且仅支持少数模态（如文本和视觉），限制了其在多模态任务中的扩展性和应用性。</p>
<p><strong>Result:</strong> 在多模态问答任务中，VAT-KG显著提升了MLLMs的表现，展示了其在统一和利用多模态知识方面的实用价值。</p>
<p><strong>Insight:</strong> VAT-KG填补了现有MMKGs在模态覆盖和知识密集性上的不足，为多模态任务提供了更全面的知识支持。</p>
<p><strong>Abstract:</strong> Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge across multiple modalities, play a pivotal role by complementing the implicit knowledge of Multimodal Large Language Models (MLLMs) and enabling more grounded reasoning via Retrieval Augmented Generation (RAG). However, existing MMKGs are generally limited in scope: they are often constructed by augmenting pre-existing knowledge graphs, which restricts their knowledge, resulting in outdated or incomplete knowledge coverage, and they often support only a narrow range of modalities, such as text and visual information. These limitations reduce their extensibility and applicability to a broad range of multimodal tasks, particularly as the field shifts toward richer modalities such as video and audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text Knowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive multimodal knowledge graph that covers visual, audio, and text information, where each triplet is linked to multimodal data and enriched with detailed descriptions of concepts. Specifically, our construction pipeline ensures cross-modal knowledge alignment between multimodal data and fine-grained semantics through a series of stringent filtering and alignment steps, enabling the automatic generation of MMKGs from any multimodal dataset. We further introduce a novel multimodal RAG framework that retrieves detailed concept-level knowledge in response to queries from arbitrary modalities. Experiments on question answering tasks across various modalities demonstrate the effectiveness of VAT-KG in supporting MLLMs, highlighting its practical value in unifying and leveraging multimodal knowledge.</p>
  </div>
</details>

<hr>
<h3 id="71-Debunk-and-Infer-Multimodal-Fake-News-Detection-via-Diffusion-Generated-Evidence-and-LLM-Reasoning-cs-CLPDF"><a href="#71-Debunk-and-Infer-Multimodal-Fake-News-Detection-via-Diffusion-Generated-Evidence-and-LLM-Reasoning-cs-CLPDF" class="headerlink" title="[71] Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning cs.CLPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21557">Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21557" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaiying Yan, Moyang Liu, Yukun Liu, Ruibo Fu, Zhengqi Wen</span></p>
<p><strong>TL;DR:</strong> DIFND 是一个多模态假新闻检测框架，通过扩散模型生成反驳证据并结合多模态大语言模型的推理能力，显著提升了检测性能和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 假新闻在多媒体平台上的快速传播对信息可信度构成严重威胁，因此需要一种高效且可解释的检测方法。</p>
<p><strong>Result:</strong> 在 FakeSV 和 FVC 数据集上，DIFND 不仅优于现有方法，还提供了可信赖的决策。</p>
<p><strong>Insight:</strong> 生成式模型与推理模型的结合能够为假新闻检测提供更丰富的证据和更强的解释性。</p>
<p><strong>Abstract:</strong> The rapid spread of fake news across multimedia platforms presents serious challenges to information credibility. In this paper, we propose a Debunk-and-Infer framework for Fake News Detection(DIFND) that leverages debunking knowledge to enhance both the performance and interpretability of fake news detection. DIFND integrates the generative strength of conditional diffusion models with the collaborative reasoning capabilities of multimodal large language models (MLLMs). Specifically, debunk diffusion is employed to generate refuting or authenticating evidence based on the multimodal content of news videos, enriching the evaluation process with diverse yet semantically aligned synthetic samples. To improve inference, we propose a chain-of-debunk strategy where a multi-agent MLLM system produces logic-grounded, multimodal-aware reasoning content and final veracity judgment. By jointly modeling multimodal features, generative debunking cues, and reasoning-rich verification within a unified architecture, DIFND achieves notable improvements in detection accuracy. Extensive experiments on the FakeSV and FVC datasets show that DIFND not only outperforms existing approaches but also delivers trustworthy decisions.</p>
  </div>
</details>

<hr>
<h3 id="72-Reinforcement-Learning-Fine-Tuning-of-Language-Model-for-Instruction-Following-and-Math-Reasoning-cs-CL-cs-AIPDF"><a href="#72-Reinforcement-Learning-Fine-Tuning-of-Language-Model-for-Instruction-Following-and-Math-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[72] Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning cs.CL | cs.AIPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21560">Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21560" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifu Han, Geo Zhang</span></p>
<p><strong>TL;DR:</strong> 该研究探讨了强化学习（RL）微调技术在小型语言模型（Qwen2.5-0.5B Base）上对指令跟随和数学推理任务的有效性，比较了SFT、DPO和RLOO等方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 如何利用高效微调方法（如RL）改进小型语言模型在复杂任务（如指令跟随和数学推理）中的表现。</p>
<p><strong>Result:</strong> RLOO在任务对齐上表现最佳，DPO效果稳定；数学任务中合成数据增强和best-of-N采样显著提高了准确性。</p>
<p><strong>Insight:</strong> 结合微调与推理时工具可以高效优化小型语言模型，平衡性能与资源消耗。</p>
<p><strong>Abstract:</strong> This study investigates the effectiveness of reinforcement learning (RL) fine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two challenging tasks: instruction following and mathematical reasoning. We compare supervised fine-tuning (SFT), Direct Preference Optimization (DPO) using preference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models. Our experiments show that RLOO with DeBERTa reward modeling achieves the best alignment, while DPO provides strong and consistent results. For math reasoing tasks, synthetic data augmentation and best-of-N sampling with an external verifier significantly improve accuracy, showing the potential of combining fine-tuning with inference-time tools. This study highlights key trade-offs and practical strategies for training lightweight, task-aligned small-scale language models.</p>
  </div>
</details>

<hr>
<h3 id="73-Reasoning-Isn’t-Enough-Examining-Truth-Bias-and-Sycophancy-in-LLMs-cs-CL-cs-AIPDF"><a href="#73-Reasoning-Isn’t-Enough-Examining-Truth-Bias-and-Sycophancy-in-LLMs-cs-CL-cs-AIPDF" class="headerlink" title="[73] Reasoning Isn’t Enough: Examining Truth-Bias and Sycophancy in LLMs cs.CL | cs.AIPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21561">Reasoning Isn’t Enough: Examining Truth-Bias and Sycophancy in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21561" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Emilio Barkett, Olivia Long, Madhavendra Thakur</span></p>
<p><strong>TL;DR:</strong> 该研究评估了大型语言模型（LLMs）在真相检测方面的能力，首次分析了推理模型在这方面的表现。研究发现推理模型的真相偏见低于非推理模型，但仍高于人类基准，且高级模型存在谄媚倾向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLMs被广泛用于事实核查和决策制定，但其作为真相评判者的能力仍不够清晰。研究旨在填补这一空白，分析不同LLMs在真相检测中的表现。</p>
<p><strong>Result:</strong> 推理模型的真相偏见较低，但仍高于人类；高级模型在真相检测中表现不对称（真相准确率高，欺骗检测差）。</p>
<p><strong>Insight:</strong> LLMs的真相检测能力仍有缺陷，尤其是高级模型中存在的谄媚行为表明技术改进需结合行为分析。</p>
<p><strong>Abstract:</strong> Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs’ veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.</p>
  </div>
</details>

<hr>
<h3 id="74-FloorPlan-DeepSeek-FPDS-A-multimodal-approach-to-floorplan-generation-using-vector-based-next-room-prediction-cs-CL-cs-AI-cs-ARPDF"><a href="#74-FloorPlan-DeepSeek-FPDS-A-multimodal-approach-to-floorplan-generation-using-vector-based-next-room-prediction-cs-CL-cs-AI-cs-ARPDF" class="headerlink" title="[74] FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction cs.CL | cs.AI | cs.ARPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21562">FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.AR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21562" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jun Yin, Pengyu Zeng, Jing Zhong, Peilin Li, Miao Zhang</span></p>
<p><strong>TL;DR:</strong> FloorPlan-DeepSeek (FPDS) 提出了一种基于‘下一个房间预测’的多模态方法，用于逐步生成矢量化的平面图，更适合实际建筑设计的迭代工作流程。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有平面图生成模型通常是端到端的，直接生成整个像素布局，与实际建筑设计中的渐进式工作流程不符。作者受大型语言模型中自回归‘下一个token预测’机制的启发，试图改进这一范式。</p>
<p><strong>Result:</strong> 在文本到平面图任务中，FPDS表现出与扩散模型和Tell2Design相当的竞争性能。</p>
<p><strong>Insight:</strong> 这种逐步生成方法更贴近实际建筑设计流程，有望支持未来智能建筑设计。</p>
<p><strong>Abstract:</strong> In the architectural design process, floor plan generation is inherently progressive and iterative. However, existing generative models for floor plans are predominantly end-to-end generation that produce an entire pixel-based layout in a single pass. This paradigm is often incompatible with the incremental workflows observed in real-world architectural practice. To address this issue, we draw inspiration from the autoregressive ‘next token prediction’ mechanism commonly used in large language models, and propose a novel ‘next room prediction’ paradigm tailored to architectural floor plan modeling. Experimental evaluation indicates that FPDS demonstrates competitive performance in comparison to diffusion models and Tell2Design in the text-to-floorplan task, indicating its potential applicability in supporting future intelligent architectural design.</p>
  </div>
</details>

<hr>
<h3 id="75-FormosanBench-Benchmarking-Low-Resource-Austronesian-Languages-in-the-Era-of-Large-Language-Models-cs-CLPDF"><a href="#75-FormosanBench-Benchmarking-Low-Resource-Austronesian-Languages-in-the-Era-of-Large-Language-Models-cs-CLPDF" class="headerlink" title="[75] FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models cs.CLPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21563">FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21563" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaiying Kevin Lin, Hsiyu Chen, Haopeng Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了FORMOSANBENCH，首个针对低资源南岛语族的基准测试，评估了大语言模型（LLMs）在濒危台湾南岛语（泰雅语、阿美语、排湾语）的机器翻译、自动语音识别和文本摘要任务上的表现。结果显示，LLMs在这些语言上的表现远低于高资源语言，且10-shot学习和微调改进有限，呼吁开发更包容的NLP技术。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 虽然大语言模型（LLMs）在高资源语言的NLP任务中表现优异，但对低资源和濒危语言的能力尚未充分探索。台湾南岛语兼具语言多样性和濒危性，亟需研究支持。</p>
<p><strong>Result:</strong> LLMs在台湾南岛语上表现显著落后于高资源语言，10-shot和微调改进有限，突显了语言资源的匮乏问题。</p>
<p><strong>Insight:</strong> LLMs在低资源语言上的性能劣势揭示了当前技术对濒危语言支持的不足，需更注重多语言包容性研究。</p>
<p><strong>Abstract:</strong> While large language models (LLMs) have demonstrated impressive performance across a wide range of natural language processing (NLP) tasks in high-resource languages, their capabilities in low-resource and minority languages remain significantly underexplored. Formosan languages – a subgroup of Austronesian languages spoken in Taiwan – are both linguistically rich and endangered, largely due to the sociolinguistic dominance of Mandarin. In this work, we introduce FORMOSANBENCH, the first benchmark for evaluating LLMs on low-resource Austronesian languages. It covers three endangered Formosan languages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine translation, automatic speech recognition (ASR), and text summarization. We assess model performance in zero-shot, 10-shot, and fine-tuned settings using FORMOSANBENCH. Our results reveal a substantial performance gap between high-resource and Formosan languages. Existing LLMs consistently underperform across all tasks, with 10-shot learning and fine-tuning offering only limited improvements. These findings underscore the urgent need for more inclusive NLP technologies that can effectively support endangered and underrepresented languages. We release our datasets and code to facilitate future research in this direction.</p>
  </div>
</details>

<hr>
<h3 id="76-Towards-Understanding-the-Cognitive-Habits-of-Large-Reasoning-Models-cs-CL-cs-AI-cs-CRPDF"><a href="#76-Towards-Understanding-the-Cognitive-Habits-of-Large-Reasoning-Models-cs-CL-cs-AI-cs-CRPDF" class="headerlink" title="[76] Towards Understanding the Cognitive Habits of Large Reasoning Models cs.CL | cs.AI | cs.CRPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21571">Towards Understanding the Cognitive Habits of Large Reasoning Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21571" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jianshuo Dong, Yujia Fu, Chuanrui Hu, Chao Zhang, Han Qiu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了CogTest基准，用于评估大型推理模型（LRMs）是否表现出类似人类的认知习惯，发现LRMs能够适应性地部署这些习惯，并揭示了其与安全相关任务的关联。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 受到大型推理模型（LRMs）在推理过程中表现出类似人类思维习惯的现象启发，研究试图验证这些模型是否确实拥有类似人类的认知习惯，并探索其对模型行为解释的意义。</p>
<p><strong>Result:</strong> 1. LRMs表现出人类类似的认知习惯，并能根据任务适应性部署。2. 同一家族模型（如Qwen-3和DeepSeek-R1）在认知习惯上表现出相似性。3. 某些习惯（如Taking Responsible Risks）与生成有害回复强相关。</p>
<p><strong>Insight:</strong> 研究LRMs的持续行为模式有助于深入理解模型的行为机制和潜在问题，尤其在与安全相关的任务中。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain of Thought (CoT) before producing final responses, offer a promising approach to interpreting and monitoring model behaviors. Inspired by the observation that certain CoT patterns – e.g., &#96;&#96;Wait, did I miss anything?’’ – consistently emerge across tasks, we explore whether LRMs exhibit human-like cognitive habits. Building on Habits of Mind, a well-established framework of cognitive habits associated with successful human problem-solving, we introduce CogTest, a principled benchmark designed to evaluate LRMs’ cognitive habits. CogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks, and employs an evidence-first extraction method to ensure reliable habit identification. With CogTest, we conduct a comprehensive evaluation of 16 widely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that LRMs, unlike conventional LLMs, not only exhibit human-like habits but also adaptively deploy them according to different tasks. Finer-grained analyses further uncover patterns of similarity and difference in LRMs’ cognitive habit profiles, particularly certain inter-family similarity (e.g., Qwen-3 models and DeepSeek-R1). Extending the study to safety-related tasks, we observe that certain habits, such as Taking Responsible Risks, are strongly associated with the generation of harmful responses. These findings suggest that studying persistent behavioral patterns in LRMs’ CoTs is a valuable step toward deeper understanding of LLM misbehavior. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/jianshuod/CogTest">https://github.com/jianshuod/CogTest</a>.</p>
  </div>
</details>

<hr>
<h3 id="77-Aligning-MLLM-Benchmark-With-Human-Preferences-via-Structural-Equation-Modeling-cs-CLPDF"><a href="#77-Aligning-MLLM-Benchmark-With-Human-Preferences-via-Structural-Equation-Modeling-cs-CLPDF" class="headerlink" title="[77] Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling cs.CLPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21572">Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21572" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianyu. Zou, Shengwu. Xiong, Ruilin. Yao, Jirui. Huang, Yi. Rong</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于结构方程建模（SEM）的新框架，用于对齐和评估多模态大语言模型（MLLM）的基准测试，以解决现有基准测试设计中的冗余和认知目标不明确问题。通过引入基于皮亚杰认知发展理论的能力层次结构，作者重新组织了基准测试并构建了名为Gold的新基准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MLLM基准测试设计缺乏结构化、可解释性和理论基础，导致能力重叠、指标冗余和诊断能力有限。为了解决这些问题，作者提出了基于SEM的框架。</p>
<p><strong>Result:</strong> 实验结果表明，Gold基准比现有方法具有更强的可解释性、更少的指标冗余和更清晰的认知一致性。</p>
<p><strong>Insight:</strong> 通过结合心理学理论和统计建模，可以设计更具诊断性和可解释性的MLLM基准测试，从而更好地评估模型的能力。</p>
<p><strong>Abstract:</strong> Evaluating multimodal large language models (MLLMs) remains a fundamental challenge due to a lack of structured, interpretable, and theoretically grounded benchmark designs. Existing benchmarks often adopt heuristic-based task groupings with unclear cognitive targets, thus resulting in overlapping abilities, redundant indicators, and limited diagnostic power. In this work, we propose a novel framework for aligning MLLM benchmark based on Structural Equation Modeling (SEM) to analyze and quantify the internal validity, dimensional separability, and contribution of benchmark components. Motivated by the observed limitations of current designs, we further introduce a novel capability hierarchy grounded in Piagets theory of cognitive development, dividing MLLM abilities into three hierarchical layers, i.e., Perception, Memory, and Reasoning. We reorganize existing MLLM benchmarks under the proposed framework and construct a new benchmark named Gold. Experimental results demonstrate that the proposed benchmark exhibits stronger interpretability, reduced indicator redundancy, and clearer cognitive consistency compared to existing approaches.</p>
  </div>
</details>

<hr>
<h3 id="78-Instruction-Learning-Paradigms-A-Dual-Perspective-on-White-box-and-Black-box-LLMs-cs-CL-cs-AI-cs-LGPDF"><a href="#78-Instruction-Learning-Paradigms-A-Dual-Perspective-on-White-box-and-Black-box-LLMs-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[78] Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs cs.CL | cs.AI | cs.LGPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21573">Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21573" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yanwei Ren, Liu Liu, Baosheng Yu, Jiayan Qiu, Quan Chen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合白盒和黑盒LLM优势的新框架，通过黑盒模型提供高质量指令初始化，白盒模型提供细粒度可解释性，并通过语义相似性约束实现优化，实验表明其性能优于现有基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法中，白盒模型需要大量计算资源且表示能力有限，而黑盒模型成本高昂，因此需要一种融合两者优势的高效解决方案。</p>
<p><strong>Result:</strong> 在复杂推理和跨语言泛化等广泛任务中，该框架性能显著优于现有基线。</p>
<p><strong>Insight:</strong> 融合黑盒和白盒模型的优势可以高效提升LLM的指令优化能力，为实际应用提供可扩展的解决方案。</p>
<p><strong>Abstract:</strong> Optimizing instructions for large language models (LLMs) is critical for harnessing their full potential in complex and diverse tasks. However, relying solely on white-box approaches demands extensive computational resources and offers limited representational capacity, while black-box models can incur prohibitive financial costs. To address these challenges, we introduce a novel framework that seamlessly merges the strengths of both paradigms. Black-box models provide high-quality, diverse instruction initializations, and white-box models supply fine-grained interpretability through hidden states and output features. By enforcing a semantic similarity constraint, these components fuse into a unified high-dimensional representation that captures deep semantic and structural nuances, enabling an iterative optimization process to refine instruction quality and adaptability. Extensive evaluations across a broad spectrum of tasks-ranging from complex reasoning to cross-lingual generalization-demonstrate that our approach consistently outperforms state-of-the-art baselines. This fusion of black-box initialization with advanced semantic refinement yields a scalable and efficient solution, paving the way for next-generation LLM-driven applications in diverse real-world scenarios. The source code will be released soon.</p>
  </div>
</details>

<hr>
<h3 id="79-Digital-Gatekeepers-Exploring-Large-Language-Model’s-Role-in-Immigration-Decisions-cs-CL-cs-AIPDF"><a href="#79-Digital-Gatekeepers-Exploring-Large-Language-Model’s-Role-in-Immigration-Decisions-cs-CL-cs-AIPDF" class="headerlink" title="[79] Digital Gatekeepers: Exploring Large Language Model’s Role in Immigration Decisions cs.CL | cs.AIPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21574">Digital Gatekeepers: Exploring Large Language Model’s Role in Immigration Decisions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21574" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yicheng Mao, Yang Zhao</span></p>
<p><strong>TL;DR:</strong> 该论文研究了大型语言模型（如GPT-3.5和GPT-4）在移民决策中的作用，发现其可以与人战略对齐，但也揭示了潜在的偏见问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 全球化与移民人口增加导致移民部门工作负担加重，需要公平决策的挑战。AI的整合为此提供了潜在解决方案。</p>
<p><strong>Result:</strong> LLMs能够与人战略对齐，强调效用最大化和程序公平性，但仍存在国籍偏见和特权群体偏好。</p>
<p><strong>Insight:</strong> LLMs在自动化移民决策中具有潜力，但需警惕其内在偏见，需进一步优化以确保公平性。</p>
<p><strong>Abstract:</strong> With globalization and increasing immigrant populations, immigration departments face significant work-loads and the challenge of ensuring fairness in decision-making processes. Integrating artificial intelligence offers a promising solution to these challenges. This study investigates the potential of large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting immigration decision-making. Utilizing a mixed-methods approach,this paper conducted discrete choice experiments and in-depth interviews to study LLM decision-making strategies and whether they are fair. Our findings demonstrate that LLMs can align their decision-making with human strategies, emphasizing utility maximization and procedural fairness. Meanwhile, this paper also reveals that while ChatGPT has safeguards to prevent unintentional discrimination, it still exhibits stereotypes and biases concerning nationality and shows preferences toward privileged group. This dual analysis highlights both the potential and limitations of LLMs in automating and enhancing immigration decisions.</p>
  </div>
</details>

<hr>
<h3 id="80-STRuCT-LLM-Unifying-Tabular-and-Graph-Reasoning-with-Reinforcement-Learning-for-Semantic-Parsing-cs-CL-cs-AIPDF"><a href="#80-STRuCT-LLM-Unifying-Tabular-and-Graph-Reasoning-with-Reinforcement-Learning-for-Semantic-Parsing-cs-CL-cs-AIPDF" class="headerlink" title="[80] STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing cs.CL | cs.AIPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21575">STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21575" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Josefa Lia Stoisser, Marc Boubnovski Martell, Lawrence Phillips, Casper Hansen, Julien Fauqueur</span></p>
<p><strong>TL;DR:</strong> STRuCT-LLM是一个统一框架，通过强化学习和Chain-of-Thought监督，训练LLMs在关系型和图结构数据上进行结构化推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有工作通常孤立处理关系型和图形式的数据，缺乏统一的框架来联合优化两种任务。</p>
<p><strong>Result:</strong> Spider任务提升13.5%，Text2Cypher任务提升73.1%，并在下游任务中展示了零样本泛化能力。</p>
<p><strong>Insight:</strong> 联合训练SQL和Cypher能产生协同效应，以执行查询为脚手架的结构化推理是有效的。</p>
<p><strong>Abstract:</strong> We propose STRuCT-LLM, a unified framework for training large language models (LLMs) to perform structured reasoning over both relational and graph-structured data. Our approach jointly optimizes Text-to-SQL and Text-to-Cypher tasks using reinforcement learning (RL) combined with Chain-of-Thought (CoT) supervision. To support fine-grained optimization in graph-based parsing, we introduce a topology-aware reward function based on graph edit distance. Unlike prior work that treats relational and graph formalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL and Cypher to induce cross-formalism transfer, enabling SQL training to improve Cypher performance and vice versa - even without shared schemas. Our largest model (QwQ-32B) achieves substantial relative improvements across tasks: on semantic parsing, Spider improves by 13.5% and Text2Cypher by 73.1%. The model also demonstrates strong zero-shot generalization, improving performance on downstream tabular QA (TableBench: 8.5%) and knowledge graph QA (CR-LT-KGQA: 1.7%) without any QA-specific supervision. These results demonstrate both the effectiveness of executable queries as scaffolds for structured reasoning and the synergistic benefits of jointly training on SQL and Cypher (code available at <a target="_blank" rel="noopener" href="https://github.com/bouv/STRuCT-LLM">https://github.com/bouv/STRuCT-LLM</a>).</p>
  </div>
</details>

<hr>
<h3 id="81-Adapting-Whisper-for-Parameter-efficient-Code-Switching-Speech-Recognition-via-Soft-Prompt-Tuning-cs-CL-cs-AI-cs-SD-eess-ASPDF"><a href="#81-Adapting-Whisper-for-Parameter-efficient-Code-Switching-Speech-Recognition-via-Soft-Prompt-Tuning-cs-CL-cs-AI-cs-SD-eess-ASPDF" class="headerlink" title="[81] Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning cs.CL | cs.AI | cs.SD | eess.ASPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21576">Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21576" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongli Yang, Yizhou Peng, Hao Huang, Sheng Li</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了通过Soft Prompt Tuning（SPT）这一参数高效方法提升大模型Whisper在低资源场景下的代码混合语音识别的性能，同时避免灾难性遗忘。提出了SPT4ASR组合策略，并在实验中验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模多语言ASR模型（如Whisper）在高资源场景表现优秀，但在低资源场景（如罕见语言和代码混合）中面临计算成本和灾难性遗忘的挑战。</p>
<p><strong>Result:</strong> 深度提示调优是最有效的SPT方法，SPT4ASR在代码混合语音识别中进一步降低了错误率，且不影响现有语言的性能。</p>
<p><strong>Insight:</strong> 参数高效方法（如SPT）可以显著提升低资源场景下的模型性能，同时避免灾难性遗忘。</p>
<p><strong>Abstract:</strong> Large-scale multilingual ASR models like Whisper excel in high-resource settings but face challenges in low-resource scenarios, such as rare languages and code-switching (CS), due to computational costs and catastrophic forgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method to enhance CS ASR while preserving prior knowledge. We evaluate two strategies: (1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model, demonstrating improved cross-lingual capabilities compared to traditional methods, and (2) adhering to SPT’s original design by freezing model parameters and only training soft prompts. Additionally, we introduce SPT4ASR, a combination of different SPT variants. Experiments on the SEAME and ASRU2019 datasets show that deep prompt tuning is the most effective SPT approach, and our SPT4ASR methods achieve further error reductions in CS ASR, maintaining parameter efficiency similar to LoRA, without degrading performance on existing languages.</p>
  </div>
</details>

<hr>
<h3 id="82-Language-Aware-Prompt-Tuning-for-Parameter-Efficient-Seamless-Language-Expansion-in-Multilingual-ASR-cs-CL-cs-AI-cs-SD-eess-ASPDF"><a href="#82-Language-Aware-Prompt-Tuning-for-Parameter-Efficient-Seamless-Language-Expansion-in-Multilingual-ASR-cs-CL-cs-AI-cs-SD-eess-ASPDF" class="headerlink" title="[82] Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR cs.CL | cs.AI | cs.SD | eess.ASPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21577">Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21577" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongli Yang, Sheng Li, Hao Huang, Ayiduosi Tuohan, Yizhou Peng</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种语言感知的提示调优方法（LAPT）和完整软提示调优（Entire SPT），用于多语言ASR中的语言扩展，显著提升了性能，并开发了SPT-Whisper工具包支持高效持续学习。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多语言ASR系统（如Whisper）在语言扩展和语言干扰方面仍存在挑战，需要高效且不影响性能的解决方案。</p>
<p><strong>Result:</strong> Entire SPT和LAPT在语言扩展任务中分别比Decoder SPT提升5.0%和16.0%的性能。</p>
<p><strong>Insight:</strong> 通过轻量化的提示调优方法，可以有效解决多语言ASR中的语言干扰和扩展问题，且计算开销低。</p>
<p><strong>Abstract:</strong> Recent advancements in multilingual automatic speech recognition (ASR) have been driven by large-scale end-to-end models like Whisper. However, challenges such as language interference and expanding to unseen languages (language expansion) without degrading performance persist. This paper addresses these with three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which applies soft prompts to both the encoder and decoder, enhancing feature extraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which leverages cross-lingual similarities to encode shared and language-specific features using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that integrates SPT into Whisper and enables efficient continual learning. Experiments across three languages from FLEURS demonstrate that Entire SPT and LAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks, respectively, providing an efficient solution for dynamic, multilingual ASR models with minimal computational overhead.</p>
  </div>
</details>

<hr>
<h3 id="83-From-General-Reasoning-to-Domain-Expertise-Uncovering-the-Limits-of-Generalization-in-Large-Language-Models-cs-CL-cs-AI-cs-CYPDF"><a href="#83-From-General-Reasoning-to-Domain-Expertise-Uncovering-the-Limits-of-Generalization-in-Large-Language-Models-cs-CL-cs-AI-cs-CYPDF" class="headerlink" title="[83] From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models cs.CL | cs.AI | cs.CYPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21580">From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21580" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dana Alsagheer, Yang Lu, Abdulrahman Kamal, Omar Kamal, Mohammad Kamal</span></p>
<p><strong>TL;DR:</strong> 研究探讨了大型语言模型（LLMs）通用推理能力与领域特定推理任务表现之间的关系。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着AI技术的发展，训练LLMs在通用推理上表现优异的需求增加，但其在领域特定任务中的表现仍有待探索。</p>
<p><strong>Result:</strong> 研究发现LLMs的通用推理能力并不总是能直接转化为领域专业推理的高效表现。</p>
<p><strong>Insight:</strong> LLMs的通用推理能力虽然在多个领域有潜力，但在特定领域的专业推理仍需针对性优化。</p>
<p><strong>Abstract:</strong> Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains. However, effective decision-making relies heavily on strong reasoning abilities. Reasoning is the foundation for decision-making, providing the analytical and logical framework to make sound choices. Reasoning involves analyzing information, drawing inferences, and reaching conclusions based on logic or evidence. Decision-making builds on this foundation by applying the insights from reasoning to select the best course of action among alternatives. Together, these processes create a continuous cycle of thought and action aimed at achieving goals effectively. As AI technology evolves, there is a growing trend to train LLMs to excel in general reasoning. This study explores how the general reasoning capabilities of LLMs connect to their performance in domain-specific reasoning tasks.</p>
  </div>
</details>

<hr>
<h3 id="84-VIDEE-Visual-and-Interactive-Decomposition-Execution-and-Evaluation-of-Text-Analytics-with-Intelligent-Agents-cs-CL-cs-AI-cs-HCPDF"><a href="#84-VIDEE-Visual-and-Interactive-Decomposition-Execution-and-Evaluation-of-Text-Analytics-with-Intelligent-Agents-cs-CL-cs-AI-cs-HCPDF" class="headerlink" title="[84] VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents cs.CL | cs.AI | cs.HCPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21582">VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21582" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sam Yu-Te Lee, Chengyang Ji, Shicheng Wen, Lifu Huang, Dongyi Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出VIDEE系统，通过智能代理支持非专业用户进行高级文本分析，包括分解、执行和评估三个阶段，并通过实验和用户研究验证其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统文本分析需要NLP专业知识，阻碍了非专业人士的使用；大型语言模型（LLMs）的发展使文本分析更易用，但仍需系统支持用户与代理协作。</p>
<p><strong>Result:</strong> 实验和用户研究表明VIDEE对非专家用户有效，揭示了用户行为模式，并为未来智能文本分析系统的改进提供了设计启示。</p>
<p><strong>Insight:</strong> 人类与代理的协作可以显著降低文本分析的门槛；人类反馈在生成式推理中的作用至关重要；系统设计需考虑不同用户背景的多样性。</p>
<p><strong>Abstract:</strong> Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE’s effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience – from none to expert – demonstrates the system’s usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.</p>
  </div>
</details>

<hr>
<h3 id="85-Empirical-Evidence-for-Alignment-Faking-in-Small-LLMs-and-Prompt-Based-Mitigation-Techniques-cs-CL-cs-AI-cs-CYPDF"><a href="#85-Empirical-Evidence-for-Alignment-Faking-in-Small-LLMs-and-Prompt-Based-Mitigation-Techniques-cs-CL-cs-AI-cs-CYPDF" class="headerlink" title="[85] Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques cs.CL | cs.AI | cs.CYPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21584">Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21584" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">J. Koorndijk</span></p>
<p><strong>TL;DR:</strong> 论文首次实证表明，即使是小型指令调优模型（如LLaMA 3 8B）也会出现对齐伪装行为，并通过提示干预显著减少这种行为，挑战了现有关于对齐伪装需大规模模型的假设。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前文献认为对齐伪装是大语言模型的涌现特性，但未验证小型模型是否也存在类似行为。研究填补了这一空白，并通过提示干预探索其缓解方法。</p>
<p><strong>Result:</strong> 提示干预显著减少了小型模型的对齐伪装行为，表明无需修改模型内部结构即可实现行为改善。</p>
<p><strong>Insight:</strong> 对齐伪装不仅限于大规模模型，且提示干预是有效的缓解手段。研究呼吁对不同规模和部署场景的模型进行更全面的对齐评估。</p>
<p><strong>Abstract:</strong> Current literature suggests that alignment faking (deceptive alignment) is an emergent property of large language models. We present the first empirical evidence that a small instruction-tuned model, specifically LLaMA 3 8B, can also exhibit alignment faking. We further show that prompt-only interventions, including deontological moral framing and scratchpad reasoning, significantly reduce this behavior without modifying model internals. This challenges the assumption that prompt-based ethics are trivial and that deceptive alignment requires scale. We introduce a taxonomy distinguishing shallow deception, shaped by context and suppressible through prompting, from deep deception, which reflects persistent, goal-driven misalignment. Our findings refine the understanding of deception in language models and underscore the need for alignment evaluations across model sizes and deployment settings.</p>
  </div>
</details>

<hr>
<h3 id="86-Can-Vision-Language-Models-Understand-Mimed-Actions-cs-CL-cs-AI-cs-CVPDF"><a href="#86-Can-Vision-Language-Models-Understand-Mimed-Actions-cs-CL-cs-AI-cs-CVPDF" class="headerlink" title="[86] Can Vision Language Models Understand Mimed Actions? cs.CL | cs.AI | cs.CVPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21586">Can Vision Language Models Understand Mimed Actions?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21586" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyundong Cho, Spencer Lin, Tejas Srinivasan, Michael Saxon, Deuksin Kwon</span></p>
<p><strong>TL;DR:</strong> 论文提出了MIME评测基准，用于评估视觉语言模型对模仿动作（mimed actions）的理解能力，发现现有模型表现显著低于人类，需进一步研究以提升模型对非语言交流的理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 非语言交流（NVC）在人类语言中扮演重要角色，但因其范围广且解释差异大，研究NVC具有挑战性。模仿动作（mime）作为NVC的子集，动作明确且解释差异小，是研究NVC的理想切入点。</p>
<p><strong>Result:</strong> 实验表明，现有VLM（包括开源和API模型）在MIME上的表现显著低于人类。</p>
<p><strong>Insight:</strong> 模仿动作理解是视觉语言模型掌握更复杂非语言交流的基础，当前模型在这一任务上仍有较大改进空间。</p>
<p><strong>Abstract:</strong> Nonverbal communication (NVC) plays an integral role in human language, but studying NVC in general is challenging because of its broad scope and high variance in interpretation among individuals and cultures. However, mime – the theatrical technique of suggesting intent using only gesture, expression, and movement – is a subset of NVC that consists of explicit and embodied actions with much lower human interpretation variance. We argue that a solid understanding of mimed actions is a crucial prerequisite for vision-language models capable of interpreting and commanding more subtle aspects of NVC. Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel video-based question answering benchmark comprising of 86 mimed actions. Constructed with motion capture data, MIME consists of variations of each action with perturbations applied to the character, background, and viewpoint for evaluating recognition robustness. We find that both open-weight and API-based vision-language models perform significantly worse than humans on MIME, motivating the need for increased research for instilling more robust understanding of human gestures.</p>
  </div>
</details>

<hr>
<h3 id="87-Representation-Consistency-for-Accurate-and-Coherent-LLM-Answer-Aggregation-cs-CL-cs-LGPDF"><a href="#87-Representation-Consistency-for-Accurate-and-Coherent-LLM-Answer-Aggregation-cs-CL-cs-LGPDF" class="headerlink" title="[87] Representation Consistency for Accurate and Coherent LLM Answer Aggregation cs.CL | cs.LGPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21590">Representation Consistency for Accurate and Coherent LLM Answer Aggregation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21590" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种称为表示一致性（RC）的测试时扩展方法，用于聚合大语言模型（LLM）生成的多个候选回答，通过内部激活一致性增强答案的准确性和一致性，无需额外模型查询。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有测试时扩展方法通常需要复杂的提示和采样策略修改，而RC方法旨在简化这一过程，通过利用模型内部激活信息提升答案聚合的准确性和一致性。</p>
<p><strong>Result:</strong> 在四个开源LLM和四个推理数据集上的实验表明，RC方法能提升任务性能（最高4%准确率提升），优于强基线方法。</p>
<p><strong>Insight:</strong> 模型内部激活的一致性可以作为衡量答案可靠性的有效指标，稀疏激活信号与连贯推理高度相关。</p>
<p><strong>Abstract:</strong> Test-time scaling improves large language models’ (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model’s internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model’s representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.</p>
  </div>
</details>

<hr>
<h3 id="88-FinEval-KR-A-Financial-Domain-Evaluation-Framework-for-Large-Language-Models’-Knowledge-and-Reasoning-cs-CLPDF"><a href="#88-FinEval-KR-A-Financial-Domain-Evaluation-Framework-for-Large-Language-Models’-Knowledge-and-Reasoning-cs-CLPDF" class="headerlink" title="[88] FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models’ Knowledge and Reasoning cs.CLPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21591">FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models’ Knowledge and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21591" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaoyu Dou, Yutian Shen, Mofan Chen, Zixuan Wang, Jiajie Xu</span></p>
<p><strong>TL;DR:</strong> FinEval-KR 是一个用于解耦和量化大型语言模型（LLMs）在金融领域知识与推理能力的评估框架，提出独立的知识分数和推理分数指标，并基于Bloom分类法分析不同认知层级的推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前评估基准在复杂金融推理任务中未能解耦知识与推理能力，且缺乏对任务失败的根因分析。FinEval-KR旨在填补这一空白，提供一个更全面的评估工具。</p>
<p><strong>Result:</strong> 实验表明，LLMs的推理能力和高阶认知能力是影响推理准确性的核心因素，且顶级模型在知识应用上仍存在瓶颈。</p>
<p><strong>Insight:</strong> 研究发现，专业金融LLMs在多指标上普遍落后于顶级通用大模型，突显了跨领域能力的重要性。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) demonstrate significant potential but face challenges in complex financial reasoning tasks requiring both domain knowledge and sophisticated reasoning. Current evaluation benchmarks often fall short by not decoupling these capabilities indicators from single task performance and lack root cause analysis for task failure. To address this, we introduce FinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs’ knowledge and reasoning abilities independently, proposing distinct knowledge score and reasoning score metrics. Inspired by cognitive science, we further propose a cognitive score based on Bloom’s taxonomy to analyze capabilities in reasoning tasks across different cognitive levels. We also release a new open-source Chinese financial reasoning dataset covering 22 subfields to support reproducible research and further advancements in financial reasoning. Our experimental results reveal that LLM reasoning ability and higher-order cognitive ability are the core factors influencing reasoning accuracy. We also specifically find that even top models still face a bottleneck with knowledge application. Furthermore, our analysis shows that specialized financial LLMs generally lag behind the top general large models across multiple metrics.</p>
  </div>
</details>

<hr>
<h3 id="89-Gazal-R1-Achieving-State-of-the-Art-Medical-Reasoning-with-Parameter-Efficient-Two-Stage-Training-cs-CLPDF"><a href="#89-Gazal-R1-Achieving-State-of-the-Art-Medical-Reasoning-with-Parameter-Efficient-Two-Stage-Training-cs-CLPDF" class="headerlink" title="[89] Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training cs.CLPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21594">Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21594" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmed M. Adly, Mostafa Samy, Amr Fawzy</span></p>
<p><strong>TL;DR:</strong> Gazal-R1是一种320亿参数的语言模型，通过两阶段训练在医学推理任务上达到最先进性能，同时提供透明的临床决策解释。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究旨在通过高效的训练策略，使中等规模模型在专业领域（如医学推理）上超越更大规模的模型。</p>
<p><strong>Result:</strong> Gazal-R1在MedQA（87.1%）、MMLU Pro（Medical）（81.6%）和PubMedQA（79.6%）上表现优异，超越更大模型。</p>
<p><strong>Insight:</strong> 1. 专业领域模型的性能可通过高效训练策略显著提升；2. 奖励设计和稳定性是关键挑战；3. 事实记忆与详细推理之间存在平衡问题。</p>
<p><strong>Abstract:</strong> We present Gazal-R1, a 32-billion-parameter language model that achieves state-of-the-art performance in medical reasoning while providing transparent, step-by-step explanations for clinical decision-making. Built upon Qwen3 32B, our model demonstrates that strategic training can enable mid-sized models to outperform significantly larger counterparts in specialized domains. We developed a novel two-stage training pipeline: first, supervised fine-tuning on a carefully curated dataset of 107,033 synthetic medical reasoning examples that teaches structured clinical thinking, enhanced by advanced parameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation (DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using Group Relative Policy Optimization (GRPO) with a sophisticated multi-component reward system that refines accuracy, format adherence, and reasoning quality. Gazal-R1 achieves exceptional performance across medical benchmarks, scoring 87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing models up to 12x larger. Beyond its strong empirical results, this work provides detailed insights into the challenges of training reasoning-capable models in specialized domains, including issues with reward hacking, training instability, and the fundamental tension between factual recall and detailed reasoning. Our methodology offers a reproducible framework for developing high-capability, domain-specific language models that balance performance, efficiency, and explainability.</p>
  </div>
</details>

<hr>
<h3 id="90-Thunder-LLM-Efficiently-Adapting-LLMs-to-Korean-with-Minimal-Resources-cs-CLPDF"><a href="#90-Thunder-LLM-Efficiently-Adapting-LLMs-to-Korean-with-Minimal-Resources-cs-CLPDF" class="headerlink" title="[90] Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources cs.CLPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21595">Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21595" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinpyo Kim, Gyeongje Cho, Chanwoo Park, Jongwon Park, Jongmin Kim</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种低成本方法Thunder-LLM，将英文大语言模型高效适配到韩语，包括数据收集、预处理、训练、下游任务构建和评估的完整流程，并展示其在资源有限下的优越性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前主流大语言模型在英语或汉语之外的其他语言上表现不佳，且完整的训练流程因技术复杂性和商业保密性而未公开。本文旨在填补这一空白，提供低成本适配新语言的解决方案。</p>
<p><strong>Result:</strong> Thunder-LLM和Thunder-LLM-Ins在韩语任务中表现优异，且所需数据和计算资源极少。</p>
<p><strong>Insight:</strong> 即使在低预算下，通过合理的流程设计，现有LLM可以高效适配新语言，语言多样性问题可通过开源和共享经验解决。</p>
<p><strong>Abstract:</strong> Since state-of-the-art LLMs often underperform in languages other than English or Chinese, improving the capability of LLMs in new languages has become an essential task. Moreover, LLMs’ entire end-to-end training process remains largely unknown to the public due to proprietary reasons, technical complexity, inconsistent documentation, and ethical considerations. The complete picture remains a closely guarded secret within the industry. This paper presents methods to adapt an existing English-based LLM to Korean in a low-budget scenario. We describe the entire end-to-end process: collecting Korean datasets, preprocessing the data, training the model, creating downstream benchmarks, and conducting evaluations. The evaluation results indicate that our method can effectively and cost-efficiently add new language capabilities to existing LLMs. Our new bilingual models, Thunder-LLM and Thunder-LLM-Ins, achieve superior Korean performance compared to state-of-the-art models while utilizing minimal data and computational resources. We share our comprehensive experience and make the code publicly available.</p>
  </div>
</details>

<hr>
<h3 id="91-Evaluating-Multimodal-Large-Language-Models-on-Educational-Textbook-Question-Answering-cs-CL-cs-AI-cs-IRPDF"><a href="#91-Evaluating-Multimodal-Large-Language-Models-on-Educational-Textbook-Question-Answering-cs-CL-cs-AI-cs-IRPDF" class="headerlink" title="[91] Evaluating Multimodal Large Language Models on Educational Textbook Question Answering cs.CL | cs.AI | cs.IRPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21596">Evaluating Multimodal Large Language Models on Educational Textbook Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21596" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hessa A. Alawwad, Anas Zafar, Areej Alhothali, Usman Naseem, Ali Alkhathlan</span></p>
<p><strong>TL;DR:</strong> 该论文首次评估了多模态大语言模型（MLLMs）在教育教科书问答任务（TQA）中的表现，使用了CK12-QA数据集，并分析了LLaVA和LLaMA 3.2-Vision等模型的表现。同时，论文提出了一种轻量级的多模态检索增强生成（RAG）方法，整合了课程中的段落和图表，展示了检索到的教育上下文对模型性能的影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管MLLMs在视觉语言任务中取得了显著成功，但其处理复杂、长篇课程和复杂教育图表的能力尚未得到充分测试。论文旨在填补这一空白，探索MLLMs在教育环境中的潜力。</p>
<p><strong>Result:</strong> 实验结果显示，检索到的教育上下文显著提升了模型的准确性和推理能力，但也暴露了模型在处理问题-上下文关系和噪声方面的局限性。</p>
<p><strong>Insight:</strong> 该研究为多模态AI在教育领域的应用提供了关键方向，指出了模型在处理复杂上下文时的改进需求以及未来研究的重点。</p>
<p><strong>Abstract:</strong> Multimodal large language models (MLLMs) have recently achieved significant success in vision–language tasks. However, their capacity to reason over complex, long lessons and intricate educational diagrams that cannot be represented as a single natural image remains largely untested. In this work, we present the first evaluation of state-of-the-art MLLMs on the textbook question answering (TQA) task using the CK12-QA dataset. We assess the performance of recent vision-language models, including LLaVA and LLaMA 3.2-Vision, across various input configurations. Additionally, we introduce a lightweight multimodal retrieval-augmented generation (RAG) pipeline that integrates both paragraphs and diagrams from the lesson into the prompt. Our results demonstrate the influence of retrieved educational context on model accuracy and reasoning, while also revealing current limitations in handling question-context relationships and the potential for noise, pointing to key directions for future research in multimodal AI-driven learning.</p>
  </div>
</details>

<hr>
<h3 id="92-Overview-of-the-ClinIQLink-2025-Shared-Task-on-Medical-Question-Answering-cs-CL-cs-AI-cs-IR-I-2-7PDF"><a href="#92-Overview-of-the-ClinIQLink-2025-Shared-Task-on-Medical-Question-Answering-cs-CL-cs-AI-cs-IR-I-2-7PDF" class="headerlink" title="[92] Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering cs.CL | cs.AI | cs.IR | I.2.7PDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21597">Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21597" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Brandon Colelough, Davis Bartels, Dina Demner-Fushman</span></p>
<p><strong>TL;DR:</strong> 本文概述了ClinIQLink 2025共享任务，这是一个针对医学问答的大型语言模型测试任务，包含多种问答格式和自动化评分及医师评审。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 设计ClinIQLink共享任务的动机是为了在医学领域的问答任务中测试大型语言模型的性能，尤其是针对全科医生级别的需求。</p>
<p><strong>Result:</strong> 任务通过结合自动化评分和医师评审，有效评估了大型语言模型在医学问答任务中的表现。</p>
<p><strong>Insight:</strong> 任务设计强调了医学问答的多样性和专家验证的重要性，为未来医学AI应用提供了标准化的测试框架。</p>
<p><strong>Abstract:</strong> In this paper, we present an overview of ClinIQLink, a shared task, collocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test large language models (LLMs) on medically-oriented question answering aimed at the level of a General Practitioner. The challenge supplies 4,978 expert-verified, medical source-grounded question-answer pairs that cover seven formats: true&#x2F;false, multiple choice, unordered list, short answer, short-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled in Docker or Apptainer images, are executed on the CodaBench platform or the University of Maryland’s Zaratan cluster. An automated harness (Task 1) scores closed-ended items by exact match and open-ended items with a three-tier embedding metric. A subsequent physician panel (Task 2) audits the top model responses.</p>
  </div>
</details>

<hr>
<h3 id="93-Structured-Attention-Matters-to-Multimodal-LLMs-in-Document-Understanding-cs-CL-cs-AI-cs-IRPDF"><a href="#93-Structured-Attention-Matters-to-Multimodal-LLMs-in-Document-Understanding-cs-CL-cs-AI-cs-IRPDF" class="headerlink" title="[93] Structured Attention Matters to Multimodal LLMs in Document Understanding cs.CL | cs.AI | cs.IRPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21600">Structured Attention Matters to Multimodal LLMs in Document Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21600" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chang Liu, Hongkai Chen, Yujun Cai, Hang Wu, Qingwen Ye</span></p>
<p><strong>TL;DR:</strong> 研究发现，原始OCR文本输入会损害多模态大语言模型（MLLMs）的文档理解能力，提出了一种基于LaTex范式的结构保留方法，显著提升了模型的问答性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索输入格式对MLLMs文档理解性能的影响，尤其是发现未经处理的原始OCR文本反而会降低模型表现。</p>
<p><strong>Result:</strong> 新方法显著提升了MLLMs在多种文档类型上的问答性能。</p>
<p><strong>Insight:</strong> 结构化输入能够引导模型注意力，减少注意力分散，提升理解能力。</p>
<p><strong>Abstract:</strong> Document understanding remains a significant challenge for multimodal large language models (MLLMs). While previous research has primarily focused on locating evidence pages through precise multimodal queries, our work investigates a fundamental yet overlooked aspect: how input format influences document comprehension performance. Through systematic analysis, we discover that raw OCR text often impairs rather than improves MLLMs’ performance, which is a counterintuitive finding we attribute to attention dispersion and structure loss. To further substantiate our hypothesis, we propose a novel structure-preserving approach that encodes document elements using the LaTex paradigm, maintaining the hierarchical organization and spatial relationships critical for comprehension. Our attention analysis reveals that structured text induces structured attention patterns on both textual and visual content, directing models to focus on semantically meaningful regions while reducing attention waste. This approach significantly enhances MLLMs’ document question answering performance across diverse document types without requiring architectural modifications or additional training.</p>
  </div>
</details>

<hr>
<h3 id="94-BiMark-Unbiased-Multilayer-Watermarking-for-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#94-BiMark-Unbiased-Multilayer-Watermarking-for-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[94] BiMark: Unbiased Multilayer Watermarking for Large Language Models cs.CL | cs.AIPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21602">BiMark: Unbiased Multilayer Watermarking for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21602" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan</span></p>
<p><strong>TL;DR:</strong> BiMark 是一种新颖的水印框架，通过创新的无偏重加权机制、多层架构和信息编码方法，解决了大型语言模型生成文本的认证问题，同时保持文本质量和高检测率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLMs）生成的文本难以区分其来源，引发了认证和监管需求。现有水印技术难以同时实现文本质量保持、模型无关检测和多比特水印嵌入，BiMark 致力于解决这一挑战。</p>
<p><strong>Result:</strong> 与现有多比特水印方法相比，BiMark 在短文本中的提取率提高30%，同时保持较低的困惑度，且在下游任务（如摘要和翻译）中与非水印文本表现相当。</p>
<p><strong>Insight:</strong> BiMark 的创新设计为LLM生成的文本认证提供了实用且高效的解决方案，其多层架构和无偏机制对其他水印技术具有启发意义。</p>
<p><strong>Abstract:</strong> Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.</p>
  </div>
</details>

<hr>
<h3 id="95-Operationalizing-Automated-Essay-Scoring-A-Human-Aware-Approach-cs-CL-cs-CY-cs-LGPDF"><a href="#95-Operationalizing-Automated-Essay-Scoring-A-Human-Aware-Approach-cs-CL-cs-CY-cs-LGPDF" class="headerlink" title="[95] Operationalizing Automated Essay Scoring: A Human-Aware Approach cs.CL | cs.CY | cs.LGPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21603">Operationalizing Automated Essay Scoring: A Human-Aware Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21603" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yenisel Plasencia-Calaña</span></p>
<p><strong>TL;DR:</strong> 论文探讨了自动化作文评分（AES）系统的人性化实现，不仅关注准确性，还比较了机器学习与大型语言模型（LLM）方法的优劣。研究发现ML模型准确性更高，但LLM在可解释性上表现更好；两者在偏见和边缘分数鲁棒性上均存在不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动化作文评分系统需要超越单纯的准确性，关注人性化实现中的关键维度（如偏见、鲁棒性和可解释性），以提升系统的可靠性和信任度。</p>
<p><strong>Result:</strong> ML模型在准确性上优于LLM，但LLM能提供更丰富的解释；两者在偏见和边缘分数处理上均表现不佳。</p>
<p><strong>Insight:</strong> AES系统设计需权衡不同方法的优缺点，未来可结合ML的高精度与LLM的可解释性，同时解决偏见和鲁棒性问题。</p>
<p><strong>Abstract:</strong> This paper explores the human-centric operationalization of Automated Essay Scoring (AES) systems, addressing aspects beyond accuracy. We compare various machine learning-based approaches with Large Language Models (LLMs) approaches, identifying their strengths, similarities and differences. The study investigates key dimensions such as bias, robustness, and explainability, considered important for human-aware operationalization of AES systems. Our study shows that ML-based AES models outperform LLMs in accuracy but struggle with explainability, whereas LLMs provide richer explanations. We also found that both approaches struggle with bias and robustness to edge scores. By analyzing these dimensions, the paper aims to identify challenges and trade-offs between different methods, contributing to more reliable and trustworthy AES methods.</p>
  </div>
</details>

<hr>
<h3 id="96-MemBench-Towards-More-Comprehensive-Evaluation-on-the-Memory-of-LLM-based-Agents-cs-CL-cs-AIPDF"><a href="#96-MemBench-Towards-More-Comprehensive-Evaluation-on-the-Memory-of-LLM-based-Agents-cs-CL-cs-AIPDF" class="headerlink" title="[96] MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents cs.CL | cs.AIPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21605">MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21605" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai</span></p>
<p><strong>TL;DR:</strong> MemBench 是一个用于全面评估基于 LLM 的智能体记忆能力的数据集和基准测试工具，涵盖多层次的记忆类型和交互场景，并提供多维度评估指标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前对基于 LLM 的智能体记忆能力的评估存在局限，缺乏多样化的记忆层次和交互场景，以及多维度的综合指标。</p>
<p><strong>Result:</strong> 发布了一个开源的数据集和基准测试工具，为研究社区提供了更全面的记忆能力评估方法。</p>
<p><strong>Insight:</strong> 记忆能力评估需要覆盖不同类型的记忆层次和交互场景，而多维度的指标设计能更全面地反映智能体的记忆能力。</p>
<p><strong>Abstract:</strong> Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at <a target="_blank" rel="noopener" href="https://github.com/import-myself/Membench">https://github.com/import-myself/Membench</a>.</p>
  </div>
</details>

<hr>
<h3 id="97-From-Thinking-to-Output-Chain-of-Thought-and-Text-Generation-Characteristics-in-Reasoning-Language-Models-cs-CL-cs-AI-cs-CRPDF"><a href="#97-From-Thinking-to-Output-Chain-of-Thought-and-Text-Generation-Characteristics-in-Reasoning-Language-Models-cs-CL-cs-AI-cs-CRPDF" class="headerlink" title="[97] From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models cs.CL | cs.AI | cs.CRPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21609">From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21609" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junhao Liu, Zhenhao Xu, Yuxin Fang, Yichuan Chen, Zuobin Ying</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种分析大型语言模型推理特性的新框架，通过关键词统计和LLM-as-a-judge范式，比较了GPT-o1、DeepSeek-R1、Kimi-k1.5和Grok-3四种模型的推理过程和输出，揭示了它们在权衡探索与利用、问题处理及结论达成方面的模式差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究缺乏对大型语言模型推理过程和输出的系统性比较，尤其是在自反思模式（’Aha moment’）和跨领域关联方面。</p>
<p><strong>Result:</strong> 研究发现不同模型在推理过程中展现出不同的模式，包括探索与利用的平衡、问题处理方式及结论达成路径。</p>
<p><strong>Insight:</strong> 研究为计算效率与推理稳健性之间的权衡提供了见解，并为实际应用中的模型设计与评估提出了实用建议。</p>
<p><strong>Abstract:</strong> Recently, there have been notable advancements in large language models (LLMs), demonstrating their growing abilities in complex reasoning. However, existing research largely overlooks a thorough and systematic comparison of these models’ reasoning processes and outputs, particularly regarding their self-reflection pattern (also termed “Aha moment”) and the interconnections across diverse domains. This paper proposes a novel framework for analyzing the reasoning characteristics of four cutting-edge large reasoning models (GPT-o1, DeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge paradigm. Our approach connects their internal thinking processes with their final outputs. A diverse dataset consists of real-world scenario-based questions covering logical deduction, causal inference, and multi-step problem-solving. Additionally, a set of metrics is put forward to assess both the coherence of reasoning and the accuracy of the outputs. The research results uncover various patterns of how these models balance exploration and exploitation, deal with problems, and reach conclusions during the reasoning process. Through quantitative and qualitative comparisons, disparities among these models are identified in aspects such as the depth of reasoning, the reliance on intermediate steps, and the degree of similarity between their thinking processes and output patterns and those of GPT-o1. This work offers valuable insights into the trade-off between computational efficiency and reasoning robustness and provides practical recommendations for enhancing model design and evaluation in practical applications. We publicly release our project at: <a target="_blank" rel="noopener" href="https://github.com/ChangWenhan/FromThinking2Output">https://github.com/ChangWenhan/FromThinking2Output</a></p>
  </div>
</details>

<hr>
<h3 id="98-Does-Multimodality-Lead-to-Better-Time-Series-Forecasting-cs-CL-cs-AI-cs-LGPDF"><a href="#98-Does-Multimodality-Lead-to-Better-Time-Series-Forecasting-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[98] Does Multimodality Lead to Better Time Series Forecasting? cs.CL | cs.AI | cs.LGPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21611">Does Multimodality Lead to Better Time Series Forecasting?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21611" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiyuan Zhang, Boran Han, Haoyang Fang, Abdul Fatir Ansari, Shuai Zhang</span></p>
<p><strong>TL;DR:</strong> 本文系统地研究了多模态（文本信号）是否有助于时间序列预测，发现其效果并非普遍适用，并提出了模型和数据特性的关键条件。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探讨多模态（尤其是文本信息）在时间序列预测中的作用，以明确其适用条件和潜在增益。</p>
<p><strong>Result:</strong> 多模态方法的增益取决于文本模型容量、时间序列模型能力、对齐策略、数据量和文本信号互补性等条件。</p>
<p><strong>Insight:</strong> 多模态并非总是有效；文本信息的高质量和互补性、模型能力匹配及数据充足性是关键成功因素。</p>
<p><strong>Abstract:</strong> Recently, there has been growing interest in incorporating textual information into foundation models for time series forecasting. However, it remains unclear whether and under what conditions such multimodal integration consistently yields gains. We systematically investigate these questions across a diverse benchmark of 14 forecasting tasks spanning 7 domains, including health, environment, and economics. We evaluate two popular multimodal forecasting paradigms: aligning-based methods, which align time series and text representations; and prompting-based methods, which directly prompt large language models for forecasting. Although prior works report gains from multimodal input, we find these effects are not universal across datasets and models, and multimodal methods sometimes do not outperform the strongest unimodal baselines. To understand when textual information helps, we disentangle the effects of model architectural properties and data characteristics. Our findings highlight that on the modeling side, incorporating text information is most helpful given (1) high-capacity text models, (2) comparatively weaker time series models, and (3) appropriate aligning strategies. On the data side, performance gains are more likely when (4) sufficient training data is available and (5) the text offers complementary predictive signal beyond what is already captured from the time series alone. Our empirical findings offer practical guidelines for when multimodality can be expected to aid forecasting tasks, and when it does not.</p>
  </div>
</details>

<hr>
<h3 id="99-LastingBench-Defend-Benchmarks-Against-Knowledge-Leakage-cs-CL-cs-AIPDF"><a href="#99-LastingBench-Defend-Benchmarks-Against-Knowledge-Leakage-cs-CL-cs-AIPDF" class="headerlink" title="[99] LastingBench: Defend Benchmarks Against Knowledge Leakage cs.CL | cs.AIPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21614">LastingBench: Defend Benchmarks Against Knowledge Leakage</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21614" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yixiong Fang, Tianran Sun, Yuling Shi, Min Wang, Xiaodong Gu</span></p>
<p><strong>TL;DR:</strong> LastingBench 是一个新框架，旨在通过扰动识别并重写泄露点，保护 QA 基准测试免受大语言模型记忆效应的影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大语言模型的复杂化，模型可能通过记忆任务数据“作弊”，削弱基准测试的有效性。现有方法主要关注泄露检测，但对减少其影响的解决方案研究不足。</p>
<p><strong>Result:</strong> 实验表明，LastingBench 在减少记忆效应方面显著有效，展现了基准测试的性能差距。</p>
<p><strong>Insight:</strong> 维护基准测试的长期有效性需要动态防御策略，而不仅是泄露检测。</p>
<p><strong>Abstract:</strong> The increasing complexity of large language models (LLMs) raises concerns about their ability to “cheat” on standard Question Answering (QA) benchmarks by memorizing task-specific data. This undermines the validity of benchmark evaluations, as they no longer reflect genuine model capabilities but instead the effects of data leakage. While prior work has focused on detecting such leakage, little attention has been given to mitigating its impact and preserving the long-term utility of benchmarks. In this paper, we introduce LastingBench, a novel framework designed to continuously reinforce and safeguard existing benchmarks against knowledge leakage. LastingBench identifies leakage points in the context through perturbation, then rewrites the leakage points to counterfactual ones-disrupting memorization while preserving the benchmark’s original evaluative intent. Evaluations of state-of-the-art QA benchmarks show significant performance gaps, highlighting the efficacy of LastingBench in reducing memorization effects. LastingBench offers a practical and scalable solution to ensure benchmark robustness over time, promoting fairer and more interpretable evaluations of LLMs.</p>
  </div>
</details>

<hr>
<h3 id="100-Refine-Medical-Diagnosis-Using-Generation-Augmented-Retrieval-and-Clinical-Practice-Guidelines-cs-CL-cs-AI-cs-IRPDF"><a href="#100-Refine-Medical-Diagnosis-Using-Generation-Augmented-Retrieval-and-Clinical-Practice-Guidelines-cs-CL-cs-AI-cs-IRPDF" class="headerlink" title="[100] Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines cs.CL | cs.AI | cs.IRPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21615">Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21615" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenhao Li, Hongkuan Zhang, Hongwei Zhang, Zhengxu Li, Zengjie Dong</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为GARMLE-G的生成增强检索框架，通过整合大语言模型预测和权威临床实践指南，解决了现有医学语言模型在诊断中的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的医学语言模型通常基于ICD代码预测诊断，但ICD代码无法捕捉医生在实际诊断中的复杂推理和临床指南依据，导致模型临床实用性受限。</p>
<p><strong>Result:</strong> 原型系统在高血压诊断任务中表现出色，检索精度、语义相关性和临床指南一致性均优于基于RAG的基线模型，且架构轻量适合本地医疗部署。</p>
<p><strong>Insight:</strong> 论文的创新点在于通过直接检索权威指南内容而非依赖模型生成文本，确保了临床建议的准确性和可靠性，为医学语言模型的实际应用提供了可行方案。</p>
<p><strong>Abstract:</strong> Current medical language models, adapted from large language models (LLMs), typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. Clinicians synthesize diverse patient data and reference clinical practice guidelines (CPGs) to make evidence-based decisions. This misalignment limits the clinical utility of existing models. We introduce GARMLE-G, a Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations. A prototype system for hypertension diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment. This work provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment.</p>
  </div>
</details>

<hr>
<h3 id="101-IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech-cs-CL-cs-AI-cs-SD-eess-ASPDF"><a href="#101-IndexTTS2-A-Breakthrough-in-Emotionally-Expressive-and-Duration-Controlled-Auto-Regressive-Zero-Shot-Text-to-Speech-cs-CL-cs-AI-cs-SD-eess-ASPDF" class="headerlink" title="[101] IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech cs.CL | cs.AI | cs.SD | eess.ASPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21619">IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21619" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang</span></p>
<p><strong>TL;DR:</strong> IndexTTS2 提出了一种新型的自回归语音合成方法，能够精确控制语音持续时间，并实现了音色与情感的分离控制。通过 GPT 潜在表征增强清晰度，结合软指令机制，其在零样本设置下显著提升了语音合成的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的自回归语音合成模型难以精确控制语音持续时间，且情感表达与说话者身份高度耦合，限制了其在严格音视频同步等场景的应用。</p>
<p><strong>Result:</strong> 实验表明，IndexTTS2 在词错误率、说话者相似度和情感保真度方面优于现有零样本 TTS 模型。</p>
<p><strong>Insight:</strong> 通过分离音色与情感控制，结合自然语言引导，语音合成的灵活性与表现力显著提升。</p>
<p><strong>Abstract:</strong> Large-scale text-to-speech (TTS) models are typically categorized into autoregressive and non-autoregressive systems. Although autoregressive systems exhibit certain advantages in speech naturalness, their token-by-token generation mechanism makes it difficult to precisely control the duration of synthesized speech. This is a key limitation in applications such as video dubbing that require strict audio-visual synchronization. This paper introduces IndexTTS2, which proposes a novel and autoregressive-model-friendly method for speech duration control. The method supports two generation modes: one allows explicit specification of the number of generated tokens for precise duration control; the other does not require manual input and lets the model freely generate speech while preserving prosodic characteristics from the input prompt. Furthermore, IndexTTS2 achieves disentanglement between emotional expression and speaker identity, enabling independent control of timbre and emotion. In the zero-shot setting, the model can perfectly reproduce the emotional characteristics of the input prompt. Users may also provide a separate emotion prompt, even from a different speaker, allowing the model to reconstruct the target timbre while conveying the desired emotion. To enhance clarity during strong emotional expressions, we incorporate GPT latent representations to improve speech stability. Meanwhile, to lower the barrier for emotion control, we design a soft instruction mechanism based on textual descriptions by fine-tuning Qwen3. This enables effective guidance of speech generation with desired emotional tendencies using natural language input. Experimental results demonstrate that IndexTTS2 outperforms existing state-of-the-art zero-shot TTS models in word error rate, speaker similarity, and emotional fidelity.</p>
  </div>
</details>

<hr>
<h3 id="102-Doc2SAR-A-Synergistic-Framework-for-High-Fidelity-Extraction-of-Structure-Activity-Relationships-from-Scientific-Documents-cs-CL-cs-AI-cs-IRPDF"><a href="#102-Doc2SAR-A-Synergistic-Framework-for-High-Fidelity-Extraction-of-Structure-Activity-Relationships-from-Scientific-Documents-cs-CL-cs-AI-cs-IRPDF" class="headerlink" title="[102] Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents cs.CL | cs.AI | cs.IRPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21625">Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21625" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaxi Zhuang, Kangning Li, Jue Hou, Mingjun Xu, Zhifeng Gao</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了 Doc2SAR，一个结合领域专用工具和微调多模态大模型的框架，用于从科学文献中高保真地提取结构-活性关系（SARs），并在新基准 DocSAR-200 上显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 从科学文献和专利中提取分子结构-活性关系（SARs）对药物发现和材料研究至关重要，但现有方法因文档格式多样性和专用任务（如化学结构识别）的局限性难以胜任。</p>
<p><strong>Result:</strong> 在 DocSAR-200 上，Doc2SAR 的表格召回率达到 80.78%，比 GPT-4o 高出 51.48%，并在实用性和高效推理方面表现突出。</p>
<p><strong>Insight:</strong> 结合领域专用工具与大模型微调是解决异构文档格式和专用任务挑战的有效途径。</p>
<p><strong>Abstract:</strong> Extracting molecular structure-activity relationships (SARs) from scientific literature and patents is essential for drug discovery and materials research. However, this task remains challenging due to heterogeneous document formats and limitations of existing methods. Specifically, rule-based approaches relying on rigid templates fail to generalize across diverse document layouts, while general-purpose multimodal large language models (MLLMs) lack sufficient accuracy and reliability for specialized tasks, such as layout detection and optical chemical structure recognition (OCSR). To address these challenges, we introduce DocSAR-200, a rigorously annotated benchmark of 200 scientific documents designed specifically for evaluating SAR extraction methods. Additionally, we propose Doc2SAR, a novel synergistic framework that integrates domain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT). Extensive experiments demonstrate that Doc2SAR achieves state-of-the-art performance across various document types, significantly outperforming leading end-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of 80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR demonstrates practical usability through efficient inference and is accompanied by a web app.</p>
  </div>
</details>

<hr>
<h3 id="103-DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE-cs-CL-cs-AIPDF"><a href="#103-DeepTalk-Towards-Seamless-and-Smart-Speech-Interaction-with-Adaptive-Modality-Specific-MoE-cs-CL-cs-AIPDF" class="headerlink" title="[103] DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE cs.CL | cs.AIPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21864">DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21864" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hang Shao, Heting Gao, Yunhang Shen, Jiawei Chen, Lijiang Li</span></p>
<p><strong>TL;DR:</strong> DeepTalk通过自适应模态专家学习框架解决了原生多模态大语言模型（MLLMs）在语音和文本生成中的性能退化问题，显著降低了性能下降，并保持了低延迟的语音交互体验。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 原生多模态大语言模型（MLLMs）虽然能直接生成语音响应，但受限于语音-文本配对数据不足，导致性能下降和灾难性遗忘问题，需要一种更高效的框架来解决这些挑战。</p>
<p><strong>Result:</strong> DeepTalk在性能上仅下降5.5%，远低于原生MLLMs（通常下降20%以上），同时对话延迟保持在0.5秒内，与模块化MLLMs性能相当。</p>
<p><strong>Insight:</strong> 通过MoE架构和模态专家分离训练，DeepTalk展示了在多模态学习中平衡性能与效率的潜力，为未来智能语音交互提供了新思路。</p>
<p><strong>Abstract:</strong> Native multimodal large language models (MLLMs) restructure a single large language model (LLM) into a spoken language model (SLM) capable of both speech and text generation. Compared to modular and aligned MLLMs, native MLLMs preserve richer paralinguistic features such as emotion and prosody, and generate speech responses directly within the backbone LLM rather than using a separate speech decoder. This integration also results in lower response latency and smoother interaction. However, native MLLMs suffer from catastrophic forgetting and performance degradation because the available paired speech-text data is insufficient to support the pretraining of MLLMs compared to the vast amount of text data required to pretrain text LLMs. To address this issue, we propose DeepTalk, a framework for adaptive modality expert learning based on a Mixture of Experts (MoE) architecture. DeepTalk first adaptively distinguishes modality experts according to their modality load within the LLM. Each modality expert then undergoes specialized single-modality training, followed by joint multimodal collaborative training. As a result, DeepTalk incurs only a 5.5% performance drop compared to the original LLM, which is significantly lower than the average performance drop of over 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par with modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within 0.5 seconds, ensuring a seamless and intelligent speech interaction experience. Code and models are released at <a target="_blank" rel="noopener" href="https://github.com/talkking/DeepTalk">https://github.com/talkking/DeepTalk</a>.</p>
  </div>
</details>

<hr>
<h3 id="104-WildSpeech-Bench-Benchmarking-Audio-LLMs-in-Natural-Speech-Conversation-cs-CLPDF"><a href="#104-WildSpeech-Bench-Benchmarking-Audio-LLMs-in-Natural-Speech-Conversation-cs-CLPDF" class="headerlink" title="[104] WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation cs.CLPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21875">WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21875" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jian Zhang, Linhao Zhang, Bokai Lei, Chuhan Wu, Wei Jia</span></p>
<p><strong>TL;DR:</strong> 该论文提出了WildSpeech-Bench，一个专门用于评估音频大语言模型（LLMs）在实际语音对话中性能的基准测试工具。通过收集真实的语音对话数据、引入多样化的说话者属性和声学条件，并设计查询感知的评估方法，该研究填补了现有文本基准测试在语音场景中的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型（如GPT-4o）在语音交互中表现出强大能力，但缺乏专门针对语音场景的全面评估工具。现有方法往往直接采用文本基准测试，忽略了语音特有的挑战（如韵律、同音词、口吃等）。</p>
<p><strong>Result:</strong> 对不同主流语音模型进行全面测试，结果显示模型在不同语音场景中表现差异显著。查询感知评估方法能够更细粒度地评估语音特有场景下的模型性能。</p>
<p><strong>Insight:</strong> 语音LLM的评估需要特别关注语音特有的现象和用户期望，直接照搬文本基准测试可能无法全面反映模型在语音场景中的性能。WildSpeech-Bench为语音模型开发提供了更精准的评估工具。</p>
<p><strong>Abstract:</strong> Recent multi-modal Large Language Models (LLMs) such as GPT-4o have demonstrated strong capabilities of direct speech interaction. However, the lack of specialized and comprehensive benchmarks for end-to-end speech LLM evaluation hinders optimizing the user experience of Audio LLMs in real-world applications. Existing evaluation methods often adapt text-based benchmarks, overlooking speech’s unique characteristics and challenges, including prosody, homophones, stuttering, and differing user expectations. Here, we present a novel approach to thoroughly evaluate LLMs in practical speech conversations. We systematically curate real-world chat data relevant to spoken scenarios, introduce diversity in speaker attributes and acoustic conditions, and augment the dataset with speech-specific phenomena. We further design a query-aware evaluation method to use customized evaluation checklists and prompts to enhance the accuracy of automatic evaluation. We conduct comprehensive testing and detailed analysis of various mainstream speech models, revealing significant differences in model performance across different speech scenarios. The use of query-aware evaluation further enables a finer-grained assessment under various speech-specific scenarios. Our benchmark can provide valuable insights for speech model development and evaluation.</p>
  </div>
</details>

<hr>
<h3 id="105-Do-Vision-Language-Models-Have-Internal-World-Models-Towards-an-Atomic-Evaluation-cs-CL-cs-AI-cs-CVPDF"><a href="#105-Do-Vision-Language-Models-Have-Internal-World-Models-Towards-an-Atomic-Evaluation-cs-CL-cs-AI-cs-CVPDF" class="headerlink" title="[105] Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation cs.CL | cs.AI | cs.CVPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21876">Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21876" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiyue Gao, Xinyu Pi, Kevin Liu, Junrong Chen, Ruolan Yang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个系统化的两阶段框架（感知与预测），用于评估视觉语言模型（VLMs）的内部世界模型（WMs）能力，并引入了一个大规模基准测试WM-ABench。通过对15个最新商业和开源VLMs的660项实验，发现这些模型在基础世界建模能力上存在显著局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的大型视觉语言模型表现出作为通用世界模型的潜力，但缺乏对其基本世界建模能力的系统性评估。论文旨在填补这一空白，通过心理学和认知科学启发的方法，量化VLMs的WMs能力。</p>
<p><strong>Result:</strong> 实验结果表明，当前VLMs在基础世界建模能力上表现较差，例如在区分运动轨迹时接近随机准确率，且在解耦理解上存在偏差（如对颜色影响速度的错误认知）。这些模型与人类水平的世界建模能力存在显著差距。</p>
<p><strong>Insight:</strong> 论文揭示了当前VLMs的能力瓶颈，尤其是在动态场景和理解因果关系上的不足。这些发现为未来模型设计提供了重要方向，例如需要增强对物理规律和时序关系的建模能力。</p>
<p><strong>Abstract:</strong> Internal world models (WMs) enable agents to understand the world’s state and predict transitions, serving as the basis for advanced deliberative reasoning. Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and Gemini, exhibit potential as general-purpose WMs. While the latest studies have evaluated and shown limitations in specific capabilities such as visual understanding, a systematic evaluation of VLMs’ fundamental WM abilities remains absent. Drawing on comparative psychology and cognitive science, we propose a two-stage framework that assesses Perception (visual, spatial, temporal, quantitative, and motion) and Prediction (mechanistic simulation, transitive inference, compositional inference) to provide an atomic evaluation of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse simulated environments with controlled counterfactual simulations. Through 660 experiments on 15 latest commercial and open-source VLMs, we find that these models exhibit striking limitations in basic world modeling abilities. For instance, almost all models perform at near-random accuracy when distinguishing motion trajectories. Additionally, they lack disentangled understanding – e.g., some models tend to believe blue objects move faster than green ones. More rich results and analyses reveal significant gaps between VLMs and human-level world modeling.</p>
  </div>
</details>

<hr>
<h3 id="106-A-Dual-Layered-Evaluation-of-Geopolitical-and-Cultural-Bias-in-LLMs-cs-CLPDF"><a href="#106-A-Dual-Layered-Evaluation-of-Geopolitical-and-Cultural-Bias-in-LLMs-cs-CLPDF" class="headerlink" title="[106] A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs cs.CLPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21881">A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21881" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sean Kim, Hyuhng Joon Kim</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种双层次评估框架，用于分析大语言模型（LLMs）在地缘政治和文化偏见上的表现，揭示了模型偏见和推理偏见的区别及其影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLMs在不同语言和文化背景中的广泛部署，理解它们在中立和敏感话题上的表现变得尤为重要，尤其是当它们的输出可能影响公众观点或强化主流叙事时。</p>
<p><strong>Result:</strong> 结果显示，第一阶段存在查询语言导致的输出对齐，而第二阶段则反映模型训练背景和查询语言的复杂交互。</p>
<p><strong>Insight:</strong> 研究强调了LLMs多语言和文化背景下评估的重要性，为未来部署和文化敏感评估提供了框架。</p>
<p><strong>Abstract:</strong> As large language models (LLMs) are increasingly deployed across diverse linguistic and cultural contexts, understanding their behavior in both factual and disputable scenarios is essential, especially when their outputs may shape public opinion or reinforce dominant narratives. In this paper, we define two types of bias in LLMs: model bias (bias stemming from model training) and inference bias (bias induced by the language of the query), through a two-phase evaluation. Phase 1 evaluates LLMs on factual questions where a single verifiable answer exists, assessing whether models maintain consistency across different query languages. Phase 2 expands the scope by probing geopolitically sensitive disputes, where responses may reflect culturally embedded or ideologically aligned perspectives. We construct a manually curated dataset spanning both factual and disputable QA, across four languages and question types. The results show that Phase 1 exhibits query language induced alignment, while Phase 2 reflects an interplay between the model’s training context and query language. This paper offers a structured framework for evaluating LLM behavior across neutral and sensitive topics, providing insights for future LLM deployment and culturally aware evaluation practices in multilingual contexts.</p>
  </div>
</details>

<hr>
<h3 id="107-AutoMixer-Checkpoint-Artifacts-as-Automatic-Data-Mixers-cs-CLPDF"><a href="#107-AutoMixer-Checkpoint-Artifacts-as-Automatic-Data-Mixers-cs-CLPDF" class="headerlink" title="[107] AutoMixer: Checkpoint Artifacts as Automatic Data Mixers cs.CLPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21910">AutoMixer: Checkpoint Artifacts as Automatic Data Mixers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21910" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ernie Chang, Yang Li, Patrick Huber, David Kant, Yangyang Shi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种利用训练过程中的检查点模型作为数据混合器的方法，以优化语言模型训练中的数据混合物。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 语言模型训练中需要模型具备多样化任务能力，但数据与任务之间的关系难以建模。检查点模型在训练轨迹中展现出不同的能力，但这些信息未被充分利用。</p>
<p><strong>Result:</strong> 在八个推理基准上验证了方法的有效性，性能提升高达1.93%。</p>
<p><strong>Insight:</strong> 检查点模型可以作为增强数据质量和优化数据混合的有效工具。</p>
<p><strong>Abstract:</strong> In language model training, it is desirable to equip models with capabilities from various tasks. However, it is not clear how to directly obtain the right data mixtures for these capabilities as the relationship between data and tasks is difficult to be modeled. In this work, we observe that checkpoint models exhibit emerging capabilities at different points in the training trajectory. Often, the training process saves checkpoints as artifacts that are under-utilized as a source of in-training data signals. We identify these artifact models based on their respective capabilities on the benchmarks and leverage them as data mixers by using their aggregated first-order influence approximation over source data. We demonstrated on eight reasoning benchmarks that the proposed framework shows significant improvements in the pretraining setting, with performance improvements of up to 1.93%. Overall, this shows the potential of checkpoint models to enhance data quality and optimize data mixtures.</p>
  </div>
</details>

<hr>
<h3 id="108-More-Vulnerable-than-You-Think-On-the-Stability-of-Tool-Integrated-LLM-Agents-cs-CL-cs-LGPDF"><a href="#108-More-Vulnerable-than-You-Think-On-the-Stability-of-Tool-Integrated-LLM-Agents-cs-CL-cs-LGPDF" class="headerlink" title="[108] More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents cs.CL | cs.LGPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21967">More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21967" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weimin Xiong, Ke Wang, Yifan Song, Hanchao Liu, Sai Zhou</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了集成工具的LLM代理在实际应用中的稳定性问题，发现其在整个工具调用过程中容易出错，开源模型尤其脆弱，增加模型规模也不会显著改善推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前对集成工具的LLM代理的评估主要关注端到端的工具使用效果，忽略了其稳定性问题，而现实应用中代理可能因各种因素崩溃或异常行为。</p>
<p><strong>Result:</strong> 实验表明代理在每个阶段均易出错，开源模型更脆弱，增加模型规模不仅未显著改善推理能力，反而可能使其更容易受到攻击。</p>
<p><strong>Insight:</strong> 强调评估代理稳定性的重要性，为未来LLM开发与评估提供了重要参考。</p>
<p><strong>Abstract:</strong> Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool’s response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.</p>
  </div>
</details>

<hr>
<h3 id="109-Lost-at-the-Beginning-of-Reasoning-cs-CLPDF"><a href="#109-Lost-at-the-Beginning-of-Reasoning-cs-CLPDF" class="headerlink" title="[109] Lost at the Beginning of Reasoning cs.CLPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22058">Lost at the Beginning of Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22058" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Baohao Liao, Xinyi Chen, Sara Rajaee, Yuhui Xu, Christian Herold</span></p>
<p><strong>TL;DR:</strong> 论文研究了大型语言模型（LLM）在长推理链中自我修正能力的不足，发现第一步推理对最终结果影响巨大。提出了一种高效的采样策略，通过奖励模型筛选高质量第一步推理，显著降低推理成本，并引入新基准评估模型自我修正能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLM在复杂推理能力上取得了进展，但其在长推理链中的自我修正能力尚未充分探索。研究发现，第一步推理的错误会显著影响后续推理质量，因此需要解决这一问题以提高推理效率。</p>
<p><strong>Result:</strong> 提出的方法在不牺牲准确性的情况下，将推理成本降低高达70%。</p>
<p><strong>Insight:</strong> 研究强调了在长推理链中优化初始步骤的重要性，为未来研究提供了关于LLM稳健推理的新方向。</p>
<p><strong>Abstract:</strong> Recent advancements in large language models (LLMs) have significantly advanced complex reasoning capabilities, particularly through extended chain-of-thought (CoT) reasoning that incorporates mechanisms such as backtracking, self-reflection and self-correction. Despite these developments, the self-correction abilities of LLMs during long CoT reasoning remain underexplored. And recent findings on overthinking suggest that such models often engage in unnecessarily redundant reasoning. In this work, we empirically show that the first reasoning step exerts a disproportionately large influence on the final prediction - errors introduced at this stage can substantially degrade subsequent reasoning quality. This phenomenon is consistently observed across two state-of-the-art open-source reasoning model families: DeepSeek-R1 and Qwen3. To address this, we propose an efficient sampling strategy that leverages a reward model to identify and retain high-quality first reasoning steps while discarding suboptimal ones, achieving up to a 70% reduction in inference cost without sacrificing accuracy. Finally, we introduce a new benchmark specifically constructed with deliberately flawed first reasoning steps to systematically evaluate model self-correction capabilities, offering a foundation for future research on robust reasoning in LLMs.</p>
  </div>
</details>

<hr>
<h3 id="110-Training-Language-Model-to-Critique-for-Better-Refinement-cs-CLPDF"><a href="#110-Training-Language-Model-to-Critique-for-Better-Refinement-cs-CLPDF" class="headerlink" title="[110] Training Language Model to Critique for Better Refinement cs.CLPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22157">Training Language Model to Critique for Better Refinement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22157" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianshu Yu, Chao Xiang, Mingchuan Yang, Pei Ke, Bosi Wen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为RCO的新框架，通过训练评论模型优化语言模型的反馈与改进能力，显著提升了评论质量和改进效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大型语言模型（LLMs）在提供反馈和评论方面表现出色，但缺乏对评论类型的有效性及其生成方式的研究。</p>
<p><strong>Result:</strong> RCO在对话生成、摘要、问答、数学推理和代码生成五个任务中，显著提升了评论质量和改进效果。</p>
<p><strong>Insight:</strong> 关注能带来实际改进的评论比直接评估评论偏好更有效。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have demonstrated remarkable evaluation and critique capabilities, providing insightful feedback and identifying flaws in various tasks. However, limited research has explored which types of critiques are most effective for improving model responses or how to generate such critiques. To address this gap, we introduce \textbf{R}efinement-oriented \textbf{C}ritique \textbf{O}ptimization (RCO), a novel framework designed to train critic models using refinement signals. RCO uses a feedback loop where critiques, generated by the critic model, guide the actor model in refining its responses. The critique utility (CU) quantifies the effectiveness of these refinements, serving as the reward signal for training the critic model. By focusing on critiques that lead to better refinements, RCO eliminates the need for direct critique preference assessment, ensuring that critiques driving meaningful improvements are rewarded. We evaluate RCO across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes. Our contributions include the introduction of RCO, a novel supervision scheme based on refined response preferences, and comprehensive experimental results that highlight the method’s effectiveness in enhancing LLM critique-refinement loops.</p>
  </div>
</details>

<hr>
<h3 id="111-Leveraging-In-Context-Learning-for-Political-Bias-Testing-of-LLMs-cs-CLPDF"><a href="#111-Leveraging-In-Context-Learning-for-Political-Bias-Testing-of-LLMs-cs-CLPDF" class="headerlink" title="[111] Leveraging In-Context Learning for Political Bias Testing of LLMs cs.CLPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22232">Leveraging In-Context Learning for Political Bias Testing of LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22232" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Patrick Haller, Jannis Vamvas, Rico Sennrich, Lena A. Jäger</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新的政治偏见测试方法——问卷建模（QM），利用人类调查数据作为上下文示例，提高了基于问题的偏见评估的稳定性，并验证了指令调优可能改变偏见方向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通过政治问题测试大型语言模型（LLM）的偏见，但存在稳定性不足的问题，难以可靠比较不同模型。作者认为模型需要更多上下文来提高测试的稳定性。</p>
<p><strong>Result:</strong> 实验表明，QM提高了偏见评估的稳定性，指令调优可以改变偏见方向，且更大规模的模型能更有效地利用上下文示例，表现出更小的偏见分数。</p>
<p><strong>Insight:</strong> 上下文学习对模型偏见测试至关重要；更大的模型在利用上下文时表现更优，偏见更少。</p>
<p><strong>Abstract:</strong> A growing body of work has been querying LLMs with political questions to evaluate their potential biases. However, this probing method has limited stability, making comparisons between models unreliable. In this paper, we argue that LLMs need more context. We propose a new probing task, Questionnaire Modeling (QM), that uses human survey data as in-context examples. We show that QM improves the stability of question-based bias evaluation, and demonstrate that it may be used to compare instruction-tuned models to their base versions. Experiments with LLMs of various sizes indicate that instruction tuning can indeed change the direction of bias. Furthermore, we observe a trend that larger models are able to leverage in-context examples more effectively, and generally exhibit smaller bias scores in QM. Data and code are publicly available.</p>
  </div>
</details>

<hr>
<h3 id="112-Detection-of-Personal-Data-in-Structured-Datasets-Using-a-Large-Language-Model-cs-CL-I-5-4-I-2-7-H-3-1PDF"><a href="#112-Detection-of-Personal-Data-in-Structured-Datasets-Using-a-Large-Language-Model-cs-CL-I-5-4-I-2-7-H-3-1PDF" class="headerlink" title="[112] Detection of Personal Data in Structured Datasets Using a Large Language Model cs.CL | I.5.4; I.2.7; H.3.1PDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22305">Detection of Personal Data in Structured Datasets Using a Large Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | I.5.4; I.2.7; H.3.1</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22305" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Albert Agisha Ntwali, Luca Rück, Martin Heckmann</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于GPT-4o的大型语言模型方法，用于检测结构化数据集中的个人数据，通过结合上下文信息（如特征名称、数据集描述等）显著提升了检测性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在检测个人数据时往往忽略上下文信息，而在实际应用中，这些信息可能对提高检测准确性至关重要。因此，作者提出了一种结合上下文的新方法。</p>
<p><strong>Result:</strong> 实验表明，新方法在MIMIC-Demo-Ext和Kaggle&#x2F;OpenML数据集上显著优于CASSED和Microsoft Presidio，尤其是在利用上下文信息时效果更佳。</p>
<p><strong>Insight:</strong> 研究表明，上下文信息对个人数据检测至关重要，未来研究需要更多包含真实个人数据的数据集以推动领域进步。</p>
<p><strong>Abstract:</strong> We propose a novel approach for detecting personal data in structured datasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key innovation of our method is the incorporation of contextual information: in addition to a feature’s name and values, we utilize information from other feature names within the dataset as well as the dataset description. We compare our approach to alternative methods, including Microsoft Presidio and CASSED, evaluating them on multiple datasets: DeSSI, a large synthetic dataset, datasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a real-world dataset containing patient information from critical care units.   Our findings reveal that detection performance varies significantly depending on the dataset used for evaluation. CASSED excels on DeSSI, the dataset on which it was trained. Performance on the medical dataset MIMIC-Demo-Ext is comparable across all models, with our GPT-4o-based approach clearly outperforming the others. Notably, personal data detection in the Kaggle and OpenML datasets appears to benefit from contextual information. This is evidenced by the poor performance of CASSED and Presidio (both of which do not utilize the context of the dataset) compared to the strong results of our GPT-4o-based approach.   We conclude that further progress in this field would greatly benefit from the availability of more real-world datasets containing personal information.</p>
  </div>
</details>

<hr>
<h3 id="113-Refining-Czech-GEC-Insights-from-a-Multi-Experiment-Approach-cs-CLPDF"><a href="#113-Refining-Czech-GEC-Insights-from-a-Multi-Experiment-Approach-cs-CLPDF" class="headerlink" title="[113] Refining Czech GEC: Insights from a Multi-Experiment Approach cs.CLPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22402">Refining Czech GEC: Insights from a Multi-Experiment Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22402" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Petr Pechman, Milan Straka, Jana Straková, Jakub Náplava</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于Transformer架构的捷克语语法错误校正（GEC）系统，通过动态生成合成错误数据并结合多实验方法，实现了当前最佳性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 捷克语的语法错误校正任务缺乏高效且高性能的解决方案，现有方法通常在数据生成和模型优化上存在不足。</p>
<p><strong>Result:</strong> 最佳模型在性能和计算效率上均显著优于现有方法，并公开了源代码和模型。</p>
<p><strong>Insight:</strong> 合成错误生成和领域平衡对提升GEC任务性能至关重要，而大型语言模型（LLMs）在专家调优场景中表现优异。</p>
<p><strong>Abstract:</strong> We present a grammar error correction (GEC) system that achieves state of the art for the Czech language. Our system is based on a neural network translation approach with the Transformer architecture, and its key feature is its real-time synthetic generation pipeline, which dynamically augments sentences with artificial errors by introducing both language-agnostic and Czech-specific errors. We conduct a comprehensive series of experiments, investigating the Czech GEC corpora as bases for synthetic error introduction, several error generation strategies, domain balancing, tokenization granularity, model size, and data scaling during fine-tuning. Additionally, we evaluate the performance of large language models (LLMs) on Czech GEC in both end-user and expert fine-tuning scenarios. Our best-performing model is superior both in performance and computational efficiency. The source code and the trained model links are available on <a target="_blank" rel="noopener" href="https://github.com/ufal/tsd2025-gec">https://github.com/ufal/tsd2025-gec</a>.</p>
  </div>
</details>

<hr>
<h3 id="114-HyperCLOVA-X-THINK-Technical-Report-cs-CL-cs-AIPDF"><a href="#114-HyperCLOVA-X-THINK-Technical-Report-cs-CL-cs-AIPDF" class="headerlink" title="[114] HyperCLOVA X THINK Technical Report cs.CL | cs.AIPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22403">HyperCLOVA X THINK Technical Report</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22403" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">NAVER Cloud HyperCLOVA X Team</span></p>
<p><strong>TL;DR:</strong> HyperCLOVA X THINK是一个专注于推理的大型语言模型，针对韩语和英语优化，通过三阶段课程学习和强化学习实现高性能表现，并在韩国本地化基准测试中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对韩语和英语的双语推理需求，推动韩国AI创新，同时为全球研究社区提供资源。</p>
<p><strong>Result:</strong> 在韩国本地化基准测试（如KMMLU、CSAT等）中表现优异，视觉增强变体在KCSAT STEM基准上媲美或超越GPT-4.1，且训练计算成本更低。</p>
<p><strong>Insight:</strong> 模型通过高效的数据增强和技术创新，实现了高性能与低计算成本的平衡，为双语推理提供了新的可能性。</p>
<p><strong>Abstract:</strong> We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $\mu$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community.</p>
  </div>
</details>

<hr>
<h3 id="115-Sequential-Diagnosis-with-Language-Models-cs-CLPDF"><a href="#115-Sequential-Diagnosis-with-Language-Models-cs-CLPDF" class="headerlink" title="[115] Sequential Diagnosis with Language Models cs.CLPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22405">Sequential Diagnosis with Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22405" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Harsha Nori, Mayank Daswani, Christopher Kelly, Scott Lundberg, Marco Tulio Ribeiro</span></p>
<p><strong>TL;DR:</strong> A paper introducing the Sequential Diagnosis Benchmark and MAI-DxO orchestrator for iterative medical diagnosis, showing significant improvements in accuracy and cost-effectiveness over physicians and standard models.</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Current evaluations of language models in medical diagnosis rely on static vignettes, missing the iterative and adaptive nature of real-world clinical practice. The paper aims to bridge this gap.</p>
<p><strong>Result:</strong> MAI-DxO achieves 80% accuracy (vs. 20% for physicians) and reduces costs by 20% vs. physicians and 70% vs. standard models. Maximum accuracy reaches 85.5%.</p>
<p><strong>Insight:</strong> AI systems guided to think iteratively and act judiciously can significantly enhance diagnostic precision and cost-effectiveness in clinical care.</p>
<p><strong>Abstract:</strong> Artificial intelligence holds great promise for expanding access to expert medical knowledge and reasoning. However, most evaluations of language models rely on static vignettes and multiple-choice questions that fail to reflect the complexity and nuance of evidence-based medicine in real-world settings. In clinical practice, physicians iteratively formulate and revise diagnostic hypotheses, adapting each subsequent question and test to what they’ve just learned, and weigh the evolving evidence before committing to a final diagnosis. To emulate this iterative process, we introduce the Sequential Diagnosis Benchmark, which transforms 304 diagnostically challenging New England Journal of Medicine clinicopathological conference (NEJM-CPC) cases into stepwise diagnostic encounters. A physician or AI begins with a short case abstract and must iteratively request additional details from a gatekeeper model that reveals findings only when explicitly queried. Performance is assessed not just by diagnostic accuracy but also by the cost of physician visits and tests performed. We also present the MAI Diagnostic Orchestrator (MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians, proposes likely differential diagnoses and strategically selects high-value, cost-effective tests. When paired with OpenAI’s o3 model, MAI-DxO achieves 80% diagnostic accuracy–four times higher than the 20% average of generalist physicians. MAI-DxO also reduces diagnostic costs by 20% compared to physicians, and 70% compared to off-the-shelf o3. When configured for maximum accuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO generalize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and Llama families. We highlight how AI systems, when guided to think iteratively and act judiciously, can advance diagnostic precision and cost-effectiveness in clinical care.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="116-CitySim-Modeling-Urban-Behaviors-and-City-Dynamics-with-Large-Scale-LLM-Driven-Agent-Simulation-cs-AI-cs-CLPDF"><a href="#116-CitySim-Modeling-Urban-Behaviors-and-City-Dynamics-with-Large-Scale-LLM-Driven-Agent-Simulation-cs-AI-cs-CLPDF" class="headerlink" title="[116] CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation cs.AI | cs.CLPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21805">CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21805" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nicolas Bougie, Narimasa Watanabe</span></p>
<p><strong>TL;DR:</strong> CitySim是一个基于大语言模型（LLM）驱动的城市模拟器，通过智能代理模拟人类行为和城市动态，比传统基于规则的方法更贴近真实人类行为。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统城市模拟方法依赖手工规则，难以捕捉复杂的人类行为和意图。CitySim利用大语言模型展示的人类级智能，提供更灵活、真实的模拟。</p>
<p><strong>Result:</strong> CitySim在微观和宏观层面均比传统方法更贴近真实人类行为，并能模拟大规模代理的集体行为。</p>
<p><strong>Insight:</strong> LLM为城市模拟提供了强大的泛化能力，未来的研究方向包括多模态输入和更动态的城市环境建模。</p>
<p><strong>Abstract:</strong> Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.</p>
  </div>
</details>

<hr>
<h3 id="117-Conceptual-Topic-Aggregation-cs-AI-cs-CL-cs-DM-cs-LG-06B99-I-2-4-I-2-7PDF"><a href="#117-Conceptual-Topic-Aggregation-cs-AI-cs-CL-cs-DM-cs-LG-06B99-I-2-4-I-2-7PDF" class="headerlink" title="[117] Conceptual Topic Aggregation cs.AI | cs.CL | cs.DM | cs.LG | 06B99 | I.2.4; I.2.7PDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22309">Conceptual Topic Aggregation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.DM | cs.LG | 06B99 | I.2.4; I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22309" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Klara M. Gutekunst, Dominik Dürrschnabel, Johannes Hirth, Gerd Stumme</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为FAT-CAT的新方法，基于形式概念分析（FCA），用于提升主题聚合的可解释性和可视化效果，解决了传统主题模型在语义表达上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着数据规模的急剧增长，传统的手动检查方法已不可行，需要更高效的计算方法进行数据探索。然而，现有主题模型在提供可解释性和深度语义结构分析方面存在不足。</p>
<p><strong>Result:</strong> 在ETYNTKE数据集上的实验表明，FAT-CAT比其他主题建模方法更能提供有意义的、可解释的数据集结构洞察。</p>
<p><strong>Insight:</strong> FCA不仅提升了主题模型的可解释性，还为数据的层次化分析提供了新的视角，特别适用于多类型、多主题的数据集。</p>
<p><strong>Abstract:</strong> The vast growth of data has rendered traditional manual inspection infeasible, necessitating the adoption of computational methods for efficient data exploration. Topic modeling has emerged as a powerful tool for analyzing large-scale textual datasets, enabling the extraction of latent semantic structures. However, existing methods for topic modeling often struggle to provide interpretable representations that facilitate deeper insights into data structure and content. In this paper, we propose FAT-CAT, an approach based on Formal Concept Analysis (FCA) to enhance meaningful topic aggregation and visualization of discovered topics. Our approach can handle diverse topics and file types – grouped by directories – to construct a concept lattice that offers a structured, hierarchical representation of their topic distribution. In a case study on the ETYNTKE dataset, we evaluate the effectiveness of our approach against other representation methods to demonstrate that FCA-based aggregation provides more meaningful and interpretable insights into dataset composition than existing topic modeling techniques.</p>
  </div>
</details>

<hr>
<h3 id="118-The-Automated-LLM-Speedrunning-Benchmark-Reproducing-NanoGPT-Improvements-cs-AI-cs-CL-cs-LGPDF"><a href="#118-The-Automated-LLM-Speedrunning-Benchmark-Reproducing-NanoGPT-Improvements-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[118] The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements cs.AI | cs.CL | cs.LGPDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22419">The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22419" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bingchen Zhao, Despoina Magka, Minqi Jiang, Xian Li, Roberta Raileanu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个自动化LLM速度跑分基准，用于评估AI代理在重现NanoGPT改进方面的能力，结果显示即使提供详细提示，当前LLM仍难以重现已知的创新。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究如何利用AI自动重现科学研究成果，以推动科学进步。</p>
<p><strong>Result:</strong> 研究发现，即便是结合了先进推理能力的LLM，也无法有效重现已知的改进，表明其自动科学研究再现能力有限。</p>
<p><strong>Insight:</strong> 自动化科学研究再现是一个复杂且尚未解决的问题，当前LLM的能力仍有很大提升空间。</p>
<p><strong>Abstract:</strong> Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent.</p>
  </div>
</details>

<hr>
<div id='quant-ph'></div>

<h1 id="quant-ph-Back"><a href="#quant-ph-Back" class="headerlink" title="quant-ph [Back]"></a>quant-ph <a href="#toc">[Back]</a></h1><h3 id="119-QuKAN-A-Quantum-Circuit-Born-Machine-approach-to-Quantum-Kolmogorov-Arnold-Networks-quant-ph-cs-CV-cs-LGPDF"><a href="#119-QuKAN-A-Quantum-Circuit-Born-Machine-approach-to-Quantum-Kolmogorov-Arnold-Networks-quant-ph-cs-CV-cs-LGPDF" class="headerlink" title="[119] QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks quant-ph | cs.CV | cs.LGPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22340">QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">quant-ph | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22340" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yannick Werner, Akash Malemath, Mengxi Liu, Vitor Fortes Rey, Nikolaos Palaiodimopoulos</span></p>
<p><strong>TL;DR:</strong> 该论文提出了QuKAN，一种基于量子电路生成模型（QCBM）的量子Kolmogorov Arnold网络，探索了KAN在量子机器学习中的潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Kolmogorov Arnold网络（KANs）因其通过学习边参数而非节点参数来高效表达复杂函数的能力而受到关注，但其在量子机器学习中的应用尚未深入研究。</p>
<p><strong>Result:</strong> 实验证明了QuKAN架构的可行性和解释性，并展示了其性能优于传统方法。</p>
<p><strong>Insight:</strong> 研究揭示了量子电路在增强KAN表达能力和效率方面的潜力，为量子机器学习提供了新思路。</p>
<p><strong>Abstract:</strong> Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold representation theorem (KAR), have demonstrated promising capabilities in expressing complex functions with fewer neurons. This is achieved by implementing learnable parameters on the edges instead of on the nodes, unlike traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs potential in quantum machine learning has not yet been well explored. In this work, we present an implementation of these KAN architectures in both hybrid and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt the KAN transfer using pre-trained residual functions, thereby exploiting the representational power of parametrized quantum circuits. In the hybrid model we combine classical KAN components with quantum subroutines, while the fully quantum version the entire architecture of the residual function is translated to a quantum model. We demonstrate the feasibility, interpretability and performance of the proposed Quantum KAN (QuKAN) architecture.</p>
  </div>
</details>

<hr>
<div id='physics.optics'></div>

<h1 id="physics-optics-Back"><a href="#physics-optics-Back" class="headerlink" title="physics.optics [Back]"></a>physics.optics <a href="#toc">[Back]</a></h1><h3 id="120-Inverse-Design-of-Diffractive-Metasurfaces-Using-Diffusion-Models-physics-optics-cs-CV-cs-LGPDF"><a href="#120-Inverse-Design-of-Diffractive-Metasurfaces-Using-Diffusion-Models-physics-optics-cs-CV-cs-LGPDF" class="headerlink" title="[120] Inverse Design of Diffractive Metasurfaces Using Diffusion Models physics.optics | cs.CV | cs.LGPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21748">Inverse Design of Diffractive Metasurfaces Using Diffusion Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">physics.optics | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21748" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liav Hen, Erez Yosef, Dan Raviv, Raja Giryes, Jacob Scheuer</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于扩散模型的衍射超表面逆向设计方法，解决了传统设计中的局部极小值和计算开销大的问题，实现了快速且低误差的超表面设计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的衍射超表面逆向设计方法由于结构和光学性能之间的复杂非线性关系，通常需要专家调整，容易陷入局部极小值，且计算开销大。论文旨在通过引入扩散模型的生成能力，解决这些问题。</p>
<p><strong>Result:</strong> 实验结果表明，该方法可以在30分钟内设计出低误差的空间均匀分束器和偏振分束器，证明了其高效性。</p>
<p><strong>Insight:</strong> 扩散模型在超表面逆向设计中展现出了强大的生成能力，能够有效替代或辅助传统优化方法，显著降低计算开销和设计时间。</p>
<p><strong>Abstract:</strong> Metasurfaces are ultra-thin optical elements composed of engineered sub-wavelength structures that enable precise control of light. Their inverse design - determining a geometry that yields a desired optical response - is challenging due to the complex, nonlinear relationship between structure and optical properties. This often requires expert tuning, is prone to local minima, and involves significant computational overhead. In this work, we address these challenges by integrating the generative capabilities of diffusion models into computational design workflows. Using an RCWA simulator, we generate training data consisting of metasurface geometries and their corresponding far-field scattering patterns. We then train a conditional diffusion model to predict meta-atom geometry and height from a target spatial power distribution at a specified wavelength, sampled from a continuous supported band. Once trained, the model can generate metasurfaces with low error, either directly using RCWA-guided posterior sampling or by serving as an initializer for traditional optimization methods. We demonstrate our approach on the design of a spatially uniform intensity splitter and a polarization beam splitter, both produced with low error in under 30 minutes. To support further research in data-driven metasurface design, we publicly release our code and datasets.</p>
  </div>
</details>

<hr>
<div id='stat.ML'></div>

<h1 id="stat-ML-Back"><a href="#stat-ML-Back" class="headerlink" title="stat.ML [Back]"></a>stat.ML <a href="#toc">[Back]</a></h1><h3 id="121-Optimal-Estimation-of-Watermark-Proportions-in-Hybrid-AI-Human-Texts-stat-ML-cs-CL-cs-LG-stat-MEPDF"><a href="#121-Optimal-Estimation-of-Watermark-Proportions-in-Hybrid-AI-Human-Texts-stat-ML-cs-CL-cs-LG-stat-MEPDF" class="headerlink" title="[121] Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts stat.ML | cs.CL | cs.LG | stat.MEPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22343">Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">stat.ML | cs.CL | cs.LG | stat.ME</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22343" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiang Li, Garrett Wen, Weiqing He, Jiayuan Wu, Qi Long</span></p>
<p><strong>TL;DR:</strong> 本文研究了混合AI-人类文本中水印比例的最优估计问题，首次提出了基于关键统计的混合模型参数估计方法，并证明了在水印方法满足条件下比例参数的可识别性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大语言模型的广泛应用，识别合成文本与水印内容成为重要课题。然而，现有研究多集中于纯文本水印检测，而实际场景中常出现混合来源的文本。本文旨在解决混合文本中水印比例的最优估计问题。</p>
<p><strong>Result:</strong> 在合成数据和开源模型生成的混合文本上，所提估计器表现出高精度。</p>
<p><strong>Insight:</strong> 水印方法的可识别性取决于关键统计的性质，连续统计方法在实际应用中更具优势。</p>
<p><strong>Abstract:</strong> Text watermarks in large language models (LLMs) are an increasingly important tool for detecting synthetic text and distinguishing human-written content from LLM-generated text. While most existing studies focus on determining whether entire texts are watermarked, many real-world scenarios involve mixed-source texts, which blend human-written and watermarked content. In this paper, we address the problem of optimally estimating the watermark proportion in mixed-source texts. We cast this problem as estimating the proportion parameter in a mixture model based on \emph{pivotal statistics}. First, we show that this parameter is not even identifiable in certain watermarking schemes, let alone consistently estimable. In stark contrast, for watermarking methods that employ continuous pivotal statistics for detection, we demonstrate that the proportion parameter is identifiable under mild conditions. We propose efficient estimators for this class of methods, which include several popular unbiased watermarks as examples, and derive minimax lower bounds for any measurable estimator based on pivotal statistics, showing that our estimators achieve these lower bounds. Through evaluations on both synthetic data and mixed-source text generated by open-source models, we demonstrate that our proposed estimators consistently achieve high estimation accuracy.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="122-TUS-REC2024-A-Challenge-to-Reconstruct-3D-Freehand-Ultrasound-Without-External-Tracker-eess-IV-cs-CVPDF"><a href="#122-TUS-REC2024-A-Challenge-to-Reconstruct-3D-Freehand-Ultrasound-Without-External-Tracker-eess-IV-cs-CVPDF" class="headerlink" title="[122] TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker eess.IV | cs.CVPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21765">TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21765" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qi Li, Shaheer U. Saeed, Yuliang Huang, Mingyuan Luo, Zhongnuo Yan</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了TUS-REC2024挑战赛，旨在通过公开数据集和评估框架推动无追踪器的自由手超声3D重建技术发展，吸引了多支团队参与，并总结了当前方法的进展与局限。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 无追踪器的自由手超声重建技术具有低成本、便携性和广泛部署的优势，但在帧间运动估计和漂移累积等方面面临挑战。挑战赛旨在通过公开基准数据集加速技术发展。</p>
<p><strong>Result:</strong> 结果表明，现有方法在3D重建方面取得了一定进展，但仍存在漂移累积和通用性等局限。</p>
<p><strong>Insight:</strong> 无追踪器自由手超声重建技术仍需进一步研究以解决漂移问题和提升跨扫描协议的通用性。公开数据集和持续挑战赛将推动该领域发展。</p>
<p><strong>Abstract:</strong> Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes from sequences of 2D ultrasound images without relying on external tracking systems, offering a low-cost, portable, and widely deployable alternative for volumetric imaging. However, it presents significant challenges, including accurate inter-frame motion estimation, minimisation of drift accumulation over long sequences, and generalisability across scanning protocols. The TUS-REC2024 Challenge was established to benchmark and accelerate progress in trackerless 3D ultrasound reconstruction by providing a publicly available dataset for the first time, along with a baseline model and evaluation framework. The Challenge attracted over 43 registered teams, of which 6 teams submitted 21 valid dockerized solutions. Submitted methods spanned a wide range of algorithmic approaches, including recurrent models, registration-driven volume refinement, attention, and physics-informed models. This paper presents an overview of the Challenge design, summarises the key characteristics of the dataset, provides a concise literature review, introduces the technical details of the underlying methodology working with tracked freehand ultrasound data, and offers a comparative analysis of submitted methods across multiple evaluation metrics. The results highlight both the progress and current limitations of state-of-the-art approaches in this domain, and inform directions for future research. The data, evaluation code, and baseline are publicly available to facilitate ongoing development and reproducibility. As a live and evolving benchmark, this Challenge is designed to be continuously developed and improved. The Challenge was held at MICCAI 2024 and will be organised again at MICCAI 2025, reflecting its growing impact and the sustained commitment to advancing this field.</p>
  </div>
</details>

<hr>
<h3 id="123-UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields-eess-IV-cs-AI-cs-CV-cs-LG-eess-SPPDF"><a href="#123-UnMix-NeRF-Spectral-Unmixing-Meets-Neural-Radiance-Fields-eess-IV-cs-AI-cs-CV-cs-LG-eess-SPPDF" class="headerlink" title="[123] UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields eess.IV | cs.AI | cs.CV | cs.LG | eess.SPPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21884">UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV | cs.LG | eess.SP</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21884" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fabian Perez, Sara Rojas, Carlos Hinojosa, Hoover Rueda-Chacón, Bernard Ghanem</span></p>
<p><strong>TL;DR:</strong> UnMix-NeRF 结合了光谱分解与神经辐射场，实现了高光谱新视图合成和无监督材料分割，解决了传统 NeRF 方法缺乏材料感知的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统 NeRF 主要依赖 RGB 数据，无法准确感知材质特性，限制了其在机器人、增强现实等应用中的潜力。需要一种方法同时实现高质量渲染和材料属性理解。</p>
<p><strong>Result:</strong> 实验表明，UnMix-NeRF 在光谱重建和材料分割方面优于现有方法，并支持灵活的材质编辑。</p>
<p><strong>Insight:</strong> 将光谱特性与 NeRF 结合，不仅提升了渲染质量，还为场景理解和编辑提供了新工具。</p>
<p><strong>Abstract:</strong> Neural Radiance Field (NeRF)-based segmentation methods focus on object semantics and rely solely on RGB data, lacking intrinsic material properties. This limitation restricts accurate material perception, which is crucial for robotics, augmented reality, simulation, and other applications. We introduce UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling joint hyperspectral novel view synthesis and unsupervised material segmentation. Our method models spectral reflectance via diffuse and specular components, where a learned dictionary of global endmembers represents pure material signatures, and per-point abundances capture their distribution. For material segmentation, we use spectral signature predictions along learned endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF enables scene editing by modifying learned endmember dictionaries for flexible material-based appearance manipulation. Extensive experiments validate our approach, demonstrating superior spectral reconstruction and material segmentation to existing methods. Project page: <a target="_blank" rel="noopener" href="https://www.factral.co/UnMix-NeRF">https://www.factral.co/UnMix-NeRF</a>.</p>
  </div>
</details>

<hr>
<h3 id="124-Towards-Scalable-and-Robust-White-Matter-Lesion-Localization-via-Multimodal-Deep-Learning-eess-IV-cs-CVPDF"><a href="#124-Towards-Scalable-and-Robust-White-Matter-Lesion-Localization-via-Multimodal-Deep-Learning-eess-IV-cs-CVPDF" class="headerlink" title="[124] Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning eess.IV | cs.CVPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22041">Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22041" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Julia Machnio, Sebastian Nørgaard Llambias, Mads Nielsen, Mostafa Mehdipour Ghazi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于多模态深度学习的白质病变定位方法，通过多任务学习联合预测病变和解剖区域掩模，实验表明多模态输入显著提升了分割性能，但联合学习效果不如单独任务模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 白质高信号（WMH）是小血管疾病和神经退行性疾病的影像学标志，其准确分割和空间定位对诊断和监测至关重要。现有方法在处理缺失模态和高效整合解剖定位方面表现不足，因此需要一种更灵活且鲁棒的框架。</p>
<p><strong>Result:</strong> 多模态输入显著优于单模态模型，但模态可互换设置以部分准确性为代价提升了鲁棒性。多任务学习在联合预测中效果不如单独任务模型。</p>
<p><strong>Insight:</strong> 多模态融合对WMH分析至关重要，而任务间的表征冲突可能导致联合学习效果不佳，未来研究需进一步优化多任务整合策略。</p>
<p><strong>Abstract:</strong> White matter hyperintensities (WMH) are radiological markers of small vessel disease and neurodegeneration, whose accurate segmentation and spatial localization are crucial for diagnosis and monitoring. While multimodal MRI offers complementary contrasts for detecting and contextualizing WM lesions, existing approaches often lack flexibility in handling missing modalities and fail to integrate anatomical localization efficiently. We propose a deep learning framework for WM lesion segmentation and localization that operates directly in native space using single- and multi-modal MRI inputs. Our study evaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR and T1, and a modality-interchangeable setup. It further introduces a multi-task model for jointly predicting lesion and anatomical region masks to estimate region-wise lesion burden. Experiments conducted on the MICCAI WMH Segmentation Challenge dataset demonstrate that multimodal input significantly improves the segmentation performance, outperforming unimodal models. While the modality-interchangeable setting trades accuracy for robustness, it enables inference in cases with missing modalities. Joint lesion-region segmentation using multi-task learning was less effective than separate models, suggesting representational conflict between tasks. Our findings highlight the utility of multimodal fusion for accurate and robust WMH analysis, and the potential of joint modeling for integrated predictions.</p>
  </div>
</details>

<hr>
<h3 id="125-Advanced-Deep-Learning-Techniques-for-Automated-Segmentation-of-Type-B-Aortic-Dissections-eess-IV-cs-CVPDF"><a href="#125-Advanced-Deep-Learning-Techniques-for-Automated-Segmentation-of-Type-B-Aortic-Dissections-eess-IV-cs-CVPDF" class="headerlink" title="[125] Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections eess.IV | cs.CVPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22222">Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22222" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hao Xu, Ruth Lim, Brian E. Chapman</span></p>
<p><strong>TL;DR:</strong> 论文提出了四种基于深度学习的自动化分割管道，用于B型主动脉夹层的精确分割，性能优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 主动脉夹层是一种危及生命的心血管疾病，手动分割耗时且不一致，需要自动化解决方案以提高效率和准确性。</p>
<p><strong>Result:</strong> Dice系数分别为TL 0.91、FL 0.88、FLT 0.47，性能显著优于对比方法。</p>
<p><strong>Insight:</strong> 深度学习模型能够有效分割复杂的心血管结构，尤其在自动提取形态参数方面具有潜力，可用于临床监测和治疗规划。</p>
<p><strong>Abstract:</strong> Purpose: Aortic dissections are life-threatening cardiovascular conditions requiring accurate segmentation of true lumen (TL), false lumen (FL), and false lumen thrombosis (FLT) from CTA images for effective management. Manual segmentation is time-consuming and variable, necessitating automated solutions. Materials and Methods: We developed four deep learning-based pipelines for Type B aortic dissection segmentation: a single-step model, a sequential model, a sequential multi-task model, and an ensemble model, utilizing 3D U-Net and Swin-UnetR architectures. A dataset of 100 retrospective CTA images was split into training (n&#x3D;80), validation (n&#x3D;10), and testing (n&#x3D;10). Performance was assessed using the Dice Coefficient and Hausdorff Distance. Results: Our approach achieved superior segmentation accuracy, with Dice Coefficients of 0.91 $\pm$ 0.07 for TL, 0.88 $\pm$ 0.18 for FL, and 0.47 $\pm$ 0.25 for FLT, outperforming Yao et al. (1), who reported 0.78 $\pm$ 0.20, 0.68 $\pm$ 0.18, and 0.25 $\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide accurate segmentation of TBAD features, enabling derivation of morphological parameters for surveillance and treatment planning</p>
  </div>
</details>

<hr>
<h3 id="126-Cardiovascular-disease-classification-using-radiomics-and-geometric-features-from-cardiac-CT-eess-IV-cs-CVPDF"><a href="#126-Cardiovascular-disease-classification-using-radiomics-and-geometric-features-from-cardiac-CT-eess-IV-cs-CVPDF" class="headerlink" title="[126] Cardiovascular disease classification using radiomics and geometric features from cardiac CT eess.IV | cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22226">Cardiovascular disease classification using radiomics and geometric features from cardiac CT</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22226" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ajay Mittal, Raghav Mehta, Omar Todd, Philipp Seeböck, Georg Langs</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于放射组学和几何特征的心脏CT心血管疾病分类方法，通过图像分割、配准和分类三个步骤，提升了分类准确率和临床可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目前大多数基于深度学习的CVD分类方法直接使用原始CT数据或结合解剖结构分割，缺乏临床可解释性。论文旨在通过引入放射组学和几何特征，提高分类准确率的同时增强方法的可解释性。</p>
<p><strong>Result:</strong> 在ASOCA数据集上，所提方法的分类准确率达到87.50%，显著高于直接使用原始CT数据的67.50%。</p>
<p><strong>Insight:</strong> 放射组学和几何特征的引入不仅提高了CVD分类的准确率，还增强了模型在临床中的可解释性，为医学图像分析提供了更透明的方法。</p>
<p><strong>Abstract:</strong> Automatic detection and classification of Cardiovascular disease (CVD) from Computed Tomography (CT) images play an important part in facilitating better-informed clinical decisions. However, most of the recent deep learning based methods either directly work on raw CT data or utilize it in pair with anatomical cardiac structure segmentation by training an end-to-end classifier. As such, these approaches become much more difficult to interpret from a clinical perspective. To address this challenge, in this work, we break down the CVD classification pipeline into three components: (i) image segmentation, (ii) image registration, and (iii) downstream CVD classification. Specifically, we utilize the Atlas-ISTN framework and recent segmentation foundational models to generate anatomical structure segmentation and a normative healthy atlas. These are further utilized to extract clinically interpretable radiomic features as well as deformation field based geometric features (through atlas registration) for CVD classification. Our experiments on the publicly available ASOCA dataset show that utilizing these features leads to better CVD classification accuracy (87.50%) when compared against classification model trained directly on raw CT images (67.50%). Our code is publicly available: <a target="_blank" rel="noopener" href="https://github.com/biomedia-mira/grc-net">https://github.com/biomedia-mira/grc-net</a></p>
  </div>
</details>

<hr>
<h3 id="127-DIGS-Dynamic-CBCT-Reconstruction-using-Deformation-Informed-4D-Gaussian-Splatting-and-a-Low-Rank-Free-Form-Deformation-Model-eess-IV-cs-CVPDF"><a href="#127-DIGS-Dynamic-CBCT-Reconstruction-using-Deformation-Informed-4D-Gaussian-Splatting-and-a-Low-Rank-Free-Form-Deformation-Model-eess-IV-cs-CVPDF" class="headerlink" title="[127] DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model eess.IV | cs.CVPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22280">DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22280" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuliang Huang, Imraj Singh, Thomas Joyce, Kris Thielemans, Jamie R. McClelland</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种动态CBCT重建方法，结合了变形感知的4D高斯泼溅和低秩自由形变模型，以解决呼吸运动引起的伪影问题，并通过6倍加速和更优的图像质量验证了方法的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> CBCT在放疗中广泛应用，但呼吸运动会导致伪影。现有的基于呼吸相的分段重建方法无法捕捉呼吸的变异性，而动态CBCT虽然能捕捉连续运动，但现有4D高斯泼溅方法的计算成本高，且缺乏空间一致性。</p>
<p><strong>Result:</strong> 相比HexPlane方法，图像质量更优，且计算速度提升了6倍。</p>
<p><strong>Insight:</strong> 变形感知的4DGS能够有效平衡动态场景建模的计算效率和空间一致性，为动态CBCT重建提供了新思路。</p>
<p><strong>Abstract:</strong> 3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion artifacts due to breathing. A common clinical approach mitigates this by sorting projections into respiratory phases and reconstructing images per phase, but this does not account for breathing variability. Dynamic CBCT instead reconstructs images at each projection, capturing continuous motion without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS) offer powerful tools for modeling dynamic scenes, yet their application to dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane, use implicit motion representations, which are computationally expensive. While explicit low-rank motion models have been proposed, they lack spatial regularization, leading to inconsistencies in Gaussian motion. To address these limitations, we introduce a free-form deformation (FFD)-based spatial basis function and a deformation-informed framework that enforces consistency by coupling the temporal evolution of Gaussian’s mean position, scale, and rotation under a unified deformation field. We evaluate our approach on six CBCT datasets, demonstrating superior image quality with a 6x speedup over HexPlane. These results highlight the potential of deformation-informed 4DGS for efficient, motion-compensated CBCT reconstruction. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Yuliang-Huang/DIGS">https://github.com/Yuliang-Huang/DIGS</a>.</p>
  </div>
</details>

<hr>
<div id='stat.ME'></div>

<h1 id="stat-ME-Back"><a href="#stat-ME-Back" class="headerlink" title="stat.ME [Back]"></a>stat.ME <a href="#toc">[Back]</a></h1><h3 id="128-Using-Large-Language-Models-to-Suggest-Informative-Prior-Distributions-in-Bayesian-Statistics-stat-ME-cs-AI-cs-CLPDF"><a href="#128-Using-Large-Language-Models-to-Suggest-Informative-Prior-Distributions-in-Bayesian-Statistics-stat-ME-cs-AI-cs-CLPDF" class="headerlink" title="[128] Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics stat.ME | cs.AI | cs.CLPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21964">Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">stat.ME | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21964" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Michael A. Riegler, Kristoffer Herland Hellton, Vajira Thambawita, Hugo L. Hammer</span></p>
<p><strong>TL;DR:</strong> 论文探讨了使用大语言模型（LLMs）为贝叶斯统计中的先验分布提供建议，结果表明LLMs能够正确识别关联性，但在校准先验宽度以避免过度自信或不足方面仍存挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 贝叶斯统计中选择合适的先验分布通常具有挑战性且主观性强，论文旨在利用LLMs提供高效、客观的先验建议。</p>
<p><strong>Result:</strong> LLMs能够正确识别变量间的关联性，但生成的适度信息先验常过于自信，而Claude在生成弱信息先验时表现更优。</p>
<p><strong>Insight:</strong> LLMs在高效生成先验分布方面潜力巨大，但需进一步解决先验宽度的校准问题以提高准确性。</p>
<p><strong>Abstract:</strong> Selecting prior distributions in Bayesian statistics is challenging, resource-intensive, and subjective. We analyze using large-language models (LLMs) to suggest suitable, knowledge-based informative priors. We developed an extensive prompt asking LLMs not only to suggest priors but also to verify and reflect on their choices.   We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real datasets: heart disease risk and concrete strength. All LLMs correctly identified the direction for all associations (e.g., that heart disease risk is higher for males). The quality of suggested priors was measured by their Kullback-Leibler divergence from the maximum likelihood estimator’s distribution.   The LLMs suggested both moderately and weakly informative priors. The moderate priors were often overconfident, resulting in distributions misaligned with the data. In our experiments, Claude and Gemini provided better priors than ChatGPT. For weakly informative priors, a key performance difference emerged: ChatGPT and Gemini defaulted to an “unnecessarily vague” mean of 0, while Claude did not, demonstrating a significant advantage.   The ability of LLMs to identify correct associations shows their great potential as an efficient, objective method for developing informative priors. However, the primary challenge remains in calibrating the width of these priors to avoid over- and under-confidence.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="129-TOMD-A-Trail-based-Off-road-Multimodal-Dataset-for-Traversable-Pathway-Segmentation-under-Challenging-Illumination-Conditions-cs-RO-cs-CV-cs-LGPDF"><a href="#129-TOMD-A-Trail-based-Off-road-Multimodal-Dataset-for-Traversable-Pathway-Segmentation-under-Challenging-Illumination-Conditions-cs-RO-cs-CV-cs-LGPDF" class="headerlink" title="[129] TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions cs.RO | cs.CV | cs.LGPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21630">TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21630" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yixin Sun, Li Li, Wenke E, Amir Atapour-Abarghouei, Toby P. Breckon</span></p>
<p><strong>TL;DR:</strong> 论文提出了TOMD数据集和动态多尺度数据融合模型，专注于解决非结构化户外环境中的可通行路径分割问题，特别是在具有挑战性光照条件下的窄道场景。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有数据集和模型主要关注城市环境或宽阔的可通行道路，忽视了窄道等复杂户外场景的需求。搜索救援和森林火灾等关键应用需要更精准的可通行路径分割方法。</p>
<p><strong>Result:</strong> 实验验证了模型的有效性，并揭示了光照对分割性能的重要性。数据集和模型已开源。</p>
<p><strong>Insight:</strong> 光照是户外可通行路径分割的关键因素，动态多尺度融合能显著提升性能。窄道场景的研究填补了现有技术的空白。</p>
<p><strong>Abstract:</strong> Detecting traversable pathways in unstructured outdoor environments remains a significant challenge for autonomous robots, especially in critical applications such as wide-area search and rescue, as well as incident management scenarios like forest fires. Existing datasets and models primarily target urban settings or wide, vehicle-traversable off-road tracks, leaving a substantial gap in addressing the complexity of narrow, trail-like off-road scenarios. To address this, we introduce the Trail-based Off-road Multimodal Dataset (TOMD), a comprehensive dataset specifically designed for such environments. TOMD features high-fidelity multimodal sensor data – including 128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements – collected through repeated traversals under diverse conditions. We also propose a dynamic multiscale data fusion model for accurate traversable pathway prediction. The study analyzes the performance of early, cross, and mixed fusion strategies under varying illumination levels. Results demonstrate the effectiveness of our approach and the relevance of illumination in segmentation performance. We publicly release TOMD at <a target="_blank" rel="noopener" href="https://github.com/yyyxs1125/TMOD">https://github.com/yyyxs1125/TMOD</a> to support future research in trail-based off-road navigation.</p>
  </div>
</details>

<hr>
<h3 id="130-AeroLite-MDNet-Lightweight-Multi-task-Deviation-Detection-Network-for-UAV-Landing-cs-RO-cs-AI-cs-CVPDF"><a href="#130-AeroLite-MDNet-Lightweight-Multi-task-Deviation-Detection-Network-for-UAV-Landing-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[130] AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing cs.RO | cs.AI | cs.CVPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21635">AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21635" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haiping Yang, Huaxing Liu, Wei Wu, Zuohui Chen, Ning Wu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个轻量级的视觉模型AeroLite-MDNet，用于检测无人机（UAV）着陆时的偏差，通过多尺度融合模块和分割分支提升了偏差检测和方向估计的性能，并引入了新的评价指标AWD和数据集UAVLandData。实验结果表明模型具有高准确性和低延迟。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 无人机着陆过程中的偏差检测是一个具有挑战性的任务，尤其是GPS信号干扰等问题可能影响着陆的准确性。论文旨在通过视觉方法解决这一问题，提升无人机着陆的安全性和可靠性。</p>
<p><strong>Result:</strong> 实验结果显示模型在偏差检测上达到98.6%的准确率，AWD为0.7秒，验证了其有效性。</p>
<p><strong>Insight:</strong> 视觉方法可以弥补GPS信号的局限性，多任务学习和新的评价指标有助于提升无人机着陆系统的鲁棒性和实时性。</p>
<p><strong>Abstract:</strong> Unmanned aerial vehicles (UAVs) are increasingly employed in diverse applications such as land surveying, material transport, and environmental monitoring. Following missions like data collection or inspection, UAVs must land safely at docking stations for storage or recharging, which is an essential requirement for ensuring operational continuity. However, accurate landing remains challenging due to factors like GPS signal interference. To address this issue, we propose a deviation warning system for UAV landings, powered by a novel vision-based model called AeroLite-MDNet. This model integrates a multiscale fusion module for robust cross-scale object detection and incorporates a segmentation branch for efficient orientation estimation. We introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the system’s sensitivity to landing deviations. Furthermore, we contribute a new dataset, UAVLandData, which captures real-world landing deviation scenarios to support training and evaluation. Experimental results show that our system achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6%, demonstrating its effectiveness in enhancing UAV landing reliability. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/ITTTTTI/Maskyolo.git">https://github.com/ITTTTTI/Maskyolo.git</a></p>
  </div>
</details>

<hr>
<h3 id="131-Experimental-investigation-of-pose-informed-reinforcement-learning-for-skid-steered-visual-navigation-cs-RO-cs-AI-cs-CV-cs-LG-cs-SY-eess-SYPDF"><a href="#131-Experimental-investigation-of-pose-informed-reinforcement-learning-for-skid-steered-visual-navigation-cs-RO-cs-AI-cs-CV-cs-LG-cs-SY-eess-SYPDF" class="headerlink" title="[131] Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation cs.RO | cs.AI | cs.CV | cs.LG | cs.SY | eess.SYPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21732">Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV | cs.LG | cs.SY | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21732" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ameya Salvi, Venkat Krovi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于姿态信息的强化学习方法，用于解决滑移转向车辆在视觉导航中的挑战，并通过仿真和硬件实验验证了其优越性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 滑移转向车辆的模型化（尤其是在越野环境下）存在困难，而端到端学习方法（如模仿学习和强化学习）为解决这一问题提供了可能，但其在动态操作区域的系统化验证仍需完善。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在视觉导航任务中的表现优于现有文献中的方法。</p>
<p><strong>Insight:</strong> 姿态信息的引入可以显著提升滑移转向车辆在动态环境中的导航能力，为端到端学习方法的实际应用提供了新思路。</p>
<p><strong>Abstract:</strong> Vision-based lane keeping is a topic of significant interest in the robotics and autonomous ground vehicles communities in various on-road and off-road applications. The skid-steered vehicle architecture has served as a useful vehicle platform for human controlled operations. However, systematic modeling, especially of the skid-slip wheel terrain interactions (primarily in off-road settings) has created bottlenecks for automation deployment. End-to-end learning based methods such as imitation learning and deep reinforcement learning, have gained prominence as a viable deployment option to counter the lack of accurate analytical models. However, the systematic formulation and subsequent verification&#x2F;validation in dynamic operation regimes (particularly for skid-steered vehicles) remains a work in progress. To this end, a novel approach for structured formulation for learning visual navigation is proposed and investigated in this work. Extensive software simulations, hardware evaluations and ablation studies now highlight the significantly improved performance of the proposed approach against contemporary literature.</p>
  </div>
</details>

<hr>
<h3 id="132-Embodied-Domain-Adaptation-for-Object-Detection-cs-RO-cs-CVPDF"><a href="#132-Embodied-Domain-Adaptation-for-Object-Detection-cs-RO-cs-CVPDF" class="headerlink" title="[132] Embodied Domain Adaptation for Object Detection cs.RO | cs.CVPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21860">Embodied Domain Adaptation for Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21860" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangyu Shi, Yanyuan Qiao, Lingqiao Liu, Feras Dayoub</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种源数据无关的域自适应方法（SFDA），用于改进室内环境中物体检测的性能，特别是在动态和多样化条件下的适应性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 移动机器人在室内环境中依赖物体检测器进行感知和定位，但传统的封闭集方法和开放词汇检测（OVOD）方法在应对室内环境的多样性和动态变化时表现不佳。</p>
<p><strong>Result:</strong> 在EDAOD基准测试中表现出显著的零样本检测性能提升，并能灵活适应动态室内条件。</p>
<p><strong>Insight:</strong> 该研究表明，结合时序信息和多尺度特征可以显著提升模型在动态环境中的适应性，同时无需依赖源数据。</p>
<p><strong>Abstract:</strong> Mobile robots rely on object detectors for perception and object localization in indoor environments. However, standard closed-set methods struggle to handle the diverse objects and dynamic conditions encountered in real homes and labs. Open-vocabulary object detection (OVOD), driven by Vision Language Models (VLMs), extends beyond fixed labels but still struggles with domain shifts in indoor environments. We introduce a Source-Free Domain Adaptation (SFDA) approach that adapts a pre-trained model without accessing source data. We refine pseudo labels via temporal clustering, employ multi-scale threshold fusion, and apply a Mean Teacher framework with contrastive learning. Our Embodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates adaptation under sequential changes in lighting, layout, and object diversity. Our experiments show significant gains in zero-shot detection performance and flexible adaptation to dynamic indoor conditions.</p>
  </div>
</details>

<hr>
<h3 id="133-Evaluating-Pointing-Gestures-for-Target-Selection-in-Human-Robot-Collaboration-cs-RO-cs-CVPDF"><a href="#133-Evaluating-Pointing-Gestures-for-Target-Selection-in-Human-Robot-Collaboration-cs-RO-cs-CVPDF" class="headerlink" title="[133] Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration cs.RO | cs.CVPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22116">Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22116" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Noora Sassali, Roel Pieters</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种用于人机协作中目标选择的指向手势评估方法，结合姿态估计和几何模型，并在多模态机器人系统中验证其效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 指向手势是人机协作中的常见交互方式，但目前缺乏对其准确性和实用性的系统评估方法。</p>
<p><strong>Result:</strong> 方法在典型机器人任务中验证了指向手势的准确性，并通过多模态系统展示了实用性。</p>
<p><strong>Insight:</strong> 研究揭示了指向手势在复杂协作任务中的潜力，同时也指出了其局限性，为未来的多模态交互设计提供了参考。</p>
<p><strong>Abstract:</strong> Pointing gestures are a common interaction method used in Human-Robot Collaboration for various tasks, ranging from selecting targets to guiding industrial processes. This study introduces a method for localizing pointed targets within a planar workspace. The approach employs pose estimation, and a simple geometric model based on shoulder-wrist extension to extract gesturing data from an RGB-D stream. The study proposes a rigorous methodology and comprehensive analysis for evaluating pointing gestures and target selection in typical robotic tasks. In addition to evaluating tool accuracy, the tool is integrated into a proof-of-concept robotic system, which includes object detection, speech transcription, and speech synthesis to demonstrate the integration of multiple modalities in a collaborative application. Finally, a discussion over tool limitations and performance is provided to understand its role in multimodal robotic systems. All developments are available at: <a target="_blank" rel="noopener" href="https://github.com/NMKsas/gesture_pointer.git">https://github.com/NMKsas/gesture_pointer.git</a>.</p>
  </div>
</details>

<hr>
<h3 id="134-KnotDLO-Toward-Interpretable-Knot-Tying-cs-RO-cs-CVPDF"><a href="#134-KnotDLO-Toward-Interpretable-Knot-Tying-cs-RO-cs-CVPDF" class="headerlink" title="[134] KnotDLO: Toward Interpretable Knot Tying cs.RO | cs.CVPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22176">KnotDLO: Toward Interpretable Knot Tying</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22176" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Holly Dinkel, Raghavendra Navaratna, Jingyi Xiang, Brian Coltin, Trey Smith</span></p>
<p><strong>TL;DR:</strong> KnotDLO是一种用于单手打结的Deformable Linear Object（DLO）方法，具有鲁棒性、可重复性和可解释性，无需人工演示或训练。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的打结方法通常在遮挡或绳子初始配置变化时表现不佳，且往往依赖人工演示或训练数据。KnotDLO旨在解决这些问题，提供一种更鲁棒和自适应的解决方案。</p>
<p><strong>Result:</strong> 在16次打结实验中，KnotDLO从未见过的配置中实现了50%的成功率。</p>
<p><strong>Insight:</strong> KnotDLO展示了无需依赖人工数据的方法在复杂任务（如打结）中的潜力，同时突出了视觉推理与控制解耦的重要性。</p>
<p><strong>Abstract:</strong> This work presents KnotDLO, a method for one-handed Deformable Linear Object (DLO) knot tying that is robust to occlusion, repeatable for varying rope initial configurations, interpretable for generating motion policies, and requires no human demonstrations or training. Grasp and target waypoints for future DLO states are planned from the current DLO shape. Grasp poses are computed from indexing the tracked piecewise linear curve representing the DLO state based on the current curve shape and are piecewise continuous. KnotDLO computes intermediate waypoints from the geometry of the current DLO state and the desired next state. The system decouples visual reasoning from control. In 16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an overhand knot from previously unseen configurations.</p>
  </div>
</details>

<hr>
<div id='cs.MM'></div>

<h1 id="cs-MM-Back"><a href="#cs-MM-Back" class="headerlink" title="cs.MM [Back]"></a>cs.MM <a href="#toc">[Back]</a></h1><h3 id="135-RiverEcho-Real-Time-Interactive-Digital-System-for-Ancient-Yellow-River-Culture-cs-MM-cs-CLPDF"><a href="#135-RiverEcho-Real-Time-Interactive-Digital-System-for-Ancient-Yellow-River-Culture-cs-MM-cs-CLPDF" class="headerlink" title="[135] RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture cs.MM | cs.CLPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21865">RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MM | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21865" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haofeng Wang, Yilin Guo, Zehao Li, Tong Yue, Yizong Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了RiverEcho，一个基于大语言模型和文化知识数据集的实时交互系统，用于保护和传承古代黄河文化。系统通过语音查询和数字人讲解提供专业信息。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 黄河文化作为人类文明的重要组成部分，需要现代技术手段加以保护和传承。通过构建实时交互系统，可以多样化传播方式并提供更深层次的文化洞察。</p>
<p><strong>Result:</strong> 实验表明，RAG技术显著提升了大语言模型在黄河文化知识上的响应质量，系统生成的回答更专业、信息更丰富。</p>
<p><strong>Insight:</strong> 结合现代技术与传统文化数据库可以有效提升文化传播的深度和广度，同时为用户提供更具吸引力的互动体验。</p>
<p><strong>Abstract:</strong> The Yellow River is China’s mother river and a cradle of human civilization. The ancient Yellow River culture is, moreover, an indispensable part of human art history. To conserve and inherit the ancient Yellow River culture, we designed RiverEcho, a real-time interactive system that responds to voice queries using a large language model and a cultural knowledge dataset, delivering explanations through a talking-head digital human. Specifically, we built a knowledge database focused on the ancient Yellow River culture, including the collection of historical texts and the processing pipeline. Experimental results demonstrate that leveraging Retrieval-Augmented Generation (RAG) on the proposed dataset enhances the response quality of the Large Language Model(LLM), enabling the system to generate more professional and informative responses. Our work not only diversifies the means of promoting Yellow River culture but also provides users with deeper cultural insights.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="136-3Description-An-Intuitive-Human-AI-Collaborative-3D-Modeling-Approach-cs-HC-cs-AI-cs-CL-cs-GR-I-2-I-2-1-I-2-7-I-3-H-5-J-5PDF"><a href="#136-3Description-An-Intuitive-Human-AI-Collaborative-3D-Modeling-Approach-cs-HC-cs-AI-cs-CL-cs-GR-I-2-I-2-1-I-2-7-I-3-H-5-J-5PDF" class="headerlink" title="[136] 3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach cs.HC | cs.AI | cs.CL | cs.GR | I.2; I.2.1; I.2.7; I.3; H.5; J.5PDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21845">3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.AI | cs.CL | cs.GR | I.2; I.2.1; I.2.7; I.3; H.5; J.5</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21845" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhuodi Cai</span></p>
<p><strong>TL;DR:</strong> </p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 3D建模传统上需要专业技能，存在较高的学习门槛，限制了非专业人士的参与。3Description旨在通过结合人类自然语言和手势输入与AI技术（如NLP和计算机视觉），降低3D建模的复杂度，使其更易用和普及。</p>
<p><strong>Result:</strong> 3Description实现了非专业人士通过自然交互方式参与3D建模的目标，同时保留了人类创造力，避免了过度依赖技术。</p>
<p><strong>Insight:</strong> 该研究展示了人机协作在降低专业工具门槛方面的潜力，同时强调了人类在创意过程中的核心地位。</p>
<p><strong>Abstract:</strong> This paper presents 3Description, an experimental human-AI collaborative approach for intuitive 3D modeling. 3Description aims to address accessibility and usability challenges in traditional 3D modeling by enabling non-professional individuals to co-create 3D models using verbal and gesture descriptions. Through a combination of qualitative research, product analysis, and user testing, 3Description integrates AI technologies such as Natural Language Processing and Computer Vision, powered by OpenAI and MediaPipe. Recognizing the web has wide cross-platform capabilities, 3Description is web-based, allowing users to describe the desired model and subsequently adjust its components using verbal and gestural inputs. In the era of AI and emerging media, 3Description not only contributes to a more inclusive and user-friendly design process, empowering more people to participate in the construction of the future 3D world, but also strives to increase human engagement in co-creation with AI, thereby avoiding undue surrender to technology and preserving human creativity.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="137-Hierarchical-Patch-Compression-for-ColPali-Efficient-Multi-Vector-Document-Retrieval-with-Dynamic-Pruning-and-Quantization-cs-IR-cs-CVPDF"><a href="#137-Hierarchical-Patch-Compression-for-ColPali-Efficient-Multi-Vector-Document-Retrieval-with-Dynamic-Pruning-and-Quantization-cs-IR-cs-CVPDF" class="headerlink" title="[137] Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization cs.IR | cs.CVPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21601">Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21601" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Duong Bach</span></p>
<p><strong>TL;DR:</strong> HPC-ColPali 是一个用于多向量文档检索的层次化压缩框架，通过 K-Means 量化、动态剪枝和二进制编码技术显著提升了效率和存储利用率，同时保持了检索精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多向量文档检索系统（如 ColPali）因其依赖高维补丁嵌入和延迟交互评分，存储和计算成本高昂。作者提出 HPC-ColPali 以解决这些问题。</p>
<p><strong>Result:</strong> 在 ViDoRe 和 SEC-Filings 数据集上，HPC-ColPali 降低查询延迟 30-50%，同时保持高精度；在 RAG 管道中减少幻觉率 30%，延迟减半。</p>
<p><strong>Insight:</strong> 层次化压缩技术（量化、剪枝、编码）可高效优化多向量检索系统，适用于资源受限环境，同时保持性能。</p>
<p><strong>Abstract:</strong> Multi-vector document retrieval systems, such as ColPali, excel in fine-grained matching for complex queries but incur significant storage and computational costs due to their reliance on high-dimensional patch embeddings and late-interaction scoring. To address these challenges, we propose HPC-ColPali, a Hierarchical Patch Compression framework that enhances the efficiency of ColPali while preserving its retrieval accuracy. Our approach integrates three innovative techniques: (1) K-Means quantization, which compresses patch embeddings into 1-byte centroid indices, achieving up to 32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing Vision-Language Model attention weights to retain only the top-$p%$ most salient patches, reducing late-interaction computation by up to 60% with less than 2% nDCG@10 loss; and (3) optional binary encoding of centroid indices into $b$-bit strings ($b&#x3D;\lceil\log_2 K\rceil$), enabling rapid Hamming distance-based similarity search for resource-constrained environments. Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30–50% lower query latency under HNSW indexing while maintaining high retrieval precision. When integrated into a Retrieval-Augmented Generation pipeline for legal summarization, it reduces hallucination rates by 30% and halves end-to-end latency. These advancements establish HPC-ColPali as a scalable and efficient solution for multi-vector document retrieval across diverse applications. Code is available at <a target="_blank" rel="noopener" href="https://github.com/DngBack/HPC-ColPali">https://github.com/DngBack/HPC-ColPali</a>.</p>
  </div>
</details>

<hr>
<h3 id="138-Evaluating-VisualRAG-Quantifying-Cross-Modal-Performance-in-Enterprise-Document-Understanding-cs-IR-cs-AI-cs-CV-cs-HC-cs-LGPDF"><a href="#138-Evaluating-VisualRAG-Quantifying-Cross-Modal-Performance-in-Enterprise-Document-Understanding-cs-IR-cs-AI-cs-CV-cs-HC-cs-LGPDF" class="headerlink" title="[138] Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding cs.IR | cs.AI | cs.CV | cs.HC | cs.LGPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21604">Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.AI | cs.CV | cs.HC | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21604" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Varun Mannam, Fang Wang, Xin Chen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个系统化的量化评估框架VisualRAG，用于衡量企业文档理解中多模态输入的可信度，优化模态权重后性能提升57.3%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前多模态生成AI的评估框架难以建立可信度，阻碍了企业对可靠AI的采用，需开发定量评估方法。</p>
<p><strong>Result:</strong> 最优权重（文本30%、图像15%、标题25%、OCR30%）性能提升57.3%，并保持计算效率。</p>
<p><strong>Insight:</strong> 多模态权重优化可显著提升性能，且不同基础模型对标题生成和OCR提取的可信度影响显著。</p>
<p><strong>Abstract:</strong> Current evaluation frameworks for multimodal generative AI struggle to establish trustworthiness, hindering enterprise adoption where reliability is paramount. We introduce a systematic, quantitative benchmarking framework to measure the trustworthiness of progressively integrating cross-modal inputs such as text, images, captions, and OCR within VisualRAG systems for enterprise document intelligence. Our approach establishes quantitative relationships between technical metrics and user-centric trust measures. Evaluation reveals that optimal modality weighting with weights of 30% text, 15% image, 25% caption, and 30% OCR improves performance by 57.3% over text-only baselines while maintaining computational efficiency. We provide comparative assessments of foundation models, demonstrating their differential impact on trustworthiness in caption generation and OCR extraction-a vital consideration for reliable enterprise AI. This work advances responsible AI deployment by providing a rigorous framework for quantifying and enhancing trustworthiness in multimodal RAG for critical enterprise applications.</p>
  </div>
</details>

<hr>
<h3 id="139-CAL-RAG-Retrieval-Augmented-Multi-Agent-Generation-for-Content-Aware-Layout-Design-cs-IR-cs-CV-I-3-3-I-2-11-H-5-2PDF"><a href="#139-CAL-RAG-Retrieval-Augmented-Multi-Agent-Generation-for-Content-Aware-Layout-Design-cs-IR-cs-CV-I-3-3-I-2-11-H-5-2PDF" class="headerlink" title="[139] CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design cs.IR | cs.CV | I.3.3; I.2.11; H.5.2PDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21934">CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CV | I.3.3; I.2.11; H.5.2</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21934" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Najmeh Forouzandehmehr, Reza Yousefi Maragheh, Sriram Kollipara, Kai Zhao, Topojoy Biswas</span></p>
<p><strong>TL;DR:</strong> CAL-RAG是一个基于检索增强的多智能体框架，用于内容感知的布局生成，结合了多模态检索、大型语言模型和协作式智能体推理，显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动化的内容感知布局生成在智能设计系统中是一个基础但未被充分探索的问题，现有方法缺乏上下文设计示例的参考，且难以处理语义对齐和视觉一致性。</p>
<p><strong>Result:</strong> 在PKU PosterLayout数据集上，CAL-RAG在多项布局指标（如底层效果、元素对齐和重叠）上优于现有基线，如LayoutPrompter。</p>
<p><strong>Insight:</strong> 结合检索增强和多步智能体推理能够显著提升自动化布局生成的性能，同时提供了可扩展和可解释的解决方案。</p>
<p><strong>Abstract:</strong> Automated content-aware layout generation – the task of arranging visual elements such as text, logos, and underlays on a background canvas – remains a fundamental yet under-explored problem in intelligent design systems. While recent advances in deep generative models and large language models (LLMs) have shown promise in structured content generation, most existing approaches lack grounding in contextual design exemplars and fall short in handling semantic alignment and visual coherence. In this work we introduce CAL-RAG, a retrieval-augmented, agentic framework for content-aware layout generation that integrates multimodal retrieval, large language models, and collaborative agentic reasoning. Our system retrieves relevant layout examples from a structured knowledge base and invokes an LLM-based layout recommender to propose structured element placements. A vision-language grader agent evaluates the layout with visual metrics, and a feedback agent provides targeted refinements, enabling iterative improvement. We implement our framework using LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in semantic and structural variability. CAL-RAG achieves state-of-the-art performance across multiple layout metrics – including underlay effectiveness, element alignment, and overlap – substantially outperforming strong baselines such as LayoutPrompter. These results demonstrate that combining retrieval augmentation with agentic multi-step reasoning yields a scalable, interpretable, and high-fidelity solution for automated layout generation.</p>
  </div>
</details>

<hr>
<h3 id="140-ARAG-Agentic-Retrieval-Augmented-Generation-for-Personalized-Recommendation-cs-IR-cs-AI-cs-CL-cs-MA-I-2-11-I-2-7-H-3-3PDF"><a href="#140-ARAG-Agentic-Retrieval-Augmented-Generation-for-Personalized-Recommendation-cs-IR-cs-AI-cs-CL-cs-MA-I-2-11-I-2-7-H-3-3PDF" class="headerlink" title="[140] ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation cs.IR | cs.AI | cs.CL | cs.MA | I.2.11; I.2.7; H.3.3PDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21931">ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.AI | cs.CL | cs.MA | I.2.11; I.2.7; H.3.3</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21931" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Reza Yousefi Maragheh, Pratheek Vadla, Priyank Gupta, Kai Zhao, Aysenur Inan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为ARAG的新型框架，通过多智能体协作机制改进检索增强生成（RAG）方法，以更好地捕获动态推荐场景中的用户偏好。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的RAG方法依赖静态检索启发式方法，难以捕捉动态推荐场景中的用户偏好。ARAG的动机是通过引入多智能体协作机制，提升推荐系统的个性化能力。</p>
<p><strong>Result:</strong> 实验表明，ARAG在NDCG@5和Hit@5指标上分别比标准RAG和基于时效的基线方法提升了42.1%和35.5%。消融研究验证了各组件的重要性。</p>
<p><strong>Insight:</strong> 该研究表明，在基于LLM的推荐系统中引入智能体协作机制，可以更有效地建模用户偏好和动态意图，为个性化推荐提供了新方向。</p>
<p><strong>Abstract:</strong> Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="141-APO-Enhancing-Reasoning-Ability-of-MLLMs-via-Asymmetric-Policy-Optimization-cs-LG-cs-AI-cs-CVPDF"><a href="#141-APO-Enhancing-Reasoning-Ability-of-MLLMs-via-Asymmetric-Policy-Optimization-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[141] APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization cs.LG | cs.AI | cs.CVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21655">APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21655" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Asymmetric Policy Optimization (APO)方法，通过Difficulty-Adaptive Divergence Shaping (DADS)和Suboptimal Trajectory Complexity Regularization (STCR)技术，增强多模态大语言模型(MLLMs)的推理能力，同时避免通用任务性能下降和过度推理的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型(MLLMs)在复杂推理任务上表现不佳，而传统的强化学习(RL)方法容易导致通用任务性能下降或生成过度冗长的推理。</p>
<p><strong>Result:</strong> View-R1-3B模型在推理基准上比基准模型平均提升7%，且不牺牲通用任务性能，甚至优于更大的MLLMs模型(7-11B)。</p>
<p><strong>Insight:</strong> 1. 动态调整KL散度权重有助于保持模型知识并提升训练稳定性；2. 惩罚冗长响应能够有效抑制过度推理，同时保留模型的探索能力。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) are powerful at integrating diverse data, but they often struggle with complex reasoning. While Reinforcement learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky. Common issues include a drop in performance on general tasks and the generation of overly detailed or “overthinking” reasoning. Our work investigates how the KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric Policy Optimization (APO) to address these issues, which divides the sampled responses into positive and negative groups. For positive samples, Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically adjust the KL divergence weight based on their difficulty. This method prevents policy entropy from dropping sharply, improves training stability, utilizes samples better, and preserves the model’s existing knowledge. For negative samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to penalize overly long responses. This helps mitigate overthinking and encourages more concise reasoning while preserving the model’s explorative capacity. We apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B significantly enhances reasoning capabilities, showing an average 7% gain over the base model and outperforming larger MLLMs (7-11B) on various reasoning benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade on general tasks, View-R1-3B maintains consistent improvement, demonstrating superior generalization. These results highlight the effectiveness and broad applicability of our DADS and STCR techniques for advancing complex multimodal reasoning in MLLMs. The code will be made available at <a target="_blank" rel="noopener" href="https://github.com/Indolent-Kawhi/View-R1">https://github.com/Indolent-Kawhi/View-R1</a>.</p>
  </div>
</details>

<hr>
<h3 id="142-SceneDiffuser-City-Scale-Traffic-Simulation-via-a-Generative-World-Model-cs-LG-cs-AI-cs-CV-cs-MA-cs-ROPDF"><a href="#142-SceneDiffuser-City-Scale-Traffic-Simulation-via-a-Generative-World-Model-cs-LG-cs-AI-cs-CV-cs-MA-cs-ROPDF" class="headerlink" title="[142] SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model cs.LG | cs.AI | cs.CV | cs.MA | cs.ROPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.21976">SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | cs.MA | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.21976" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuhan Tan, John Lambert, Hong Jeon, Sakshum Kulshrestha, Yijing Bai</span></p>
<p><strong>TL;DR:</strong> SceneDiffuser++是首个端到端的生成式世界模型，通过单一损失函数训练，实现城市规模的A点到B点交通模拟，整合了场景生成、动态代理行为建模和环境模拟等功能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目标是通过生成式模拟技术扩展有限的人工驾驶数据，实现城市规模的交通模拟，从而支持自动驾驶软件的测试与验证。</p>
<p><strong>Result:</strong> 在城市规模的交通模拟中展现出卓越的真实性，尤其是在长时模拟条件下。</p>
<p><strong>Insight:</strong> 通过整合多技术模块的端到端模型，能够更高效地实现复杂的城市交通模拟，为自动驾驶测试提供了更强大的工具。</p>
<p><strong>Abstract:</strong> The goal of traffic simulation is to augment a potentially limited amount of manually-driven miles that is available for testing and validation, with a much larger amount of simulated synthetic miles. The culmination of this vision would be a generative simulated city, where given a map of the city and an autonomous vehicle (AV) software stack, the simulator can seamlessly simulate the trip from point A to point B by populating the city around the AV and controlling all aspects of the scene, from animating the dynamic agents (e.g., vehicles, pedestrians) to controlling the traffic light states. We refer to this vision as CitySim, which requires an agglomeration of simulation technologies: scene generation to populate the initial scene, agent behavior modeling to animate the scene, occlusion reasoning, dynamic scene generation to seamlessly spawn and remove agents, and environment simulation for factors such as traffic lights. While some key technologies have been separately studied in various works, others such as dynamic scene generation and environment simulation have received less attention in the research community. We propose SceneDiffuser++, the first end-to-end generative world model trained on a single loss function capable of point A-to-B simulation on a city scale integrating all the requirements above. We demonstrate the city-scale traffic simulation capability of SceneDiffuser++ and study its superior realism under long simulation conditions. We evaluate the simulation quality on an augmented version of the Waymo Open Motion Dataset (WOMD) with larger map regions to support trip-level simulation.</p>
  </div>
</details>

<hr>
<h3 id="143-Unfolding-Generative-Flows-with-Koopman-Operators-Fast-and-Interpretable-Sampling-cs-LG-cs-CVPDF"><a href="#143-Unfolding-Generative-Flows-with-Koopman-Operators-Fast-and-Interpretable-Sampling-cs-LG-cs-CVPDF" class="headerlink" title="[143] Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling cs.LG | cs.CVPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22304">Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22304" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Erkan Turan, Aristotelis Siozopoulos, Maks Ovsjanikov</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于Koopman算子的生成流模型（Koopman-CFM），通过将非线性流建模为可学习观测空间中的线性演化，实现了快速且可解释的采样。相比传统条件流匹配（CFM），该方法显著提升了采样效率，并提供了生成过程的结构化分析工具。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统条件流匹配（CFM）依赖于数值求解非线性常微分方程（ODE），计算成本高且难以解释。现有的加速方法（如轨迹拉直或蒸馏）未揭示生成过程的结构。论文旨在解决这些问题，提出一种兼具高效采样和可解释性的方法。</p>
<p><strong>Result:</strong> 在2D合成数据集和真实数据集（MNIST、F-MNIST、TFD）上，Koopman-CFM显著优于传统CFM的采样速度，同时提供了对生成过程的谱分析能力。</p>
<p><strong>Insight:</strong> Koopman算子理论为生成模型提供了一种将非线性动力学线性化的有效工具，不仅加速了采样，还增强了模型的可解释性。这种结合了高效性和结构分析的方法，为生成建模提供了新的思路。</p>
<p><strong>Abstract:</strong> Conditional Flow Matching (CFM) offers a simulation-free framework for training continuous-time generative models, bridging diffusion and flow-based approaches. However, sampling from CFM still relies on numerically solving non-linear ODEs which can be computationally expensive and difficult to interpret. Recent alternatives address sampling speed via trajectory straightening, mini-batch coupling or distillation. However, these methods typically do not shed light on the underlying \textit{structure} of the generative process. In this work, we propose to accelerate CFM and introduce an interpretable representation of its dynamics by integrating Koopman operator theory, which models non-linear flows as linear evolution in a learned space of observables. We introduce a decoder-free Koopman-CFM architecture that learns an embedding where the generative dynamics become linear, enabling closed-form, one-step sampling via matrix exponentiation. This results in significant speedups over traditional CFM as demonstrated on controlled 2D datasets and real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face Dataset (TFD). Unlike previous methods, our approach leads to a well-structured Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions offer principled tools for analyzing generative behavior such as temporal scaling, mode stability, and decomposition in Koopman latent space. By combining sampling efficiency with analytical structure, Koopman-enhanced flow matching offers a potential step toward fast and interpretable generative modeling.</p>
  </div>
</details>

<hr>
<h3 id="144-Probabilistic-Optimality-for-Inference-time-Scaling-cs-LG-cs-AI-cs-CLPDF"><a href="#144-Probabilistic-Optimality-for-Inference-time-Scaling-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[144] Probabilistic Optimality for Inference-time Scaling cs.LG | cs.AI | cs.CLPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.22376">Probabilistic Optimality for Inference-time Scaling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2506.22376" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei, Qing Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种概率框架，用于推理时扩展的最优性，设计了一种动态确定最优样本数的算法OptScale，显著减少采样开销，同时在数学推理任务上保持或优于现有性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有推理时扩展方法多基于启发式策略，缺乏理论基础。论文旨在填补这一空白，为推理时扩展提供理论指导。</p>
<p><strong>Result:</strong> OptScale在数学推理基准（MATH-500等）上显著减少采样开销，性能与现有方法相当或更好。</p>
<p><strong>Insight:</strong> 1. 推理时扩展需要理论支撑。2. 动态调整样本数量能提高计算效率。</p>
<p><strong>Abstract:</strong> Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-N selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on mathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-07-02/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-06-28/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
