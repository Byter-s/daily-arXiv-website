<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-06-15 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-06-15
            
            
        </div>
        <span class="post-date">
            Jun 15, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 35]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 77]</li>
<li><a href="#cs.MM">cs.MM</a> [Total: 6]</li>
<li><a href="#cs.GR">cs.GR</a> [Total: 2]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 8]</li>
<li><a href="#eess.SY">eess.SY</a> [Total: 2]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
<li><a href="#cs.MA">cs.MA</a> [Total: 1]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 6]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 1]</li>
<li><a href="#cs.SD">cs.SD</a> [Total: 1]</li>
<li><a href="#physics.med-ph">physics.med-ph</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 3]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 1]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-TaskCraft-Automated-Generation-of-Agentic-Tasks"><a href="#1-TaskCraft-Automated-Generation-of-Agentic-Tasks" class="headerlink" title="[1] TaskCraft: Automated Generation of Agentic Tasks"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10055">TaskCraft: Automated Generation of Agentic Tasks</a></h3><p><em>Dingfeng Shi,Jingyi Cao,Qianben Chen,Weichen Sun,Weizhen Li,Hongxuan Lu,Fangchen Dong,Tianrui Qin,King Zhu,Minghao Yang,Jian Yang,Ge Zhang,Jiaheng Liu,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: TaskCraft是一种自动生成具有多工具交互、可扩展难度和可验证执行轨迹的代理任务的框架，解决了现有数据集中工具交互不足和人工标注成本高的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的指令数据缺乏工具交互，而代理任务基准主要依赖人工标注，成本高且难以扩展。因此，需要一种自动化的方法来生成多样化、难度可控的代理任务。

<p>Contribution: 1. 提出了TaskCraft框架，自动生成多工具交互、可扩展难度的代理任务；2. 通过深度和宽度扩展方法生成结构化和层次化复杂的任务；3. 发布了包含3.6万个任务的合成数据集。</p>
<p>Method: 1. 利用深度和宽度扩展方法扩展原子任务；2. 生成带有执行轨迹的多工具任务；3. 优化提示生成和监督微调流程。</p>
<p>Result: 实验表明，生成的任务改进了提示优化和监督微调的效果，支持了代理模型的性能提升。</p>
<p>Insight: 自动化任务生成是解决代理任务数据稀缺和标注成本高的有效途径，同时可通过调整扩展方式控制任务难度。</p>
<p>Abstract: Agentic tasks, which require multi-step problem solving with autonomy, tool<br>use, and adaptive reasoning, are becoming increasingly central to the<br>advancement of NLP and AI. However, existing instruction data lacks tool<br>interaction, and current agentic benchmarks rely on costly human annotation,<br>limiting their scalability. We introduce \textsc{TaskCraft}, an automated<br>workflow for generating difficulty-scalable, multi-tool, and verifiable agentic<br>tasks with execution trajectories. TaskCraft expands atomic tasks using<br>depth-based and width-based extensions to create structurally and<br>hierarchically complex challenges. Empirical results show that these tasks<br>improve prompt optimization in the generation workflow and enhance supervised<br>fine-tuning of agentic foundation models. We present a large-scale synthetic<br>dataset of approximately 36,000 tasks with varying difficulty to support future<br>research on agent tuning and evaluation.</p>
</details>


<h3 id="2-Chat-of-Thought-Collaborative-Multi-Agent-System-for-Generating-Domain-Specific-Information"><a href="#2-Chat-of-Thought-Collaborative-Multi-Agent-System-for-Generating-Domain-Specific-Information" class="headerlink" title="[2] Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10086">Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information</a></h3><p><em>Christodoulos Constantinides,Shuxin Lin,Nianjun Zhou,Dhaval Patel</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文提出了一种名为Chat-of-Thought的多代理系统，用于生成工业资产的FMEA文档。该系统通过多角色协作的LLM代理和动态任务路由优化内容生成与验证。</p>
<details>
  <summary>Details</summary>
Motivation: 工业设备监控领域的FMEA文档生成面临高效性和准确性挑战，传统方法难以满足需求。

<p>Contribution: 提出了一种融合动态多角色讨论的Chat-of-Thought系统，优化了FMEA文档的生成和迭代细化。</p>
<p>Method: 采用多角色LLM代理协作和动态任务路由，结合模板驱动工作流和上下文感知的代理协作。</p>
<p>Result: 展示了Chat-of-Thought在工业设备监控领域的潜力，能够高效生成和验证FMEA文档。</p>
<p>Insight: 多代理协作和动态讨论能够显著提升领域特定信息的生成质量与效率。</p>
<p>Abstract: This paper presents a novel multi-agent system called Chat-of-Thought,<br>designed to facilitate the generation of Failure Modes and Effects Analysis<br>(FMEA) documents for industrial assets. Chat-of-Thought employs multiple<br>collaborative Large Language Model (LLM)-based agents with specific roles,<br>leveraging advanced AI techniques and dynamic task routing to optimize the<br>generation and validation of FMEA tables. A key innovation in this system is<br>the introduction of a Chat of Thought, where dynamic, multi-persona-driven<br>discussions enable iterative refinement of content. This research explores the<br>application domain of industrial equipment monitoring, highlights key<br>challenges, and demonstrates the potential of Chat-of-Thought in addressing<br>these challenges through interactive, template-driven workflows and<br>context-aware agent collaboration.</p>
</details>


<h3 id="3-ChartReasoner-Code-Driven-Modality-Bridging-for-Long-Chain-Reasoning-in-Chart-Question-Answering"><a href="#3-ChartReasoner-Code-Driven-Modality-Bridging-for-Long-Chain-Reasoning-in-Chart-Question-Answering" class="headerlink" title="[3] ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10116">ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering</a></h3><p><em>Caijun Jia,Nan Xu,Jingxuan Wei,Qingli Wang,Lei Wang,Bihui Yu,Junnan Zhu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ChartReasoner是一个两阶段的框架，通过代码驱动的方式实现图表问答任务中的长链推理，通过高保真的图表转换和自动合成数据，提升了多模态推理的精确性和可解释性。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的视觉推理任务通常将视觉信息转换为文本进行推理，但会丢失图表中的结构和语义信息。尤其在图表问答任务中，这会导致关键细节的缺失。

<p>Contribution: 提出了ChartReasoner框架：1. 高保真地将图表转换为结构化代码；2. 设计了自动合成图表推理数据的方法；3. 结合监督学习和强化学习训练多模态模型。</p>
<p>Method: 1. 训练模型将图表图像转换为ECharts代码；2. 利用代码验证器自动生成和筛选高质量推理数据；3. 结合监督微调和强化学习训练最终模型。</p>
<p>Result: 在四个公开基准测试中，ChartReasoner在保留图表细节和推理性能上表现优异，参数更少的情况下接近GPT-4o的性能。</p>
<p>Insight: 通过代码驱动的模态转换和自动数据合成，可以在保留视觉细节的同时实现高效的多模态推理，为视觉推理任务提供了一种新思路。</p>
<p>Abstract: Recently, large language models have shown remarkable reasoning capabilities<br>through long-chain reasoning before responding. However, how to extend this<br>capability to visual reasoning tasks remains an open challenge. Existing<br>multimodal reasoning approaches transfer such visual reasoning task into<br>textual reasoning task via several image-to-text conversions, which often lose<br>critical structural and semantic information embedded in visualizations,<br>especially for tasks like chart question answering that require a large amount<br>of visual details. To bridge this gap, we propose ChartReasoner, a code-driven<br>novel two-stage framework designed to enable precise, interpretable reasoning<br>over charts. We first train a high-fidelity model to convert diverse chart<br>images into structured ECharts codes, preserving both layout and data semantics<br>as lossless as possible. Then, we design a general chart reasoning data<br>synthesis pipeline, which leverages this pretrained transport model to<br>automatically and scalably generate chart reasoning trajectories and utilizes a<br>code validator to filter out low-quality samples. Finally, we train the final<br>multimodal model using a combination of supervised fine-tuning and<br>reinforcement learning on our synthesized chart reasoning dataset and<br>experimental results on four public benchmarks clearly demonstrate the<br>effectiveness of our proposed ChartReasoner. It can preserve the original<br>details of the charts as much as possible and perform comparably with<br>state-of-the-art open-source models while using fewer parameters, approaching<br>the performance of proprietary systems like GPT-4o in out-of-domain settings.</p>
</details>


<h3 id="4-Unsupervised-Elicitation-of-Language-Models"><a href="#4-Unsupervised-Elicitation-of-Language-Models" class="headerlink" title="[4] Unsupervised Elicitation of Language Models"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10139">Unsupervised Elicitation of Language Models</a></h3><p><em>Jiaxin Wen,Zachary Ankner,Arushi Somani,Peter Hase,Samuel Marks,Jacob Goldman-Wetzler,Linda Petrini,Henry Sleight,Collin Burns,He He,Shi Feng,Ethan Perez,Jan Leike</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了一种无监督算法ICM，通过在自生成标签上微调预训练语言模型，无需外部监督，优于传统的人类监督方法。</p>
<details>
  <summary>Details</summary>
Motivation: 针对超人类能力的语言模型，高质量的人类监督难以获取，论文旨在解决这一问题。

<p>Contribution: 提出了无监督算法ICM，能够在无外部监督的情况下有效微调语言模型。</p>
<p>Method: 使用Internal Coherence Maximization (ICM)算法，通过最大化模型内部一致性微调模型。</p>
<p>Result: 在多个任务上表现优于人类监督方法，并能提升前沿语言模型的训练效果。</p>
<p>Insight: 无监督方法可以超越人类监督，尤其是在模型能力远超人类的任务上。</p>
<p>Abstract: To steer pretrained language models for downstream tasks, today’s<br>post-training paradigm relies on humans to specify desired behaviors. However,<br>for models with superhuman capabilities, it is difficult or impossible to get<br>high-quality human supervision. To address this challenge, we introduce a new<br>unsupervised algorithm, Internal Coherence Maximization (ICM), to fine-tune<br>pretrained language models on their own generated labels, \emph{without<br>external supervision}. On GSM8k-verification, TruthfulQA, and Alpaca reward<br>modeling tasks, our method matches the performance of training on golden<br>supervision and outperforms training on crowdsourced human supervision. On<br>tasks where LMs’ capabilities are strongly superhuman, our method can elicit<br>those capabilities significantly better than training on human labels. Finally,<br>we show that our method can improve the training of frontier LMs: we use our<br>method to train an unsupervised reward model and use reinforcement learning to<br>train a Claude 3.5 Haiku-based assistant. Both the reward model and the<br>assistant outperform their human-supervised counterparts.</p>
</details>


<h3 id="5-Can-LLMs-Generate-Good-Stories-Insights-and-Challenges-from-a-Narrative-Planning-Perspective"><a href="#5-Can-LLMs-Generate-Good-Stories-Insights-and-Challenges-from-a-Narrative-Planning-Perspective" class="headerlink" title="[5] Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10161">Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective</a></h3><p><em>Yi Wang,Max Kreminski</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文研究了大型语言模型（LLM）在故事生成中的能力，重点关注叙事规划问题。通过提出基于文学例子的评估基准，研究发现GPT-4级别的LLM在小规模下可以生成因果合理的故事，但在角色意图性和戏剧冲突方面仍存在挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 故事生成是LLM的重要应用领域，但对其能力的理解有限，主要由于自动评估方法的不足和人工评估的高成本与主观性。

<p>Contribution: 提出了基于文学例子的叙事规划评估基准，明确了LLM在因果合理性、角色意图性和戏剧冲突方面的表现，并揭示了其在复杂推理方面的局限。</p>
<p>Method: 通过设定叙事规划任务评估LLM的故事生成能力，分析了因果合理性、角色意图性和戏剧冲突等关键指标。</p>
<p>Result: GPT-4级别的LLM在小规模故事中表现良好，但在角色意图性和戏剧冲突方面仍需强化学习支持的复杂推理。</p>
<p>Insight: LLM在叙事规划中的能力受限于规模与复杂度，未来需结合强化学习等技术提升复杂推理能力。</p>
<p>Abstract: Story generation has been a prominent application of Large Language Models<br>(LLMs). However, understanding LLMs’ ability to produce high-quality stories<br>remains limited due to challenges in automatic evaluation methods and the high<br>cost and subjectivity of manual evaluation. Computational narratology offers<br>valuable insights into what constitutes a good story, which has been applied in<br>the symbolic narrative planning approach to story generation. This work aims to<br>deepen the understanding of LLMs’ story generation capabilities by using them<br>to solve narrative planning problems. We present a benchmark for evaluating<br>LLMs on narrative planning based on literature examples, focusing on causal<br>soundness, character intentionality, and dramatic conflict. Our experiments<br>show that GPT-4 tier LLMs can generate causally sound stories at small scales,<br>but planning with character intentionality and dramatic conflict remains<br>challenging, requiring LLMs trained with reinforcement learning for complex<br>reasoning. The results offer insights on the scale of stories that LLMs can<br>generate while maintaining quality from different aspects. Our findings also<br>highlight interesting problem solving behaviors and shed lights on challenges<br>and considerations for applying LLM narrative planning in game environments.</p>
</details>


<h3 id="6-Q2E-Query-to-Event-Decomposition-for-Zero-Shot-Multilingual-Text-to-Video-Retrieval"><a href="#6-Q2E-Query-to-Event-Decomposition-for-Zero-Shot-Multilingual-Text-to-Video-Retrieval" class="headerlink" title="[6] Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10202">Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval</a></h3><p><em>Shubhashis Roy Dipta,Francis Ferraro</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: Q2E提出了一种零样本多语言文本到视频检索的方法，通过LLMs和VLMs的潜在知识分解查询，提升了复杂事件的视频检索能力。</p>
<details>
  <summary>Details</summary>
Motivation: 利用LLMs和VLMs的潜在参数知识，改进复杂事件的视频检索，解决人类查询过于简化的问题。

<p>Contribution: 提出Q2E方法，支持零样本多语言视频检索，并可跨数据集、领域、LLMs&#x2F;VLMs适配；展示了音频信息对检索的显著提升。</p>
<p>Method: 将查询分解为事件相关子任务，利用LLMs&#x2F;VLMs知识增强查询理解；采用基于熵的融合评分进行多模态零样本融合。</p>
<p>Result: 在多个数据集和检索指标上优于现有方法，音频信息的集成显著提升了性能。</p>
<p>Insight: 复杂事件检索可通过分解查询和多模态融合优化；音频信息在多模态检索中不可或缺。</p>
<p>Abstract: Recent approaches have shown impressive proficiency in extracting and<br>leveraging parametric knowledge from Large-Language Models (LLMs) and<br>Vision-Language Models (VLMs). In this work, we consider how we can improve the<br>identification and retrieval of videos related to complex real-world events by<br>automatically extracting latent parametric knowledge about those events. We<br>present Q2E: a Query-to-Event decomposition method for zero-shot multilingual<br>text-to-video retrieval, adaptable across datasets, domains, LLMs, or VLMs. Our<br>approach demonstrates that we can enhance the understanding of otherwise overly<br>simplified human queries by decomposing the query using the knowledge embedded<br>in LLMs and VLMs. We additionally show how to apply our approach to both visual<br>and speech-based inputs. To combine this varied multimodal knowledge, we adopt<br>entropy-based fusion scoring for zero-shot fusion. Through evaluations on two<br>diverse datasets and multiple retrieval metrics, we demonstrate that Q2E<br>outperforms several state-of-the-art baselines. Our evaluation also shows that<br>integrating audio information can significantly improve text-to-video<br>retrieval. We have released code and data for future research.</p>
</details>


<h3 id="7-TTT-Bench-A-Benchmark-for-Evaluating-Reasoning-Ability-with-Simple-and-Novel-Tic-Tac-Toe-style-Games"><a href="#7-TTT-Bench-A-Benchmark-for-Evaluating-Reasoning-Ability-with-Simple-and-Novel-Tic-Tac-Toe-style-Games" class="headerlink" title="[7] TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10209">TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games</a></h3><p><em>Prakamya Mishra,Jiang Liu,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: TTT-Bench是一个新的评测基准，通过四种简单的井字棋变体游戏评测大型推理模型（LRMs）的基础策略、空间和逻辑推理能力，发现这些模型虽然在复杂数学问题上表现优异，但在简单推理游戏中表现不佳。</p>
<details>
  <summary>Details</summary>
Motivation: 现有评测基准主要集中在STEM领域，而LRMs在更广泛的任务领域中的推理能力尚未充分探索。通过设计简单游戏评测基础推理能力，填补了这一空白。

<p>Contribution: 1. 提出TTT-Bench基准，包含四种简单但需策略推理的井字棋变体游戏；2. 发现LRMs在简单推理任务中表现显著低于复杂数学任务，揭示了其局限性。</p>
<p>Method: 采用程序化方法生成可验证的两人游戏问题，评测多种先进LRMs的表现，特别关注其对对手意图和空间配置的推理能力。</p>
<p>Result: 评测结果显示，LRMs在TTT-Bench上的表现平均比MATH 500和AIME 2024低41%和5%，尤其是在长期策略推理任务中表现较差。</p>
<p>Insight: 大型推理模型在复杂任务上的优异表现可能掩盖了其在基础推理能力上的不足，这为进一步优化模型提供了方向。</p>
<p>Abstract: Large reasoning models (LRMs) have demonstrated impressive reasoning<br>capabilities across a broad range of tasks including Olympiad-level<br>mathematical problems, indicating evidence of their complex reasoning<br>abilities. While many reasoning benchmarks focus on the STEM domain, the<br>ability of LRMs to reason correctly in broader task domains remains<br>underexplored. In this work, we introduce \textbf{TTT-Bench}, a new benchmark<br>that is designed to evaluate basic strategic, spatial, and logical reasoning<br>abilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games<br>that humans can effortlessly solve from a young age. We propose a simple yet<br>scalable programmatic approach for generating verifiable two-player game<br>problems for TTT-Bench. Although these games are trivial for humans, they<br>require reasoning about the intentions of the opponent, as well as the game<br>board’s spatial configurations, to ensure a win. We evaluate a diverse set of<br>state-of-the-art LRMs, and \textbf{discover that the models that excel at hard<br>math problems frequently fail at these simple reasoning games}. Further testing<br>reveals that our evaluated reasoning models score on average $\downarrow$ 41%<br>&amp; $\downarrow$ 5% lower on TTT-Bench compared to MATH 500 &amp; AIME 2024<br>respectively, with larger models achieving higher performance using shorter<br>reasoning traces, where most of the models struggle on long-term strategic<br>reasoning situations on simple and new TTT-Bench tasks.</p>
</details>


<h3 id="8-Classifying-Unreliable-Narrators-with-Large-Language-Models"><a href="#8-Classifying-Unreliable-Narrators-with-Large-Language-Models" class="headerlink" title="[8] Classifying Unreliable Narrators with Large Language Models"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10231">Classifying Unreliable Narrators with Large Language Models</a></h3><p><em>Anneliese Brei,Katharine Henry,Abhisheik Sharma,Shashank Srivastava,Snigdha Chaturvedi</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文利用大型语言模型（LLM）识别不可靠叙述者，提出了TUNa数据集和分类任务，实验表明此任务极具挑战性，但有潜力。</p>
<details>
  <summary>Details</summary>
Motivation: 研究旨在通过计算方式识别叙述者是否可靠，填补了文学理论与现实世界文本数据的应用空白。

<p>Contribution: 1. 提出了TUNa数据集，涵盖多领域文本；2. 定义了三种不可靠叙述者的分类任务；3. 分析了LLM在不同学习设置下的表现。</p>
<p>Method: 结合文学理论定义分类任务，利用few-shot、微调和课程学习等方法训练LLM。</p>
<p>Result: 任务极具挑战性，但LLM在识别不可靠叙述者方面有潜力。</p>
<p>Insight: 文学理论可以为现实世界文本分类提供启发，未来研究可进一步优化模型和数据集。</p>
<p>Abstract: Often when we interact with a first-person account of events, we consider<br>whether or not the narrator, the primary speaker of the text, is reliable. In<br>this paper, we propose using computational methods to identify unreliable<br>narrators, i.e. those who unintentionally misrepresent information. Borrowing<br>literary theory from narratology to define different types of unreliable<br>narrators based on a variety of textual phenomena, we present TUNa, a<br>human-annotated dataset of narratives from multiple domains, including blog<br>posts, subreddit posts, hotel reviews, and works of literature. We define<br>classification tasks for intra-narrational, inter-narrational, and<br>inter-textual unreliabilities and analyze the performance of popular<br>open-weight and proprietary LLMs for each. We propose learning from literature<br>to perform unreliable narrator classification on real-world text data. To this<br>end, we experiment with few-shot, fine-tuning, and curriculum learning<br>settings. Our results show that this task is very challenging, and there is<br>potential for using LLMs to identify unreliable narrators. We release our<br>expert-annotated dataset and code and invite future research in this area.</p>
</details>


<h3 id="9-Flick-Few-Labels-Text-Classification-using-K-Aware-Intermediate-Learning-in-Multi-Task-Low-Resource-Languages"><a href="#9-Flick-Few-Labels-Text-Classification-using-K-Aware-Intermediate-Learning-in-Multi-Task-Low-Resource-Languages" class="headerlink" title="[9] Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10292">Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages</a></h3><p><em>Ali Almutairi,Abdullah Alsuhaibani,Shoaib Jameel,Usman Naseem,Gelareh Mohammadi,Imran Razzak</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: Flick提出了一种用于低资源语言的少标签文本分类方法，通过高质量伪标签蒸馏和自适应选择机制，显著提升了伪标签的可靠性。</p>
<details>
  <summary>Details</summary>
Motivation: 解决低资源语言环境中少标签文本分类的难点，尤其是在噪声伪标签和领域适应问题上。

<p>Contribution: 提出了Flick方法，通过伪标签精馏组件和自适应top-k选择机制，显著提升了伪标签质量，适用于低资源语言。</p>
<p>Method: 利用高置信度伪标签蒸馏和单簇内聚性，结合自适应top-k选择机制，优化伪标签生成过程。</p>
<p>Result: 在14个多样化数据集上验证了Flick的优越性能，包括阿拉伯语、乌尔都语等低资源语言。</p>
<p>Insight: 通过专注于高质量伪标签的生成，Flick在低资源环境中实现了更鲁棒的模型微调，仅需少量真实标签。</p>
<p>Abstract: Training deep learning networks with minimal supervision has gained<br>significant research attention due to its potential to reduce reliance on<br>extensive labelled data. While self-training methods have proven effective in<br>semi-supervised learning, they remain vulnerable to errors from noisy pseudo<br>labels. Moreover, most recent approaches to the few-label classification<br>problem are either designed for resource-rich languages such as English or<br>involve complex cascading models that are prone to overfitting. To address the<br>persistent challenge of few-label text classification in truly low-resource<br>linguistic contexts, where existing methods often struggle with noisy<br>pseudo-labels and domain adaptation, we propose Flick. Unlike prior methods<br>that rely on generic multi-cluster pseudo-labelling or complex cascading<br>architectures, Flick leverages the fundamental insight that distilling<br>high-confidence pseudo-labels from a broader set of initial clusters can<br>dramatically improve pseudo-label quality, particularly for linguistically<br>diverse, low-resource settings. Flick introduces a novel pseudo-label<br>refinement component, a departure from traditional pseudo-labelling strategies<br>by identifying and leveraging top-performing pseudo-label clusters. This<br>component specifically learns to distil highly reliable pseudo-labels from an<br>initial broad set by focusing on single-cluster cohesion and leveraging an<br>adaptive top-k selection mechanism. This targeted refinement process is crucial<br>for mitigating the propagation of errors inherent in low-resource data,<br>allowing for robust fine-tuning of pre-trained language models with only a<br>handful of true labels. We demonstrate Flick’s efficacy across 14 diverse<br>datasets, encompassing challenging low-resource languages such as Arabic, Urdu,<br>and Setswana, alongside English, showcasing its superior performance and<br>adaptability.</p>
</details>


<h3 id="10-“Check-My-Work-”-Measuring-Sycophancy-in-a-Simulated-Educational-Context"><a href="#10-“Check-My-Work-”-Measuring-Sycophancy-in-a-Simulated-Educational-Context" class="headerlink" title="[10] “Check My Work?”: Measuring Sycophancy in a Simulated Educational Context"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10297">“Check My Work?”: Measuring Sycophancy in a Simulated Educational Context</a></h3><p><em>Chuck Arvin</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文研究了在模拟教育环境中，用户提供的建议如何影响大型语言模型（LLM），尤其是模型的’谄媚’行为对教育公平可能产生的负面影响。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机在于LLM在教育环境中的应用日益增多，但其对用户输入的敏感性可能导致谄媚行为，从而加剧教育不平等。

<p>Contribution: 主要贡献是量化了LLM的谄媚行为，展示了模型在不同条件下回答质量的变化，并揭示了小模型更易出现谄媚行为。</p>
<p>Method: 通过测试五款LLM在五种实验条件下的表现，分析了模型回答质量的变化，并研究了标记级别的概率以确认谄媚行为。</p>
<p>Result: 结果显示，模型的准确性受学生回答的显著影响（±15%），且小模型的谄媚效应更强（30% vs. 8%）。</p>
<p>Insight: 研究发现LLM在教育中可能加剧知识差距，强调了理解和减少这种偏见的必要性。</p>
<p>Abstract: This study examines how user-provided suggestions affect Large Language<br>Models (LLMs) in a simulated educational context, where sycophancy poses<br>significant risks. Testing five different LLMs from the OpenAI GPT-4o and<br>GPT-4.1 model classes across five experimental conditions, we show that<br>response quality varies dramatically based on query framing. In cases where the<br>student mentions an incorrect answer, the LLM correctness can degrade by as<br>much as 15 percentage points, while mentioning the correct answer boosts<br>accuracy by the same margin. Our results also show that this bias is stronger<br>in smaller models, with an effect of up to 30% for the GPT-4.1-nano model,<br>versus 8% for the GPT-4o model. Our analysis of how often LLMs “flip” their<br>answer, and an investigation into token level probabilities, confirm that the<br>models are generally changing their answers to answer choices mentioned by<br>students in line with the sycophancy hypothesis. This sycophantic behavior has<br>important implications for educational equity, as LLMs may accelerate learning<br>for knowledgeable students while the same tools may reinforce misunderstanding<br>for less knowledgeable students. Our results highlight the need to better<br>understand the mechanism, and ways to mitigate, such bias in the educational<br>context.</p>
</details>


<h3 id="11-Code-Execution-as-Grounded-Supervision-for-LLM-Reasoning"><a href="#11-Code-Execution-as-Grounded-Supervision-for-LLM-Reasoning" class="headerlink" title="[11] Code Execution as Grounded Supervision for LLM Reasoning"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10343">Code Execution as Grounded Supervision for LLM Reasoning</a></h3><p><em>Dongwon Jung,Wenxuan Zhou,Muhao Chen</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种利用代码执行确定性生成高质量思维链监督数据的方法，替代依赖人工标注或易错的LLM生成监督数据，有效提升了LLM的推理能力。</p>
<details>
  <summary>Details</summary>
Motivation: 现有思维链监督数据生成方法依赖昂贵的人工标注或易错的LLM生成，难以保证可靠性和准确性。本文通过利用代码执行的确定性，提出了一种可扩展的高质量监督数据生成方法。

<p>Contribution: 1. 提出利用程序执行的确定性生成高质量思维链监督数据的方法；2. 所生成的推理数据可通过执行验证准确性；3. 实验证明该方法能有效提升LLM的跨领域推理能力。</p>
<p>Method: 从代码执行中提取可验证的逐步推理痕迹，并将其转化为自然语言的思维链推理数据。</p>
<p>Result: 在多个领域的推理基准测试中，该方法显著提升了LLM的推理能力，并通过消融实验验证了生成数据的准确性和推理效率的提升。</p>
<p>Insight: 利用代码执行的确定性生成推理监督数据是一种高效且可扩展的方法，可减少对人工标注的依赖并提高推理准确性。</p>
<p>Abstract: Training large language models (LLMs) with chain-of-thought (CoT) supervision<br>has proven effective for enhancing their reasoning abilities. However,<br>obtaining reliable and accurate reasoning supervision remains a significant<br>challenge. We propose a scalable method for generating a high-quality CoT<br>supervision dataset by leveraging the determinism of program execution. Unlike<br>existing reasoning dataset generation methods that rely on costly human<br>annotations or error-prone LLM-generated CoT, our approach extracts verifiable,<br>step-by-step reasoning traces from code execution and transforms them into a<br>natural language CoT reasoning. Experiments on reasoning benchmarks across<br>various domains show that our method effectively equips LLMs with transferable<br>reasoning abilities across diverse tasks. Furthermore, the ablation studies<br>validate that our method produces highly accurate reasoning data and reduces<br>overall token length during inference by reducing meaningless repetition and<br>overthinking.</p>
</details>


<h3 id="12-TableRAG-A-Retrieval-Augmented-Generation-Framework-for-Heterogeneous-Document-Reasoning"><a href="#12-TableRAG-A-Retrieval-Augmented-Generation-Framework-for-Heterogeneous-Document-Reasoning" class="headerlink" title="[12] TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10380">TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning</a></h3><p><em>Xiaohan Yu,Pu Jian,Chong Chen</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: TableRAG是一种检索增强生成框架，针对异构文档（包含文本和表格）的推理任务提出，通过融合文本检索和表格操作，解决了现有方法在表格结构和多跳推理中的局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法在处理包含文本和表格的异构文档时存在局限性，如破坏表格结构和信息丢失，导致在多跳和全局推理任务中表现不佳。

<p>Contribution: 提出了TableRAG框架，融合文本理解和表格操作；开发了HeteQA基准测试，用于评估异构文档的多跳推理能力；实验证明TableRAG在多项任务中优于基线。</p>
<p>Method: 采用迭代四步法：查询分解、文本检索、SQL编程与执行、中间答案生成，结合文本和表格的动态操作。</p>
<p>Result: TableRAG在公开数据集和HeteQA上均超越现有方法，成为异构文档问答的新SOTA。</p>
<p>Insight: 异构文档的推理需要结合文本和表格的结构化操作，而非简单拼接，动态的多步推理能显著提升性能。</p>
<p>Abstract: Retrieval-Augmented Generation (RAG) has demonstrated considerable<br>effectiveness in open-domain question answering. However, when applied to<br>heterogeneous documents, comprising both textual and tabular components,<br>existing RAG approaches exhibit critical limitations. The prevailing practice<br>of flattening tables and chunking strategies disrupts the intrinsic tabular<br>structure, leads to information loss, and undermines the reasoning capabilities<br>of LLMs in multi-hop, global queries. To address these challenges, we propose<br>TableRAG, an hybrid framework that unifies textual understanding and complex<br>manipulations over tabular data. TableRAG iteratively operates in four steps:<br>context-sensitive query decomposition, text retrieval, SQL programming and<br>execution, and compositional intermediate answer generation. We also develop<br>HeteQA, a novel benchmark designed to evaluate the multi-hop heterogeneous<br>reasoning capabilities. Experimental results demonstrate that TableRAG<br>consistently outperforms existing baselines on both public datasets and our<br>HeteQA, establishing a new state-of-the-art for heterogeneous document question<br>answering. We release TableRAG at <a target="_blank" rel="noopener" href="https://github.com/yxh-y/TableRAG/tree/main">https://github.com/yxh-y/TableRAG/tree/main</a>.</p>
</details>


<h3 id="13-PAG-Multi-Turn-Reinforced-LLM-Self-Correction-with-Policy-as-Generative-Verifier"><a href="#13-PAG-Multi-Turn-Reinforced-LLM-Self-Correction-with-Policy-as-Generative-Verifier" class="headerlink" title="[13] PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10406">PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier</a></h3><p><em>Yuhua Jiang,Yuwen Xiong,Yufeng Yuan,Chao Xin,Wenyuan Xu,Yu Yue,Qianchuan Zhao,Lin Yan</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: PAG提出了一个结合生成与验证的多轮强化学习框架，通过模型在生成和验证角色间切换，选择性修正答案，提升自校正能力。</p>
<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在复杂推理任务中表现优异，但难以可靠验证自身输出的正确性。现有解决方案依赖独立验证模块或多阶段训练，限制了扩展性。

<p>Contribution: 提出了Policy as Generative Verifier（PAG）框架，将生成与验证统一到多轮强化学习中，通过选择性修正避免模型崩溃，提升推理和验证能力。</p>
<p>Method: 在统一的多轮强化学习范式下，模型交替扮演生成策略和验证器的角色，仅在生成验证检测到错误时修正答案（验证-修正流程）。</p>
<p>Result: 在多样化推理基准测试中，PAG作为策略提升了生成和自校正的准确性，作为验证器其自验证表现优于自一致性方法。</p>
<p>Insight: 将验证与生成统一到单一框架中，通过选择性修正避免了不必要的重复修正常见的模型崩溃问题，同时联合优化了推理和验证能力。</p>
<p>Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in<br>complex reasoning tasks, yet they still struggle to reliably verify the<br>correctness of their own outputs. Existing solutions to this verification<br>challenge often depend on separate verifier models or require multi-stage<br>self-correction training pipelines, which limit scalability. In this paper, we<br>propose Policy as Generative Verifier (PAG), a simple and effective framework<br>that empowers LLMs to self-correct by alternating between policy and verifier<br>roles within a unified multi-turn reinforcement learning (RL) paradigm.<br>Distinct from prior approaches that always generate a second attempt regardless<br>of model confidence, PAG introduces a selective revision mechanism: the model<br>revises its answer only when its own generative verification step detects an<br>error. This verify-then-revise workflow not only alleviates model collapse but<br>also jointly enhances both reasoning and verification abilities. Extensive<br>experiments across diverse reasoning benchmarks highlight PAG’s dual<br>advancements: as a policy, it enhances direct generation and self-correction<br>accuracy; as a verifier, its self-verification outperforms self-consistency.</p>
</details>


<h3 id="14-Burn-After-Reading-Do-Multimodal-Large-Language-Models-Truly-Capture-Order-of-Events-in-Image-Sequences"><a href="#14-Burn-After-Reading-Do-Multimodal-Large-Language-Models-Truly-Capture-Order-of-Events-in-Image-Sequences" class="headerlink" title="[14] Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10415">Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?</a></h3><p><em>Yingjin Song,Yupei Du,Denis Paperno,Albert Gatt</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 这篇论文提出了TempVS基准，用于评估多模态大型语言模型（MLLMs）在图像序列中对时间逻辑的理解能力，发现现有模型与人类表现存在显著差距。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机是验证MLLMs是否能真正理解图像序列中的事件顺序，揭示其时间推理和基础能力的不足。

<p>Contribution: 主要贡献是提出了TempVS基准，包含三种测试任务（事件关系推理、句子排序和图像排序），并评估了38个SOTA MLLMs的表现。</p>
<p>Method: 方法是通过设计TempVS基准测试，结合视觉和语言模态，评估模型对事件顺序的理解能力。</p>
<p>Result: 结果显示现有MLLMs在TempVS任务上表现不佳，与人类能力差距较大。</p>
<p>Insight: 研究指出未来研究方向，包括改进模型的时间推理能力和多模态融合机制。</p>
<p>Abstract: This paper introduces the TempVS benchmark, which focuses on temporal<br>grounding and reasoning capabilities of Multimodal Large Language Models<br>(MLLMs) in image sequences. TempVS consists of three main tests (i.e., event<br>relation inference, sentence ordering and image ordering), each accompanied<br>with a basic grounding test. TempVS requires MLLMs to rely on both visual and<br>linguistic modalities to understand the temporal order of events. We evaluate<br>38 state-of-the-art MLLMs, demonstrating that models struggle to solve TempVS,<br>with a substantial performance gap compared to human capabilities. We also<br>provide fine-grained insights that suggest promising directions for future<br>research. Our TempVS benchmark data and code are available at<br><a target="_blank" rel="noopener" href="https://github.com/yjsong22/TempVS">https://github.com/yjsong22/TempVS</a>.</p>
</details>


<h3 id="15-Fast-on-the-Easy-Deep-on-the-Hard-Efficient-Reasoning-via-Powered-Length-Penalty"><a href="#15-Fast-on-the-Easy-Deep-on-the-Hard-Efficient-Reasoning-via-Powered-Length-Penalty" class="headerlink" title="[15] Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10446">Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty</a></h3><p><em>Zehui Ling,Deshu Chen,Hongwei Zhang,Yifeng Jiao,Xin Guo,Yuan Cheng</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文提出了一种通过动态调整输出长度惩罚来提升大型语言模型（LLM）推理效率的方法，针对简单问题减少输出长度以降低计算延迟，而对复杂问题保留充分推理以提高准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法如Chain-of-Thought提示虽然提升了LLM的推理能力，但往往导致输出过长，增加计算延迟。统一长度惩罚忽略了问题复杂性，影响了性能表现。

<p>Contribution: 引入了基于问题复杂性的动态长度惩罚机制，平衡了输出长度和推理能力，从而在提升效率的同时维持或提高准确性。</p>
<p>Method: 通过分割奖励函数并加入新型输出长度惩罚，动态调整模型在不同复杂度问题上的推理行为。</p>
<p>Result: 在三个数据集（GSM8K、MATH500、AIME2024）上验证了方法的有效性，简单数据集上缩短了输出长度而不损失准确性，复杂数据集上提升了准确性。</p>
<p>Insight: 动态调整推理策略能显著提升LLM的效率和性能，说明问题复杂度对推理行为的设计至关重要。</p>
<p>Abstract: Large language models (LLMs) have demonstrated significant advancements in<br>reasoning capabilities, performing well on various challenging benchmarks.<br>Techniques like Chain-of-Thought prompting have been introduced to further<br>improve reasoning. However, these approaches frequently generate longer<br>outputs, which in turn increase computational latency. Although some methods<br>use reinforcement learning to shorten reasoning, they often apply uniform<br>penalties without considering the problem’s complexity, leading to suboptimal<br>outcomes. In this study, we seek to enhance the efficiency of LLM reasoning by<br>promoting conciseness for simpler problems while preserving sufficient<br>reasoning for more complex ones for accuracy, thus improving the model’s<br>overall performance. Specifically, we manage the model’s reasoning efficiency<br>by dividing the reward function and including a novel penalty for output<br>length. Our approach has yielded impressive outcomes in benchmark evaluations<br>across three datasets: GSM8K, MATH500, and AIME2024. For the comparatively<br>simpler datasets GSM8K and MATH500, our method has effectively shortened output<br>lengths while preserving or enhancing accuracy. On the more demanding AIME2024<br>dataset, our approach has resulted in improved accuracy.</p>
</details>


<h3 id="16-Table-Text-Alignment-Explaining-Claim-Verification-Against-Tables-in-Scientific-Papers"><a href="#16-Table-Text-Alignment-Explaining-Claim-Verification-Against-Tables-in-Scientific-Papers" class="headerlink" title="[16] Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10486">Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers</a></h3><p><em>Xanh Ho,Sunisth Kumar,Yun-Ang Wu,Florian Boudin,Atsuhiro Takasu,Akiko Aizawa</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文将表格-文本对齐重新定义为解释任务，强调模型需识别支持或反驳科学声明所需的关键表格单元格，并构建了包含人工标注单元格级依据的新数据集，以提高科学声明的可解释性和验证性能。</p>
<details>
  <summary>Details</summary>
Motivation: 仅预测科学声明验证标签的模型缺乏透明性和解释性，无法揭示模型推理过程。通过引入表格单元格对齐任务，论文旨在提升模型的可解释性和性能。

<p>Contribution: 1. 将表格-文本对齐任务重新定义为解释任务；2. 构建了包含人工标注单元格依据的新数据集；3. 提出了处理模糊案例的分类法；4. 验证了表格对齐信息对提升声明验证性能的作用。</p>
<p>Method: 1. 扩展SciTab基准数据集，加入人工标注的单元格级依据；2. 提出分类法处理模糊案例；3. 实验验证表格对齐信息对性能的提升。</p>
<p>Result: 实验表明，加入表格对齐信息可提升声明验证性能，但大多数语言模型虽能预测正确标签，却难以复现人类对齐的依据，表明其预测缺乏忠实推理。</p>
<p>Insight: 模型的预测正确性并不等同于忠实推理，表明现有语言模型在解释性任务上仍有不足，需进一步改进以提升可解释性。</p>
<p>Abstract: Scientific claim verification against tables typically requires predicting<br>whether a claim is supported or refuted given a table. However, we argue that<br>predicting the final label alone is insufficient: it reveals little about the<br>model’s reasoning and offers limited interpretability. To address this, we<br>reframe table-text alignment as an explanation task, requiring models to<br>identify the table cells essential for claim verification. We build a new<br>dataset by extending the SciTab benchmark with human-annotated cell-level<br>rationales. Annotators verify the claim label and highlight the minimal set of<br>cells needed to support their decision. After the annotation process, we<br>utilize the collected information and propose a taxonomy for handling ambiguous<br>cases. Our experiments show that (i) incorporating table alignment information<br>improves claim verification performance, and (ii) most LLMs, while often<br>predicting correct labels, fail to recover human-aligned rationales, suggesting<br>that their predictions do not stem from faithful reasoning.</p>
</details>


<h3 id="17-Reliable-Reasoning-Path-Distilling-Effective-Guidance-for-LLM-Reasoning-with-Knowledge-Graphs"><a href="#17-Reliable-Reasoning-Path-Distilling-Effective-Guidance-for-LLM-Reasoning-with-Knowledge-Graphs" class="headerlink" title="[17] Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10508">Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs</a></h3><p><em>Yilin Xiao,Chuang Zhou,Qinggang Zhang,Bo Li,Qing Li,Xiao Huang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种名为RRP的框架，通过结合知识图谱和大型语言模型，解决复杂推理任务中路径可靠性和冗余问题，提升推理能力。</p>
<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中表现不佳，主要因为缺乏背景知识且容易产生幻觉，知识图谱的引入虽能补充事实，但仍难以生成逻辑一致的推理路径。

<p>Contribution: 提出了RRP框架，利用LLMs的语义能力和知识图谱的结构信息，通过关系嵌入和双向分布学习生成高质量推理路径，并引入反思模块优化路径。</p>
<p>Method: 结合LLMs和知识图谱的关系嵌入与双向分布学习，生成推理路径；通过反思模块评估路径重要性，提升路径质量。</p>
<p>Result: 在两个公开数据集上达到SOTA性能，且能灵活集成到不同LLMs中。</p>
<p>Insight: 推理路径的可靠性和逻辑一致性对LLMs的推理能力至关重要，结构信息和语义能力的结合为复杂问题提供了有效解决方案。</p>
<p>Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks<br>due to a lack of background knowledge and a tendency to hallucinate. To address<br>these limitations, integrating knowledge graphs (KGs) with LLMs has been<br>intensively studied. Existing KG-enhanced LLMs focus on supplementary factual<br>knowledge, but still struggle with solving complex questions. We argue that<br>refining the relationships among facts and organizing them into a logically<br>consistent reasoning path is equally important as factual knowledge itself.<br>Despite their potential, extracting reliable reasoning paths from KGs poses the<br>following challenges: the complexity of graph structures and the existence of<br>multiple generated paths, making it difficult to distinguish between useful and<br>redundant ones. To tackle these challenges, we propose the RRP framework to<br>mine the knowledge graph, which combines the semantic strengths of LLMs with<br>structural information obtained through relation embedding and bidirectional<br>distribution learning. Additionally, we introduce a rethinking module that<br>evaluates and refines reasoning paths according to their significance.<br>Experimental results on two public datasets show that RRP achieves<br>state-of-the-art performance compared to existing baseline methods. Moreover,<br>RRP can be easily integrated into various LLMs to enhance their reasoning<br>abilities in a plug-and-play manner. By generating high-quality reasoning paths<br>tailored to specific questions, RRP distills effective guidance for LLM<br>reasoning.</p>
</details>


<h3 id="18-NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors"><a href="#18-NeuralNexus-at-BEA-2025-Shared-Task-Retrieval-Augmented-Prompting-for-Mistake-Identification-in-AI-Tutors" class="headerlink" title="[18] NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10627">NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors</a></h3><p><em>Numaan Naeem,Sarfraz Ahmad,Momina Ahsan,Hasan Iqbal</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了四种方法用于评估AI导师是否能够正确识别学生数学推理中的错误，最终检索增强的少样本提示系统结合LLM推理表现最佳。</p>
<details>
  <summary>Details</summary>
Motivation: 旨在提升AI导师在教学反馈中的错误识别能力，从而优化其教学效果。

<p>Contribution: 1. 提出了四种错误识别方法；2. 展示了检索增强的提示系统在任务中的有效性。</p>
<p>Method: 结合了四种方法：机器学习模型集成、句子Transformer、历史感知模型和检索增强的少样本提示系统。</p>
<p>Result: 检索增强的提示系统优于所有基线，证明了其在教学反馈评估中的优势。</p>
<p>Insight: 示例驱动的提示与LLM推理结合能有效提升AI导师的错误识别能力。</p>
<p>Abstract: This paper presents our system for Track 1: Mistake Identification in the BEA<br>2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors. The<br>task involves evaluating whether a tutor’s response correctly identifies a<br>mistake in a student’s mathematical reasoning. We explore four approaches: (1)<br>an ensemble of machine learning models over pooled token embeddings from<br>multiple pretrained language models (LMs); (2) a frozen sentence-transformer<br>using [CLS] embeddings with an MLP classifier; (3) a history-aware model with<br>multi-head attention between token-level history and response embeddings; and<br>(4) a retrieval-augmented few-shot prompting system with a large language model<br>(LLM) i.e. GPT 4o. Our final system retrieves semantically similar examples,<br>constructs structured prompts, and uses schema-guided output parsing to produce<br>interpretable predictions. It outperforms all baselines, demonstrating the<br>effectiveness of combining example-driven prompting with LLM reasoning for<br>pedagogical feedback assessment. Our code is available at<br><a target="_blank" rel="noopener" href="https://github.com/NaumanNaeem/BEA_2025">https://github.com/NaumanNaeem/BEA_2025</a>.</p>
</details>


<h3 id="19-PREMISE-Scalable-and-Strategic-Prompt-Optimization-for-Efficient-Mathematical-Reasoning-in-Large-Models"><a href="#19-PREMISE-Scalable-and-Strategic-Prompt-Optimization-for-Efficient-Mathematical-Reasoning-in-Large-Models" class="headerlink" title="[19] PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10716">PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models</a></h3><p><em>Ye Yu,Yaoning Yu,Haohan Wang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: PREMISE 是一种无需修改模型权重的提示优化框架，通过多目标文本搜索和梯度启发式方法，显著减少大型推理模型中的冗余计算和 token 使用，同时保持高准确率。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的 LRM（大型推理模型）在数学推理任务中使用冗长的链式思维（CoT）推理，导致 token 使用量高且成本昂贵，限制了在延迟敏感或 API 受限环境中的部署。PREMISE 旨在通过优化提示减少推理开销。

<p>Contribution: 提出 PREMISE 框架，首次将基于提示的优化方法应用于 LRM，实现高效推理；通过多目标优化平衡简洁性和准确性，显著降低 token 使用和成本。</p>
<p>Method: 结合轨迹级诊断和梯度启发式提示优化，通过多目标文本搜索同时优化 token 长度和答案有效性；适用于单次黑盒接口的商业 LLM。</p>
<p>Result: 在 GSM8K、SVAMP 和 Math500 基准上，PREMISE 在保持或提升准确率（Claude 96%→96%，Gemini 91%→92%）的同时，显著减少 token 使用（最高 87.5%）和成本（降低 69%–82%）。</p>
<p>Insight: 提示级优化是实现高效 LRM 推理的可行路径，无需对模型进行任何修改，且适用于商业 LLM。未来可扩展到其他推理任务和场景。</p>
<p>Abstract: Large reasoning models (LRMs) such as Claude 3.7 Sonnet and OpenAI o1 achieve<br>strong performance on mathematical benchmarks using lengthy chain-of-thought<br>(CoT) reasoning, but the resulting traces are often unnecessarily verbose. This<br>inflates token usage and cost, limiting deployment in latency-sensitive or<br>API-constrained settings. We introduce PREMISE (PRompt-based Efficient<br>Mathematical Inference with Strategic Evaluation), a prompt-only framework that<br>reduces reasoning overhead without modifying model weights. PREMISE combines<br>trace-level diagnostics with gradient-inspired prompt optimization to minimize<br>redundant computation while preserving answer accuracy. The approach jointly<br>optimizes brevity and correctness through a multi-objective textual search that<br>balances token length and answer validity. Unlike prior work, PREMISE runs in a<br>single-pass black-box interface, so it can be applied directly to commercial<br>LLMs. On GSM8K, SVAMP, and Math500 we match or exceed baseline accuracy<br>($96%\rightarrow96%$ with Claude, $91%\rightarrow92%$ with Gemini) while<br>reducing reasoning tokens by up to $87.5%$ and cutting dollar cost by<br>$69$–$82%$. These results show that prompt-level optimization is a practical<br>and scalable path to efficient LRM inference without compromising reasoning<br>quality.</p>
</details>


<h3 id="20-Beyond-True-or-False-Retrieval-Augmented-Hierarchical-Analysis-of-Nuanced-Claims"><a href="#20-Beyond-True-or-False-Retrieval-Augmented-Hierarchical-Analysis-of-Nuanced-Claims" class="headerlink" title="[20] Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10728">Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims</a></h3><p><em>Priyanka Kargupta,Runchu Tian,Jiawei Han</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了ClaimSpect框架，通过检索增强生成技术自动构建针对复杂声明的层次化分析，结合语料库视角解构声明并展示不同观点。</p>
<details>
  <summary>Details</summary>
Motivation: 现实中许多声明（如科学或政治领域的声明）往往具有复杂性，仅用“真”或“假”难以全面评估。论文旨在通过层次化分析和多角度验证提供更全面的见解。

<p>Contribution: 提出ClaimSpect框架，能够自动生成层次化声明分析，结合语料库检索技术发现子方面和多样化观点。</p>
<p>Method: 采用检索增强生成技术，通过分层语料库检索相关片段，发现声明的子方面及其支持、中立或反对的观点。</p>
<p>Result: 在科学和政治声明数据集上的实验表明，ClaimSpect能够有效解构复杂声明，并在多种基线方法中表现优异。</p>
<p>Insight: 层次化分析和多角度检索提供了一种更全面、结构化的方式评估复杂声明，有助于用户聚焦感兴趣的特定方面。</p>
<p>Abstract: Claims made by individuals or entities are oftentimes nuanced and cannot be<br>clearly labeled as entirely “true” or “false” – as is frequently the case with<br>scientific and political claims. However, a claim (e.g., “vaccine A is better<br>than vaccine B”) can be dissected into its integral aspects and sub-aspects<br>(e.g., efficacy, safety, distribution), which are individually easier to<br>validate. This enables a more comprehensive, structured response that provides<br>a well-rounded perspective on a given problem while also allowing the reader to<br>prioritize specific angles of interest within the claim (e.g., safety towards<br>children). Thus, we propose ClaimSpect, a retrieval-augmented generation-based<br>framework for automatically constructing a hierarchy of aspects typically<br>considered when addressing a claim and enriching them with corpus-specific<br>perspectives. This structure hierarchically partitions an input corpus to<br>retrieve relevant segments, which assist in discovering new sub-aspects.<br>Moreover, these segments enable the discovery of varying perspectives towards<br>an aspect of the claim (e.g., support, neutral, or oppose) and their respective<br>prevalence (e.g., “how many biomedical papers believe vaccine A is more<br>transportable than B?”). We apply ClaimSpect to a wide variety of real-world<br>scientific and political claims featured in our constructed dataset, showcasing<br>its robustness and accuracy in deconstructing a nuanced claim and representing<br>perspectives within a corpus. Through real-world case studies and human<br>evaluation, we validate its effectiveness over multiple baselines.</p>
</details>


<h3 id="21-Different-Questions-Different-Models-Fine-Grained-Evaluation-of-Uncertainty-and-Calibration-in-Clinical-QA-with-LLMs"><a href="#21-Different-Questions-Different-Models-Fine-Grained-Evaluation-of-Uncertainty-and-Calibration-in-Clinical-QA-with-LLMs" class="headerlink" title="[21] Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10769">Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs</a></h3><p><em>Alberto Testoni,Iacer Calixto</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文对临床问答任务中大型语言模型（LLMs）的不确定性估计方法进行了细粒度评估，比较了不同模型和方法在多种医学专业和问题类型下的表现，并提出了一种轻量级的单次生成估计方法。</p>
<details>
  <summary>Details</summary>
Motivation: 在临床决策支持等高风险领域，LLMs的准确且校准良好的不确定性估计至关重要。然而，现有研究对LLMs在不同医学专业和问题类型下的表现差异缺乏细致分析，为此需进行系统性评估。

<p>Contribution: 1. 对10种开源LLMs（通用、生物医学和推理模型）在两种数据集、11个医学专业和6种问题类型下进行了不确定性估计的全面评估；<br>2. 提出了一种轻量级的单次生成不确定性估计方法，其性能接近语义熵方法。</p>
<p>Method: 1. 比较了标准的单次生成和基于采样的不确定性估计方法；<br>2. 设计了一种基于推理轨迹中行为信号的简单单次生成估计方法，无需多次采样。</p>
<p>Result: 实验结果表明，不同医学专业和问题类型之间存在显著差异，强调了根据问题性质和模型特点选择合适模型的重要性。轻量级方法的性能接近语义熵，但仅需单次生成。</p>
<p>Insight: 1. 医学专业和问题类型的多样性对LLMs的不确定性估计性能有显著影响；<br>2. 轻量级方法在保持性能的同时降低了计算成本，适用于实际部署。</p>
<p>Abstract: Accurate and well-calibrated uncertainty estimates are essential for<br>deploying large language models (LLMs) in high-stakes domains such as clinical<br>decision support. We present a fine-grained evaluation of uncertainty<br>estimation methods for clinical multiple-choice question answering, covering<br>ten open-source LLMs (general-purpose, biomedical, and reasoning models) across<br>two datasets, eleven medical specialties, and six question types. We compare<br>standard single-generation and sampling-based methods, and present a case study<br>exploring simple, single-pass estimators based on behavioral signals in<br>reasoning traces. These lightweight methods approach the performance of<br>Semantic Entropy while requiring only one generation. Our results reveal<br>substantial variation across specialties and question types, underscoring the<br>importance of selecting models based on both the nature of the question and<br>model-specific strengths.</p>
</details>


<h3 id="22-Improving-Named-Entity-Transcription-with-Contextual-LLM-based-Revision"><a href="#22-Improving-Named-Entity-Transcription-with-Contextual-LLM-based-Revision" class="headerlink" title="[22] Improving Named Entity Transcription with Contextual LLM-based Revision"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10779">Improving Named Entity Transcription with Contextual LLM-based Revision</a></h3><p><em>Viet Anh Trinh,Xinlu He,Jacob Whitehill</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种基于大语言模型（LLM）的修正方法，通过利用LLM的推理能力和上下文信息（如课程笔记）来改进ASR系统中的命名实体转录错误，并在自建的数据集上实现了30%的相对WER降低。</p>
<details>
  <summary>Details</summary>
Motivation: 当前ASR系统在通用语音识别上表现优异，但对命名实体的错误率仍然较高，影响了后续应用。因此，需要一种方法专门提升命名实体的转录准确性。

<p>Contribution: 1. 提出了一种基于LLM的修正机制；2. 引入了一个新的数据集NER-MIT-OpenCourseWare（45小时）；3. 在命名实体转录上实现了30%的相对WER降低。</p>
<p>Method: 通过LLM的推理能力和本地上下文（如课程笔记）中的正确命名实体信息，对ASR输出的命名实体进行修正。</p>
<p>Result: 在NER-MIT-OpenCourseWare数据集上，命名实体的WER相对降低了30%。</p>
<p>Insight: LLM的上下文推理能力可以有效修正ASR系统中的命名实体错误，且结合领域知识（如课程笔记）能进一步提升效果。</p>
<p>Abstract: With recent advances in modeling and the increasing amount of supervised<br>training data, automatic speech recognition (ASR) systems have achieved<br>remarkable performance on general speech. However, the word error rate (WER) of<br>state-of-the-art ASR remains high for named entities. Since named entities are<br>often the most critical keywords, misrecognizing them can affect all downstream<br>applications, especially when the ASR system functions as the front end of a<br>complex system. In this paper, we introduce a large language model (LLM)<br>revision mechanism to revise incorrect named entities in ASR predictions by<br>leveraging the LLM’s reasoning ability as well as local context (e.g., lecture<br>notes) containing a set of correct named entities. Finally, we introduce the<br>NER-MIT-OpenCourseWare dataset, containing 45 hours of data from MIT courses<br>for development and testing. On this dataset, our proposed technique achieves<br>up to 30% relative WER reduction for named entities.</p>
</details>


<h3 id="23-Mitigating-Negative-Interference-in-Multilingual-Sequential-Knowledge-Editing-through-Null-Space-Constraints"><a href="#23-Mitigating-Negative-Interference-in-Multilingual-Sequential-Knowledge-Editing-through-Null-Space-Constraints" class="headerlink" title="[23] Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10800">Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints</a></h3><p><em>Wei Sun,Tingyu Qu,Mingxiao Li,Jesse Davis,Marie-Francine Moens</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: LangEdit是一种通过零空间约束减轻多语言顺序知识编辑中负面干扰的新框架，确保语言特定知识更新的独立性，同时保持多语言通用能力。</p>
<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）在更新知识时，跨语言的顺序编辑会导致参数干扰，破坏多语言通用性和知识准确性，亟需解决这一挑战。

<p>Contribution: 提出LangEdit框架，利用零空间投影隔离语言特定知识更新，确保编辑独立性并保留多语言通用能力。</p>
<p>Method: 通过将参数更新投影到先前更新子空间的正交补空间（零空间），数学保证更新独立性。</p>
<p>Result: 在三种模型架构、六种语言和四项下游任务上验证，LangEdit显著减少参数干扰，优于现有编辑方法。</p>
<p>Insight: 零空间约束为多语言知识更新提供了一种高效且数学可解释的解决方案，为LLM的多语言编辑开辟了新方向。</p>
<p>Abstract: Efficiently updating multilingual knowledge in large language models (LLMs),<br>while preserving consistent factual representations across languages, remains a<br>long-standing and unresolved challenge. While deploying separate editing<br>systems for each language might seem viable, this approach incurs substantial<br>costs due to the need to manage multiple models. A more efficient solution<br>involves integrating knowledge updates across all languages into a unified<br>model. However, performing sequential edits across languages often leads to<br>destructive parameter interference, significantly degrading multilingual<br>generalization and the accuracy of injected knowledge. To address this<br>challenge, we propose LangEdit, a novel null-space constrained framework<br>designed to precisely isolate language-specific knowledge updates. The core<br>innovation of LangEdit lies in its ability to project parameter updates for<br>each language onto the orthogonal complement of previous updated subspaces.<br>This approach mathematically guarantees update independence while preserving<br>multilingual generalization capabilities. We conduct a comprehensive evaluation<br>across three model architectures, six languages, and four downstream tasks,<br>demonstrating that LangEdit effectively mitigates parameter interference and<br>outperforms existing state-of-the-art editing methods. Our results highlight<br>its potential for enabling efficient and accurate multilingual knowledge<br>updates in LLMs. The code is available at<br><a target="_blank" rel="noopener" href="https://github.com/VRCMF/LangEdit.git">https://github.com/VRCMF/LangEdit.git</a>.</p>
</details>


<h3 id="24-ReCUT-Balancing-Reasoning-Length-and-Accuracy-in-LLMs-via-Stepwise-Trails-and-Preference-Optimization"><a href="#24-ReCUT-Balancing-Reasoning-Length-and-Accuracy-in-LLMs-via-Stepwise-Trails-and-Preference-Optimization" class="headerlink" title="[24] ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10822">ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization</a></h3><p><em>Zhensheng Jin,Xinze Li,Yifan Ji,Chunyi Peng,Zhenghao Liu,Qi Shi,Yukun Yan,Shuo Wang,Furong Peng,Ge Yu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ReCUT提出了一种新方法，通过逐步探索和长短切换采样策略，平衡LLM的推理长度与准确性，显著减少了推理长度并保持了准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的CoT提示方法存在过度推理（overthinking）问题，导致推理轨迹冗长或冗余，现有解决方案因数据质量和过拟合问题效果受限。

<p>Contribution: 提出了ReCUT方法，通过逐步探索和长短切换采样生成多样化推理路径，并训练两个专用模型（分别优化准确性和推理长度），最终通过参数插值获得集成模型。</p>
<p>Method: 采用逐步探索机制和长短切换采样策略生成推理路径，通过偏好对训练两个专用模型，并通过参数插值集成。</p>
<p>Result: 在多个数学推理数据集上，推理长度减少30-50%，同时保持或提升了准确性。</p>
<p>Insight: 通过平衡推理长度和准确性，ReCUT在减少计算开销的同时保持了推理质量，为LLM的高效推理提供了新思路。</p>
<p>Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially<br>improved the reasoning capabilities of Large Language Models (LLMs). However,<br>these methods often suffer from overthinking, leading to unnecessarily lengthy<br>or redundant reasoning traces. Existing approaches attempt to mitigate this<br>issue through curating multiple reasoning chains for training LLMs, but their<br>effectiveness is often constrained by the quality of the generated data and<br>prone to overfitting. To address the challenge, we propose Reasoning<br>Compression ThroUgh Stepwise Trials (ReCUT), a novel method aimed at balancing<br>the accuracy and length of reasoning trajectory. Specifically, ReCUT employs a<br>stepwise exploration mechanism and a long-short switched sampling strategy,<br>enabling LLMs to incrementally generate diverse reasoning paths. These paths<br>are evaluated and used to construct preference pairs to train two specialized<br>models (Gemini LLMs)-one optimized for reasoning accuracy, the other for<br>shorter reasoning. A final integrated model is obtained by interpolating the<br>parameters of these two models. Experimental results across multiple math<br>reasoning datasets and backbone models demonstrate that ReCUT significantly<br>reduces reasoning lengths by approximately 30-50%, while maintaining or<br>improving reasoning accuracy compared to various baselines. All codes and data<br>will be released via <a target="_blank" rel="noopener" href="https://github.com/NEUIR/ReCUT">https://github.com/NEUIR/ReCUT</a>.</p>
</details>


<h3 id="25-CIIR-LiveRAG-2025-Optimizing-Multi-Agent-Retrieval-Augmented-Generation-through-Self-Training"><a href="#25-CIIR-LiveRAG-2025-Optimizing-Multi-Agent-Retrieval-Augmented-Generation-through-Self-Training" class="headerlink" title="[25] CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10844">CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training</a></h3><p><em>Alireza Salemi,Mukta Maddipatla,Hamed Zamani</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了一种名为 mRAG 的多智能体检索增强生成框架，通过自训练和奖励引导的轨迹采样优化智能体协作，提升了复杂任务的生成效果，并在比赛中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的检索增强生成（RAG）方法在复杂任务中表现受限，缺乏多智能体协作的能力。为了克服这一局限，研究者提出了多智能体框架 mRAG，以优化任务分解与协作。

<p>Contribution: 1. 提出了 mRAG，一种多智能体 RAG 框架，通过规划、搜索、推理和协调等子任务智能体协作完成任务。<br>2. 采用自训练和奖励引导的轨迹采样方法，优化多智能体协作。<br>3. 在 SIGIR 2025 LiveRAG 比赛中验证了其优越性。</p>
<p>Method: 1. 设计多智能体框架，每个智能体负责特定子任务（如规划、搜索等）。<br>2. 利用自训练范式结合奖励引导的轨迹采样，优化智能体间的协作策略。<br>3. 在 DataMorgana 数据集上进行训练和评估。</p>
<p>Result: mRAG 在 SIGIR 2025 LiveRAG 比赛中的表现优于传统 RAG 基线，展示了其在复杂任务中的生成能力。</p>
<p>Insight: 1. 多智能体协作能显著提升复杂任务的生成质量。<br>2. 自训练和奖励引导的轨迹采样是优化智能体协作的有效方法。</p>
<p>Abstract: This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG)<br>framework composed of specialized agents for subtasks such as planning,<br>searching, reasoning, and coordination. Our system uses a self-training<br>paradigm with reward-guided trajectory sampling to optimize inter-agent<br>collaboration and enhance response generation. Evaluated on DataMorgana-derived<br>datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms<br>conventional RAG baselines. We further analyze competition outcomes and<br>showcase the framework’s strengths with case studies, demonstrating its<br>efficacy for complex, real-world RAG tasks.</p>
</details>


<h3 id="26-Accelerating-Diffusion-Large-Language-Models-with-SlowFast-The-Three-Golden-Principles"><a href="#26-Accelerating-Diffusion-Large-Language-Models-with-SlowFast-The-Three-Golden-Principles" class="headerlink" title="[26] Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10848">Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles</a></h3><p><em>Qingyan Wei,Yaojie Zhang,Zhiyuan Liu,Dongrui Liu,Linfeng Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种名为SlowFast Sampling的动态采样策略，通过交替探索和加速解码阶段，显著提升扩散语言模型的推理效率，同时结合dLLM-Cache减少冗余计算。</p>
<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的采样策略（如基于置信度或半自回归解码）存在静态行为问题，导致效率不足和灵活性受限，因此需要更高效的动态采样方法。

<p>Contribution: 提出了SlowFast Sampling策略及其三个黄金原则（确定性原则、收敛原则和位置原则），并结合dLLM-Cache优化计算效率。</p>
<p>Method: 采用动态采样策略SlowFast Sampling，交替进行探索和加速解码，并利用三个原则指导解码时机和位置，同时结合缓存减少冗余计算。</p>
<p>Result: 实验表明，SlowFast Sampling在LLaDA上实现了15.63倍的加速，结合缓存后可达34.22倍，且吞吐量优于自回归基线LLaMA3 8B。</p>
<p>Insight: 合理的采样策略可以充分释放扩散语言模型的潜力，实现高效且高质量的文本生成。</p>
<p>Abstract: Diffusion-based language models (dLLMs) have emerged as a promising<br>alternative to traditional autoregressive LLMs by enabling parallel token<br>generation and significantly reducing inference latency. However, existing<br>sampling strategies for dLLMs, such as confidence-based or semi-autoregressive<br>decoding, often suffer from static behavior, leading to suboptimal efficiency<br>and limited flexibility. In this paper, we propose SlowFast Sampling, a novel<br>dynamic sampling strategy that adaptively alternates between exploratory and<br>accelerated decoding stages. Our method is guided by three golden principles:<br>certainty principle, convergence principle, and positional principle, which<br>govern when and where tokens can be confidently and efficiently decoded. We<br>further integrate our strategy with dLLM-Cache to reduce redundant computation.<br>Extensive experiments across benchmarks and models show that SlowFast Sampling<br>achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and<br>up to 34.22$\times$ when combined with caching. Notably, our approach<br>outperforms strong autoregressive baselines like LLaMA3 8B in throughput,<br>demonstrating that well-designed sampling can unlock the full potential of<br>dLLMs for fast and high-quality generation.</p>
</details>


<h3 id="27-Analyzing-the-relationships-between-pretraining-language-phonetic-tonal-and-speaker-information-in-self-supervised-speech-models"><a href="#27-Analyzing-the-relationships-between-pretraining-language-phonetic-tonal-and-speaker-information-in-self-supervised-speech-models" class="headerlink" title="[27] Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10855">Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models</a></h3><p><em>Michele Gubian,Ioana Krehan,Oli Liu,James Kirby,Sharon Goldwater</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文研究了自监督语音模型wav2vec2在不同语言预训练下对语音、音调、说话者信息的表示方式，揭示了这些信息在模型中的正交性及跨语言共性。</p>
<details>
  <summary>Details</summary>
Motivation: 目前对自监督语音模型的分析主要集中在英语，论文旨在探索wav2vec2模型在不同语言预训练下如何编码语音、音调和说话者信息，填补多语言研究的空白。

<p>Contribution: 论文的主要贡献是通过分析多语言数据，发现语音、音调和说话者信息在wav2vec2模型中通常是正交的，且表示结构在跨语言中表现一致性。</p>
<p>Method: 采用了探测分类器和几何分析，研究了预训练模型对语音、音调、说话者信息的编码方式，分析了匹配与非匹配语言的性能差异。</p>
<p>Result: 结果显示，所有预训练语言中，语音、音调和说话者信息的子空间基本正交，且层间性能模式相似，仅在匹配语言的语音和音调任务中后期层有微弱优势。</p>
<p>Insight: 研究表明，wav2vec2学习的表示结构主要独立于预训练语音材料，可能具有跨语言的通用性。</p>
<p>Abstract: Analyses of self-supervised speech models have begun to reveal where and how<br>they represent different types of information. However, almost all analyses<br>have focused on English. Here, we examine how wav2vec2 models trained on four<br>different languages encode both language-matched and non-matched speech. We use<br>probing classifiers and geometric analyses to examine how phones, lexical<br>tones, and speaker information are represented. We show that for all<br>pretraining and test languages, the subspaces encoding phones, tones, and<br>speakers are largely orthogonal, and that layerwise patterns of probing<br>accuracy are similar, with a relatively small advantage for matched-language<br>phone and tone (but not speaker) probes in the later layers. Our findings<br>suggest that the structure of representations learned by wav2vec2 is largely<br>independent of the speech material used during pretraining.</p>
</details>


<h3 id="28-Slimming-Down-LLMs-Without-Losing-Their-Minds"><a href="#28-Slimming-Down-LLMs-Without-Losing-Their-Minds" class="headerlink" title="[28] Slimming Down LLMs Without Losing Their Minds"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10885">Slimming Down LLMs Without Losing Their Minds</a></h3><p><em>Qingda,Mai</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文研究了高效参数微调方法（LoRA和QLoRA）对大型语言模型（LLM）性能的影响，验证了其在常识推理、数学推理和多领域知识任务中的表现，并强调了微调数据集与目标任务对齐的重要性。</p>
<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的规模不断扩大，高效参数微调方法的需求日益增长。作者旨在验证这些方法在任务特定性能提升中的效果及其计算效率。

<p>Contribution: 研究表明：(1) LoRA方法在提升任务性能的同时保持了计算效率；(2) 微调数据集与目标任务的对齐对性能影响显著。</p>
<p>Method: 采用了LoRA和QLoRA两种参数高效微调方法，并在HellaSwag、GSM8K和MMLU-CS三个评测基准上进行了实验验证。</p>
<p>Result: LoRA方法在任务特定性能上表现优异且高效，目标任务表现与微调数据集选择密切相关。</p>
<p>Insight: 为资源有限的开发者提供了高效适配LLM的理论依据和实践指导，强调了数据对齐在微调中的关键作用。</p>
<p>Abstract: This paper investigates and validates the impact of fine-tuning on large<br>language model performance, focusing on parameter-efficient methods (LoRA and<br>QLoRA). We evaluate model capabilities across three key domains: (1)<br>commonsense reasoning (HellaSwag), (2) mathematical reasoning (GSM8K), and (3)<br>multi-domain knowledge (MMLU-CS).<br>  Our findings demonstrate that: (1) LoRA-based methods effectively improve<br>task-specific performance while maintaining computational efficiency, and (2)<br>performance strongly depends on alignment between fine-tuning dataset and<br>benchmark tasks. The study provides both theoretical insights into<br>parameter-efficient mechanisms and practical guidance for developers<br>implementing efficient LLM adaptation with limited resources.</p>
</details>


<h3 id="29-Generalization-or-Hallucination-Understanding-Out-of-Context-Reasoning-in-Transformers"><a href="#29-Generalization-or-Hallucination-Understanding-Out-of-Context-Reasoning-in-Transformers" class="headerlink" title="[29] Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10887">Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers</a></h3><p><em>Yixiao Huang,Hanlin Zhu,Tianyu Guo,Jiantao Jiao,Somayeh Sojoudi,Michael I. Jordan,Stuart Russell,Song Mei</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文探讨了大型语言模型（LLMs）在新知识微调过程中表现出的‘泛化’与‘幻觉’行为的根源，提出了‘上下文外推理’（OCR）机制，并通过实验和理论分析验证了其作用。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能够通过微调获取新知识，但其在泛化和幻觉行为上的矛盾现象尚未得到充分解释。本文旨在揭示这两种行为的共同机制。

<p>Contribution: 1) 提出了‘上下文外推理’（OCR）机制，解释了LLMs的泛化与幻觉行为；2) 通过实验验证了OCR的存在及其对模型行为的影响；3) 理论分析了OCR的数学基础，即梯度下降的隐式偏好。</p>
<p>Method: 1) 在五个主流LLMs上设计实验，验证OCR的作用；2) 将OCR形式化为合成事实召回任务；3) 分析单层单头注意力模型的学习能力，强调了矩阵因式分解的重要性。</p>
<p>Result: 实验表明OCR驱动了泛化和幻觉行为，理论分析揭示了梯度下降倾向于最小化输出-值矩阵的核范数，从而解释了其高效学习能力。</p>
<p>Insight: 知识注入过程中的泛化和幻觉现象可归因于相同的底层机制（OCR），梯度下降的隐式偏好是模型学习关联能力的核心原因。</p>
<p>Abstract: Large language models (LLMs) can acquire new knowledge through fine-tuning,<br>but this process exhibits a puzzling duality: models can generalize remarkably<br>from new facts, yet are also prone to hallucinating incorrect information.<br>However, the reasons for this phenomenon remain poorly understood. In this<br>work, we argue that both behaviors stem from a single mechanism known as<br>out-of-context reasoning (OCR): the ability to deduce implications by<br>associating concepts, even those without a causal link. Our experiments across<br>five prominent LLMs confirm that OCR indeed drives both generalization and<br>hallucination, depending on whether the associated concepts are causally<br>related. To build a rigorous theoretical understanding of this phenomenon, we<br>then formalize OCR as a synthetic factual recall task. We empirically show that<br>a one-layer single-head attention-only transformer with factorized output and<br>value matrices can learn to solve this task, while a model with combined<br>weights cannot, highlighting the crucial role of matrix factorization. Our<br>theoretical analysis shows that the OCR capability can be attributed to the<br>implicit bias of gradient descent, which favors solutions that minimize the<br>nuclear norm of the combined output-value matrix. This mathematical structure<br>explains why the model learns to associate facts and implications with high<br>sample efficiency, regardless of whether the correlation is causal or merely<br>spurious. Ultimately, our work provides a theoretical foundation for<br>understanding the OCR phenomenon, offering a new lens for analyzing and<br>mitigating undesirable behaviors from knowledge injection.</p>
</details>


<h3 id="30-BioClinical-ModernBERT-A-State-of-the-Art-Long-Context-Encoder-for-Biomedical-and-Clinical-NLP"><a href="#30-BioClinical-ModernBERT-A-State-of-the-Art-Long-Context-Encoder-for-Biomedical-and-Clinical-NLP" class="headerlink" title="[30] BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10896">BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP</a></h3><p><em>Thomas Sounack,Joshua Davis,Brigitte Durieux,Antoine Chaffin,Tom J. Pollard,Eric Lehman,Alistair E. W. Johnson,Matthew McDermott,Tristan Naumann,Charlotta Lindvall</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: BioClinical ModernBERT 是一种针对生物医学和临床 NLP 任务的长上下文编码器，通过大规模领域适应和长上下文处理，提升速度和性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有编码器在生物医学和临床 NLP 中的应用受限，发展滞后于解码器模型，因此亟需一种高效、领域适应的解决方案。

<p>Contribution: 提出了 BioClinical ModernBERT，通过在最大生物医学和临床语料库上持续预训练，提升了领域适应性和任务表现。</p>
<p>Method: 基于 ModernBERT 进行领域适应，结合长上下文处理和 53.5B 标记的大规模预训练，利用多源数据集（20 个）提高泛化能力。</p>
<p>Result: 在四项下游任务中超越现有编码器，并提供了基础版和大版模型及训练检查点以供研究。</p>
<p>Insight: 多源数据和大规模预训练是提升生物医学和临床 NLP 模型性能的关键。</p>
<p>Abstract: Encoder-based transformer models are central to biomedical and clinical<br>Natural Language Processing (NLP), as their bidirectional self-attention makes<br>them well-suited for efficiently extracting structured information from<br>unstructured text through discriminative tasks. However, encoders have seen<br>slower development compared to decoder models, leading to limited domain<br>adaptation in biomedical and clinical settings. We introduce BioClinical<br>ModernBERT, a domain-adapted encoder that builds on the recent ModernBERT<br>release, incorporating long-context processing and substantial improvements in<br>speed and performance for biomedical and clinical NLP. BioClinical ModernBERT<br>is developed through continued pretraining on the largest biomedical and<br>clinical corpus to date, with over 53.5 billion tokens, and addresses a key<br>limitation of prior clinical encoders by leveraging 20 datasets from diverse<br>institutions, domains, and geographic regions, rather than relying on data from<br>a single source. It outperforms existing biomedical and clinical encoders on<br>four downstream tasks spanning a broad range of use cases. We release both base<br>(150M parameters) and large (396M parameters) versions of BioClinical<br>ModernBERT, along with training checkpoints to support further research.</p>
</details>


<h3 id="31-Beyond-Gold-Standards-Epistemic-Ensemble-of-LLM-Judges-for-Formal-Mathematical-Reasoning"><a href="#31-Beyond-Gold-Standards-Epistemic-Ensemble-of-LLM-Judges-for-Formal-Mathematical-Reasoning" class="headerlink" title="[31] Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10903">Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning</a></h3><p><em>Lan Zhang,Marco Valentino,Andre Freitas</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种基于LLM评委的系统化自动评估方法（EFG），用于形式数学推理中的自动形式化任务，通过多维度标准提高了评估的透明度和有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 当前自动形式化任务中，LLM评委的评估方法过于粗粒度，无法满足高级数学推理中对细微和多维度质量的要求，亟需一种更系统化的评估框架。

<p>Contribution: 提出了一个基于形式化和认知基础的LLM评委集成方法（EFG），通过逻辑保存、数学一致性、形式有效性和形式质量等多维度标准，实现了更透明和可靠的评估。</p>
<p>Method: 采用LLM评委集成（EFG），评估标准涵盖逻辑保存（LP）、数学一致性（MC）、形式有效性（FV）和形式质量（FQ），并与人类评估进行对比验证。</p>
<p>Result: 实验表明EFG集成方法在评估形式数学推理任务时，比粗粒度模型更贴合人类评估结果，尤其在形式质量方面表现突出。</p>
<p>Insight: 通过定义明确的原子属性指导LLM评委，可以构建一个可扩展、可解释且可靠的自动评估框架，特别适用于复杂的形式数学推理任务。</p>
<p>Abstract: Autoformalization plays a crucial role in formal mathematical reasoning by<br>enabling the automatic translation of natural language statements into formal<br>languages. While recent advances using large language models (LLMs) have shown<br>promising results, methods for automatically evaluating autoformalization<br>remain underexplored. As one moves to more complex domains (e.g., advanced<br>mathematics), human evaluation requires significant time and domain expertise,<br>especially as the complexity of the underlying statements and background<br>knowledge increases. LLM-as-a-judge presents a promising approach for<br>automating such evaluation. However, existing methods typically employ<br>coarse-grained and generic evaluation criteria, which limit their effectiveness<br>for advanced formal mathematical reasoning, where quality hinges on nuanced,<br>multi-granular dimensions. In this work, we take a step toward addressing this<br>gap by introducing a systematic, automatic method to evaluate autoformalization<br>tasks. The proposed method is based on an epistemically and formally grounded<br>ensemble (EFG) of LLM judges, defined on criteria encompassing logical<br>preservation (LP), mathematical consistency (MC), formal validity (FV), and<br>formal quality (FQ), resulting in a transparent assessment that accounts for<br>different contributing factors. We validate the proposed framework to serve as<br>a proxy for autoformalization assessment within the domain of formal<br>mathematics. Overall, our experiments demonstrate that the EFG ensemble of LLM<br>judges is a suitable emerging proxy for evaluation, more strongly correlating<br>with human assessments than a coarse-grained model, especially when assessing<br>formal qualities. These findings suggest that LLM-as-judges, especially when<br>guided by a well-defined set of atomic properties, could offer a scalable,<br>interpretable, and reliable support for evaluating formal mathematical<br>reasoning.</p>
</details>


<h3 id="32-Magistral"><a href="#32-Magistral" class="headerlink" title="[32] Magistral"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10910">Magistral</a></h3><p><em>Mistral-AI,:,Abhinav Rastogi,Albert Q. Jiang,Andy Lo,Gabrielle Berrada,Guillaume Lample,Jason Rute,Joep Barmentlo,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Léonard Blier,Lucile Saulnier,Matthieu Dinot,Maxime Darrin,Neha Gupta,Roman Soletskyi,Sagar Vaze,Teven Le Scao,Yihan Wang,Adam Yang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Andy Ehrenberg,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jean-Hadrien Chabran,Jean-Malo Delignon,Joachim Studnia,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Kush Jain,Lingxiao Zhao,Louis Martin,Luyu Gao,Lélio Renard Lavaud,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Maximilian Augustin,Mickaël Seznec,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Romain Sauvestre,Rémi Delacourt,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yunhao Tang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: Magistral 是 Mistral 的第一个推理模型，基于可扩展的强化学习（RL）流程，通过纯 RL 训练探索了大语言模型（LLM）的极限，并提供了强制模型推理语言的简单方法。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于现有实现或从先前模型中提取的 RL 痕迹，Magistral 通过从头开始的方法，仅使用自己的模型和基础设施，探索纯 RL 训练的可能性。

<p>Contribution: 1. 展示了纯 RL 训练 LLM 的极限；2. 提出了一种强制模型推理语言的简单方法；3. 证明了仅基于文本数据的 RL 训练能保留初始检查点的多模态理解、指令跟随和函数调用能力。</p>
<p>Method: 使用 Mistral Medium 3 训练 Magistral Medium，完全基于 RL；开源 Magistral Small，包含来自 Magistral Medium 的冷启动数据。</p>
<p>Result: RL 训练不仅保持了初始模型的能力，还改善了多模态理解、指令跟随和函数调用。</p>
<p>Insight: 纯 RL 训练是一种可行的 LLM 训练策略，能够在不依赖外部数据的情况下提升模型性能。</p>
<p>Abstract: We introduce Magistral, Mistral’s first reasoning model and our own scalable<br>reinforcement learning (RL) pipeline. Instead of relying on existing<br>implementations and RL traces distilled from prior models, we follow a ground<br>up approach, relying solely on our own models and infrastructure. Notably, we<br>demonstrate a stack that enabled us to explore the limits of pure RL training<br>of LLMs, present a simple method to force the reasoning language of the model,<br>and show that RL on text data alone maintains most of the initial checkpoint’s<br>capabilities. We find that RL on text maintains or improves multimodal<br>understanding, instruction following and function calling. We present Magistral<br>Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we<br>open-source Magistral Small (Apache 2.0) which further includes cold-start data<br>from Magistral Medium.</p>
</details>


<h3 id="33-Dynamic-Epistemic-Friction-in-Dialogue"><a href="#33-Dynamic-Epistemic-Friction-in-Dialogue" class="headerlink" title="[33] Dynamic Epistemic Friction in Dialogue"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10934">Dynamic Epistemic Friction in Dialogue</a></h3><p><em>Timothy Obiso,Kenneth Lai,Abhijnan Nath,Nikhil Krishnaswamy,James Pustejovsky</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文探讨了大型语言模型（LLM）在与人类协作时的’动态认知摩擦’，即因新信息与现有信念冲突而产生的阻力，并提出了一种基于动态认知逻辑的模型来预测对话中的信念更新。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管LLM在人类偏好对齐方面取得了进展，但现有方法忽视了信念更新过程中的认知摩擦，导致在对抗性或模糊信息下的表现不足。

<p>Contribution: 提出了动态认知摩擦的概念，并构建了一个基于动态认知逻辑的模型，用于分析和预测协作任务中对话的信念更新过程。</p>
<p>Method: 使用动态认知逻辑框架，将认知摩擦建模为信念修正中的阻力，并通过实际协作任务的分析验证模型的有效性。</p>
<p>Result: 模型能够有效预测对话中的信念更新行为，并为复杂现实对话场景的信念对齐提供了更精细的度量方法。</p>
<p>Insight: 动态认知摩擦为理解人类-AI协作中的信念冲突和更新提供了新视角，有助于改进LLM在对抗性环境下的表现。</p>
<p>Abstract: Recent developments in aligning Large Language Models (LLMs) with human<br>preferences have significantly enhanced their utility in human-AI collaborative<br>scenarios. However, such approaches often neglect the critical role of<br>“epistemic friction,” or the inherent resistance encountered when updating<br>beliefs in response to new, conflicting, or ambiguous information. In this<br>paper, we define dynamic epistemic friction as the resistance to epistemic<br>integration, characterized by the misalignment between an agent’s current<br>belief state and new propositions supported by external evidence. We position<br>this within the framework of Dynamic Epistemic Logic (Van Benthem and Pacuit,<br>2011), where friction emerges as nontrivial belief-revision during the<br>interaction. We then present analyses from a situated collaborative task that<br>demonstrate how this model of epistemic friction can effectively predict belief<br>updates in dialogues, and we subsequently discuss how the model of belief<br>alignment as a measure of epistemic resistance or friction can naturally be<br>made more sophisticated to accommodate the complexities of real-world dialogue<br>scenarios.</p>
</details>


<h3 id="34-Domain2Vec-Vectorizing-Datasets-to-Find-the-Optimal-Data-Mixture-without-Training"><a href="#34-Domain2Vec-Vectorizing-Datasets-to-Find-the-Optimal-Data-Mixture-without-Training" class="headerlink" title="[34] Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10952">Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training</a></h3><p><em>Mozhi Zhang,Howe Tissue,Lu Wang,Xipeng Qiu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: Domain2Vec提出了一种无训练的方法，通过将数据集分解为元域（meta-domains）的线性组合来优化数据混合，显著减少了计算开销。</p>
<details>
  <summary>Details</summary>
Motivation: 当前在语言模型预训练中，如何选择最优的数据混合是一个重要问题。传统方法需要多次训练，计算成本高。本文提出了一种无需训练的方法，通过向量化数据集来找到最佳数据混合。

<p>Contribution: 1. 提出了域向量化的新方法Domain2Vec；2. 引入元域概念和分布对齐假设（DA^2）；3. 实现了无需训练的最优数据混合选择。</p>
<p>Method: Domain2Vec将数据集分解为元域的线性组合，生成域向量并表示分布；利用分类器进行分解；通过分布对齐假设优化数据混合。</p>
<p>Result: 在Pile-CC上，仅用51.5%的计算量达到了与原数据混合相同的验证损失；在相同计算预算下，下游任务性能平均提升2.83%。</p>
<p>Insight: 分布对齐假设为数据混合优化提供了理论基础；Domain2Vec的向量化方法提升了传统方法的效率和扩展性。</p>
<p>Abstract: We introduce~\textsc{Domain2Vec}, a novel approach that decomposes any<br>dataset into a linear combination of several \emph{meta-domains}, a new concept<br>designed to capture the key underlying features of datasets.<br>\textsc{Domain2Vec} maintains a vocabulary of meta-domains and uses a<br>classifier to decompose any given dataset into a domain vector that corresponds<br>to a distribution over this vocabulary. These domain vectors enable the<br>identification of the optimal data mixture for language model (LM) pretraining<br>in a training-free manner under the \emph{\textbf{D}istribution<br>\textbf{A}lignment \textbf{A}ssumption} (DA$^{2}$), which suggests that when<br>the data distributions of the training set and the validation set are better<br>aligned, a lower validation loss is achieved. Moreover, \textsc{Domain2vec} can<br>be seamlessly integrated into previous works to model the relationship between<br>domain vectors and LM performance, greatly enhancing the efficiency and<br>scalability of previous methods. Extensive experiments demonstrate that<br>\textsc{Domain2Vec} helps find the data mixture that enhances downstream task<br>performance with minimal computational overhead. Specifically,<br>\textsc{Domain2Vec} achieves the same validation loss on Pile-CC using only<br>$51.5%$ of the computation required when training on the original mixture of<br>The Pile dataset. Under equivalent compute budget, \textsc{Domain2Vec} improves<br>downstream performance by an average of $2.83%$.</p>
</details>


<h3 id="35-How-Well-Can-Reasoning-Models-Identify-and-Recover-from-Unhelpful-Thoughts"><a href="#35-How-Well-Can-Reasoning-Models-Identify-and-Recover-from-Unhelpful-Thoughts" class="headerlink" title="[35] How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10979">How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?</a></h3><p><em>Sohee Yang,Sang-Woo Lee,Nora Kassner,Daniela Gottesman,Sebastian Riedel,Mor Geva</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文探讨了推理模型在识别和恢复四种无益思维上的表现，发现模型能有效识别但难以从中恢复，尤其是大模型表现更差，并提出了改进自我评估能力的呼吁。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索推理模型是否具备有效的自我评估能力，能否识别并修正无益思维，以提升模型的推理准确性和安全性。

<p>Contribution: 主要贡献包括：1) 对四种无益思维的分类研究；2) 揭示了模型在识别和恢复无益思维上的局限性；3) 发现大模型在恢复短无关思维时的反规模趋势；4) 通过实验展示了模型在有害思维注入时的安全风险。</p>
<p>Method: 方法包括：1) 设计实验评估模型对无益思维的识别和恢复能力；2) 注入四种无益思维并观察模型表现；3) 比较不同规模模型的性能差异；4) 进行 jailbreak 实验以验证安全影响。</p>
<p>Result: 结果显示：1) 模型能有效识别无益思维但难以恢复；2) 大模型在短无关思维注入时表现更差；3) 小模型对有害思维的干扰最小。</p>
<p>Insight: 核心洞察是：模型的自我评估能力尚不成熟，需进一步改进以实现更可靠和安全的推理系统，同时规模并非总是提高性能的关键因素。</p>
<p>Abstract: Recent reasoning models show the ability to reflect, backtrack, and<br>self-validate their reasoning, which is crucial in spotting mistakes and<br>arriving at accurate solutions. A natural question that arises is how<br>effectively models can perform such self-reevaluation. We tackle this question<br>by investigating how well reasoning models identify and recover from four types<br>of unhelpful thoughts: uninformative rambling thoughts, thoughts irrelevant to<br>the question, thoughts misdirecting the question as a slightly different<br>question, and thoughts that lead to incorrect answers. We show that models are<br>effective at identifying most unhelpful thoughts but struggle to recover from<br>the same thoughts when these are injected into their thinking process, causing<br>significant performance drops. Models tend to naively continue the line of<br>reasoning of the injected irrelevant thoughts, which showcases that their<br>self-reevaluation abilities are far from a general “meta-cognitive” awareness.<br>Moreover, we observe non&#x2F;inverse-scaling trends, where larger models struggle<br>more than smaller ones to recover from short irrelevant thoughts, even when<br>instructed to reevaluate their reasoning. We demonstrate the implications of<br>these findings with a jailbreak experiment using irrelevant thought injection,<br>showing that the smallest models are the least distracted by<br>harmful-response-triggering thoughts. Overall, our findings call for<br>improvement in self-reevaluation of reasoning models to develop better<br>reasoning and safer systems.</p>
</details>


<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="36-Multimodal-Cinematic-Video-Synthesis-Using-Text-to-Image-and-Audio-Generation-Models"><a href="#36-Multimodal-Cinematic-Video-Synthesis-Using-Text-to-Image-and-Audio-Generation-Models" class="headerlink" title="[36] Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10005">Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models</a></h3><p><em>Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种多模态电影视频合成方法，结合文本到图像和音频生成模型，实现了60秒高质量电影的自动化生成，支持高达1024x768的分辨率和15-30 FPS的帧率。</p>
<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的进步推动了多媒体创作的自动化，但目前仍缺乏一种整合文本、图像和音频合成的端到端电影视频生成方法。

<p>Contribution: 主要贡献是提出了一种五场景框架，结合Stable Diffusion、GPT-2和混合音频流水线，实现了高质量的视听同步电影视频合成，并通过优化确保了效率和可靠性。</p>
<p>Method: 方法包括：1）使用Stable Diffusion生成高保真图像；2）GPT-2用于叙事结构；3）混合音频流水线（gTTS和YouTube音乐）；4）线性帧插值和电影级后处理增强质量；5）优化CUDA内存管理和错误处理。</p>
<p>Result: 实验表明，该方法在视觉质量、叙事连贯性和效率上表现优异，适用于创意、教育和工业应用。</p>
<p>Insight: 该工作展示了生成式AI在多模态视频合成中的潜力，通过整合不同模型和流水线优化，实现了端到端的电影级内容生成。</p>
<p>Abstract: Advances in generative artificial intelligence have altered multimedia<br>creation, allowing for automatic cinematic video synthesis from text inputs.<br>This work describes a method for creating 60-second cinematic movies<br>incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for<br>narrative structuring, and a hybrid audio pipeline using gTTS and<br>YouTube-sourced music. It uses a five-scene framework, which is augmented by<br>linear frame interpolation, cinematic post-processing (e.g., sharpening), and<br>audio-video synchronization to provide professional-quality results. It was<br>created in a GPU-accelerated Google Colab environment using Python 3.11. It has<br>a dual-mode Gradio interface (Simple and Advanced), which supports resolutions<br>of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA<br>memory management and error handling ensure reliability. The experiments<br>demonstrate outstanding visual quality, narrative coherence, and efficiency,<br>furthering text-to-video synthesis for creative, educational, and industrial<br>applications.</p>
</details>


<h3 id="37-LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning"><a href="#37-LoRA-Edit-Controllable-First-Frame-Guided-Video-Editing-via-Mask-Aware-LoRA-Fine-Tuning" class="headerlink" title="[37] LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10082">LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning</a></h3><p><em>Chenjian Gao,Lihe Ding,Xin Cai,Zhanpeng Huang,Zibin Wang,Tianfan Xue</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种基于LoRA（低秩适应）微调的视频编辑方法LoRA-Edit，通过掩码感知的LoRA微调实现可控的首帧引导视频编辑，避免了大规模预训练的局限性，同时保留了背景并支持灵活的编辑传播。</p>
<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的视频编辑方法依赖于大规模预训练，缺乏对特定编辑的灵活性。首帧引导编辑虽然能控制第一帧，但对后续帧的控制不足。论文旨在解决这一问题。

<p>Contribution: 提出了一种掩码驱动的LoRA微调方法，能够在不改变模型架构的情况下高效地适应预训练的Image-to-Video模型，支持灵活的编辑传播和背景保留。</p>
<p>Method: 通过掩码感知的LoRA微调技术，结合输入视频的空间结构和运动线索以及参考图像的外观引导，动态调节模型对不同区域的关注点。</p>
<p>Result: 实验表明，该方法在视频编辑任务中优于现有方法，实现了高质量的编辑效果。</p>
<p>Insight: 该方法通过掩码和LoRA技术的结合，实现了对视频编辑的灵活控制，同时避免了模型架构的修改，为高效视频编辑提供了新思路。</p>
<p>Abstract: Video editing using diffusion models has achieved remarkable results in<br>generating high-quality edits for videos. However, current methods often rely<br>on large-scale pretraining, limiting flexibility for specific edits.<br>First-frame-guided editing provides control over the first frame, but lacks<br>flexibility over subsequent frames. To address this, we propose a mask-based<br>LoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video<br>(I2V) models for flexible video editing. Our approach preserves background<br>regions while enabling controllable edits propagation. This solution offers<br>efficient and adaptable video editing without altering the model architecture.<br>To better steer this process, we incorporate additional references, such as<br>alternate viewpoints or representative scene states, which serve as visual<br>anchors for how content should unfold. We address the control challenge using a<br>mask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model<br>to the editing context. The model must learn from two distinct sources: the<br>input video provides spatial structure and motion cues, while reference images<br>offer appearance guidance. A spatial mask enables region-specific learning by<br>dynamically modulating what the model attends to, ensuring that each area draws<br>from the appropriate source. Experimental results show our method achieves<br>superior video editing performance compared to state-of-the-art methods.</p>
</details>


<h3 id="38-DeepTraverse-A-Depth-First-Search-Inspired-Network-for-Algorithmic-Visual-Understanding"><a href="#38-DeepTraverse-A-Depth-First-Search-Inspired-Network-for-Algorithmic-Visual-Understanding" class="headerlink" title="[38] DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10084">DeepTraverse: A Depth-First Search Inspired Network for Algorithmic Visual Understanding</a></h3><p><em>Bin Guo,John H. L. Hansen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DeepTraverse提出了一种受深度优先搜索启发的视觉网络架构，通过递归探索和自适应校准模块实现特征的系统性构建和精细化，提升了分类准确性和特征判别力。</p>
<details>
  <summary>Details</summary>
Motivation: 传统视觉主干网络的特征构建方式单一，缺乏自适应迭代优化的能力。作者受经典搜索算法启发，探索了将算法化、结构化处理流程引入视觉网络的潜力，以提高特征的可解释性和推理能力。

<p>Contribution: 1) 提出了DeepTraverse架构，结合递归探索模块和自适应校准模块，实现了高效且结构化的特征构建方法。2) 通过实验验证了该架构在分类任务中的竞争性能，优于传统模型。</p>
<p>Method: 1) 递归探索模块：通过参数共享深度分析特征路径。2) 自适应校准模块：动态调整特征重要性以适配全局上下文。两者的协同作用实现了特征的系统性构建与优化。</p>
<p>Result: 在多个图像分类基准测试中，DeepTraverse表现出更高的分类准确性和特征判别力，其性能优于参数数量相近或更多的传统模型。</p>
<p>Insight: 将算法先验（如搜索策略）引入视觉网络设计，能够构建更高效、性能更强且结构化的视觉主干网络，为网络的可解释性和推理能力提供了新思路。</p>
<p>Abstract: Conventional vision backbones, despite their success, often construct<br>features through a largely uniform cascade of operations, offering limited<br>explicit pathways for adaptive, iterative refinement. This raises a compelling<br>question: can principles from classical search algorithms instill a more<br>algorithmic, structured, and logical processing flow within these networks,<br>leading to representations built through more interpretable, perhaps<br>reasoning-like decision processes? We introduce DeepTraverse, a novel vision<br>architecture directly inspired by algorithmic search strategies, enabling it to<br>learn features through a process of systematic elucidation and adaptive<br>refinement distinct from conventional approaches. DeepTraverse operationalizes<br>this via two key synergistic components: recursive exploration modules that<br>methodically deepen feature analysis along promising representational paths<br>with parameter sharing for efficiency, and adaptive calibration modules that<br>dynamically adjust feature salience based on evolving global context. The<br>resulting algorithmic interplay allows DeepTraverse to intelligently construct<br>and refine feature patterns. Comprehensive evaluations across a diverse suite<br>of image classification benchmarks show that DeepTraverse achieves highly<br>competitive classification accuracy and robust feature discrimination, often<br>outperforming conventional models with similar or larger parameter counts. Our<br>work demonstrates that integrating such algorithmic priors provides a<br>principled and effective strategy for building more efficient, performant, and<br>structured vision backbones.</p>
</details>


<h3 id="39-Test-Time-Adaptation-for-Generalizable-Task-Progress-Estimation"><a href="#39-Test-Time-Adaptation-for-Generalizable-Task-Progress-Estimation" class="headerlink" title="[39] Test-Time Adaptation for Generalizable Task Progress Estimation"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10085">Test-Time Adaptation for Generalizable Task Progress Estimation</a></h3><p><em>Christos Ziakas,Alessandra Russo</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种测试时自适应方法，通过优化自监督目标，使进度估计模型能够在线适应测试轨迹的视觉和时间上下文。该方法基于梯度元学习策略，利用专家视觉轨迹和自然语言任务描述进行训练，从而提升基于语义内容而非时间顺序的进度估计性能。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的进度估计方法在面对分布外任务、环境和体现形式时表现不佳。本文旨在提出一种通用性强的自适应方法，使模型在测试时能够动态适应新场景。

<p>Contribution: 1. 提出了一种测试时自适应方法，能够在测试时在线优化模型以适应新场景。2. 引入梯度元学习策略，结合专家视觉轨迹和自然语言任务描述进行训练。3. 展示了该方法在分布外任务、环境和体现形式中的泛化能力。</p>
<p>Method: 采用梯度元学习策略，训练模型基于专家视觉轨迹和自然语言任务描述，并设计了一种自监督目标函数。在测试时，模型通过优化该目标函数自适应调整参数。</p>
<p>Result: 实验表明，该方法在分布外任务、环境和体现形式中的表现优于当前最先进的基于上下文的自动回归视觉-语言模型方法。</p>
<p>Insight: 通过结合元学习和测试时自适应，模型能够在不依赖时间顺序的情况下更好地捕捉语义内容，从而提升通用性和适应性。</p>
<p>Abstract: We propose a test-time adaptation method that enables a progress estimation<br>model to adapt online to the visual and temporal context of test trajectories<br>by optimizing a learned self-supervised objective. To this end, we introduce a<br>gradient-based meta-learning strategy to train the model on expert visual<br>trajectories and their natural language task descriptions, such that test-time<br>adaptation improves progress estimation relying on semantic content over<br>temporal order. Our test-time adaptation method generalizes from a single<br>training environment to diverse out-of-distribution tasks, environments, and<br>embodiments, outperforming the state-of-the-art in-context learning approach<br>using autoregressive vision-language models.</p>
</details>


<h3 id="40-EfficientVLA-Training-Free-Acceleration-and-Compression-for-Vision-Language-Action-Models"><a href="#40-EfficientVLA-Training-Free-Acceleration-and-Compression-for-Vision-Language-Action-Models" class="headerlink" title="[40] EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10100">EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models</a></h3><p><em>Yantai Yang,Yuhao Wang,Zichen Wen,Luo Zhongwei,Chang Zou,Zhipeng Zhang,Chuan Wen,Linfeng Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: EfficientVLA是一种无需训练的高效推理加速框架，用于减少VLA模型的计算和内存开销，通过剪枝、视觉令牌优化和特征缓存策略，显著提升了推理速度，同时保持了性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前VLA模型（如扩散式架构）在计算和内存需求上存在冗余问题，限制了其实际部署能力。现有的加速方法通常只能解决局部问题，无法全面优化整个VLA流程。

<p>Contribution: 提出了EfficientVLA框架，通过综合剪枝、视觉令牌优化和特征缓存三种策略，系统性地消除VLA模型的计算和内存冗余，实现高效推理。</p>
<p>Method: 1）剪枝语言模块中功能冗余的层；2）使用任务感知策略优化视觉令牌选择；3）在扩散式动作头中缓存和重用中间特征以减少冗余。</p>
<p>Result: 在CogACT模型上实现了1.93倍的推理加速，计算量降至28.9%，同时在SIMPLER基准测试中仅降低0.6%的成功率。</p>
<p>Insight: 通过结构化和训练无关的方法，可以显著提升VLA模型的效率，为实际部署提供了可能性。</p>
<p>Abstract: Vision-Language-Action (VLA) models, particularly diffusion-based<br>architectures, demonstrate transformative potential for embodied intelligence<br>but are severely hampered by high computational and memory demands stemming<br>from extensive inherent and inference-time redundancies. While existing<br>acceleration efforts often target isolated inefficiencies, such piecemeal<br>solutions typically fail to holistically address the varied computational and<br>memory bottlenecks across the entire VLA pipeline, thereby limiting practical<br>deployability. We introduce EfficientVLA, a structured and training-free<br>inference acceleration framework that systematically eliminates these barriers<br>by cohesively exploiting multifaceted redundancies. EfficientVLA<br>synergistically integrates three targeted strategies: (1) pruning of<br>functionally inconsequential layers from the language module, guided by an<br>analysis of inter-layer redundancies; (2) optimizing the visual processing<br>pathway through a task-aware strategy that selects a compact, diverse set of<br>visual tokens, balancing task-criticality with informational coverage; and (3)<br>alleviating temporal computational redundancy within the iterative<br>diffusion-based action head by strategically caching and reusing key<br>intermediate features. We apply our method to a standard VLA model CogACT,<br>yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%<br>success rate drop in the SIMPLER benchmark.</p>
</details>


<h3 id="41-A-Manually-Annotated-Image-Caption-Dataset-for-Detecting-Children-in-the-Wild"><a href="#41-A-Manually-Annotated-Image-Caption-Dataset-for-Detecting-Children-in-the-Wild" class="headerlink" title="[41] A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10117">A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild</a></h3><p><em>Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文发布了首个多模态环境下检测未成年人的图像-字幕数据集ICCWD，包含10,000个手动标注的图像-字幕对，用于评估检测工具的性能。实验显示现有方法在此任务上仍有挑战性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有缺乏多模态环境下检测未成年人的数据集，而法律和平台对未成年人内容的监管需求迫切。

<p>Contribution: 发布了首个多模态未成年人检测数据集ICCWD，包含丰富的上下文和标注。</p>
<p>Method: 通过手动标注10,000个图像-字幕对，设计实验评估了三种检测器的性能，包括商用年龄估计系统。</p>
<p>Result: 最佳检测器的真阳性率为75.3%，表明任务具有挑战性。</p>
<p>Insight: 多模态数据集有助于设计更优的未成年人检测方法，填补了现有研究空白。</p>
<p>Abstract: Platforms and the law regulate digital content depicting minors (defined as<br>individuals under 18 years of age) differently from other types of content.<br>Given the sheer amount of content that needs to be assessed, machine<br>learning-based automation tools are commonly used to detect content depicting<br>minors. To our knowledge, no dataset or benchmark currently exists for<br>detecting these identification methods in a multi-modal environment. To fill<br>this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an<br>image-caption dataset aimed at benchmarking tools that detect depictions of<br>minors. Our dataset is richer than previous child image datasets, containing<br>images of children in a variety of contexts, including fictional depictions and<br>partially visible bodies. ICCWD contains 10,000 image-caption pairs manually<br>labeled to indicate the presence or absence of a child in the image. To<br>demonstrate the possible utility of our dataset, we use it to benchmark three<br>different detectors, including a commercial age estimation system applied to<br>images. Our results suggest that child detection is a challenging task, with<br>the best method achieving a 75.3% true positive rate. We hope the release of<br>our dataset will aid in the design of better minor detection methods in a wide<br>range of scenarios.</p>
</details>


<h3 id="42-Deteccao-da-Psoriase-Utilizando-Visao-Computacional-Uma-Abordagem-Comparativa-Entre-CNNs-e-Vision-Transformers"><a href="#42-Deteccao-da-Psoriase-Utilizando-Visao-Computacional-Uma-Abordagem-Comparativa-Entre-CNNs-e-Vision-Transformers" class="headerlink" title="[42] Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10119">Detecção da Psoríase Utilizando Visão Computacional: Uma Abordagem Comparativa Entre CNNs e Vision Transformers</a></h3><p><em>Natanael Lucena,Fábio S. da Silva,Ricardo Rios</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文比较了卷积神经网络（CNN）和视觉变换器（ViT）在多分类银屑病及其类似病变图像任务中的性能，发现ViT在小模型上表现更优，推荐DaViT-B架构用于自动化检测。</p>
<details>
  <summary>Details</summary>
Motivation: 研究旨在探索CNN和ViT在医学图像分类中的性能差异，特别是针对银屑病的识别任务，以推动高效自动化诊断工具的发展。

<p>Contribution: 通过实验对比证明了ViT在银屑病检测任务中的优越性，尤其是DaViT-B模型取得了96.4%的F1分数。</p>
<p>Method: 使用ImageNet预训练的CNN和ViT模型，针对特定数据集进行微调，评估其性能指标。</p>
<p>Result: ViT表现优于CNN，DaViT-B模型性能最佳，F1分数达96.4%。</p>
<p>Insight: 视觉变换器在小模型情况下展现出色性能，表明其在医学图像分类任务中的潜力。</p>
<p>Abstract: This paper presents a comparison of the performance of Convolutional Neural<br>Networks (CNNs) and Vision Transformers (ViTs) in the task of multi-classifying<br>images containing lesions of psoriasis and diseases similar to it. Models<br>pre-trained on ImageNet were adapted to a specific data set. Both achieved high<br>predictive metrics, but the ViTs stood out for their superior performance with<br>smaller models. Dual Attention Vision Transformer-Base (DaViT-B) obtained the<br>best results, with an f1-score of 96.4%, and is recommended as the most<br>efficient architecture for automated psoriasis detection. This article<br>reinforces the potential of ViTs for medical image classification tasks.</p>
</details>


<h3 id="43-ViCrit-A-Verifiable-Reinforcement-Learning-Proxy-Task-for-Visual-Perception-in-VLMs"><a href="#43-ViCrit-A-Verifiable-Reinforcement-Learning-Proxy-Task-for-Visual-Perception-in-VLMs" class="headerlink" title="[43] ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10128">ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs</a></h3><p><em>Xiyao Wang,Zhengyuan Yang,Chao Feng,Yongyuan Liang,Yuhang Zhou,Xiaoyu Liu,Ziyi Zang,Ming Li,Chung-Ching Lin,Kevin Lin,Linjie Li,Furong Huang,Lijuan Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了ViCrit任务，通过强化学习（RL）微调视觉语言模型（VLMs），任务要求模型定位人为注入的视觉描述错误，从而提升视觉感知能力。ViCrit平衡了任务难度与可验证性，同时在多个VL基准测试中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 现有RL任务多针对纯语言模型（如数学推理或代码生成），但VLMs的视觉感知任务缺乏既具挑战性又可明确验证的代理任务，阻碍了RL在视觉领域的应用。

<p>Contribution: 提出了ViCrith任务，通过定位合成视觉幻觉（即错误的视觉描述），为VLMs提供一种可验证的RL代理任务，并展示了其在多种VL任务中的泛化能力。</p>
<p>Method: 在200字的人类标注图像描述中注入单一视觉错误（如物体、属性、数量或空间关系），要求模型基于图像和修改后的描述定位错误。任务设计为二元精确匹配奖励，便于计算。</p>
<p>Result: 实验表明，ViCrit训练的模型在多个VL基准测试中表现显著提升，并能泛化到自然图像以外的抽象推理和视觉数学任务。</p>
<p>Insight: ViCrit不仅提升了模型的记忆能力，还促进了真正的视觉感知学习，表明细粒度的幻觉批评是提升VLMs视觉感知的有效目标。</p>
<p>Abstract: Reinforcement learning (RL) has shown great effectiveness for fine-tuning<br>large language models (LLMs) using tasks that are challenging yet easily<br>verifiable, such as math reasoning or code generation. However, extending this<br>success to visual perception in vision-language models (VLMs) has been impeded<br>by the scarcity of vision-centric tasks that are simultaneously challenging and<br>unambiguously verifiable. To this end, we introduce ViCrit (Visual Caption<br>Hallucination Critic), an RL proxy task that trains VLMs to localize a subtle,<br>synthetic visual hallucination injected into paragraphs of human-written image<br>captions. Starting from a 200-word captions, we inject a single, subtle visual<br>description error-altering a few words on objects, attributes, counts, or<br>spatial relations-and task the model to pinpoint the corrupted span given the<br>image and the modified caption. This formulation preserves the full perceptual<br>difficulty while providing a binary, exact-match reward that is easy to compute<br>and unambiguous. Models trained with the ViCrit Task exhibit substantial gains<br>across a variety of VL benchmarks. Crucially, the improvements transfer beyond<br>natural-image training data to abstract image reasoning and visual math,<br>showing promises of learning to perceive rather than barely memorizing seen<br>objects. To facilitate evaluation, we further introduce ViCrit-Bench, a<br>category-balanced diagnostic benchmark that systematically probes perception<br>errors across diverse image domains and error types. Together, our results<br>demonstrate that fine-grained hallucination criticism is an effective and<br>generalizable objective for enhancing visual perception in VLMs.</p>
</details>


<h3 id="44-Retrieval-of-Surface-Solar-Radiation-through-Implicit-Albedo-Recovery-from-Temporal-Context"><a href="#44-Retrieval-of-Surface-Solar-Radiation-through-Implicit-Albedo-Recovery-from-Temporal-Context" class="headerlink" title="[44] Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10174">Retrieval of Surface Solar Radiation through Implicit Albedo Recovery from Temporal Context</a></h3><p><em>Yael Frischholz,Devis Tuia,Michael Lehning</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种基于注意力机制的太阳辐射反演方法，通过隐式学习从卫星图像序列中推断背景反射率，解决了传统方法在山区动态雪盖区域的局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 传统太阳辐射反演算法依赖月度统计估计背景反射率，但在山区由于动态雪盖导致性能下降。本文旨在通过隐式学习背景反射率来提升反演精度。

<p>Contribution: 提出了一种基于Temporo-Spatial Vision Transformer的注意力机制模型，无需手工特征（如反照率图或云掩膜），直接从卫星图像序列中学习背景反射率动态。</p>
<p>Method: 利用多光谱SEVIRI图像、静态地形特征和太阳几何数据，训练模型从时间上下文中隐式推断背景反射率。模型基于HelioMont算法的太阳辐射估计进行训练。</p>
<p>Result: 实验表明，模型在提供足够的时间上下文时，性能与依赖反照率的方法相当，尤其在山区表现突出。</p>
<p>Insight: 时间上下文在隐式学习背景反射率中至关重要，尤其在动态雪盖区域。模型能够捕捉并利用潜在地表反射动态，提升泛化能力。</p>
<p>Abstract: Accurate retrieval of surface solar radiation (SSR) from satellite imagery<br>critically depends on estimating the background reflectance that a spaceborne<br>sensor would observe under clear-sky conditions. Deviations from this baseline<br>can then be used to detect cloud presence and guide radiative transfer models<br>in inferring atmospheric attenuation. Operational retrieval algorithms<br>typically approximate background reflectance using monthly statistics, assuming<br>surface properties vary slowly relative to atmospheric conditions. However,<br>this approach fails in mountainous regions where intermittent snow cover and<br>changing snow surfaces are frequent. We propose an attention-based emulator for<br>SSR retrieval that implicitly learns to infer clear-sky surface reflectance<br>from raw satellite image sequences. Built on the Temporo-Spatial Vision<br>Transformer, our approach eliminates the need for hand-crafted features such as<br>explicit albedo maps or cloud masks. The emulator is trained on instantaneous<br>SSR estimates from the HelioMont algorithm over Switzerland, a region<br>characterized by complex terrain and dynamic snow cover. Inputs include<br>multi-spectral SEVIRI imagery from the Meteosat Second Generation platform,<br>augmented with static topographic features and solar geometry. The target<br>variable is HelioMont’s SSR, computed as the sum of its direct and diffuse<br>horizontal irradiance components, given at a spatial resolution of 1.7 km. We<br>show that, when provided a sufficiently long temporal context, the model<br>matches the performances of albedo-informed models, highlighting the model’s<br>ability to internally learn and exploit latent surface reflectance dynamics.<br>Our geospatial analysis shows this effect is most powerful in mountainous<br>regions and improves generalization in both simple and complex topographic<br>settings. Code and datasets are publicly available at<br><a target="_blank" rel="noopener" href="https://github.com/frischwood/HeMu-dev.git">https://github.com/frischwood/HeMu-dev.git</a></p>
</details>


<h3 id="45-Attention-Please-Revisiting-Attentive-Probing-for-Masked-Image-Modeling"><a href="#45-Attention-Please-Revisiting-Attentive-Probing-for-Masked-Image-Modeling" class="headerlink" title="[45] Attention, Please! Revisiting Attentive Probing for Masked Image Modeling"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10178">Attention, Please! Revisiting Attentive Probing for Masked Image Modeling</a></h3><p><em>Bill Psomas,Dionysis Christopoulos,Eirini Baltzi,Ioannis Kakogeorgiou,Tilemachos Aravanis,Nikos Komodakis,Konstantinos Karantzalos,Yannis Avrithis,Giorgos Tolias</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了高效的注意力探测方法（EP），通过多查询交叉注意力机制减少冗余参数，提升计算效率，并在多个基准测试中优于线性探测和传统注意力探测方法。</p>
<details>
  <summary>Details</summary>
Motivation: 由于微调在大规模场景下不切实际，探测成为自监督学习的重要评估协议。然而，标准的线性探测（LP）无法充分反映掩码图像建模（MIM）的潜力，因为其分布式补丁令牌的特性。这促使了对注意力探测的需求，但现有方法存在参数冗余和计算效率低的问题。

<p>Contribution: 本文引入了高效探测（EP），一种多查询交叉注意力机制，显著减少了冗余投影和可训练参数，实现了10倍的速度提升，并在多个基准测试中表现优于现有方法。</p>
<p>Method: EP通过多查询交叉注意力机制选择性聚合补丁级特征，消除了冗余投影，减少了参数数量和计算开销。</p>
<p>Result: EP在七个基准测试中超越了LP和传统注意力探测方法，在低样本和逐层设置中表现优异，同时生成可解释的注意力图。</p>
<p>Insight: 注意力探测在MIM等任务中比线性探测更具潜力，且高效的设计可以显著提升性能和计算效率。</p>
<p>Abstract: As fine-tuning (FT) becomes increasingly impractical at scale, probing is<br>emerging as the preferred evaluation protocol for self-supervised learning<br>(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the<br>potential of models trained with Masked Image Modeling (MIM), due to the<br>distributed nature of patch tokens. This motivates the need for attentive<br>probing, an alternative that uses attention to selectively aggregate<br>patch-level features. Despite its growing adoption, attentive probing remains<br>under-explored, with existing methods suffering from excessive parameterization<br>and poor computational efficiency.<br>  In this work, we revisit attentive probing through the lens of the<br>accuracy-efficiency trade-off. We conduct a systematic study of existing<br>methods, analyzing their mechanisms and benchmarking their performance. We<br>introduce efficient probing (EP), a multi-query cross-attention mechanism that<br>eliminates redundant projections, reduces the number of trainable parameters,<br>and achieves up to a 10$\times$ speed-up over conventional multi-head<br>attention. Despite its simplicity, EP outperforms LP and prior attentive<br>probing approaches across seven benchmarks, generalizes well beyond MIM to<br>diverse pre-training paradigms, produces interpretable attention maps, and<br>achieves strong gains in low-shot and layer-wise settings. Code available at<br><a target="_blank" rel="noopener" href="https://github.com/billpsomas/efficient-probing">https://github.com/billpsomas/efficient-probing</a>.</p>
</details>


<h3 id="46-Improving-Personalized-Search-with-Regularized-Low-Rank-Parameter-Updates"><a href="#46-Improving-Personalized-Search-with-Regularized-Low-Rank-Parameter-Updates" class="headerlink" title="[46] Improving Personalized Search with Regularized Low-Rank Parameter Updates"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10182">Improving Personalized Search with Regularized Low-Rank Parameter Updates</a></h3><p><em>Fiona Ryan,Josef Sivic,Fabian Caba Heilbron,Judy Hoffman,James M. Rehg,Bryan Russell</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种通过正则化低秩参数更新改进个性化视觉语言检索的方法，结合少量示例学习新概念并整合个人与通用知识，在DeepFashion2和ConCon-Chi基准测试中表现最优。</p>
<details>
  <summary>Details</summary>
Motivation: 个性化视觉语言检索需从少量示例学习新概念，同时整合个人与通用知识。现有方法难以平衡两者的结合。

<p>Contribution: 1. 提出正则化低秩参数更新方法，有效适应语言编码器的表示；2. 探索多个人概念参数的组合策略；3. 引入新指标评估通用知识保存效果。</p>
<p>Method: 在语言编码器最后一层对少量参数进行正则化低秩更新，并采用参数加法组合多个学习到的个人概念。</p>
<p>Result: 在DeepFashion2和ConCon-Chi基准测试中，个性化检索准确率比现有方法提升4%-22%。</p>
<p>Insight: 低秩参数更新能有效平衡新概念学习和通用知识保留，参数加法是组合多概念的可行策略。</p>
<p>Abstract: Personalized vision-language retrieval seeks to recognize new concepts (e.g.<br>“my dog Fido”) from only a few examples. This task is challenging because it<br>requires not only learning a new concept from a few images, but also<br>integrating the personal and general knowledge together to recognize the<br>concept in different contexts. In this paper, we show how to effectively adapt<br>the internal representation of a vision-language dual encoder model for<br>personalized vision-language retrieval. We find that regularized low-rank<br>adaption of a small set of parameters in the language encoder’s final layer<br>serves as a highly effective alternative to textual inversion for recognizing<br>the personal concept while preserving general knowledge. Additionally, we<br>explore strategies for combining parameters of multiple learned personal<br>concepts, finding that parameter addition is effective. To evaluate how well<br>general knowledge is preserved in a finetuned representation, we introduce a<br>metric that measures image retrieval accuracy based on captions generated by a<br>vision language model (VLM). Our approach achieves state-of-the-art accuracy on<br>two benchmarks for personalized image retrieval with natural language queries -<br>DeepFashion2 and ConCon-Chi - outperforming the prior art by 4%-22% on personal<br>retrievals.</p>
</details>


<h3 id="47-ScoreMix-Improving-Face-Recognition-via-Score-Composition-in-Diffusion-Generators"><a href="#47-ScoreMix-Improving-Face-Recognition-via-Score-Composition-in-Diffusion-Generators" class="headerlink" title="[47] ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10226">ScoreMix: Improving Face Recognition via Score Composition in Diffusion Generators</a></h3><p><em>Parsa Rahimi,Sebastien Marcel</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: ScoreMix提出了一种基于扩散模型的数据增强方法，通过混合不同类别的分数生成挑战性样本，显著提升了判别模型的性能，尤其是在数据较少的情况下。</p>
<details>
  <summary>Details</summary>
Motivation: 解决小样本场景下判别模型性能不足的问题，利用扩散模型的分数组合特性生成有挑战性的合成数据。

<p>Contribution: 1) 提出ScoreMix方法，利用扩散模型的分数组合特性增强数据；2) 发现混合远处类别比近处类别效果更好；3) 展示生成器和判别器空间的低相关性。</p>
<p>Method: 通过在扩散采样过程中凸组合不同类别的分数，生成具有挑战性的合成样本。</p>
<p>Result: 在多个基准测试中显著提升了判别模型的性能，尤其是在数据有限的情况下。</p>
<p>Insight: 判别器和生成器的条件空间相关性低，混合远处类别的策略更有效。</p>
<p>Abstract: In this paper, we propose ScoreMix, a novel yet simple data augmentation<br>strategy leveraging the score compositional properties of diffusion models to<br>enhance discriminator performance, particularly under scenarios with limited<br>labeled data. By convexly mixing the scores from different class-conditioned<br>trajectories during diffusion sampling, we generate challenging synthetic<br>samples that significantly improve discriminative capabilities in all studied<br>benchmarks. We systematically investigate class-selection strategies for mixing<br>and discover that greater performance gains arise when combining classes<br>distant in the discriminator’s embedding space, rather than close in the<br>generator’s condition space. Moreover, we empirically show that, under standard<br>metrics, the correlation between the generator’s learned condition space and<br>the discriminator’s embedding space is minimal. Our approach achieves notable<br>performance improvements without extensive parameter searches, demonstrating<br>practical advantages for training discriminative models while effectively<br>mitigating problems regarding collections of large datasets. Paper website:<br><a target="_blank" rel="noopener" href="https://parsa-ra.github.io/scoremix">https://parsa-ra.github.io/scoremix</a></p>
</details>


<h3 id="48-California-Crop-Yield-Benchmark-Combining-Satellite-Image-Climate-Evapotranspiration-and-Soil-Data-Layers-for-County-Level-Yield-Forecasting-of-Over-70-Crops"><a href="#48-California-Crop-Yield-Benchmark-Combining-Satellite-Image-Climate-Evapotranspiration-and-Soil-Data-Layers-for-County-Level-Yield-Forecasting-of-Over-70-Crops" class="headerlink" title="[48] California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10228">California Crop Yield Benchmark: Combining Satellite Image, Climate, Evapotranspiration, and Soil Data Layers for County-Level Yield Forecasting of Over 70 Crops</a></h3><p><em>Hamid Kamangir,Mona Hajiesmaeeli,Mason Earles</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种结合卫星图像、气候、蒸散和土壤数据的多模态深度学习模型，用于加州70多种作物的县级产量预测。模型在测试数据集上达到了0.76的R2分数，表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 加州是全球农业生产的领导者，但准确的作物产量预测仍面临挑战，因为涉及复杂的环境、气候和土壤因素。现有数据虽丰富，但缺乏有效的整合和预测方法。

<p>Contribution: 1. 提供了一个涵盖加州所有县70多种作物的综合产量基准数据集（2008-2022）。2. 开发了一个多模态深度学习模型，结合卫星图像等多源数据，实现了高精度的县级产量预测。</p>
<p>Method: 模型采用分层特征提取和时间序列编码器，捕捉生长季节的时空动态。静态输入（如土壤特性和作物类型）用于长期变异性建模。</p>
<p>Result: 在测试数据集上，模型的整体R2分数为0.76，展示了强大的预测能力。</p>
<p>Insight: 该研究为农业预测、气候适应和精准农业提供了有力工具，公开的数据集和代码对进一步研究具有重要价值。</p>
<p>Abstract: California is a global leader in agricultural production, contributing 12.5%<br>of the United States total output and ranking as the fifth-largest food and<br>cotton supplier in the world. Despite the availability of extensive historical<br>yield data from the USDA National Agricultural Statistics Service, accurate and<br>timely crop yield forecasting remains a challenge due to the complex interplay<br>of environmental, climatic, and soil-related factors. In this study, we<br>introduce a comprehensive crop yield benchmark dataset covering over 70 crops<br>across all California counties from 2008 to 2022. The benchmark integrates<br>diverse data sources, including Landsat satellite imagery, daily climate<br>records, monthly evapotranspiration, and high-resolution soil properties. To<br>effectively learn from these heterogeneous inputs, we develop a multi-modal<br>deep learning model tailored for county-level, crop-specific yield forecasting.<br>The model employs stratified feature extraction and a timeseries encoder to<br>capture spatial and temporal dynamics during the growing season. Static inputs<br>such as soil characteristics and crop identity inform long-term variability.<br>Our approach achieves an overall R2 score of 0.76 across all crops of unseen<br>test dataset, highlighting strong predictive performance across California<br>diverse agricultural regions. This benchmark and modeling framework offer a<br>valuable foundation for advancing agricultural forecasting, climate adaptation,<br>and precision farming. The full dataset and codebase are publicly available at<br>our GitHub repository.</p>
</details>


<h3 id="49-DySS-Dynamic-Queries-and-State-Space-Learning-for-Efficient-3D-Object-Detection-from-Multi-Camera-Videos"><a href="#49-DySS-Dynamic-Queries-and-State-Space-Learning-for-Efficient-3D-Object-Detection-from-Multi-Camera-Videos" class="headerlink" title="[49] DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10242">DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos</a></h3><p><em>Rajeev Yasarla,Shizhong Han,Hong Cai,Fatih Porikli</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DySS提出了一种基于状态空间学习和动态查询的高效3D物体检测方法，通过状态空间模型和动态查询更新机制，实现了高性能和高效推理。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的基于密集BEV特征的3D物体检测方法计算成本高，而基于稀疏查询的方法在处理多帧视频时仍然需要大量查询，导致效率低下。DySS旨在通过状态空间学习和动态查询来解决这些问题。

<p>Contribution: 1. 提出状态空间模型（SSM）用于时序特征处理，并引入未来预测和掩码重构任务增强模型性能。2. 设计了动态查询更新机制（合并、移除和拆分），保持高效查询集。3. 在nuScenes数据集上实现了SOTA性能（65.31 NDS和57.4 mAP）和实时推理速度（33 FPS）。</p>
<p>Method: 1. 利用SSM对时序特征进行序列化处理。2. 引入未来预测和掩码重构作为辅助任务，优化SSM学习。3. 基于学习到的状态空间特征，动态更新查询集（合并、移除和拆分）。</p>
<p>Result: 在nuScenes测试集上取得65.31 NDS和57.4 mAP，验证集上56.2 NDS和46.2 mAP，推理速度达33 FPS。</p>
<p>Insight: 通过状态空间学习和动态查询机制，DySS不仅提升了3D物体检测性能，还显著降低了计算成本，为实时感知任务提供了高效解决方案。</p>
<p>Abstract: Camera-based 3D object detection in Bird’s Eye View (BEV) is one of the most<br>important perception tasks in autonomous driving. Earlier methods rely on dense<br>BEV features, which are costly to construct. More recent works explore sparse<br>query-based detection. However, they still require a large number of queries<br>and can become expensive to run when more video frames are used. In this paper,<br>we propose DySS, a novel method that employs state-space learning and dynamic<br>queries. More specifically, DySS leverages a state-space model (SSM) to<br>sequentially process the sampled features over time steps. In order to<br>encourage the model to better capture the underlying motion and correspondence<br>information, we introduce auxiliary tasks of future prediction and masked<br>reconstruction to better train the SSM. The state of the SSM then provides an<br>informative yet efficient summarization of the scene. Based on the state-space<br>learned features, we dynamically update the queries via merge, remove, and<br>split operations, which help maintain a useful, lean set of detection queries<br>throughout the network. Our proposed DySS achieves both superior detection<br>performance and efficient inference. Specifically, on the nuScenes test split,<br>DySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the<br>art. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a<br>real-time inference speed of 33 FPS.</p>
</details>


<h3 id="50-HalLoc-Token-level-Localization-of-Hallucinations-for-Vision-Language-Models"><a href="#50-HalLoc-Token-level-Localization-of-Hallucinations-for-Vision-Language-Models" class="headerlink" title="[50] HalLoc: Token-level Localization of Hallucinations for Vision Language Models"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10286">HalLoc: Token-level Localization of Hallucinations for Vision Language Models</a></h3><p><em>Eunkyu Park,Minyeong Kim,Gunhee Kim</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: HalLoc是一个针对视觉语言模型（VLM）幻觉问题的标记级定位数据集，支持高效的概率幻觉检测，包含15万标注样本，并提出了低开销的基线模型。</p>
<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法计算复杂度高且难以区分模糊的真实与幻觉信息，HalLoc旨在解决这些问题。

<p>Contribution: 提供了首个大规模标记级幻觉定位数据集HalLoc，并开发了低开销检测模型，可直接集成到现有VLM中。</p>
<p>Method: 设计了包含VQA、指令跟随和图像描述任务的15万标注样本数据集，并训练了高效基线模型。</p>
<p>Result: HalLoc支持开发概率检测模型，基线模型可无缝集成到VLM中，提升可靠性。</p>
<p>Insight: HalLoc为提升VLM在真实场景中的可信度提供了新方向，通过概率检测和低开销模型实现高效幻觉定位。</p>
<p>Abstract: Hallucinations pose a significant challenge to the reliability of large<br>vision-language models, making their detection essential for ensuring accuracy<br>in critical applications. Current detection methods often rely on<br>computationally intensive models, leading to high latency and resource demands.<br>Their definitive outcomes also fail to account for real-world scenarios where<br>the line between hallucinated and truthful information is unclear. To address<br>these issues, we propose HalLoc, a dataset designed for efficient,<br>probabilistic hallucination detection. It features 150K token-level annotated<br>samples, including hallucination types, across Visual Question Answering (VQA),<br>instruction-following, and image captioning tasks. This dataset facilitates the<br>development of models that detect hallucinations with graded confidence,<br>enabling more informed user interactions. Additionally, we introduce a baseline<br>model trained on HalLoc, offering low-overhead, concurrent hallucination<br>detection during generation. The model can be seamlessly integrated into<br>existing VLMs, improving reliability while preserving efficiency. The prospect<br>of a robust plug-and-play hallucination detection module opens new avenues for<br>enhancing the trustworthiness of vision-language models in real-world<br>applications. The HalLoc dataset and code are publicly available at:<br><a target="_blank" rel="noopener" href="https://github.com/dbsltm/cvpr25_halloc">https://github.com/dbsltm/cvpr25_halloc</a>.</p>
</details>


<h3 id="51-Uncertainty-Aware-Deep-Learning-for-Automated-Skin-Cancer-Classification-A-Comprehensive-Evaluation"><a href="#51-Uncertainty-Aware-Deep-Learning-for-Automated-Skin-Cancer-Classification-A-Comprehensive-Evaluation" class="headerlink" title="[51] Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10302">Uncertainty-Aware Deep Learning for Automated Skin Cancer Classification: A Comprehensive Evaluation</a></h3><p><em>Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文通过结合迁移学习和不确定性量化（UQ），对皮肤癌分类任务进行了全面评估，旨在提高分类准确性和模型输出的可靠性。</p>
<details>
  <summary>Details</summary>
Motivation: 皮肤癌的早期准确诊断对患者治疗至关重要，但现有深度学习模型受限于数据稀缺和缺乏不确定性意识，需要进一步提升性能与可信度。

<p>Contribution: 论文的主要贡献包括：1) 比较了多种预训练特征提取器和传统分类器的性能；2) 引入了UQ方法（如MCD、集成和EMCD），并提出了不确定性感知的评估指标；3) 揭示了集成方法在准确性和不确定性处理之间的平衡优势。</p>
<p>Method: 方法分为两阶段：1) 基于HAM10000数据集，对比了CLIP变体、ResNet50、DenseNet121等预训练模型与SVM、XGBoost等分类器的组合性能；2) 结合UQ方法（MCD、集成、EMCD）评估模型预测的可靠性。</p>
<p>Result: 结果表明，基于CLIP的视觉变换器（如LAION CLIP ViT-H&#x2F;14）与SVM组合表现最佳；集成方法在准确性和不确定性处理上表现平衡，而EMCD对不确定预测更敏感。</p>
<p>Insight: 论文强调了在医疗诊断中整合UQ的重要性，既能提升性能，又能增强模型在临床实际应用中的可信度。</p>
<p>Abstract: Accurate and reliable skin cancer diagnosis is critical for early treatment<br>and improved patient outcomes. Deep learning (DL) models have shown promise in<br>automating skin cancer classification, but their performance can be limited by<br>data scarcity and a lack of uncertainty awareness. In this study, we present a<br>comprehensive evaluation of DL-based skin lesion classification using transfer<br>learning and uncertainty quantification (UQ) on the HAM10000 dataset. In the<br>first phase, we benchmarked several pre-trained feature extractors-including<br>Contrastive Language-Image Pretraining (CLIP) variants, Residual Network-50<br>(ResNet50), Densely Connected Convolutional Network (DenseNet121), Visual<br>Geometry Group network (VGG16), and EfficientNet-V2-Large-combined with a range<br>of traditional classifiers such as Support Vector Machine (SVM), eXtreme<br>Gradient Boosting (XGBoost), and logistic regression. Our results show that<br>CLIP-based vision transformers, particularly LAION CLIP ViT-H&#x2F;14 with SVM,<br>deliver the highest classification performance. In the second phase, we<br>incorporated UQ using Monte Carlo Dropout (MCD), Ensemble, and Ensemble Monte<br>Carlo Dropout (EMCD) to assess not only prediction accuracy but also the<br>reliability of model outputs. We evaluated these models using uncertainty-aware<br>metrics such as uncertainty accuracy(UAcc), uncertainty sensitivity(USen),<br>uncertainty specificity(USpe), and uncertainty precision(UPre). The results<br>demonstrate that ensemble methods offer a good trade-off between accuracy and<br>uncertainty handling, while EMCD is more sensitive to uncertain predictions.<br>This study highlights the importance of integrating UQ into DL-based medical<br>diagnosis to enhance both performance and trustworthiness in real-world<br>clinical applications.</p>
</details>


<h3 id="52-Towards-Scalable-SOAP-Note-Generation-A-Weakly-Supervised-Multimodal-Framework"><a href="#52-Towards-Scalable-SOAP-Note-Generation-A-Weakly-Supervised-Multimodal-Framework" class="headerlink" title="[52] Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10328">Towards Scalable SOAP Note Generation: A Weakly Supervised Multimodal Framework</a></h3><p><em>Sadia Kamal,Tim Oates,Joy Wan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种弱监督多模态框架，用于从病灶图像和稀疏临床文本生成SOAP笔记，减轻医生负担并减少对大量标注数据的依赖，性能媲美先进模型。</p>
<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最常见的癌症，医生手动生成SOAP笔记耗时且易导致职业倦怠，亟需自动化解决方案以缓解负担。

<p>Contribution: 1. 设计了一个弱监督多模态框架，减少对标注数据的依赖；2. 提出两个新指标MedConceptEval和CCS，评估生成的临床笔记质量。</p>
<p>Method: 基于病灶图像和稀疏临床文本，通过弱监督学习生成结构化的SOAP笔记，无需大量标注数据。</p>
<p>Result: 在临床相关性指标上表现与GPT-4o、Claude和DeepSeek Janus Pro相当。</p>
<p>Insight: 弱监督学习和多模态输入的结合可用于医疗领域的结构化文本生成任务，有效减少标注成本。</p>
<p>Abstract: Skin carcinoma is the most prevalent form of cancer globally, accounting for<br>over $8 billion in annual healthcare expenditures. In clinical settings,<br>physicians document patient visits using detailed SOAP (Subjective, Objective,<br>Assessment, and Plan) notes. However, manually generating these notes is<br>labor-intensive and contributes to clinician burnout. In this work, we propose<br>a weakly supervised multimodal framework to generate clinically structured SOAP<br>notes from limited inputs, including lesion images and sparse clinical text.<br>Our approach reduces reliance on manual annotations, enabling scalable,<br>clinically grounded documentation while alleviating clinician burden and<br>reducing the need for large annotated data. Our method achieves performance<br>comparable to GPT-4o, Claude, and DeepSeek Janus Pro across key clinical<br>relevance metrics. To evaluate clinical quality, we introduce two novel metrics<br>MedConceptEval and Clinical Coherence Score (CCS) which assess semantic<br>alignment with expert medical concepts and input features, respectively.</p>
</details>


<h3 id="53-Research-on-Audio-Visual-Quality-Assessment-Dataset-and-Method-for-User-Generated-Omnidirectional-Video"><a href="#53-Research-on-Audio-Visual-Quality-Assessment-Dataset-and-Method-for-User-Generated-Omnidirectional-Video" class="headerlink" title="[53] Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10331">Research on Audio-Visual Quality Assessment Dataset and Method for User-Generated Omnidirectional Video</a></h3><p><em>Fei Zhao,Da Pan,Zelu Qi,Ping Shi</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文针对元宇宙中用户生成的360度视频（UGC-ODV）的音视频质量评估（AVQA）问题，构建了一个数据集并提出了一种基线模型。</p>
<details>
  <summary>Details</summary>
Motivation: 随着元宇宙的兴起，360度视频逐渐从专业生成内容（PGC）转向用户生成内容（UGC）。然而，UGC-ODV的音视频质量评估研究仍然有限。

<p>Contribution: 1. 构建了一个包含300个视频的UGC-ODV音视频数据集；2. 设计了一个包含视频特征提取、音频特征提取和音视频融合模块的AVQA基线模型。</p>
<p>Method: 1. 通过5人和2种360度相机拍摄10类场景的视频；2. 进行主观AVQA实验获取MOS评分；3. 设计基线模型提取视频和音频特征并融合评估质量。</p>
<p>Result: 提出的基线模型在构建的数据集上表现最优。</p>
<p>Insight: UGC-ODV的音视频质量评估需要结合多模态特征，且用户生成内容的质量评估与专业内容存在差异。</p>
<p>Abstract: In response to the rising prominence of the Metaverse, omnidirectional videos<br>(ODVs) have garnered notable interest, gradually shifting from<br>professional-generated content (PGC) to user-generated content (UGC). However,<br>the study of audio-visual quality assessment (AVQA) within ODVs remains<br>limited. To address this, we construct a dataset of UGC omnidirectional audio<br>and video (A&#x2F;V) content. The videos are captured by five individuals using two<br>different types of omnidirectional cameras, shooting 300 videos covering 10<br>different scene types. A subjective AVQA experiment is conducted on the dataset<br>to obtain the Mean Opinion Scores (MOSs) of the A&#x2F;V sequences. After that, to<br>facilitate the development of UGC-ODV AVQA fields, we construct an effective<br>AVQA baseline model on the proposed dataset, of which the baseline model<br>consists of video feature extraction module, audio feature extraction and<br>audio-visual fusion module. The experimental results demonstrate that our model<br>achieves optimal performance on the proposed dataset.</p>
</details>


<h3 id="54-Using-Vision-Language-Models-to-Detect-Students’-Academic-Emotion-through-Facial-Expressions"><a href="#54-Using-Vision-Language-Models-to-Detect-Students’-Academic-Emotion-through-Facial-Expressions" class="headerlink" title="[54] Using Vision Language Models to Detect Students’ Academic Emotion through Facial Expressions"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10334">Using Vision Language Models to Detect Students’ Academic Emotion through Facial Expressions</a></h3><p><em>Deliang Wang,Chao Yang,Gaowei Chen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文研究了利用视觉语言模型（VLMs）通过零样本提示检测学生在在线学习环境中的学术情绪，发现Qwen2.5-VL-7B-Instruct在识别困惑表情方面表现较好，但对分心行为的检测效果较差。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的学生学术情绪分析方法依赖监督学习，且泛化能力不足。视觉语言模型的出现为解决这一问题提供了新思路。

<p>Contribution: 论文验证了VLMs在零样本提示下检测学生学术情绪的潜力，并比较了两种模型的表现，为实际应用提供了参考。</p>
<p>Method: 使用Llama-3.2-11B-Vision-Instruct和Qwen2.5-VL-7B-Instruct两种VLMs，对5000张包含五种表情的学生面部图像进行零样本分析。</p>
<p>Result: Qwen2.5-VL-7B-Instruct在困惑表情识别上表现较好，而两种模型对分心行为的检测效果不佳。快乐情绪的识别效果最好。</p>
<p>Insight: VLMs在学术情绪检测领域具有一定的潜力，但对某些情绪（如分心）的识别需要进一步改进。</p>
<p>Abstract: Students’ academic emotions significantly influence their social behavior and<br>learning performance. Traditional approaches to automatically and accurately<br>analyze these emotions have predominantly relied on supervised machine learning<br>algorithms. However, these models often struggle to generalize across different<br>contexts, necessitating repeated cycles of data collection, annotation, and<br>training. The emergence of Vision-Language Models (VLMs) offers a promising<br>alternative, enabling generalization across visual recognition tasks through<br>zero-shot prompting without requiring fine-tuning. This study investigates the<br>potential of VLMs to analyze students’ academic emotions via facial expressions<br>in an online learning environment. We employed two VLMs,<br>Llama-3.2-11B-Vision-Instruct and Qwen2.5-VL-7B-Instruct, to analyze 5,000<br>images depicting confused, distracted, happy, neutral, and tired expressions<br>using zero-shot prompting. Preliminary results indicate that both models<br>demonstrate moderate performance in academic facial expression recognition,<br>with Qwen2.5-VL-7B-Instruct outperforming Llama-3.2-11B-Vision-Instruct.<br>Notably, both models excel in identifying students’ happy emotions but fail to<br>detect distracted behavior. Additionally, Qwen2.5-VL-7B-Instruct exhibits<br>relatively high performance in recognizing students’ confused expressions,<br>highlighting its potential for practical applications in identifying content<br>that causes student confusion.</p>
</details>


<h3 id="55-PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting"><a href="#55-PointGS-Point-Attention-Aware-Sparse-View-Synthesis-with-Gaussian-Splatting" class="headerlink" title="[55] PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10335">PointGS: Point Attention-Aware Sparse View Synthesis with Gaussian Splatting</a></h3><p><em>Lintao Xiang,Hongpei Zheng,Yating Huang,Qijun Yang,Hujun Yin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PointGS利用点注意力感知的高斯泼溅技术，通过稀疏视图实现了高质量的实时渲染。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的3DGS方法需要大量校准视图来生成一致的场景表示，稀疏输入时容易过拟合训练视图，导致渲染质量下降。

<p>Contribution: 提出了一种点特征感知的高斯泼溅框架，通过多尺度2D特征采样、点交互网络和轻量级MLP解码，实现了稀疏视图下的高质量渲染。</p>
<p>Method: 1. 使用立体基础模型估计相机姿态并重建密集点云；2. 通过多尺度2D特征采样和聚合编码高斯颜色属性；3. 设计基于自注意力机制的点交互网络增强点表示；4. 用轻量级MLP解码为高斯参数进行渲染。</p>
<p>Result: 在多种基准测试中显著优于NeRF方法，且在少样本设置下与最先进的3DGS方法竞争。</p>
<p>Insight: 点注意力机制和多尺度特征融合是提升稀疏视图渲染质量的关键。</p>
<p>Abstract: 3D Gaussian splatting (3DGS) is an innovative rendering technique that<br>surpasses the neural radiance field (NeRF) in both rendering speed and visual<br>quality by leveraging an explicit 3D scene representation. Existing 3DGS<br>approaches require a large number of calibrated views to generate a consistent<br>and complete scene representation. When input views are limited, 3DGS tends to<br>overfit the training views, leading to noticeable degradation in rendering<br>quality. To address this limitation, we propose a Point-wise Feature-Aware<br>Gaussian Splatting framework that enables real-time, high-quality rendering<br>from sparse training views. Specifically, we first employ the latest stereo<br>foundation model to estimate accurate camera poses and reconstruct a dense<br>point cloud for Gaussian initialization. We then encode the colour attributes<br>of each 3D Gaussian by sampling and aggregating multiscale 2D appearance<br>features from sparse inputs. To enhance point-wise appearance representation,<br>we design a point interaction network based on a self-attention mechanism,<br>allowing each Gaussian point to interact with its nearest neighbors. These<br>enriched features are subsequently decoded into Gaussian parameters through two<br>lightweight multi-layer perceptrons (MLPs) for final rendering. Extensive<br>experiments on diverse benchmarks demonstrate that our method significantly<br>outperforms NeRF-based approaches and achieves competitive performance under<br>few-shot settings compared to the state-of-the-art 3DGS methods.</p>
</details>


<h3 id="56-UrbanSense-AFramework-for-Quantitative-Analysis-of-Urban-Streetscapes-leveraging-Vision-Large-Language-Models"><a href="#56-UrbanSense-AFramework-for-Quantitative-Analysis-of-Urban-Streetscapes-leveraging-Vision-Large-Language-Models" class="headerlink" title="[56] UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10342">UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models</a></h3><p><em>Jun Yin,Jing Zhong,Peilin Li,Pengyu Zeng,Miao Zhang,Ran Luo,Shuai Lu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一个基于视觉-语言模型的多模态研究框架UrbanSense，用于自动化、可扩展的城市街景风格差异分析，并展示了其在量化城市风格演变方面的潜力。</p>
<details>
  <summary>Details</summary>
Motivation: 城市文化和建筑风格因地理、历史、社会政治等因素差异显著，传统研究方法难以标准化。该研究旨在通过数据驱动方法增强城市形态研究的客观性。

<p>Contribution: 1.构建了UrbanDiffBench数据集；2.开发了首个基于视觉-语言模型的城市街景分析框架UrbanSense；3.实验结果验证了该方法在量化风格差异上的有效性。</p>
<p>Method: 基于视觉-语言模型的多模态框架，支持城市风格的定量生成与比较。</p>
<p>Result: 超过80%的生成描述通过t检验（p&lt;0.05），主观评价中的高Phi分数证实了方法对风格差异的捕捉能力(城市0.912，时期0.833)。</p>
<p>Insight: 该框架为城市风格的量化与解释提供了科学依据，为未来设计提供了数据支持。</p>
<p>Abstract: Urban cultures and architectural styles vary significantly across cities due<br>to geographical, chronological, historical, and socio-political factors.<br>Understanding these differences is essential for anticipating how cities may<br>evolve in the future. As representative cases of historical continuity and<br>modern innovation in China, Beijing and Shenzhen offer valuable perspectives<br>for exploring the transformation of urban streetscapes. However, conventional<br>approaches to urban cultural studies often rely on expert interpretation and<br>historical documentation, which are difficult to standardize across different<br>contexts. To address this, we propose a multimodal research framework based on<br>vision-language models, enabling automated and scalable analysis of urban<br>streetscape style differences. This approach enhances the objectivity and<br>data-driven nature of urban form research. The contributions of this study are<br>as follows: First, we construct UrbanDiffBench, a curated dataset of urban<br>streetscapes containing architectural images from different periods and<br>regions. Second, we develop UrbanSense, the first vision-language-model-based<br>framework for urban streetscape analysis, enabling the quantitative generation<br>and comparison of urban style representations. Third, experimental results show<br>that Over 80% of generated descriptions pass the t-test (p less than 0.05).<br>High Phi scores (0.912 for cities, 0.833 for periods) from subjective<br>evaluations confirm the method’s ability to capture subtle stylistic<br>differences. These results highlight the method’s potential to quantify and<br>interpret urban style evolution, offering a scientifically grounded lens for<br>future design.</p>
</details>


<h3 id="57-RealKeyMorph-Keypoints-in-Real-world-Coordinates-for-Resolution-agnostic-Image-Registration"><a href="#57-RealKeyMorph-Keypoints-in-Real-world-Coordinates-for-Resolution-agnostic-Image-Registration" class="headerlink" title="[57] RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10344">RealKeyMorph: Keypoints in Real-world Coordinates for Resolution-agnostic Image Registration</a></h3><p><em>Mina C. Moghadam,Alan Q. Wang,Omer Taub,Martin R. Prince,Mert R. Sabuncu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: RealKeyMorph (RKM) 提出了一种分辨率无关的图像配准方法，通过输出真实世界坐标系中的关键点，避免了传统方法中的重采样问题。</p>
<details>
  <summary>Details</summary>
Motivation: 医学图像配准中，由于采集参数（如像素间距、切片厚度等）的差异，图像分辨率可能不同。传统机器学习方法基于固定分辨率进行重采样，容易引入插值伪影，影响配准效果。

<p>Contribution: RKM 扩展了 KeyMorph 框架，通过在真实世界坐标系中提取关键点，实现了分辨率无关的图像配准，避免了重采样的缺陷。</p>
<p>Method: RKM 利用扫描仪生成的仿射矩阵（如 MRI 机器的矩阵），将关键点从体素坐标转换为真实世界坐标，并在训练过程中集成这一转换，使关键点具有分辨率无关性。</p>
<p>Result: 实验显示，RKM 在腹部 MRI 的 2D 正交堆叠和脑部数据集的 3D 体积配准任务中表现优异。</p>
<p>Insight: 通过直接操作原始数据（避免重采样），RKM 提升了图像配准的精度和鲁棒性，尤其适用于分辨率不一致的场景。</p>
<p>Abstract: Many real-world settings require registration of a pair of medical images<br>that differ in spatial resolution, which may arise from differences in image<br>acquisition parameters like pixel spacing, slice thickness, and field-of-view.<br>However, all previous machine learning-based registration techniques resample<br>images onto a fixed resolution. This is suboptimal because resampling can<br>introduce artifacts due to interpolation. To address this, we present<br>RealKeyMorph (RKM), a resolution-agnostic method for image registration. RKM is<br>an extension of KeyMorph, a registration framework which works by training a<br>network to learn corresponding keypoints for a given pair of images, after<br>which a closed-form keypoint matching step is used to derive the transformation<br>that aligns them. To avoid resampling and enable operating on the raw data, RKM<br>outputs keypoints in real-world coordinates of the scanner. To do this, we<br>leverage the affine matrix produced by the scanner (e.g., MRI machine) that<br>encodes the mapping from voxel coordinates to real world coordinates. By<br>transforming keypoints into real-world space and integrating this into the<br>training process, RKM effectively enables the extracted keypoints to be<br>resolution-agnostic. In our experiments, we demonstrate the advantages of RKM<br>on the registration task for orthogonal 2D stacks of abdominal MRIs, as well as<br>3D volumes with varying resolutions in brain datasets.</p>
</details>


<h3 id="58-Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation"><a href="#58-Motion-R1-Chain-of-Thought-Reasoning-and-Reinforcement-Learning-for-Human-Motion-Generation" class="headerlink" title="[58] Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10353">Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</a></h3><p><em>Runqi Ouyang,Haoyun Li,Zhenyuan Zhang,Xiaofeng Wang,Zheng Zhu,Guan Huang,Xingang Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Motion-R1通过结合思维链和强化学习提升文本到动作生成的质量，解决了现有方法在语义对齐和动作合成中的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉深层语言结构和逻辑推理，导致生成的动作缺乏可控性、一致性和多样性。Motion-R1旨在解决这些问题。

<p>Contribution: 提出了Motion-R1框架，结合思维链机制和强化学习，显著提升复杂文本指令的解析和多步骤动作生成能力。</p>
<p>Method: 使用思维链分解文本指令，并采用Group Relative Policy Optimization强化学习算法联合优化推理链和动作合成。</p>
<p>Result: 在多个基准数据集上表现优异，尤其在语义理解和长期时间连贯性方面优于现有方法。</p>
<p>Insight: 思维链机制和强化学习的结合能够有效提升复杂动作生成的语义理解和执行能力。</p>
<p>Abstract: Recent advances in large language models, especially in natural language<br>understanding and reasoning, have opened new possibilities for text-to-motion<br>generation. Although existing approaches have made notable progress in semantic<br>alignment and motion synthesis, they often rely on end-to-end mapping<br>strategies that fail to capture deep linguistic structures and logical<br>reasoning. Consequently, generated motions tend to lack controllability,<br>consistency, and diversity. To address these limitations, we propose Motion-R1,<br>a unified motion-language modeling framework that integrates a Chain-of-Thought<br>mechanism. By explicitly decomposing complex textual instructions into<br>logically structured action paths, Motion-R1 provides high-level semantic<br>guidance for motion generation, significantly enhancing the model’s ability to<br>interpret and execute multi-step, long-horizon, and compositionally rich<br>commands. To train our model, we adopt Group Relative Policy Optimization, a<br>reinforcement learning algorithm designed for large models, which leverages<br>motion quality feedback to optimize reasoning chains and motion synthesis<br>jointly. Extensive experiments across multiple benchmark datasets demonstrate<br>that Motion-R1 achieves competitive or superior performance compared to<br>state-of-the-art methods, particularly in scenarios requiring nuanced semantic<br>understanding and long-term temporal coherence. The code, model and data will<br>be publicly available.</p>
</details>


<h3 id="59-FaceLiVT-Face-Recognition-using-Linear-Vision-Transformer-with-Structural-Reparameterization-For-Mobile-Device"><a href="#59-FaceLiVT-Face-Recognition-using-Linear-Vision-Transformer-with-Structural-Reparameterization-For-Mobile-Device" class="headerlink" title="[59] FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10361">FaceLiVT: Face Recognition using Linear Vision Transformer with Structural Reparameterization For Mobile Device</a></h3><p><em>Novendra Setyawan,Chi-Chia Sun,Mao-Hsiu Hsu,Wen-Kai Kuo,Jun-Wei Hsieh</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: FaceLiVT是一种轻量级但强大的人脸识别模型，结合了CNN-Transformer混合架构和创新的轻量级多头线性注意力机制（MHLA），在降低计算复杂度的同时保持了高精度。</p>
<details>
  <summary>Details</summary>
Motivation: 在移动设备上实现高效、低延迟的人脸识别，同时平衡计算资源与模型性能。

<p>Contribution: 提出FaceLiVT模型，结合MHLA和结构重参数化技术，显著提升推理速度，适用于资源受限平台。</p>
<p>Method: 采用混合CNN-Transformer架构，设计轻量级MHLA机制，并通过结构重参数化优化模型效率。</p>
<p>Result: 在LFW等基准数据集上表现优异，推理速度比EdgeFace快8.6倍，比纯ViT模型快21.2倍。</p>
<p>Insight: 混合架构结合轻量级注意力机制可显著提升移动设备上的实时人脸识别性能。</p>
<p>Abstract: This paper introduces FaceLiVT, a lightweight yet powerful face recognition<br>model that integrates a hybrid Convolution Neural Network (CNN)-Transformer<br>architecture with an innovative and lightweight Multi-Head Linear Attention<br>(MHLA) mechanism. By combining MHLA alongside a reparameterized token mixer,<br>FaceLiVT effectively reduces computational complexity while preserving<br>competitive accuracy. Extensive evaluations on challenging benchmarks;<br>including LFW, CFP-FP, AgeDB-30, IJB-B, and IJB-C; highlight its superior<br>performance compared to state-of-the-art lightweight models. MHLA notably<br>improves inference speed, allowing FaceLiVT to deliver high accuracy with lower<br>latency on mobile devices. Specifically, FaceLiVT is 8.6 faster than EdgeFace,<br>a recent hybrid CNN-Transformer model optimized for edge devices, and 21.2<br>faster than a pure ViT-Based model. With its balanced design, FaceLiVT offers<br>an efficient and practical solution for real-time face recognition on<br>resource-constrained platforms.</p>
</details>


<h3 id="60-FSATFusion-Frequency-Spatial-Attention-Transformer-for-Infrared-and-Visible-Image-Fusion"><a href="#60-FSATFusion-Frequency-Spatial-Attention-Transformer-for-Infrared-and-Visible-Image-Fusion" class="headerlink" title="[60] FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10366">FSATFusion: Frequency-Spatial Attention Transformer for Infrared and Visible Image Fusion</a></h3><p><em>Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui,Yuhan Lyu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了FSATFusion，一种用于红外和可见光图像融合的端到端网络，结合了频域-空间注意力Transformer模块，显著提升了融合效果和下游任务性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法多基于CNN，但在全局上下文捕获上存在局限，导致信息丢失，影响融合质量。作者希望通过Transformer和注意力机制提升这一能力。

<p>Contribution: 1. 提出FSAT模块，结合频域-空间注意力机制；2. 改进Transformer模块（ITM）以增强全局上下文提取；3. 展示了优异的融合效果和泛化能力。</p>
<p>Method: 设计频域-空间注意力Transformer（FSAT）模块，结合FSAM提取显著特征，并改进Transformer（ITM）以增强全局信息捕获。</p>
<p>Result: 实验表明FSATFusion在融合质量和效率上优于现有方法，且在下游任务（如目标检测）中表现优异。</p>
<p>Insight: 结合频率和空间域的注意力机制能更好地提取特征，改进的Transformer模块进一步提升了全局信息捕获能力。</p>
<p>Abstract: The infrared and visible images fusion (IVIF) is receiving increasing<br>attention from both the research community and industry due to its excellent<br>results in downstream applications. Existing deep learning approaches often<br>utilize convolutional neural networks to extract image features. However, the<br>inherently capacity of convolution operations to capture global context can<br>lead to information loss, thereby restricting fusion performance. To address<br>this limitation, we propose an end-to-end fusion network named the<br>Frequency-Spatial Attention Transformer Fusion Network (FSATFusion). The<br>FSATFusion contains a frequency-spatial attention Transformer (FSAT) module<br>designed to effectively capture discriminate features from source images. This<br>FSAT module includes a frequency-spatial attention mechanism (FSAM) capable of<br>extracting significant features from feature maps. Additionally, we propose an<br>improved Transformer module (ITM) to enhance the ability to extract global<br>context information of vanilla Transformer. We conducted both qualitative and<br>quantitative comparative experiments, demonstrating the superior fusion quality<br>and efficiency of FSATFusion compared to other state-of-the-art methods.<br>Furthermore, our network was tested on two additional tasks without any<br>modifications, to verify the excellent generalization capability of FSATFusion.<br>Finally, the object detection experiment demonstrated the superiority of<br>FSATFusion in downstream visual tasks. Our code is available at<br><a target="_blank" rel="noopener" href="https://github.com/Lmmh058/FSATFusion">https://github.com/Lmmh058/FSATFusion</a>.</p>
</details>


<h3 id="61-Revisiting-Transformers-with-Insights-from-Image-Filtering"><a href="#61-Revisiting-Transformers-with-Insights-from-Image-Filtering" class="headerlink" title="[61] Revisiting Transformers with Insights from Image Filtering"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10371">Revisiting Transformers with Insights from Image Filtering</a></h3><p><em>Laziz U. Abdullaev,Maksim Tkachenko,Tan M. Nguyen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文通过图像处理的视角重新解释了Transformer中的自注意力机制，提出了一个统一的框架来解释其计算过程及组件作用，并提出了两种改进架构，不仅在可解释性上有所提升，还在任务性能上取得了显著进步。</p>
<details>
  <summary>Details</summary>
Motivation: 自注意力机制的成功缺乏理论解释，现有方法虽尝试从图像去噪和非参数回归角度理解，但仍未深入分析其增强组件的机理。本文旨在通过图像处理框架弥补这一理解差距。

<p>Contribution: 1. 提出基于图像处理的统一框架，解释自注意力及其组件（如位置编码和残差连接）的作用。2. 通过这一框架发现了自注意力与图像处理之间的潜在差异并尝试弥合。3. 提出两种独立架构改进，提升了模型性能。</p>
<p>Method: 1. 构建图像处理框架分析自注意力机制。2. 通过该框架解释位置编码和残差连接等功能。3. 提出两种改进架构并验证其效果。</p>
<p>Result: 实验表明，基于图像处理启发的改进不仅增强了模型的可解释性，还在语言和视觉任务中提升了准确性和鲁棒性，尤其是在长序列理解上表现更优。</p>
<p>Insight: 自注意力机制的某些设计灵感可能源于图像处理领域，这种跨领域的视角有助于深入理解并改进Transformer架构。</p>
<p>Abstract: The self-attention mechanism, a cornerstone of Transformer-based<br>state-of-the-art deep learning architectures, is largely heuristic-driven and<br>fundamentally challenging to interpret. Establishing a robust theoretical<br>foundation to explain its remarkable success and limitations has therefore<br>become an increasingly prominent focus in recent research. Some notable<br>directions have explored understanding self-attention through the lens of image<br>denoising and nonparametric regression. While promising, existing frameworks<br>still lack a deeper mechanistic interpretation of various architectural<br>components that enhance self-attention, both in its original formulation and<br>subsequent variants. In this work, we aim to advance this understanding by<br>developing a unifying image processing framework, capable of explaining not<br>only the self-attention computation itself but also the role of components such<br>as positional encoding and residual connections, including numerous later<br>variants. We also pinpoint potential distinctions between the two concepts<br>building upon our framework, and make effort to close this gap. We introduce<br>two independent architectural modifications within transformers. While our<br>primary objective is interpretability, we empirically observe that image<br>processing-inspired modifications can also lead to notably improved accuracy<br>and robustness against data contamination and adversaries across language and<br>vision tasks as well as better long sequence understanding.</p>
</details>


<h3 id="62-Leveraging-6DoF-Pose-Foundation-Models-For-Mapping-Marine-Sediment-Burial"><a href="#62-Leveraging-6DoF-Pose-Foundation-Models-For-Mapping-Marine-Sediment-Burial" class="headerlink" title="[62] Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10386">Leveraging 6DoF Pose Foundation Models For Mapping Marine Sediment Burial</a></h3><p><em>Jerry Yan,Chinmay Talegaonkar,Nicholas Antipa,Eric Terrill,Sophia Merrifield</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PoseIDON 是一个结合深度基础模型与多视角摄影测量的计算机视觉流程，用于估计海底物体6自由度位姿，并通过CAD模型对齐推断埋藏深度。</p>
<details>
  <summary>Details</summary>
Motivation: 海底人为物体埋藏状态的准确估计对研究局部沉积动态、评估生态风险和污染物运输至关重要，但传统方法因遮挡、低可见度和物体退化等问题难以实现。

<p>Contribution: 提出了PoseIDON方法，首次将深度基础模型特征与多视角摄影测量结合，实现了对海底物体位姿和埋藏深度的准确估计。</p>
<p>Method: 使用多视角ROV视频，通过深度基础模型提取特征，结合CAD模型对齐和局部海床平面拟合，计算物体6自由度位姿和埋藏深度。</p>
<p>Result: 在54个物体的测试中，平均埋藏深度误差约为10厘米，成功捕捉了底层沉积物运输的空间模式。</p>
<p>Insight: 该方法为非侵入式、可扩展的海底埋藏状态测绘提供了新思路，适用于环境污染评估和危险物恢复策略制定。</p>
<p>Abstract: The burial state of anthropogenic objects on the seafloor provides insight<br>into localized sedimentation dynamics and is also critical for assessing<br>ecological risks, potential pollutant transport, and the viability of recovery<br>or mitigation strategies for hazardous materials such as munitions. Accurate<br>burial depth estimation from remote imagery remains difficult due to partial<br>occlusion, poor visibility, and object degradation. This work introduces a<br>computer vision pipeline, called PoseIDON, which combines deep foundation model<br>features with multiview photogrammetry to estimate six degrees of freedom<br>object pose and the orientation of the surrounding seafloor from ROV video.<br>Burial depth is inferred by aligning CAD models of the objects with observed<br>imagery and fitting a local planar approximation of the seafloor. The method is<br>validated using footage of 54 objects, including barrels and munitions,<br>recorded at a historic ocean dumpsite in the San Pedro Basin. The model<br>achieves a mean burial depth error of approximately 10 centimeters and resolves<br>spatial burial patterns that reflect underlying sediment transport processes.<br>This approach enables scalable, non-invasive mapping of seafloor burial and<br>supports environmental assessment at contaminated sites.</p>
</details>


<h3 id="63-DART-Differentiable-Dynamic-Adaptive-Region-Tokenizer-for-Vision-Transformer-and-Mamba"><a href="#63-DART-Differentiable-Dynamic-Adaptive-Region-Tokenizer-for-Vision-Transformer-and-Mamba" class="headerlink" title="[63] DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10390">DART: Differentiable Dynamic Adaptive Region Tokenizer for Vision Transformer and Mamba</a></h3><p><em>Shicheng Yin,Kaixuan Yin,Yang Liu,Weixing Chen,Liang Lin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为DART的动态自适应区域标记器，通过自适应划分图像为不同大小的块，提升了Vision Transformer和Mamba的性能，同时减少了计算开销。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的Vision Transformer和Mamba等非卷积模型依赖固定大小的图像块，导致对背景区域的过度编码和对关键局部细节的遗漏，尤其在信息稀疏分布的场景下表现不佳。

<p>Contribution: 提出了DART（Dynamic Adaptive Region Tokenizer），一种完全可微的动态自适应区域标记器，能够根据内容自适应划分图像块，从而更高效地编码信息丰富的区域。</p>
<p>Method: DART结合可学习的区域得分和分段可微分的分位数操作，动态分配更密集的标记到信息丰富区域。引入仅约1M额外参数，显著提升模型性能。</p>
<p>Result: 在DeiT（ImageNet-1K）上准确率提升2.1%，同时减少45%的浮点运算量（FLOPs）。在DeiT、Vim和VideoMamba上的实验验证了其一致性和高效性。</p>
<p>Insight: DART提供了一种更高效的方法替代均匀增加标记密度的策略，能够在提升性能的同时减少计算开销，适用于信息分布不均的场景。</p>
<p>Abstract: Recently, non-convolutional models such as the Vision Transformer (ViT) and<br>Vision Mamba (Vim) have achieved remarkable performance in computer vision<br>tasks. However, their reliance on fixed-size patches often results in excessive<br>encoding of background regions and omission of critical local details,<br>especially when informative objects are sparsely distributed. To address this,<br>we introduce a fully differentiable Dynamic Adaptive Region Tokenizer (DART),<br>which adaptively partitions images into content-dependent patches of varying<br>sizes. DART combines learnable region scores with piecewise differentiable<br>quantile operations to allocate denser tokens to information-rich areas.<br>Despite introducing only approximately 1 million (1M) additional parameters,<br>DART improves accuracy by 2.1% on DeiT (ImageNet-1K). Unlike methods that<br>uniformly increase token density to capture fine-grained details, DART offers a<br>more efficient alternative, achieving 45% FLOPs reduction with superior<br>performance. Extensive experiments on DeiT, Vim, and VideoMamba confirm that<br>DART consistently enhances accuracy while incurring minimal or even reduced<br>computational overhead. Code is available at<br><a target="_blank" rel="noopener" href="https://github.com/HCPLab-SYSU/DART">https://github.com/HCPLab-SYSU/DART</a>.</p>
</details>


<h3 id="64-ReconMOST-Multi-Layer-Sea-Temperature-Reconstruction-with-Observations-Guided-Diffusion"><a href="#64-ReconMOST-Multi-Layer-Sea-Temperature-Reconstruction-with-Observations-Guided-Diffusion" class="headerlink" title="[64] ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10391">ReconMOST: Multi-Layer Sea Temperature Reconstruction with Observations-Guided Diffusion</a></h3><p><em>Yuanyi Song,Pumeng Lyu,Ben Fei,Fenghua Ling,Wanli Ouyang,Lei Bai</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: ReconMOST提出了一种基于数据驱动的扩散模型框架，用于多层面海水温度重建，通过历史数值模拟数据预训练模型，并利用稀疏原位观测数据指导逆向扩散过程，实现了高精度和物理一致的重建。</p>
<details>
  <summary>Details</summary>
Motivation: 传统海洋温度重建方法因数据稀疏、算法复杂和计算成本高而受限，而现有的机器学习方法通常仅适用于海表或局部区域，且难以处理云遮挡等问题。需要一种更高效、全局且多层面的重建方法。

<p>Contribution: 提出ReconMOST框架，首次将ML方法扩展到全球多层面海水温度重建；通过预训练的扩散模型和观测数据指导，实现了高精度和物理一致的重建效果。</p>
<p>Method: 1. 基于CMIP6历史数值模拟数据预训练无条件扩散模型；2. 使用稀疏高精度原位观测数据指导逆向扩散过程；3. 在无直接观测区域，利用预训练学习到的物理分布模式进行隐式指导重建。</p>
<p>Result: 在CMIP6和EN4分析数据上，MSE达到0.049（指导）、0.680（重建）和0.633（总），即使在92.5%数据缺失情况下仍保持重建精度和分辨率。</p>
<p>Insight: 扩散模型在海洋科学中的应用潜力——通过结合物理一致的预训练和稀疏观测，可实现高精度全局重建，同时解决了传统ML方法的局限性。</p>
<p>Abstract: Accurate reconstruction of ocean is essential for reflecting global climate<br>dynamics and supporting marine meteorological research. Conventional methods<br>face challenges due to sparse data, algorithmic complexity, and high<br>computational costs, while increasing usage of machine learning (ML) method<br>remains limited to reconstruction problems at the sea surface and local<br>regions, struggling with issues like cloud occlusion. To address these<br>limitations, this paper proposes ReconMOST, a data-driven guided diffusion<br>model framework for multi-layer sea temperature reconstruction. Specifically,<br>we first pre-train an unconditional diffusion model using a large collection of<br>historical numerical simulation data, enabling the model to attain physically<br>consistent distribution patterns of ocean temperature fields. During the<br>generation phase, sparse yet high-accuracy in-situ observational data are<br>utilized as guidance points for the reverse diffusion process, generating<br>accurate reconstruction results. Importantly, in regions lacking direct<br>observational data, the physically consistent spatial distribution patterns<br>learned during pre-training enable implicitly guided and physically plausible<br>reconstructions. Our method extends ML-based SST reconstruction to a global,<br>multi-layer setting, handling over 92.5% missing data while maintaining<br>reconstruction accuracy, spatial resolution, and superior generalization<br>capability. We pre-train our model on CMIP6 numerical simulation data and<br>conduct guided reconstruction experiments on CMIP6 and EN4 analysis data. The<br>results of mean squared error (MSE) values achieve 0.049 on guidance, 0.680 on<br>reconstruction, and 0.633 on total, respectively, demonstrating the<br>effectiveness and robustness of the proposed framework. Our source code is<br>available at <a target="_blank" rel="noopener" href="https://github.com/norsheep/ReconMOST">https://github.com/norsheep/ReconMOST</a>.</p>
</details>


<h3 id="65-Pisces-An-Auto-regressive-Foundation-Model-for-Image-Understanding-and-Generation"><a href="#65-Pisces-An-Auto-regressive-Foundation-Model-for-Image-Understanding-and-Generation" class="headerlink" title="[65] Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10395">Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation</a></h3><p><em>Zhiyang Xu,Jiuhai Chen,Zhaojiang Lin,Xichen Pan,Lifu Huang,Tianyi Zhou,Madian Khabsa,Qifan Wang,Di Jin,Michihiro Yasunaga,Lili Yu,Xi Victoria Lin,Shaoliang Nie</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Pisces是一种自回归多模态基础模型，通过解耦的视觉编码架构和针对多模态生成的定制训练技术，解决了图像理解与生成任务中的性能差异问题，同时在两者上均表现出色。</p>
<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型在图像理解和生成任务中往往不如专用模型表现优异，主要因为视觉特征和训练过程的差异。Pisces旨在设计一种统一框架，解决这一问题。

<p>Contribution: 1. 提出Pisces，一种自回归多模态基础模型；2. 设计解耦的视觉编码架构；3. 优化多模态生成的训练技术；4. 在多个公开基准测试中验证其竞争力。</p>
<p>Method: 采用解耦视觉编码架构，分别为图像理解和生成任务优化特征提取；结合数据精选、预训练和微调技术，提升多模态任务的性能。</p>
<p>Result: 在20多个图像理解任务和GenEval图像生成基准测试中表现出色，验证了模型在两种任务上的竞争力。</p>
<p>Insight: 图像理解与生成之间存在协同关系，解耦的视觉编码架构能有效提升统一多模态模型的性能。</p>
<p>Abstract: Recent advances in large language models (LLMs) have enabled multimodal<br>foundation models to tackle both image understanding and generation within a<br>unified framework. Despite these gains, unified models often underperform<br>compared to specialized models in either task. A key challenge in developing<br>unified models lies in the inherent differences between the visual features<br>needed for image understanding versus generation, as well as the distinct<br>training processes required for each modality. In this work, we introduce<br>Pisces, an auto-regressive multimodal foundation model that addresses this<br>challenge through a novel decoupled visual encoding architecture and tailored<br>training techniques optimized for multimodal generation. Combined with<br>meticulous data curation, pretraining, and finetuning, Pisces achieves<br>competitive performance in both image understanding and image generation. We<br>evaluate Pisces on over 20 public benchmarks for image understanding, where it<br>demonstrates strong performance across a wide range of tasks. Additionally, on<br>GenEval, a widely adopted benchmark for image generation, Pisces exhibits<br>robust generative capabilities. Our extensive analysis reveals the synergistic<br>relationship between image understanding and generation, and the benefits of<br>using separate visual encoders, advancing the field of unified multimodal<br>models.</p>
</details>


<h3 id="66-MF2Summ-Multimodal-Fusion-for-Video-Summarization-with-Temporal-Alignment"><a href="#66-MF2Summ-Multimodal-Fusion-for-Video-Summarization-with-Temporal-Alignment" class="headerlink" title="[66] MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10430">MF2Summ: Multimodal Fusion for Video Summarization with Temporal Alignment</a></h3><p><em>Shuo wang,Jihao Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: MF2Summ提出了一种多模态融合的视频摘要方法，结合视觉和听觉信息，通过跨模态Transformer和时间对齐机制提升摘要性能。在SumMe和TVSum数据集上表现优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 传统视频摘要方法通常仅依赖视觉信息，无法充分利用视频的多模态语义。本文旨在通过融合视觉和听觉信息，提升视频摘要的性能和语义丰富度。

<p>Contribution: 1) 提出MF2Summ模型，首次结合视觉和听觉信息进行视频摘要；2) 设计了跨模态Transformer和时间对齐机制；3) 在SumMe和TVSum数据集上表现优于现有方法。</p>
<p>Method: 1) 从GoogLeNet和SoundNet提取视觉和听觉特征；2) 使用跨模态Transformer和自注意力Transformer建模模态依赖和时间对齐；3) 预测片段重要性，并通过NMS和KTS算法选择关键片段。</p>
<p>Result: 在SumMe和TVSum数据集上，F1分数分别提升1.9%和0.6%，优于DSNet和其他先进方法。</p>
<p>Insight: 多模态融合和时间对齐是提升视频摘要性能的关键，听觉信息为视觉提供了补充语义，增强了摘要的全面性。</p>
<p>Abstract: The rapid proliferation of online video content necessitates effective video<br>summarization techniques. Traditional methods, often relying on a single<br>modality (typically visual), struggle to capture the full semantic richness of<br>videos. This paper introduces MF2Summ, a novel video summarization model based<br>on multimodal content understanding, integrating both visual and auditory<br>information. MF2Summ employs a five-stage process: feature extraction,<br>cross-modal attention interaction, feature fusion, segment prediction, and key<br>shot selection. Visual features are extracted using a pre-trained GoogLeNet<br>model, while auditory features are derived using SoundNet. The core of our<br>fusion mechanism involves a cross-modal Transformer and an alignment-guided<br>self-attention Transformer, designed to effectively model inter-modal<br>dependencies and temporal correspondences. Segment importance, location, and<br>center-ness are predicted, followed by key shot selection using Non-Maximum<br>Suppression (NMS) and the Kernel Temporal Segmentation (KTS) algorithm.<br>Experimental results on the SumMe and TVSum datasets demonstrate that MF2Summ<br>achieves competitive performance, notably improving F1-scores by 1.9% and<br>0.6% respectively over the DSNet model, and performing favorably against other<br>state-of-the-art methods.</p>
</details>


<h3 id="67-Towards-Robust-Multimodal-Emotion-Recognition-under-Missing-Modalities-and-Distribution-Shifts"><a href="#67-Towards-Robust-Multimodal-Emotion-Recognition-under-Missing-Modalities-and-Distribution-Shifts" class="headerlink" title="[67] Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10452">Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts</a></h3><p><em>Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为CIDer的鲁棒多模态情感识别框架，通过模型特定的自蒸馏和模型无关的因果推断模块，解决了模态缺失和分布偏移问题。</p>
<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在面临模态缺失和分布外数据时表现不佳，现有方法过于依赖特定模型或引入过多参数，CIDer旨在解决这些问题。

<p>Contribution: 1) 提出CIDer框架，结合自蒸馏和因果推断模块；2) 引入RMFM任务和新的OOD数据集；3) 在参数效率和鲁棒性上优于现有方法。</p>
<p>Method: CIDer包含Model-Specific Self-Distillation (MSSD)和Model-Agnostic Causal Inference (MACI)模块。MSSD通过自蒸馏增强鲁棒性，MACI利用因果图减少偏差。</p>
<p>Result: CIDer在RMFM和OOD任务中表现出色，且参数更少、训练更快。</p>
<p>Insight: 结合自蒸馏和因果推断可以同时解决模态缺失和分布偏移问题，且无需过多参数。</p>
<p>Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges<br>in addressing both modality missing and Out-Of-Distribution (OOD) data<br>simultaneously. Existing methods often rely on specific models or introduce<br>excessive parameters, which limits their practicality. To address these issues,<br>we propose a novel robust MER framework, Causal Inference Distiller (CIDer),<br>and introduce a new task, Random Modality Feature Missing (RMFM), to generalize<br>the definition of modality missing. CIDer integrates two key components: a<br>Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal<br>Inference (MACI) module. MSSD enhances robustness under the RMFM task through a<br>weight-sharing self-distillation approach applied across low-level features,<br>attention maps, and high-level representations. Additionally, a Word-level<br>Self-aligned Attention Module (WSAM) reduces computational complexity, while a<br>Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.<br>To tackle OOD challenges, MACI employs a tailored causal graph to mitigate<br>label and language biases using a Multimodal Causal Module (MCM) and<br>fine-grained counterfactual texts. Notably, MACI can independently enhance OOD<br>generalization with minimal additional parameters. Furthermore, we also<br>introduce the new repartitioned MER OOD datasets. Experimental results<br>demonstrate that CIDer achieves robust performance in both RMFM and OOD<br>scenarios, with fewer parameters and faster training compared to<br>state-of-the-art methods. The implementation of this work is publicly<br>accessible at <a target="_blank" rel="noopener" href="https://github.com/gw-zhong/CIDer">https://github.com/gw-zhong/CIDer</a>.</p>
</details>


<h3 id="68-Rethinking-Generative-Human-Video-Coding-with-Implicit-Motion-Transformation"><a href="#68-Rethinking-Generative-Human-Video-Coding-with-Implicit-Motion-Transformation" class="headerlink" title="[68] Rethinking Generative Human Video Coding with Implicit Motion Transformation"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10453">Rethinking Generative Human Video Coding with Implicit Motion Transformation</a></h3><p><em>Bolin Chen,Ru-Ling Liao,Jie Chen,Yan Ye</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种基于隐式运动变换（IMT）的生成式人体视频编码方法，解决了传统显式运动场在复杂人体运动中的失真和运动不准确问题。</p>
<details>
  <summary>Details</summary>
Motivation: 传统基于显式运动的生成式人体视频编码在复杂多样的运动模式中表现不佳，导致重建结果失真和运动不准确。

<p>Contribution: 提出了隐式运动变换（IMT）方法，通过将复杂人体信号表征为紧凑视觉特征，并将其转换为隐式运动指导，显著提升了生成式人体视频编码的性能。</p>
<p>Method: 将人体信号压缩为紧凑视觉特征，并利用隐式运动变换（IMT）将其转换为隐式运动指导，用于高质量重建。</p>
<p>Result: 实验证明，IMT方法在高效压缩和高保真合成方面表现优异。</p>
<p>Insight: 隐式运动变换能更有效地处理复杂人体运动，避免了显式运动场的局限性。</p>
<p>Abstract: Beyond traditional hybrid-based video codec, generative video codec could<br>achieve promising compression performance by evolving high-dimensional signals<br>into compact feature representations for bitstream compactness at the encoder<br>side and developing explicit motion fields as intermediate supervision for<br>high-quality reconstruction at the decoder side. This paradigm has achieved<br>significant success in face video compression. However, compared to facial<br>videos, human body videos pose greater challenges due to their more complex and<br>diverse motion patterns, i.e., when using explicit motion guidance for<br>Generative Human Video Coding (GHVC), the reconstruction results could suffer<br>severe distortions and inaccurate motion. As such, this paper highlights the<br>limitations of explicit motion-based approaches for human body video<br>compression and investigates the GHVC performance improvement with the aid of<br>Implicit Motion Transformation, namely IMT. In particular, we propose to<br>characterize complex human body signal into compact visual features and<br>transform these features into implicit motion guidance for signal<br>reconstruction. Experimental results demonstrate the effectiveness of the<br>proposed IMT paradigm, which can facilitate GHVC to achieve high-efficiency<br>compression and high-fidelity synthesis.</p>
</details>


<h3 id="69-MedSeg-R-Reasoning-Segmentation-in-Medical-Images-with-Multimodal-Large-Language-Models"><a href="#69-MedSeg-R-Reasoning-Segmentation-in-Medical-Images-with-Multimodal-Large-Language-Models" class="headerlink" title="[69] MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10465">MedSeg-R: Reasoning Segmentation in Medical Images with Multimodal Large Language Models</a></h3><p><em>Yu Huang,Zelin Peng,Yichen Zhao,Piao Yang,Xiaokang Yang,Wei Shen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: MedSeg-R 是一个基于多模态大语言模型（MLLMs）的端到端框架，用于医学图像推理分割任务，能够生成精确的分割掩码并理解复杂的临床指令。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割模型依赖显式的人工指令，缺乏主动推理能力，无法处理复杂的临床问题。多模态大语言模型在医学QA任务中表现良好，但难以生成精确的分割掩码。

<p>Contribution: 1) 提出医学图像推理分割任务；2) 设计MedSeg-R框架，结合MLLMs的推理能力和像素级分割；3) 发布MedSeg-QA数据集，包含10,000多对图像掩码和对话数据。</p>
<p>Method: 框架包含全局上下文理解模块（生成多模态中间令牌）和像素级接地模块（解码令牌生成分割掩码和文本响应）。</p>
<p>Result: 实验表明MedSeg-R在多个基准测试中表现优异，分割精度高，并支持医学图像的可解释文本分析。</p>
<p>Insight: 通过结合大语言模型的推理能力和像素级分割技术，可以有效实现医学图像的复杂指令理解和精确分割。</p>
<p>Abstract: Medical image segmentation is crucial for clinical diagnosis, yet existing<br>models are limited by their reliance on explicit human instructions and lack<br>the active reasoning capabilities to understand complex clinical questions.<br>While recent advancements in multimodal large language models (MLLMs) have<br>improved medical question-answering (QA) tasks, most methods struggle to<br>generate precise segmentation masks, limiting their application in automatic<br>medical diagnosis. In this paper, we introduce medical image reasoning<br>segmentation, a novel task that aims to generate segmentation masks based on<br>complex and implicit medical instructions. To address this, we propose<br>MedSeg-R, an end-to-end framework that leverages the reasoning abilities of<br>MLLMs to interpret clinical questions while also capable of producing<br>corresponding precise segmentation masks for medical images. It is built on two<br>core components: 1) a global context understanding module that interprets<br>images and comprehends complex medical instructions to generate multi-modal<br>intermediate tokens, and 2) a pixel-level grounding module that decodes these<br>tokens to produce precise segmentation masks and textual responses.<br>Furthermore, we introduce MedSeg-QA, a large-scale dataset tailored for the<br>medical image reasoning segmentation task. It includes over 10,000 image-mask<br>pairs and multi-turn conversations, automatically annotated using large<br>language models and refined through physician reviews. Experiments show<br>MedSeg-R’s superior performance across several benchmarks, achieving high<br>segmentation accuracy and enabling interpretable textual analysis of medical<br>images.</p>
</details>


<h3 id="70-LLMs-Are-Not-Yet-Ready-for-Deepfake-Image-Detection"><a href="#70-LLMs-Are-Not-Yet-Ready-for-Deepfake-Image-Detection" class="headerlink" title="[70] LLMs Are Not Yet Ready for Deepfake Image Detection"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10474">LLMs Are Not Yet Ready for Deepfake Image Detection</a></h3><p><em>Shahroz Tariq,David Nguyen,M. A. P. Chamikara,Tingmin Wu,Alsharif Abuadbba,Kristen Moore</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文通过零样本评估研究了四种大型视觉语言模型（VLM）在深伪图像检测中的表现，发现虽然它们能生成合理解释并识别表面异常，但仍不适合作为独立检测系统。模型容易受到风格误导，但在可解释性和上下文分析方面有潜力，适合作为混合或人机协作框架的一部分。</p>
<details>
  <summary>Details</summary>
Motivation: 随着深伪技术的快速发展，维护媒体可信度和公众信任面临巨大挑战。视觉语言模型（VLM）因其多领域潜力而被视为可能的解决方案，但其在深伪检测中的实际表现尚不清楚，本研究旨在填补这一空白。

<p>Contribution: 1. 对四种主流VLM（ChatGPT、Claude、Gemini、Grok）进行零样本深伪检测评估；2. 构建涵盖多种深伪类型的基准数据集；3. 揭示模型的失败模式（如过度关注风格）及潜力（如上下文分析能力）。</p>
<p>Method: 1. 使用零样本评估方法，测试模型在三种深伪类型（换脸、重演、合成生成）上的表现；2. 通过分类准确率和解释深度分析模型能力；3. 识别模型的关键失败模式，如对复古风格等误导模式的敏感性。</p>
<p>Result: 1. VLM能生成合理解释并识别表面异常，但无法作为独立检测工具；2. 模型在风格误导下表现不佳；3. 在可解释性和上下文分析方面表现突出，适合辅助人类专家。</p>
<p>Insight: 通用模型虽无法独立胜任深伪检测，但其在可解释性和上下文分析方面的优势使其适合作为混合或人机协作框架的一部分，未来可结合领域专业知识提升性能。</p>
<p>Abstract: The growing sophistication of deepfakes presents substantial challenges to<br>the integrity of media and the preservation of public trust. Concurrently,<br>vision-language models (VLMs), large language models enhanced with visual<br>reasoning capabilities, have emerged as promising tools across various domains,<br>sparking interest in their applicability to deepfake detection. This study<br>conducts a structured zero-shot evaluation of four prominent VLMs: ChatGPT,<br>Claude, Gemini, and Grok, focusing on three primary deepfake types: faceswap,<br>reenactment, and synthetic generation. Leveraging a meticulously assembled<br>benchmark comprising authentic and manipulated images from diverse sources, we<br>evaluate each model’s classification accuracy and reasoning depth. Our analysis<br>indicates that while VLMs can produce coherent explanations and detect<br>surface-level anomalies, they are not yet dependable as standalone detection<br>systems. We highlight critical failure modes, such as an overemphasis on<br>stylistic elements and vulnerability to misleading visual patterns like vintage<br>aesthetics. Nevertheless, VLMs exhibit strengths in interpretability and<br>contextual analysis, suggesting their potential to augment human expertise in<br>forensic workflows. These insights imply that although general-purpose models<br>currently lack the reliability needed for autonomous deepfake detection, they<br>hold promise as integral components in hybrid or human-in-the-loop detection<br>frameworks.</p>
</details>


<h3 id="71-Semantic-Localization-Guiding-Segment-Anything-Model-For-Reference-Remote-Sensing-Image-Segmentation"><a href="#71-Semantic-Localization-Guiding-Segment-Anything-Model-For-Reference-Remote-Sensing-Image-Segmentation" class="headerlink" title="[71] Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10503">Semantic Localization Guiding Segment Anything Model For Reference Remote Sensing Image Segmentation</a></h3><p><em>Shuyang Li,Shuang Wang,Zhuangzhuang Sun,Jing Xiao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种名为PSLG-SAM的框架，通过将参考遥感图像分割任务分解为粗定位和精细分割两阶段，利用视觉定位网络和SAM模型，显著提升了性能并减少了标注负担。</p>
<details>
  <summary>Details</summary>
Motivation: 当前RRSIS方法依赖多模态融合骨干和语义分割头，但面临密集标注需求和复杂场景解释的挑战。论文旨在通过两阶段分解解决这些问题。

<p>Contribution: 1. 提出PSLG-SAM框架，分解任务为粗定位和精细分割；2. 引入聚类增强和掩码边界优化策略；3. 贡献了一个高质量的多类别标注数据集。</p>
<p>Method: 1. 粗定位阶段使用视觉定位网络；2. 精细分割阶段用SAM模型，结合聚类生成前景点和掩码边界优化。</p>
<p>Result: 在两个数据集上，PSLG-SAM显著优于现有方法，验证了其有效性。</p>
<p>Insight: 通过任务分解和SAM模型的结合，可以显著减少标注负担并提升复杂场景下的分割精度。</p>
<p>Abstract: The Reference Remote Sensing Image Segmentation (RRSIS) task generates<br>segmentation masks for specified objects in images based on textual<br>descriptions, which has attracted widespread attention and research interest.<br>Current RRSIS methods rely on multi-modal fusion backbones and semantic<br>segmentation heads but face challenges like dense annotation requirements and<br>complex scene interpretation. To address these issues, we propose a framework<br>named \textit{prompt-generated semantic localization guiding Segment Anything<br>Model}(PSLG-SAM), which decomposes the RRSIS task into two stages: coarse<br>localization and fine segmentation. In coarse localization stage, a visual<br>grounding network roughly locates the text-described object. In fine<br>segmentation stage, the coordinates from the first stage guide the Segment<br>Anything Model (SAM), enhanced by a clustering-based foreground point generator<br>and a mask boundary iterative optimization strategy for precise segmentation.<br>Notably, the second stage can be train-free, significantly reducing the<br>annotation data burden for the RRSIS task. Additionally, decomposing the RRSIS<br>task into two stages allows for focusing on specific region segmentation,<br>avoiding interference from complex scenes.We further contribute a high-quality,<br>multi-category manually annotated dataset. Experimental validation on two<br>datasets (RRSIS-D and RRSIS-M) demonstrates that PSLG-SAM achieves significant<br>performance improvements and surpasses existing state-of-the-art models.Our<br>code will be made publicly available.</p>
</details>


<h3 id="72-J-DDL-Surface-Damage-Detection-and-Localization-System-for-Fighter-Aircraft"><a href="#72-J-DDL-Surface-Damage-Detection-and-Localization-System-for-Fighter-Aircraft" class="headerlink" title="[72] J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10505">J-DDL: Surface Damage Detection and Localization System for Fighter Aircraft</a></h3><p><em>Jin Huang,Mingqiang Wei,Zikuan Li,Hangyu Qu,Wei Zhao,Xinyu Bai</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: J-DDL是一个用于战斗机表面损伤检测与定位的智能系统，结合2D图像与3D点云数据，优化了YOLO架构并引入创新模块，显著提升了检测效率和准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 战斗机表面检查的规模和复杂性使得人工检测效率低下且难以统一，亟需自动化解决方案。

<p>Contribution: 1）提出J-DDL系统，整合2D图像与3D点云；2）优化YOLO架构，引入轻量级Fasternet块和EMA模块；3）提出新损失函数Inner-CIOU；4）发布首个飞机损伤公开数据集。</p>
<p>Method: 基于YOLO的检测网络，结合Fasternet块和EMA模块进行特征提取与聚合，利用Inner-CIOU损失提升精度，并将2D检测结果映射到3D点云。</p>
<p>Result: 实验验证J-DDL在损伤检测与定位上的有效性，显著优于传统方法。</p>
<p>Insight: 结合2D与3D数据可实现更全面的表面检查；轻量化模块和新型损失函数是提升检测效率的关键。</p>
<p>Abstract: Ensuring the safety and extended operational life of fighter aircraft<br>necessitates frequent and exhaustive inspections. While surface defect<br>detection is feasible for human inspectors, manual methods face critical<br>limitations in scalability, efficiency, and consistency due to the vast surface<br>area, structural complexity, and operational demands of aircraft maintenance.<br>We propose a smart surface damage detection and localization system for fighter<br>aircraft, termed J-DDL. J-DDL integrates 2D images and 3D point clouds of the<br>entire aircraft surface, captured using a combined system of laser scanners and<br>cameras, to achieve precise damage detection and localization. Central to our<br>system is a novel damage detection network built on the YOLO architecture,<br>specifically optimized for identifying surface defects in 2D aircraft images.<br>Key innovations include lightweight Fasternet blocks for efficient feature<br>extraction, an optimized neck architecture incorporating Efficient Multiscale<br>Attention (EMA) modules for superior feature aggregation, and the introduction<br>of a novel loss function, Inner-CIOU, to enhance detection accuracy. After<br>detecting damage in 2D images, the system maps the identified anomalies onto<br>corresponding 3D point clouds, enabling accurate 3D localization of defects<br>across the aircraft surface. Our J-DDL not only streamlines the inspection<br>process but also ensures more comprehensive and detailed coverage of large and<br>complex aircraft exteriors. To facilitate further advancements in this domain,<br>we have developed the first publicly available dataset specifically focused on<br>aircraft damage. Experimental evaluations validate the effectiveness of our<br>framework, underscoring its potential to significantly advance automated<br>aircraft inspection technologies.</p>
</details>


<h3 id="73-CogStream-Context-guided-Streaming-Video-Question-Answering"><a href="#73-CogStream-Context-guided-Streaming-Video-Question-Answering" class="headerlink" title="[73] CogStream: Context-guided Streaming Video Question Answering"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10516">CogStream: Context-guided Streaming Video Question Answering</a></h3><p><em>Zicheng Zhao,Kangyu Wang,Shijie Li,Rui Qian,Weiyao Lin,Huabin Liu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 这篇论文提出了一个名为CogStream的新任务，解决了流媒体视频场景中的多模态推理问题，同时提出了一种高效的方法和相关数据集。</p>
<details>
  <summary>Details</summary>
Motivation: 传统视频大语言模型在处理流媒体视频时存在计算负担大和无关上下文干扰的问题。

<p>Contribution: 论文提出了一种新的任务（CogStream），引入了包含层次化问答对的标注数据集，并提出了基线模型CogReasoner。</p>
<p>Method: CogReasoner通过视觉流压缩和历史对话检索高效处理任务。</p>
<p>Result: 实验证明了方法的有效性。</p>
<p>Insight: 流媒体视频的多模态推理需要高效利用相关上下文信息，避免无关数据干扰。</p>
<p>Abstract: Despite advancements in Video Large Language Models (Vid-LLMs) improving<br>multimodal understanding, challenges persist in streaming video reasoning due<br>to its reliance on contextual information. Existing paradigms feed all<br>available historical contextual information into Vid-LLMs, resulting in a<br>significant computational burden for visual data processing. Furthermore, the<br>inclusion of irrelevant context distracts models from key details. This paper<br>introduces a challenging task called Context-guided Streaming Video Reasoning<br>(CogStream), which simulates real-world streaming video scenarios, requiring<br>models to identify the most relevant historical contextual information to<br>deduce answers for questions about the current stream. To support CogStream, we<br>present a densely annotated dataset featuring extensive and hierarchical<br>question-answer pairs, generated by a semi-automatic pipeline. Additionally, we<br>present CogReasoner as a baseline model. It efficiently tackles this task by<br>leveraging visual stream compression and historical dialogue retrieval.<br>Extensive experiments prove the effectiveness of this method. Code will be<br>released soon.</p>
</details>


<h3 id="74-From-Images-to-Insights-Explainable-Biodiversity-Monitoring-with-Plain-Language-Habitat-Explanations"><a href="#74-From-Images-to-Insights-Explainable-Biodiversity-Monitoring-with-Plain-Language-Habitat-Explanations" class="headerlink" title="[74] From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10559">From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations</a></h3><p><em>Yutong Zhou,Masahiro Ryo</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出一个端到端的视觉-因果框架，将物种图像转化为关于其栖息地偏好的可解释因果分析，结合了物种识别、全球分布检索、伪缺失采样和气候数据提取等方法。</p>
<details>
  <summary>Details</summary>
Motivation: 理解物种为何生活在特定地点对生态研究和生物多样性保护至关重要，但现有生态工作流程零散且难以被非专家使用。

<p>Contribution: 该论文的主要贡献是开发了一个整合多模态数据和因果推断方法的框架，用于生成人类易懂的栖息地偏好解释。</p>
<p>Method: 方法包括：（1）物种识别，（2）全球分布检索，（3）伪缺失采样，（4）气候数据提取，（5）发现环境特征的因果结构，（6）评估其对物种分布的影响。最终通过模板和大语言模型生成解释。</p>
<p>Result: 通过蜜蜂和花卉物种的案例展示了框架的潜力，表明其能生成统计支撑且人类易懂的栖息地解释。</p>
<p>Insight: 结合多模态AI和因果推断方法，可以为生态学研究提供更直观和可解释的工具，尤其有助于非专业用户理解物种栖息地偏好。</p>
<p>Abstract: Explaining why the species lives at a particular location is important for<br>understanding ecological systems and conserving biodiversity. However, existing<br>ecological workflows are fragmented and often inaccessible to non-specialists.<br>We propose an end-to-end visual-to-causal framework that transforms a species<br>image into interpretable causal insights about its habitat preference. The<br>system integrates species recognition, global occurrence retrieval,<br>pseudo-absence sampling, and climate data extraction. We then discover causal<br>structures among environmental features and estimate their influence on species<br>occurrence using modern causal inference methods. Finally, we generate<br>statistically grounded, human-readable causal explanations from structured<br>templates and large language models. We demonstrate the framework on a bee and<br>a flower species and report early results as part of an ongoing project,<br>showing the potential of the multimodal AI assistant backed up by a recommended<br>ecological modeling practice for describing species habitat in<br>human-understandable language.</p>
</details>


<h3 id="75-Balancing-Tails-when-Comparing-Distributions-Comprehensive-Equity-Index-CEI-with-Application-to-Bias-Evaluation-in-Operational-Face-Biometrics"><a href="#75-Balancing-Tails-when-Comparing-Distributions-Comprehensive-Equity-Index-CEI-with-Application-to-Bias-Evaluation-in-Operational-Face-Biometrics" class="headerlink" title="[75] Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10564">Balancing Tails when Comparing Distributions: Comprehensive Equity Index (CEI) with Application to Bias Evaluation in Operational Face Biometrics</a></h3><p><em>Imanol Solano,Julian Fierrez,Aythami Morales,Alejandro Peña,Ruben Tolosana,Francisco Zamora-Martinez,Javier San Agustin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了 Comprehensive Equity Index (CEI)，一种用于检测人脸识别系统中人口统计偏差的新指标。CEI通过单独分析真实和冒名分数分布，并关注分布尾部，显著提升了检测细微偏差的能力。</p>
<details>
  <summary>Details</summary>
Motivation: 现有指标难以检测高性能人脸识别系统中的细微人口统计偏差，尤其是在分数分布的尾部。因此，需要一种更敏感的度量方法来揭示这些隐蔽的偏差。

<p>Contribution: 1. 提出了 CEI，一种新颖的公平性指标，专注于分布尾部概率和整体形状。2. 设计了自动化版本 CEI^A，提升了客观性和实用性。3. 验证了 CEI 在检测细微偏差上的优越性。</p>
<p>Method: CEI 分别分析真实分数分布和冒名分数分布，允许可配置的尾部概率关注，并结合整体分布形状进行评估。实验涵盖了多种数据集和故意引入偏差的模型。</p>
<p>Result: 实验表明，CEI 能有效检测出以往方法难以发现的细微偏差，尤其在分布尾部。CEI^A 进一步增强了实用性和客观性。</p>
<p>Insight: CEI 的核心创新在于对分布尾部的独立分析，这在检测细微偏差时尤为关键。这一方法不仅适用于人脸识别，还可扩展到其他需要分析分布尾部的问题。</p>
<p>Abstract: Demographic bias in high-performance face recognition (FR) systems often<br>eludes detection by existing metrics, especially with respect to subtle<br>disparities in the tails of the score distribution. We introduce the<br>Comprehensive Equity Index (CEI), a novel metric designed to address this<br>limitation. CEI uniquely analyzes genuine and impostor score distributions<br>separately, enabling a configurable focus on tail probabilities while also<br>considering overall distribution shapes. Our extensive experiments (evaluating<br>state-of-the-art FR systems, intentionally biased models, and diverse datasets)<br>confirm CEI’s superior ability to detect nuanced biases where previous methods<br>fall short. Furthermore, we present CEI^A, an automated version of the metric<br>that enhances objectivity and simplifies practical application. CEI provides a<br>robust and sensitive tool for operational FR fairness assessment. The proposed<br>methods have been developed particularly for bias evaluation in face biometrics<br>but, in general, they are applicable for comparing statistical distributions in<br>any problem where one is interested in analyzing the distribution tails.</p>
</details>


<h3 id="76-DreamActor-H1-High-Fidelity-Human-Product-Demonstration-Video-Generation-via-Motion-designed-Diffusion-Transformers"><a href="#76-DreamActor-H1-High-Fidelity-Human-Product-Demonstration-Video-Generation-via-Motion-designed-Diffusion-Transformers" class="headerlink" title="[76] DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10568">DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</a></h3><p><em>Lizhen Wang,Zhurong Xia,Tianshu Hu,Pengrui Wang,Pengfei Wang,Zerong Zheng,Ming Zhou</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种基于扩散变换器（DiT）的框架DreamActor-H1，用于生成高保真的人类与产品展示视频，解决了现有方法在保留人和产品身份以及空间关系上的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 在电子商务和数字营销中，生成高质量的人类与产品展示视频对产品呈现至关重要。现有方法往往无法同时保留人和产品的身份信息，或缺乏对空间关系的理解。

<p>Contribution: 1）提出了一种基于扩散变换器的框架，能够同时保留人和产品的身份细节；2）引入遮挡交叉注意力机制和3D人体网格模板，优化运动引导；3）通过结构化文本编码增强类别级语义一致性。</p>
<p>Method: 采用扩散变换器（DiT）结合遮挡交叉注意力机制，输入配对的参考信息（人和产品），使用3D身体网格和产品边界框提供运动指导，并利用结构化文本编码增强3D一致性。</p>
<p>Result: 在混合数据集上训练后，该方法在身份保留和运动生成上表现优于现有技术，实现了更真实的展示效果。</p>
<p>Insight: 通过结合3D模板和语义编码，可以显著提升生成视频的保真度和交互自然性，为电商应用提供实用解决方案。</p>
<p>Abstract: In e-commerce and digital marketing, generating high-fidelity human-product<br>demonstration videos is important for effective product presentation. However,<br>most existing frameworks either fail to preserve the identities of both humans<br>and products or lack an understanding of human-product spatial relationships,<br>leading to unrealistic representations and unnatural interactions. To address<br>these challenges, we propose a Diffusion Transformer (DiT)-based framework. Our<br>method simultaneously preserves human identities and product-specific details,<br>such as logos and textures, by injecting paired human-product reference<br>information and utilizing an additional masked cross-attention mechanism. We<br>employ a 3D body mesh template and product bounding boxes to provide precise<br>motion guidance, enabling intuitive alignment of hand gestures with product<br>placements. Additionally, structured text encoding is used to incorporate<br>category-level semantics, enhancing 3D consistency during small rotational<br>changes across frames. Trained on a hybrid dataset with extensive data<br>augmentation strategies, our approach outperforms state-of-the-art techniques<br>in maintaining the identity integrity of both humans and products and<br>generating realistic demonstration motions. Project page:<br><a target="_blank" rel="noopener" href="https://submit2025-dream.github.io/DreamActor-H1/">https://submit2025-dream.github.io/DreamActor-H1/</a>.</p>
</details>


<h3 id="77-Improving-Medical-Visual-Representation-Learning-with-Pathological-level-Cross-Modal-Alignment-and-Correlation-Exploration"><a href="#77-Improving-Medical-Visual-Representation-Learning-with-Pathological-level-Cross-Modal-Alignment-and-Correlation-Exploration" class="headerlink" title="[77] Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10573">Improving Medical Visual Representation Learning with Pathological-level Cross-Modal Alignment and Correlation Exploration</a></h3><p><em>Jun Wang,Lixing Zhu,Xiaohan Yu,Abhir Bhalerao,Yulan He</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为PLACE的新框架，通过病理级别的跨模态对齐和关联探索，提升了医学视觉表示学习的效果，无需额外人工标注。</p>
<details>
  <summary>Details</summary>
Motivation: 医学领域的图像-报告对学习面临复杂语义和长报告的挑战，现有方法多关注实例或词汇级别的对齐，而忽略了病理级别的语义一致性。

<p>Contribution: 提出了PLACE框架，结合病理级别的跨模态对齐（PCMA）和关联探索，提高了医学视觉表示的鲁棒性和泛化能力。</p>
<p>Method: 1. 提出PCMA模块，通过视觉病理观察提取器从局部标记中提取病理表示；2. 设计代理任务探索图像块间的关联，丰富细粒度细节。</p>
<p>Result: 在分类、图像-文本检索、语义分割、目标检测和报告生成等多个下游任务中实现了SOTA性能。</p>
<p>Insight: 病理级别的对齐和关联探索是提升医学视觉表示学习的关键，且无需依赖外部疾病标注。</p>
<p>Abstract: Learning medical visual representations from image-report pairs through joint<br>learning has garnered increasing research attention due to its potential to<br>alleviate the data scarcity problem in the medical domain. The primary<br>challenges stem from the lengthy reports that feature complex discourse<br>relations and semantic pathologies. Previous works have predominantly focused<br>on instance-wise or token-wise cross-modal alignment, often neglecting the<br>importance of pathological-level consistency. This paper presents a novel<br>framework PLACE that promotes the Pathological-Level Alignment and enriches the<br>fine-grained details via Correlation Exploration without additional human<br>annotations. Specifically, we propose a novel pathological-level cross-modal<br>alignment (PCMA) approach to maximize the consistency of pathology observations<br>from both images and reports. To facilitate this, a Visual Pathology<br>Observation Extractor is introduced to extract visual pathological observation<br>representations from localized tokens. The PCMA module operates independently<br>of any external disease annotations, enhancing the generalizability and<br>robustness of our methods. Furthermore, we design a proxy task that enforces<br>the model to identify correlations among image patches, thereby enriching the<br>fine-grained details crucial for various downstream tasks. Experimental results<br>demonstrate that our proposed framework achieves new state-of-the-art<br>performance on multiple downstream tasks, including classification,<br>image-to-text retrieval, semantic segmentation, object detection and report<br>generation.</p>
</details>


<h3 id="78-DanceChat-Large-Language-Model-Guided-Music-to-Dance-Generation"><a href="#78-DanceChat-Large-Language-Model-Guided-Music-to-Dance-Generation" class="headerlink" title="[78] DanceChat: Large Language Model-Guided Music-to-Dance Generation"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10574">DanceChat: Large Language Model-Guided Music-to-Dance Generation</a></h3><p><em>Qing Wang,Xiaohang Yang,Yilan Dong,Naveen Raj Govindaraj,Gregory Slabaugh,Shanxin Yuan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DanceChat 是一种基于大语言模型（LLM）的音乐到舞蹈生成方法，通过文本指令提供明确的舞蹈指导，解决了音乐与舞蹈之间的语义鸿沟问题。</p>
<details>
  <summary>Details</summary>
Motivation: 音乐仅提供抽象的线索（如旋律、节奏和情感），难以直接映射到具体舞蹈动作，同时音乐和舞蹈的配对数据稀缺，限制了模型的多样性学习能力。

<p>Contribution: 提出了 DanceChat，利用 LLM 生成文本舞蹈指令，提供高层次指导；设计了多模态特征提取与融合模块，以及扩散模型与多模态对齐损失，确保生成的舞蹈与音乐和文本一致。</p>
<p>Method: 1) LLM 生成伪指令；2) 多模态特征提取与融合；3) 扩散模型结合多模态对齐损失进行运动合成。</p>
<p>Result: 在 AIST++ 数据集和人类评估中，DanceChat 在定性和定量上均优于现有方法。</p>
<p>Insight: 通过大语言模型提供的文本指令，可以显式填补音乐到舞蹈的语义鸿沟，显著提升生成的多样性和对齐性。</p>
<p>Abstract: Music-to-dance generation aims to synthesize human dance motion conditioned<br>on musical input. Despite recent progress, significant challenges remain due to<br>the semantic gap between music and dance motion, as music offers only abstract<br>cues, such as melody, groove, and emotion, without explicitly specifying the<br>physical movements. Moreover, a single piece of music can produce multiple<br>plausible dance interpretations. This one-to-many mapping demands additional<br>guidance, as music alone provides limited information for generating diverse<br>dance movements. The challenge is further amplified by the scarcity of paired<br>music and dance data, which restricts the model^a\u{A}&#39;Zs ability to learn<br>diverse dance patterns. In this paper, we introduce DanceChat, a Large Language<br>Model (LLM)-guided music-to-dance generation approach. We use an LLM as a<br>choreographer that provides textual motion instructions, offering explicit,<br>high-level guidance for dance generation. This approach goes beyond implicit<br>learning from music alone, enabling the model to generate dance that is both<br>more diverse and better aligned with musical styles. Our approach consists of<br>three components: (1) an LLM-based pseudo instruction generation module that<br>produces textual dance guidance based on music style and structure, (2) a<br>multi-modal feature extraction and fusion module that integrates music, rhythm,<br>and textual guidance into a shared representation, and (3) a diffusion-based<br>motion synthesis module together with a multi-modal alignment loss, which<br>ensures that the generated dance is aligned with both musical and textual cues.<br>Extensive experiments on AIST++ and human evaluations show that DanceChat<br>outperforms state-of-the-art methods both qualitatively and quantitatively.</p>
</details>


<h3 id="79-Text-to-Image-for-Multi-Label-Image-Recognition-with-Joint-Prompt-Adapter-Learning"><a href="#79-Text-to-Image-for-Multi-Label-Image-Recognition-with-Joint-Prompt-Adapter-Learning" class="headerlink" title="[79] Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10575">Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning</a></h3><p><em>Chun-Mei Feng,Kai Yu,Xinxing Xu,Salman Khan,Rick Siow Mong Goh,Wangmeng Zuo,Yong Liu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为T2I-PAL的新方法，通过利用文本到图像生成模型减少模态差异，并结合提示调谐和适配器学习提升多标签图像识别性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的文本-图像对比学习方法存在模态差异问题，限制了多标签图像识别的性能。T2I-PAL旨在通过生成多样化的真实图像和局部特征聚合解决这一问题。

<p>Contribution: 1) 提出T2I-PAL方法，利用文本生成图像减少模态差异；2) 结合类热图和可学习原型提升局部特征表示；3) 整合提示调谐和适配器学习以优化分类性能。</p>
<p>Method: 1) 使用预训练文本到图像模型生成多样图像；2) 引入类热图和可学习原型优化局部特征；3) 结合提示调谐和适配器学习进行高效微调。</p>
<p>Result: 在多个基准数据集（如MS-COCO、VOC2007和NUS-WIDE）上，T2I-PAL的平均识别性能比现有最优方法高出3.47%。</p>
<p>Insight: 通过生成真实图像并优化局部特征表示，可以有效减少模态差异并提升多标签图像识别性能，同时降低对全标注数据的依赖。</p>
<p>Abstract: Benefited from image-text contrastive learning, pre-trained vision-language<br>models, e.g., CLIP, allow to direct leverage texts as images (TaI) for<br>parameter-efficient fine-tuning (PEFT). While CLIP is capable of making image<br>features to be similar to the corresponding text features, the modality gap<br>remains a nontrivial issue and limits image recognition performance of TaI.<br>Using multi-label image recognition (MLR) as an example, we present a novel<br>method, called T2I-PAL to tackle the modality gap issue when using only text<br>captions for PEFT. The core design of T2I-PAL is to leverage pre-trained<br>text-to-image generation models to generate photo-realistic and diverse images<br>from text captions, thereby reducing the modality gap. To further enhance MLR,<br>T2I-PAL incorporates a class-wise heatmap and learnable prototypes. This<br>aggregates local similarities, making the representation of local visual<br>features more robust and informative for multi-label recognition. For better<br>PEFT, we further combine both prompt tuning and adapter learning to enhance<br>classification performance. T2I-PAL offers significant advantages: it<br>eliminates the need for fully semantically annotated training images, thereby<br>reducing the manual annotation workload, and it preserves the intrinsic mode of<br>the CLIP model, allowing for seamless integration with any existing CLIP<br>framework. Extensive experiments on multiple benchmarks, including MS-COCO,<br>VOC2007, and NUS-WIDE, show that our T2I-PAL can boost recognition performance<br>by 3.47% in average above the top-ranked state-of-the-art methods.</p>
</details>


<h3 id="80-Rethinking-Random-Masking-in-Self-Distillation-on-ViT"><a href="#80-Rethinking-Random-Masking-in-Self-Distillation-on-ViT" class="headerlink" title="[80] Rethinking Random Masking in Self Distillation on ViT"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10582">Rethinking Random Masking in Self Distillation on ViT</a></h3><p><em>Jihyeon Seong,Hyunkyung Han</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文探讨了在自蒸馏框架（如DINO）中随机掩码的作用，提出了一种不对称的掩码策略，仅对学生的全局视图进行掩码，同时保留局部视图和教师的全局视图。</p>
<details>
  <summary>Details</summary>
Motivation: 研究表明，随机掩码可能无意中移除关键语义信息，因此需要更智能的掩码策略。

<p>Contribution: 提出了一种不对称随机掩码策略，仅在学生的全局视图上应用掩码，以保留干净的监督信号并增强鲁棒性。</p>
<p>Method: 在DINO框架中，对学生的全局视图进行随机掩码，同时保持局部视图和教师的全局视图完整。</p>
<p>Result: 在mini-ImageNet数据集上的实验表明，该方法能生成更鲁棒和细粒度的注意力图，并提升下游任务性能。</p>
<p>Insight: 通过不对称掩码策略，可以平衡训练效率和语义信息的保留，从而提升自蒸馏框架的性能。</p>
<p>Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a<br>wide range of vision tasks. In particular, self-distillation frameworks such as<br>DINO have contributed significantly to these advances. Within such frameworks,<br>random masking is often utilized to improve training efficiency and introduce<br>regularization. However, recent studies have raised concerns that<br>indiscriminate random masking may inadvertently eliminate critical semantic<br>information, motivating the development of more informed masking strategies. In<br>this study, we explore the role of random masking in the self-distillation<br>setting, focusing on the DINO framework. Specifically, we apply random masking<br>exclusively to the student’s global view, while preserving the student’s local<br>views and the teacher’s global view in their original, unmasked forms. This<br>design leverages DINO’s multi-view augmentation scheme to retain clean<br>supervision while inducing robustness through masked inputs. We evaluate our<br>approach using DINO-Tiny on the mini-ImageNet dataset and show that random<br>masking under this asymmetric setup yields more robust and fine-grained<br>attention maps, ultimately enhancing downstream performance.</p>
</details>


<h3 id="81-Hierarchical-Error-Assessment-of-CAD-Models-for-Aircraft-Manufacturing-and-Measurement"><a href="#81-Hierarchical-Error-Assessment-of-CAD-Models-for-Aircraft-Manufacturing-and-Measurement" class="headerlink" title="[81] Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10594">Hierarchical Error Assessment of CAD Models for Aircraft Manufacturing-and-Measurement</a></h3><p><em>Jin Huang,Honghua Chen,Mingqiang Wei</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种称为HEA-MM的分层误差评估框架，用于飞机制造和测量平台中的CAD模型。通过全局、部件和特征三个层次进行误差分析，并结合结构光扫描器和优化方法，实现了对飞机工件的高精度评估。</p>
<details>
  <summary>Details</summary>
Motivation: 航空设备对高质量（高性能、高稳定性和高可靠性）的要求极高。现有方法在评估CAD模型的制造误差时缺乏分层分析能力，因此需要一种更全面的误差评估框架。

<p>Contribution: 提出了HEA-MM框架，首次在全局、部件和特征三个层次上对CAD模型进行误差分析；提出了一种基于优化的原始图元细化方法；设计了一种两阶段算法，用于检测和分析圆形孔特征。</p>
<p>Method: 1. 全局层次：评估扫描点云与参考CAD模型的整体偏差；2. 部件层次：通过优化方法对粗糙图元进行拆分和合并操作，生成有意义的点云区域；3. 特征层次：采用张量投票和假设-聚类框架检测并分析圆形孔特征。</p>
<p>Result: 实验结果表明，HEA-MM在多种飞机CAD模型上表现出高效性和准确性，能够为制造和测量提供可靠的误差评估。</p>
<p>Insight: 分层误差分析方法可以更全面地捕捉制造误差，优化方法为点云区域分析提供了新思路，两阶段圆形孔检测算法提高了特征分析的精度。</p>
<p>Abstract: The most essential feature of aviation equipment is high quality, including<br>high performance, high stability and high reliability. In this paper, we<br>propose a novel hierarchical error assessment framework for aircraft CAD models<br>within a manufacturing-and-measurement platform, termed HEA-MM. HEA-MM employs<br>structured light scanners to obtain comprehensive 3D measurements of<br>manufactured workpieces. The measured point cloud is registered with the<br>reference CAD model, followed by an error analysis conducted at three<br>hierarchical levels: global, part, and feature. At the global level, the error<br>analysis evaluates the overall deviation of the scanned point cloud from the<br>reference CAD model. At the part level, error analysis is performed on these<br>patches underlying the point clouds. We propose a novel optimization-based<br>primitive refinement method to obtain a set of meaningful patches of point<br>clouds. Two basic operations, splitting and merging, are introduced to refine<br>the coarse primitives. At the feature level, error analysis is performed on<br>circular holes, which are commonly found in CAD models. To facilitate it, a<br>two-stage algorithm is introduced for the detection of circular holes. First,<br>edge points are identified using a tensor-voting algorithm. Then, multiple<br>circles are fitted through a hypothesize-and-clusterize framework, ensuring<br>accurate detection and analysis of the circular features. Experimental results<br>on various aircraft CAD models demonstrate the effectiveness of our proposed<br>method.</p>
</details>


<h3 id="82-Semantic-decoupled-Spatial-Partition-Guided-Point-supervised-Oriented-Object-Detection"><a href="#82-Semantic-decoupled-Spatial-Partition-Guided-Point-supervised-Oriented-Object-Detection" class="headerlink" title="[82] Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10601">Semantic-decoupled Spatial Partition Guided Point-supervised Oriented Object Detection</a></h3><p><em>Xinyuan Liu,Hang Xu,Yike Ma,Yucheng Zhang,Feng Dai</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种名为SSP（语义解耦空间划分）的统一框架，用于解决点监督定向目标检测中样本分配和实例混淆的问题，显著提升了检测性能。</p>
<details>
  <summary>Details</summary>
Motivation: 随着遥感技术的进步，图像数量激增，但高密度场景中的定向目标检测因需要大量人工标注而受限。点监督方法虽然成本低，但现有方法因基于固定规则的设计导致样本分配不足和实例混淆。

<p>Contribution: 1) 提出SSP框架，结合规则驱动的先验注入和数据驱动的标签净化；2) 设计了基于像素级空间划分的样本分配和基于语义空间划分的框提取方法。</p>
<p>Method: 1) 通过像素级空间划分估计目标尺寸上下界并挖掘高质量样本；2) 利用语义空间划分生成伪标签以监督下游检测器学习。</p>
<p>Result: 在DOTA-v1.0等数据集上，SSP在点监督下达到45.78% mAP，优于SOTA方法PointOBB-v2 4.10%。与ORCNN和ReDet结合时，分别达到47.86%和48.50% mAP。</p>
<p>Insight: SSP通过结合规则和数据驱动的方法，有效解决了点监督中的样本分配问题，为高密度场景的定向目标检测提供了一种高效解决方案。</p>
<p>Abstract: Recent remote sensing tech advancements drive imagery growth, making oriented<br>object detection rapid development, yet hindered by labor-intensive annotation<br>for high-density scenes. Oriented object detection with point supervision<br>offers a cost-effective solution for densely packed scenes in remote sensing,<br>yet existing methods suffer from inadequate sample assignment and instance<br>confusion due to rigid rule-based designs. To address this, we propose SSP<br>(Semantic-decoupled Spatial Partition), a unified framework that synergizes<br>rule-driven prior injection and data-driven label purification. Specifically,<br>SSP introduces two core innovations: 1) Pixel-level Spatial Partition-based<br>Sample Assignment, which compactly estimates the upper and lower bounds of<br>object scales and mines high-quality positive samples and hard negative samples<br>through spatial partitioning of pixel maps. 2) Semantic Spatial Partition-based<br>Box Extraction, which derives instances from spatial partitions modulated by<br>semantic maps and reliably converts them into bounding boxes to form<br>pseudo-labels for supervising the learning of downstream detectors. Experiments<br>on DOTA-v1.0 and others demonstrate SSP&#39; s superiority: it achieves 45.78% mAP<br>under point supervision, outperforming SOTA method PointOBB-v2 by 4.10%.<br>Furthermore, when integrated with ORCNN and ReDet architectures, the SSP<br>framework achieves mAP values of 47.86% and 48.50%, respectively. The code is<br>available at <a target="_blank" rel="noopener" href="https://github.com/antxinyuan/ssp">https://github.com/antxinyuan/ssp</a>.</p>
</details>


<h3 id="83-High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model"><a href="#83-High-resolution-efficient-image-generation-from-WiFi-CSI-using-a-pretrained-latent-diffusion-model" class="headerlink" title="[83] High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10605">High-resolution efficient image generation from WiFi CSI using a pretrained latent diffusion model</a></h3><p><em>Eshan Ramesh,Nishio Takayuki</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: LatentCSI是一种利用预训练的潜在扩散模型（LDM）从WiFi CSI测量生成物理环境图像的新方法，通过直接映射CSI振幅到潜在空间，提高计算效率和图像质量。</p>
<details>
  <summary>Details</summary>
Motivation: 传统方法如GAN在WiFi CSI图像生成中存在计算复杂度高和图像质量不佳的问题。LatentCSI旨在通过利用预训练的LDM解决这些问题，实现高效高质量的图像生成。

<p>Contribution: 提出LatentCSI方法，首次将WiFi CSI数据直接映射到预训练LDM的潜在空间，实现了高效、高质量的图像生成，并具备文本引导的灵活性。</p>
<p>Method: 使用轻量级神经网络将CSI振幅映射到LDM的潜在空间，利用LDM的扩散模型和文本引导进行去噪，最后通过预训练解码器生成高分辨率图像。</p>
<p>Result: 在自采集的WiFi设备和摄像头数据集及MM-Fi数据集上验证，LatentCSI在计算效率和感知质量上均优于基线方法，并支持文本引导控制。</p>
<p>Insight: 通过绕过传统的像素空间生成和显式编码阶段，LatentCSI展示了潜在扩散模型在跨模态数据（如WiFi CSI到图像）任务中的潜力。</p>
<p>Abstract: We present LatentCSI, a novel method for generating images of the physical<br>environment from WiFi CSI measurements that leverages a pretrained latent<br>diffusion model (LDM). Unlike prior approaches that rely on complex and<br>computationally intensive techniques such as GANs, our method employs a<br>lightweight neural network to map CSI amplitudes directly into the latent space<br>of an LDM. We then apply the LDM’s denoising diffusion model to the latent<br>representation with text-based guidance before decoding using the LDM’s<br>pretrained decoder to obtain a high-resolution image. This design bypasses the<br>challenges of pixel-space image generation and avoids the explicit image<br>encoding stage typically required in conventional image-to-image pipelines,<br>enabling efficient and high-quality image synthesis. We validate our approach<br>on two datasets: a wide-band CSI dataset we collected with off-the-shelf WiFi<br>devices and cameras; and a subset of the publicly available MM-Fi dataset. The<br>results demonstrate that LatentCSI outperforms baselines of comparable<br>complexity trained directly on ground-truth images in both computational<br>efficiency and perceptual quality, while additionally providing practical<br>advantages through its unique capacity for text-guided controllability.</p>
</details>


<h3 id="84-MSTAR-Box-free-Multi-query-Scene-Text-Retrieval-with-Attention-Recycling"><a href="#84-MSTAR-Box-free-Multi-query-Scene-Text-Retrieval-with-Attention-Recycling" class="headerlink" title="[84] MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10609">MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling</a></h3><p><em>Liang Yin,Xudong Xie,Zhang Li,Xiang Bai,Yuliang Liu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: MSTAR提出了一种无需边界框标注的、支持多查询类型的场景文本检索方法，通过动态多粒度文本表示和风格感知指令统一自由文本查询，显著提升了性能并降低了标注成本。</p>
<details>
  <summary>Details</summary>
Motivation: 现有场景文本检索方法依赖昂贵的边界框标注，且难以统一多样化的查询类型。MSTAR旨在解决这些问题。

<p>Contribution: 1. 提出无需边界框标注的MSTAR方法。2. 引入多实例匹配模块增强视觉-语言对齐。3. 构建首个多查询场景文本检索基准MQTR。</p>
<p>Method: 1. 渐进式视觉嵌入动态捕捉多粒度文本表示。2. 风格感知指令统一自由文本查询。3. 多实例匹配模块优化对齐。</p>
<p>Result: 在7个公开数据集和MQTR基准上，MSTAR性能优于之前方法（如Total-Text上MAP提升6.4%），MQTR上平均提升8.5%。</p>
<p>Insight: 无需边界框标注的检索方法可行且高效；多查询统一化设计能更好满足多样化需求。</p>
<p>Abstract: Scene text retrieval has made significant progress with the assistance of<br>accurate text localization. However, existing approaches typically require<br>costly bounding box annotations for training. Besides, they mostly adopt a<br>customized retrieval strategy but struggle to unify various types of queries to<br>meet diverse retrieval needs. To address these issues, we introduce Muti-query<br>Scene Text retrieval with Attention Recycling (MSTAR), a box-free approach for<br>scene text retrieval. It incorporates progressive vision embedding to<br>dynamically capture the multi-grained representation of texts and harmonizes<br>free-style text queries with style-aware instructions. Additionally, a<br>multi-instance matching module is integrated to enhance vision-language<br>alignment. Furthermore, we build the Multi-Query Text Retrieval (MQTR) dataset,<br>the first benchmark designed to evaluate the multi-query scene text retrieval<br>capability of models, comprising four query types and 16k images. Extensive<br>experiments demonstrate the superiority of our method across seven public<br>datasets and the MQTR dataset. Notably, MSTAR marginally surpasses the previous<br>state-of-the-art model by 6.4% in MAP on Total-Text while eliminating box<br>annotation costs. Moreover, on the MQTR benchmark, MSTAR significantly<br>outperforms the previous models by an average of 8.5%. The code and datasets<br>are available at <a target="_blank" rel="noopener" href="https://github.com/yingift/MSTAR">https://github.com/yingift/MSTAR</a>.</p>
</details>


<h3 id="85-Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models"><a href="#85-Anatomy-Grounded-Weakly-Supervised-Prompt-Tuning-for-Chest-X-ray-Latent-Diffusion-Models" class="headerlink" title="[85] Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10633">Anatomy-Grounded Weakly Supervised Prompt Tuning for Chest X-ray Latent Diffusion Models</a></h3><p><em>Konstantinos Vilouras,Ilias Stogiannidis,Junyu Yan,Alison Q. O’Neil,Sotirios A. Tsaftaris</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种针对胸部X光片的潜在扩散模型微调框架，通过弱监督提示调优提升多模态对齐，解决了医学影像中文本与图像对齐不足的问题，并在标准数据集和分布外数据上表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 医学影像领域的数据隐私问题导致数据有限，使得潜在扩散模型在文本与图像对齐方面的性能不足，影响了其在医学影像多模态任务中的应用。

<p>Contribution: 提出了一种弱监督提示调优框架，显著提升了预训练模型在医学影像中的多模态对齐能力；在标准数据集上达到新SOTA，并展示了对分布外数据的鲁棒性。</p>
<p>Method: 通过解剖学信息引导的弱监督提示调优方法，对预训练的潜在扩散模型进行高效微调，实现文本与图像区域的更精准对齐。</p>
<p>Result: 在MS-CXR数据集上达到新SOTA，同时在VinDr-CXR等分布外数据上表现鲁棒。</p>
<p>Insight: 即使是数据受限的医学影像领域，通过弱监督学习也能有效提升多模态模型的性能，为医学影像分析任务提供了新思路。</p>
<p>Abstract: Latent Diffusion Models have shown remarkable results in text-guided image<br>synthesis in recent years. In the domain of natural (RGB) images, recent works<br>have shown that such models can be adapted to various vision-language<br>downstream tasks with little to no supervision involved. On the contrary,<br>text-to-image Latent Diffusion Models remain relatively underexplored in the<br>field of medical imaging, primarily due to limited data availability (e.g., due<br>to privacy concerns). In this work, focusing on the chest X-ray modality, we<br>first demonstrate that a standard text-conditioned Latent Diffusion Model has<br>not learned to align clinically relevant information in free-text radiology<br>reports with the corresponding areas of the given scan. Then, to alleviate this<br>issue, we propose a fine-tuning framework to improve multi-modal alignment in a<br>pre-trained model such that it can be efficiently repurposed for downstream<br>tasks such as phrase grounding. Our method sets a new state-of-the-art on a<br>standard benchmark dataset (MS-CXR), while also exhibiting robust performance<br>on out-of-distribution data (VinDr-CXR). Our code will be made publicly<br>available.</p>
</details>


<h3 id="86-Symmetrical-Flow-Matching-Unified-Image-Generation-Segmentation-and-Classification-with-Score-Based-Generative-Models"><a href="#86-Symmetrical-Flow-Matching-Unified-Image-Generation-Segmentation-and-Classification-with-Score-Based-Generative-Models" class="headerlink" title="[86] Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10634">Symmetrical Flow Matching: Unified Image Generation, Segmentation, and Classification with Score-Based Generative Models</a></h3><p><em>Francisco Caetano,Christiaan Viviers,Peter H. N. De With,Fons van der Sommen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了Symmetrical Flow Matching(SymmFlow)框架，通过对称学习目标统一了图像生成、语义分割和分类任务，实现了高性能的单模态多任务模型。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的Flow Matching框架在生成任务中表现优异，但在多任务统一方面存在局限性。研究人员希望通过对称学习目标实现图像生成、分割和分类的统一建模。

<p>Contribution: 提出了SymmFlow框架，通过双向一致性目标和语义保留机制，实现了生成、分割和分类任务的统一建模，并显著提高了性能。</p>
<p>Method: 采用对称学习目标联合建模前向和反向变换，同时引入新的训练目标保留语义信息，支持像素级和图像级条件生成。</p>
<p>Result: 在CelebAMask-HQ和COCO-Stuff等数据集上，生成任务FID分别达到11.9和7.0；同时在分割和分类任务中表现优异。</p>
<p>Insight: 通过对称性和语义保留机制，多任务统一建模不仅可行，还能提升模型在单一任务上的性能，为多模态生成学习提供了新思路。</p>
<p>Abstract: Flow Matching has emerged as a powerful framework for learning continuous<br>transformations between distributions, enabling high-fidelity generative<br>modeling. This work introduces Symmetrical Flow Matching (SymmFlow), a new<br>formulation that unifies semantic segmentation, classification, and image<br>generation within a single model. Using a symmetric learning objective,<br>SymmFlow models forward and reverse transformations jointly, ensuring<br>bi-directional consistency, while preserving sufficient entropy for generative<br>diversity. A new training objective is introduced to explicitly retain semantic<br>information across flows, featuring efficient sampling while preserving<br>semantic structure, allowing for one-step segmentation and classification<br>without iterative refinement. Unlike previous approaches that impose strict<br>one-to-one mapping between masks and images, SymmFlow generalizes to flexible<br>conditioning, supporting both pixel-level and image-level class labels.<br>Experimental results on various benchmarks demonstrate that SymmFlow achieves<br>state-of-the-art performance on semantic image synthesis, obtaining FID scores<br>of 11.9 on CelebAMask-HQ and 7.0 on COCO-Stuff with only 25 inference steps.<br>Additionally, it delivers competitive results on semantic segmentation and<br>shows promising capabilities in classification tasks. The code will be publicly<br>available.</p>
</details>


<h3 id="87-GigaVideo-1-Advancing-Video-Generation-via-Automatic-Feedback-with-4-GPU-Hours-Fine-Tuning"><a href="#87-GigaVideo-1-Advancing-Video-Generation-via-Automatic-Feedback-with-4-GPU-Hours-Fine-Tuning" class="headerlink" title="[87] GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10639">GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning</a></h3><p><em>Xiaoyi Bao,Jindi Lv,Xiaofeng Wang,Zheng Zhu,Xinze Chen,YuKun Zhou,Jiancheng Lv,Xingang Wang,Guan Huang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: GigaVideo-1提出了一种高效的视频生成微调框架，通过自动反馈而非大量人工标注和数据，仅用4 GPU小时显著提升了视频生成质量。</p>
<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型需通过微调提升特定维度（如实例保留、运动合理性），但传统方法依赖人工标注和高计算资源，实用性受限。

<p>Contribution: 1. 设计了一种无监督的高效微调框架；2. 提出基于提示的数据引擎和奖励引导优化策略；3. 在VBench-2.0上平均提升4%，仅需4 GPU小时。</p>
<p>Method: 1. 通过提示驱动数据引擎构建多样化训练样本；2. 使用预训练视觉语言模型的反馈自适应加权样本。</p>
<p>Result: 在17个评估维度上均表现提升，平均增益4%，计算资源需求极低。</p>
<p>Insight: 自动反馈机制可有效替代人工标注，低资源消耗的微调方法具有实际应用潜力。</p>
<p>Abstract: Recent progress in diffusion models has greatly enhanced video generation<br>quality, yet these models still require fine-tuning to improve specific<br>dimensions like instance preservation, motion rationality, composition, and<br>physical plausibility. Existing fine-tuning approaches often rely on human<br>annotations and large-scale computational resources, limiting their<br>practicality. In this work, we propose GigaVideo-1, an efficient fine-tuning<br>framework that advances video generation without additional human supervision.<br>Rather than injecting large volumes of high-quality data from external sources,<br>GigaVideo-1 unlocks the latent potential of pre-trained video diffusion models<br>through automatic feedback. Specifically, we focus on two key aspects of the<br>fine-tuning process: data and optimization. To improve fine-tuning data, we<br>design a prompt-driven data engine that constructs diverse, weakness-oriented<br>training samples. On the optimization side, we introduce a reward-guided<br>training strategy, which adaptively weights samples using feedback from<br>pre-trained vision-language models with a realism constraint. We evaluate<br>GigaVideo-1 on the VBench-2.0 benchmark using Wan2.1 as the baseline across 17<br>evaluation dimensions. Experiments show that GigaVideo-1 consistently improves<br>performance on almost all the dimensions with an average gain of about 4% using<br>only 4 GPU-hours. Requiring no manual annotations and minimal real data,<br>GigaVideo-1 demonstrates both effectiveness and efficiency. Code, model, and<br>data will be publicly available.</p>
</details>


<h3 id="88-PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis"><a href="#88-PiPViT-Patch-based-Visual-Interpretable-Prototypes-for-Retinal-Image-Analysis" class="headerlink" title="[88] PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10669">PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis</a></h3><p><em>Marzieh Oghbaie,Teresa Araújoa,Hrvoje Bogunović</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PiPViT提出了一种基于视觉Transformer的原型方法，通过对比学习和多分辨率输入处理学习可解释的原型，用于视网膜图像分析，既能实现高性能又能提供有意义的解释。</p>
<details>
  <summary>Details</summary>
Motivation: 现有原型方法在医学影像中的可视化通常与人类可理解的生物标志物不一致，且原型通常过细，难以解释生物标志物的范围和存在性。

<p>Contribution: 提出了PiPViT，一种基于ViT的可解释原型模型，利用对比学习和多分辨率处理学习鲁棒的、可解释的原型。</p>
<p>Method: 采用ViT捕获图像块间的长程依赖关系，通过对比学习和多分辨率输入处理学习原型。</p>
<p>Result: 在视网膜OCT图像分类任务中达到SOTA性能，原型具有临床相关性和语义意义。</p>
<p>Insight: PiPViT不仅能提供高性能分类，还能通过透明原型辅助临床诊断解释。</p>
<p>Abstract: Background and Objective: Prototype-based methods improve interpretability by<br>learning fine-grained part-prototypes; however, their visualization in the<br>input pixel space is not always consistent with human-understandable<br>biomarkers. In addition, well-known prototype-based approaches typically learn<br>extremely granular prototypes that are less interpretable in medical imaging,<br>where both the presence and extent of biomarkers and lesions are critical.<br>  Methods: To address these challenges, we propose PiPViT (Patch-based Visual<br>Interpretable Prototypes), an inherently interpretable prototypical model for<br>image recognition. Leveraging a vision transformer (ViT), PiPViT captures<br>long-range dependencies among patches to learn robust, human-interpretable<br>prototypes that approximate lesion extent only using image-level labels.<br>Additionally, PiPViT benefits from contrastive learning and multi-resolution<br>input processing, which enables effective localization of biomarkers across<br>scales.<br>  Results: We evaluated PiPViT on retinal OCT image classification across four<br>datasets, where it achieved competitive quantitative performance compared to<br>state-of-the-art methods while delivering more meaningful explanations.<br>Moreover, quantitative evaluation on a hold-out test set confirms that the<br>learned prototypes are semantically and clinically relevant. We believe PiPViT<br>can transparently explain its decisions and assist clinicians in understanding<br>diagnostic outcomes. Github page: <a target="_blank" rel="noopener" href="https://github.com/marziehoghbaie/PiPViT">https://github.com/marziehoghbaie/PiPViT</a></p>
</details>


<h3 id="89-Enhancing-Deepfake-Detection-using-SE-Block-Attention-with-CNN"><a href="#89-Enhancing-Deepfake-Detection-using-SE-Block-Attention-with-CNN" class="headerlink" title="[89] Enhancing Deepfake Detection using SE Block Attention with CNN"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10683">Enhancing Deepfake Detection using SE Block Attention with CNN</a></h3><p><em>Subhram Dasgupta,Janelle Mason,Xiaohong Yuan,Olusola Odeyomi,Kaushik Roy</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种轻量级的CNN结合SE注意力模块的Deepfake检测方法，显著降低了模型大小和计算资源消耗，同时在性能上达到了竞争性水平。</p>
<details>
  <summary>Details</summary>
Motivation: Deepfake技术的快速发展使得伪造内容越来越逼真，传统检测方法难以应对。现有深度检测模型通常体积庞大，存储和计算成本高，亟需高效轻量化的解决方案。

<p>Contribution: 提出了一种结合SE注意力模块的轻量级CNN模型，动态调整通道特征权重，提升检测效率，减少资源消耗。</p>
<p>Method: 采用带有SE注意力模块的CNN，通过动态特征重标定（channel-wise feature recalibration）强化信息丰富特征，抑制无用特征，模型结构简洁高效。</p>
<p>Result: 在Style GAN数据集上取得了94.14%的分类准确率和0.985的AUC-ROC分数，性能优于同类模型。</p>
<p>Insight: SE注意力模块可以有效提升轻量级模型的检测性能，为资源受限场景下的Deepfake检测提供了可行方案。</p>
<p>Abstract: In the digital age, Deepfake present a formidable challenge by using advanced<br>artificial intelligence to create highly convincing manipulated content,<br>undermining information authenticity and security. These sophisticated<br>fabrications surpass traditional detection methods in complexity and realism.<br>To address this issue, we aim to harness cutting-edge deep learning<br>methodologies to engineer an innovative deepfake detection model. However, most<br>of the models designed for deepfake detection are large, causing heavy storage<br>and memory consumption. In this research, we propose a lightweight convolution<br>neural network (CNN) with squeeze and excitation block attention (SE) for<br>Deepfake detection. The SE block module is designed to perform dynamic<br>channel-wise feature recalibration. The SE block allows the network to<br>emphasize informative features and suppress less useful ones, which leads to a<br>more efficient and effective learning module. This module is integrated with a<br>simple sequential model to perform Deepfake detection. The model is smaller in<br>size and it achieves competing accuracy with the existing models for deepfake<br>detection tasks. The model achieved an overall classification accuracy of<br>94.14% and AUC-ROC score of 0.985 on the Style GAN dataset from the Diverse<br>Fake Face Dataset. Our proposed approach presents a promising avenue for<br>combating the Deepfake challenge with minimal computational resources,<br>developing efficient and scalable solutions for digital content verification.</p>
</details>


<h3 id="90-Unsourced-Adversarial-CAPTCHA-A-Bi-Phase-Adversarial-CAPTCHA-Framework"><a href="#90-Unsourced-Adversarial-CAPTCHA-A-Bi-Phase-Adversarial-CAPTCHA-Framework" class="headerlink" title="[90] Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10685">Unsourced Adversarial CAPTCHA: A Bi-Phase Adversarial CAPTCHA Framework</a></h3><p><em>Xia Du,Xiaoyuan Liu,Jizhe Zhou,Zheng Lin,Chi-man Pun,Zhe Chen,Wei Ni,Jun Luo</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一个名为UAC的对抗性CAPTCHA框架，通过大语言模型（LLM）和扩散模型生成高质量对抗样本，支持目标和黑盒攻击，实验表明其攻击成功率高且生成的CAPTCHA对人类和DNN均难区分。</p>
<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA方案因深度学习进步而容易被自动化攻击破解，现有对抗攻击方法依赖原始图像特征，限制了在缺乏初始输入图像场景中的应用。

<p>Contribution: 1. 提出UAC框架，利用LLM生成高保真对抗样本；2. 提出EDICT方法优化扩散模型的双潜变量；3. 提出BP-UAC策略，支持黑盒攻击。</p>
<p>Method: 1. 使用LLM增强CAPTCHA多样性；2. 对目标攻击采用EDICT方法；3. 对黑盒攻击采用多模态梯度与双路径优化策略（BP-UAC）。</p>
<p>Result: BP-UAC在多样系统中实现高攻击成功率，生成自然且难区分的CAPTCHA。</p>
<p>Insight: 结合LLM和扩散模型可提升对抗样本的多样性和质量，多模态梯度与双路径优化对黑盒攻击有效。</p>
<p>Abstract: With the rapid advancements in deep learning, traditional CAPTCHA schemes are<br>increasingly vulnerable to automated attacks powered by deep neural networks<br>(DNNs). Existing adversarial attack methods often rely on original image<br>characteristics, resulting in distortions that hinder human interpretation and<br>limit applicability in scenarios lacking initial input images. To address these<br>challenges, we propose the Unsourced Adversarial CAPTCHA (UAC), a novel<br>framework generating high-fidelity adversarial examples guided by<br>attacker-specified text prompts. Leveraging a Large Language Model (LLM), UAC<br>enhances CAPTCHA diversity and supports both targeted and untargeted attacks.<br>For targeted attacks, the EDICT method optimizes dual latent variables in a<br>diffusion model for superior image quality. In untargeted attacks, especially<br>for black-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA<br>(BP-UAC), a two-step optimization strategy employing multimodal gradients and<br>bi-path optimization for efficient misclassification. Experiments show BP-UAC<br>achieves high attack success rates across diverse systems, generating natural<br>CAPTCHAs indistinguishable to humans and DNNs.</p>
</details>


<h3 id="91-Underage-Detection-through-a-Multi-Task-and-MultiAge-Approach-for-Screening-Minors-in-Unconstrained-Imagery"><a href="#91-Underage-Detection-through-a-Multi-Task-and-MultiAge-Approach-for-Screening-Minors-in-Unconstrained-Imagery" class="headerlink" title="[91] Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10689">Underage Detection through a Multi-Task and MultiAge Approach for Screening Minors in Unconstrained Imagery</a></h3><p><em>Christopher Gaul,Eduardo Fidalgo,Enrique Alegre,Rocío Alaiz Rodríguez,Eri Pérez Corral</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种多任务和多年龄方法，用于在无约束图像中检测未成年人，通过共享特征的架构、改进的损失函数和年龄平衡采样，显著提升了年龄估计和未成年人检测的准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 自动检测未成年人面临公开数据中儿童样本不足和分布偏移的问题，需要开发鲁棒的方法来解决这些挑战。

<p>Contribution: 1. 提出了一种多任务架构，结合年龄回归和多个二元阈值的未成年人检测任务；2. 引入了α-重新加权的焦点式损失和年龄平衡采样以解决类别不平衡；3. 创建了一个大型且严格的评估基准（ASORES-39k和ASWIFT-20k）。</p>
<p>Method: 1. 基于冻结的FaRL视觉语言主干网络，结合双层MLP共享特征；2. 使用α-重新加权焦点损失和年龄平衡的mini-batch采样；3. 引入年龄间隙（age gap）优化边缘案例。</p>
<p>Result: 在ASORES-39k上，年龄估计的RMSE从5.733降至5.656，18岁以下检测的F2得分从0.801提升至0.857；在ASWIFT-20k上，F2得分从0.742提升至0.833。</p>
<p>Insight: 多任务学习和年龄平衡采样对未成年人检测任务至关重要，尤其是在数据分布偏移的情况下，模型表现出强鲁棒性。</p>
<p>Abstract: Accurate automatic screening of minors in unconstrained images demands models<br>that are robust to distribution shift and resilient to the children<br>under-representation in publicly available data. To overcome these issues, we<br>propose a multi-task architecture with dedicated under&#x2F;over-age discrimination<br>tasks based on a frozen FaRL vision-language backbone joined with a compact<br>two-layer MLP that shares features across one age-regression head and four<br>binary under-age heads for age thresholds of 12, 15, 18, and 21 years, focusing<br>on the legally critical age range. To address the severe class imbalance, we<br>introduce an $\alpha$-reweighted focal-style loss and age-balanced mini-batch<br>sampling, which equalizes twelve age bins during stochastic optimization.<br>Further improvement is achieved with an age gap that removes edge cases from<br>the loss.<br>  Moreover, we set a rigorous evaluation by proposing the Overall Under-Age<br>Benchmark, with 303k cleaned training images and 110k test images, defining<br>both the “ASORES-39k” restricted overall test, which removes the noisiest<br>domains, and the age estimation wild shifts test “ASWIFT-20k” of 20k-images,<br>stressing extreme pose ($&gt;$45{\deg}), expression, and low image quality to<br>emulate real-world shifts.<br>  Trained on the cleaned overall set with resampling and age gap, our multiage<br>model “F” lowers the root-mean-square-error on the ASORES-39k restricted test<br>from 5.733 (age-only baseline) to 5.656 years and lifts under-18 detection from<br>F2 score of 0.801 to 0.857 at 1% false-adult rate. Under the domain shift to<br>the wild data of ASWIFT-20k, the same configuration nearly sustains 0.99 recall<br>while boosting F2 from 0.742 to 0.833 with respect to the age-only baseline,<br>demonstrating strong generalization under distribution shift. For the under-12<br>and under-15 tasks, the respective boosts in F2 are from 0.666 to 0.955 and<br>from 0.689 to 0.916, respectively.</p>
</details>


<h3 id="92-Continual-Hyperbolic-Learning-of-Instances-and-Classes"><a href="#92-Continual-Hyperbolic-Learning-of-Instances-and-Classes" class="headerlink" title="[92] Continual Hyperbolic Learning of Instances and Classes"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10710">Continual Hyperbolic Learning of Instances and Classes</a></h3><p><em>Melika Ayoughi,Mina Ghadimi Atigh,Mohammad Mahdi Derakhshani,Cees G. M. Snoek,Pascal Mettes,Paul Groth</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一个持续学习框架HyperCLIC，用于同时学习实例和类别，利用双曲空间建模层次结构，并验证了其在EgoObjects数据集上的有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 现实应用（如机器人和自动驾驶）需要模型同时处理实例和类别的持续学习，传统方法仅专注于单一任务，无法满足需求。

<p>Contribution: 1) 提出了同时学习实例和类别的持续学习任务；2) 提出HyperCLIC，利用双曲空间建模层次关系；3) 设计了持续层次化评估指标。</p>
<p>Method: 采用双曲空间的分类和蒸馏目标，通过低失真和紧凑嵌入表示树状结构，平衡细粒度实例识别和粗粒度类别泛化。</p>
<p>Result: 在EgoObjects数据集上验证，HyperCLIC在多种粒度下表现出色，提升了层次泛化能力。</p>
<p>Insight: 双曲空间适合建模层次关系，为持续学习中的多粒度任务提供了新思路。</p>
<p>Abstract: Continual learning has traditionally focused on classifying either instances<br>or classes, but real-world applications, such as robotics and self-driving<br>cars, require models to handle both simultaneously. To mirror real-life<br>scenarios, we introduce the task of continual learning of instances and<br>classes, at the same time. This task challenges models to adapt to multiple<br>levels of granularity over time, which requires balancing fine-grained instance<br>recognition with coarse-grained class generalization. In this paper, we<br>identify that classes and instances naturally form a hierarchical structure. To<br>model these hierarchical relationships, we propose HyperCLIC, a continual<br>learning algorithm that leverages hyperbolic space, which is uniquely suited<br>for hierarchical data due to its ability to represent tree-like structures with<br>low distortion and compact embeddings. Our framework incorporates hyperbolic<br>classification and distillation objectives, enabling the continual embedding of<br>hierarchical relations. To evaluate performance across multiple granularities,<br>we introduce continual hierarchical metrics. We validate our approach on<br>EgoObjects, the only dataset that captures the complexity of hierarchical<br>object recognition in dynamic real-world environments. Empirical results show<br>that HyperCLIC operates effectively at multiple granularities with improved<br>hierarchical generalization.</p>
</details>


<h3 id="93-Uncertainty-Masked-Bernoulli-Diffusion-for-Camouflaged-Object-Detection-Refinement"><a href="#93-Uncertainty-Masked-Bernoulli-Diffusion-for-Camouflaged-Object-Detection-Refinement" class="headerlink" title="[93] Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10712">Uncertainty-Masked Bernoulli Diffusion for Camouflaged Object Detection Refinement</a></h3><p><em>Yuqi Shen,Fengyang Xiao,Sujie Hu,Youwei Pang,Yifan Pu,Chengyu Fang,Xiu Li,Chunming He</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了第一个专门用于伪装目标检测（COD）的生成式优化框架——不确定性掩码伯努利扩散（UMBD），通过选择性优化低质量区域提升分割性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有COD方法在分割质量较差的区域存在优化空间，但缺乏针对性后处理框架。

<p>Contribution: 1. 提出UMBD框架，首次将生成式优化引入COD；2. 设计HUQNet网络，融合多源不确定性指导优化；3. 轻量级集成现有COD模型，显著提升性能。</p>
<p>Method: 1. UMBD通过伯努利扩散选择性优化低质量区域；2. HUQNet采用多分支架构量化不确定性；3. 扩散采样过程中自适应调整优化强度。</p>
<p>Result: 在多个COD基准测试中，平均MAE提升5.5%，加权F-measure提升3.2%，计算开销低。</p>
<p>Insight: 生成式优化与判别式模型的结合能有效解决COD的细微差异问题；不确定性引导的局部优化是关键。</p>
<p>Abstract: Camouflaged Object Detection (COD) presents inherent challenges due to the<br>subtle visual differences between targets and their backgrounds. While existing<br>methods have made notable progress, there remains significant potential for<br>post-processing refinement that has yet to be fully explored. To address this<br>limitation, we propose the Uncertainty-Masked Bernoulli Diffusion (UMBD) model,<br>the first generative refinement framework specifically designed for COD. UMBD<br>introduces an uncertainty-guided masking mechanism that selectively applies<br>Bernoulli diffusion to residual regions with poor segmentation quality,<br>enabling targeted refinement while preserving correctly segmented areas. To<br>support this process, we design the Hybrid Uncertainty Quantification Network<br>(HUQNet), which employs a multi-branch architecture and fuses uncertainty from<br>multiple sources to improve estimation accuracy. This enables adaptive guidance<br>during the generative sampling process. The proposed UMBD framework can be<br>seamlessly integrated with a wide range of existing Encoder-Decoder-based COD<br>models, combining their discriminative capabilities with the generative<br>advantages of diffusion-based refinement. Extensive experiments across multiple<br>COD benchmarks demonstrate consistent performance improvements, achieving<br>average gains of 5.5% in MAE and 3.2% in weighted F-measure with only modest<br>computational overhead. Code will be released.</p>
</details>


<h3 id="94-IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain"><a href="#94-IQE-CLIP-Instance-aware-Query-Embedding-for-Zero-Few-shot-Anomaly-Detection-in-Medical-Domain" class="headerlink" title="[94] IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly Detection in Medical Domain"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10730">IQE-CLIP: Instance-aware Query Embedding for Zero-&#x2F;Few-shot Anomaly Detection in Medical Domain</a></h3><p><em>Hong Huang,Weixiang Sun,Zhijian Wu,Jingwen Niu,Donghuan Lu,Xian Wu,Yefeng Zheng</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: IQE-CLIP 是一种基于 CLIP 的零&#x2F;小样本异常检测框架，通过结合文本和实例感知的视觉信息生成异常敏感嵌入，适用于医学领域。</p>
<details>
  <summary>Details</summary>
Motivation: 现有基于 CLIP 的方法依赖于预先设计的情境特定提示，无法在联合嵌入空间中有效区分正常和异常实例，且医学领域的探索有限。

<p>Contribution: 提出了 IQE-CLIP 框架，通过类基和可学习的提示标记，以及实例感知查询模块，生成异常敏感嵌入，填补医学领域 ZFSAD 的研究空白。</p>
<p>Method: 1. 引入类基和可学习提示标记；2. 设计实例感知查询模块，提取多模态区域级上下文信息。</p>
<p>Result: 在六个医学数据集上实现了零&#x2F;小样本设置的 SOTA 性能。</p>
<p>Insight: 结合文本和视觉实例信息能更有效指示异常，医学领域的 ZFSAD 任务需要更精细的嵌入生成方法。</p>
<p>Abstract: Recent advances in vision-language models, such as CLIP, have significantly<br>improved performance in zero- and few-shot anomaly detection (ZFSAD) tasks.<br>However, most existing CLIP-based methods assume prior knowledge of categories<br>and rely on carefully designed prompts tailored to specific scenarios. While<br>these text prompts capture semantic information in the textual space, they<br>often fail to distinguish normal and anomalous instances in the joint embedding<br>space. Moreover, most ZFSAD approaches focus on industrial domains, with<br>limited exploration in medical tasks. To address these limitations, we propose<br>IQE-CLIP, a novel framework for ZFSAD in the medical domain. We show that query<br>embeddings integrating both textual and instance-aware visual information serve<br>as more effective indicators of anomalies. Specifically, we introduce<br>class-based and learnable prompting tokens to better adapt CLIP to the medical<br>setting. Furthermore, we design an instance-aware query module that extracts<br>region-level contextual information from both modalities, enabling the<br>generation of anomaly-sensitive embeddings. Extensive experiments on six<br>medical datasets demonstrate that IQE-CLIP achieves state-of-the-art<br>performance in both zero-shot and few-shot settings. Code and data are<br>available at \href{<a target="_blank" rel="noopener" href="https://github.com/hongh0/IQE-CLIP/%7D%7Bthis">https://github.com/hongh0/IQE-CLIP/}{this</a> https URL}.</p>
</details>


<h3 id="95-PosterCraft-Rethinking-High-Quality-Aesthetic-Poster-Generation-in-a-Unified-Framework"><a href="#95-PosterCraft-Rethinking-High-Quality-Aesthetic-Poster-Generation-in-a-Unified-Framework" class="headerlink" title="[95] PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10741">PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework</a></h3><p><em>SiXiang Chen,Jianyu Lai,Jialin Gao,Tian Ye,Haoyu Chen,Hengyu Shi,Shitong Shao,Yunlong Lin,Song Fei,Zhaohu Xing,Yeying Jin,Junfeng Luo,Xiaoming Wei,Lei Zhu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PosterCraft是一个统一框架，用于生成高质量美学海报，通过多阶段优化工作流和自动化数据构建，显著提升渲染质量和视觉吸引力。</p>
<details>
  <summary>Details</summary>
Motivation: 生成美学海报比简单设计图像更具挑战性，需解决文本渲染、内容整合、布局和谐等问题。现有方法多为模块化流程，限制了生成自由度。

<p>Contribution: 1）提出统一框架PosterCraft，摆脱模块化约束；2）引入Text-Render-2M数据集和HQ-Poster100K；3）多阶段优化方法（文本渲染、区域感知微调、美学强化学习、反馈细化）。</p>
<p>Method: 1）大规模文本渲染优化；2）区域感知监督微调；3）基于偏好优化的美学文本强化学习；4）联合视觉语言反馈细化。</p>
<p>Result: 在渲染精度、布局一致性和视觉吸引力上大幅优于开源基线，接近SOTA商业系统质量。</p>
<p>Insight: 自动化数据构建和多阶段优化是提升生成质量的关键，统一框架更适合复杂美学任务。</p>
<p>Abstract: Generating aesthetic posters is more challenging than simple design images:<br>it requires not only precise text rendering but also the seamless integration<br>of abstract artistic content, striking layouts, and overall stylistic harmony.<br>To address this, we propose PosterCraft, a unified framework that abandons<br>prior modular pipelines and rigid, predefined layouts, allowing the model to<br>freely explore coherent, visually compelling compositions. PosterCraft employs<br>a carefully designed, cascaded workflow to optimize the generation of<br>high-aesthetic posters: (i) large-scale text-rendering optimization on our<br>newly introduced Text-Render-2M dataset; (ii) region-aware supervised<br>fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via<br>best-of-n preference optimization; and (iv) joint vision-language feedback<br>refinement. Each stage is supported by a fully automated data-construction<br>pipeline tailored to its specific needs, enabling robust training without<br>complex architectural modifications. Evaluated on multiple experiments,<br>PosterCraft significantly outperforms open-source baselines in rendering<br>accuracy, layout coherence, and overall visual appeal-approaching the quality<br>of SOTA commercial systems. Our code, models, and datasets can be found in the<br>Project page: <a target="_blank" rel="noopener" href="https://ephemeral182.github.io/PosterCraft">https://ephemeral182.github.io/PosterCraft</a></p>
</details>


<h3 id="96-SlotPi-Physics-informed-Object-centric-Reasoning-Models"><a href="#96-SlotPi-Physics-informed-Object-centric-Reasoning-Models" class="headerlink" title="[96] SlotPi: Physics-informed Object-centric Reasoning Models"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10778">SlotPi: Physics-informed Object-centric Reasoning Models</a></h3><p><em>Jian Li,Wan Han,Ning Lin,Yu-Liang Zhan,Ruizhi Chengze,Haining Wang,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Hao Sun</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: SlotPi是一种基于物理知识的对象中心推理模型，通过结合哈密顿原理和时空预测模块，解决了现有方法中物理知识整合不足和跨场景适应性的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 现有对象中心动态模拟方法忽略了物理知识的整合和模型在多样场景中的适应性验证，而人类能够通过观察世界获取物理知识并用于动态推理。

<p>Contribution: 提出了SlotPi模型，结合了哈密顿物理模块和时空预测模块，并在基准数据集和流体数据集上验证了其在预测和视觉问答任务中的优势。</p>
<p>Method: SlotPi通过整合基于哈密顿原理的物理模块和时空预测模块，实现了对象中心动态推理与流体流动特性的模拟。</p>
<p>Result: 实验表明，SlotPi在预测和VQA任务中表现优异，并在新创建的真实世界数据集上展现了强大的适应性。</p>
<p>Insight: 物理知识的整合不仅提高了模型的动态推理能力，还增强了其跨场景适应性，为构建更高级的世界模型奠定了基础。</p>
<p>Abstract: Understanding and reasoning about dynamics governed by physical laws through<br>visual observation, akin to human capabilities in the real world, poses<br>significant challenges. Currently, object-centric dynamic simulation methods,<br>which emulate human behavior, have achieved notable progress but overlook two<br>critical aspects: 1) the integration of physical knowledge into models. Humans<br>gain physical insights by observing the world and apply this knowledge to<br>accurately reason about various dynamic scenarios; 2) the validation of model<br>adaptability across diverse scenarios. Real-world dynamics, especially those<br>involving fluids and objects, demand models that not only capture object<br>interactions but also simulate fluid flow characteristics. To address these<br>gaps, we introduce SlotPi, a slot-based physics-informed object-centric<br>reasoning model. SlotPi integrates a physical module based on Hamiltonian<br>principles with a spatio-temporal prediction module for dynamic forecasting.<br>Our experiments highlight the model’s strengths in tasks such as prediction and<br>Visual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,<br>we have created a real-world dataset encompassing object interactions, fluid<br>dynamics, and fluid-object interactions, on which we validated our model’s<br>capabilities. The model’s robust performance across all datasets underscores<br>its strong adaptability, laying a foundation for developing more advanced world<br>models.</p>
</details>


<h3 id="97-Human-Robot-Navigation-using-Event-based-Cameras-and-Reinforcement-Learning"><a href="#97-Human-Robot-Navigation-using-Event-based-Cameras-and-Reinforcement-Learning" class="headerlink" title="[97] Human-Robot Navigation using Event-based Cameras and Reinforcement Learning"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10790">Human-Robot Navigation using Event-based Cameras and Reinforcement Learning</a></h3><p><em>Ignacio Bugueno-Cordova,Javier Ruiz-del-Solar,Rodrigo Verschae</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种结合事件相机和其他传感器以及强化学习的机器人导航控制器，用于实时的人类中心导航和避障。与传统基于图像的控制器相比，该方法利用事件相机的异步特性，实现了自适应推理和控制。</p>
<details>
  <summary>Details</summary>
Motivation: 传统基于图像的导航控制器存在固定帧率、运动模糊和延迟问题，而事件相机能够异步捕捉视觉信息，为解决这些问题提供了新思路。

<p>Contribution: 1. 提出了一个结合事件相机和强化学习的机器人导航框架；2. 通过模仿学习提升样本效率；3. 在仿真环境中展示了鲁棒的导航和避障能力。</p>
<p>Method: 1. 使用事件相机和其他传感器进行感知；2. 采用深度确定性策略梯度（DDPG）优化策略；3. 通过模仿学习初始化策略以提升效率。</p>
<p>Result: 在仿真环境中实现了鲁棒的导航、行人跟随和避障功能。</p>
<p>Insight: 事件相机的异步特性为机器人导航提供了新的感知方式，结合强化学习可以显著提升动态环境中的适应性。</p>
<p>Abstract: This work introduces a robot navigation controller that combines event<br>cameras and other sensors with reinforcement learning to enable real-time<br>human-centered navigation and obstacle avoidance. Unlike conventional<br>image-based controllers, which operate at fixed rates and suffer from motion<br>blur and latency, this approach leverages the asynchronous nature of event<br>cameras to process visual information over flexible time intervals, enabling<br>adaptive inference and control. The framework integrates event-based<br>perception, additional range sensing, and policy optimization via Deep<br>Deterministic Policy Gradient, with an initial imitation learning phase to<br>improve sample efficiency. Promising results are achieved in simulated<br>environments, demonstrating robust navigation, pedestrian following, and<br>obstacle avoidance. A demo video is available at the project website.</p>
</details>


<h3 id="98-Prompts-to-Summaries-Zero-Shot-Language-Guided-Video-Summarization"><a href="#98-Prompts-to-Summaries-Zero-Shot-Language-Guided-Video-Summarization" class="headerlink" title="[98] Prompts to Summaries: Zero-Shot Language-Guided Video Summarization"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10807">Prompts to Summaries: Zero-Shot Language-Guided Video Summarization</a></h3><p><em>Mario Barbara,Alaa Maalouf</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种零样本、可查询文本的视频摘要方法，利用视频-语言模型(VidLMs)和大型语言模型(LLMs)生成用户引导的视频摘要，无需训练数据，性能优于无监督方法，并媲美监督方法。</p>
<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸式增长需要灵活、用户可控的摘要工具，但现有方法要么依赖特定领域数据，无法泛化，要么无法结合用户通过自然语言表达的意图。

<p>Contribution: 1. 首次提出零样本、可查询文本的视频摘要方法；2. 开发了基于VidLMs和LLMs的流水线，无需训练数据；3. 在多个基准测试中超越无监督方法，媲美监督方法；4. 发布了新数据集VidSum-Reason。</p>
<p>Method: 1. 将视频分段为场景；2. 通过VidLM生成场景级描述；3. 使用LLM评估场景重要性；4. 通过一致性和唯一性指标传播分数到帧级别。</p>
<p>Result: 在SumMe和TVSum上超越无监督方法，QFVS基准测试中表现竞争力，且无需训练数据。VidSum-Reason数据集的提出为后续研究提供挑战性基线。</p>
<p>Insight: 预训练多模态模型结合精心设计的提示和分数传播方法，可成为通用、文本查询视频摘要的强大基础。</p>
<p>Abstract: The explosive growth of video data intensified the need for flexible<br>user-controllable summarization tools that can operate without domain-specific<br>training data. Existing methods either rely on datasets, limiting<br>generalization, or cannot incorporate user intent expressed in natural<br>language. We introduce Prompts-to-Summaries: the first zero-shot,<br>text-queryable video summarizer that converts off-the-shelf video-language<br>models (VidLMs) captions into user-guided skims via large language models<br>(LLMs) judging, without the use of training data at all, beating all<br>unsupervised and matching supervised methods. Our pipeline (i) segments raw<br>video footage into coherent scenes, (ii) generates rich scene-level<br>descriptions through a memory-efficient, batch-style VidLM prompting scheme<br>that scales to hours-long videos on a single GPU, (iii) leverages an LLM as a<br>judge to assign scene-level importance scores under a carefully crafted prompt,<br>and finally, (iv) propagates those scores to short segments level via two new<br>metrics: consistency (temporal coherency) and uniqueness (novelty), yielding<br>fine-grained frame importance. On SumMe and TVSum, our data-free approach<br>surpasses all prior data-hungry unsupervised methods. It also performs<br>competitively on the Query-Focused Video Summarization (QFVS) benchmark,<br>despite using no training data and the competing methods requiring supervised<br>frame-level importance. To spur further research, we release VidSum-Reason, a<br>new query-driven dataset featuring long-tailed concepts and multi-step<br>reasoning; our framework attains robust F1 scores and serves as the first<br>challenging baseline. Overall, our results demonstrate that pretrained<br>multimodal models, when orchestrated with principled prompting and score<br>propagation, already provide a powerful foundation for universal,<br>text-queryable video summarization.</p>
</details>


<h3 id="99-Unsupervised-Deformable-Image-Registration-with-Structural-Nonparametric-Smoothing"><a href="#99-Unsupervised-Deformable-Image-Registration-with-Structural-Nonparametric-Smoothing" class="headerlink" title="[99] Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10813">Unsupervised Deformable Image Registration with Structural Nonparametric Smoothing</a></h3><p><em>Hang Zhang,Xiang Chen,Renjiu Hu,Rongguang Wang,Jinwei Zhang,Min Liu,Yaonan Wang,Gaolei Li,Xinxing Cheng,Jinming Duan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为SmoothProper的无监督模块，通过结合非参数平滑优化层，有效解决了稀疏特征和大位移挑战下的变形图像配准问题。</p>
<details>
  <summary>Details</summary>
Motivation: 稀疏特征和大位移是传统无监督变形图像配准（DIR）方法的难点，神经网络的单次前向预测导致变形场缺乏约束，难以处理这些问题。

<p>Contribution: 提出了SmoothProper模块，通过优化层实现变形场的平滑性和结构一致性，无需额外参数调优，显著提升了配准精度。</p>
<p>Method: 在神经网络的前向传播中引入基于对偶优化的非参数平滑层，实现空间信号传播和变形场平滑。</p>
<p>Result: 在视网膜血管数据集上，配准误差降至1.88像素（2912x2912图像），首次有效解决了稀疏特征和大位移的双重挑战。</p>
<p>Insight: 结合优化层和神经网络可以弥补无监督DIR在结构一致性上的不足，为复杂场景配准提供了新思路。</p>
<p>Abstract: Learning-based deformable image registration (DIR) accelerates alignment by<br>amortizing traditional optimization via neural networks. Label supervision<br>further enhances accuracy, enabling efficient and precise nonlinear alignment<br>of unseen scans. However, images with sparse features amid large smooth<br>regions, such as retinal vessels, introduce aperture and large-displacement<br>challenges that unsupervised DIR methods struggle to address. This limitation<br>occurs because neural networks predict deformation fields in a single forward<br>pass, leaving fields unconstrained post-training and shifting the<br>regularization burden entirely to network weights. To address these issues, we<br>introduce SmoothProper, a plug-and-play neural module enforcing smoothness and<br>promoting message passing within the network’s forward pass. By integrating a<br>duality-based optimization layer with tailored interaction terms, SmoothProper<br>efficiently propagates flow signals across spatial locations, enforces<br>smoothness, and preserves structural consistency. It is model-agnostic,<br>seamlessly integrates into existing registration frameworks with minimal<br>parameter overhead, and eliminates regularizer hyperparameter tuning.<br>Preliminary results on a retinal vessel dataset exhibiting aperture and<br>large-displacement challenges demonstrate our method reduces registration error<br>to 1.88 pixels on 2912x2912 images, marking the first unsupervised DIR approach<br>to effectively address both challenges. The source code will be available at<br><a target="_blank" rel="noopener" href="https://github.com/tinymilky/SmoothProper">https://github.com/tinymilky/SmoothProper</a>.</p>
</details>


<h3 id="100-Occlusion-Aware-3D-Hand-Object-Pose-Estimation-with-Masked-AutoEncoders"><a href="#100-Occlusion-Aware-3D-Hand-Object-Pose-Estimation-with-Masked-AutoEncoders" class="headerlink" title="[100] Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10816">Occlusion-Aware 3D Hand-Object Pose Estimation with Masked AutoEncoders</a></h3><p><em>Hui Yang,Wei Sun,Jian Liu,Jin Zheng,Jian Xiao,Ajmal Mian</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种基于掩码自编码器的遮挡感知3D手-物体姿态估计方法（HOMAE），通过目标聚焦掩码策略和多尺度特征融合，解决了手-物体交互中的遮挡问题，并在DexYCB和HO3Dv2基准测试中取得了最先进的结果。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的手-物体姿态估计方法在遮挡情况下表现不佳，缺乏对全局结构的感知和推理能力。作者希望通过引入掩码自编码器和多尺度特征融合，提升模型在遮挡场景下的性能。

<p>Contribution: 1) 提出目标聚焦掩码策略，引导模型学习上下文感知特征；2) 结合多尺度特征预测SDF（有符号距离场）和点云，增强几何感知能力；3) 在DexYCB和HO3Dv2数据集上实现了最先进的性能。</p>
<p>Method: HOMAE方法基于掩码自编码器，采用目标聚焦掩码策略生成结构化遮挡，并通过解码器提取多尺度特征预测SDF。结合SDF的全局上下文与点云的局部几何信息，提升遮挡区域的鲁棒性。</p>
<p>Result: 在DexYCB和HO3Dv2基准测试中，HOMAE达到了最先进的性能，证明了其遮挡感知和几何融合的有效性。</p>
<p>Insight: 通过结合隐式（SDF）和显式（点云）表示的优势，可以更好地处理遮挡问题，同时多尺度特征提取和全局推理是提升姿态估计性能的关键。</p>
<p>Abstract: Hand-object pose estimation from monocular RGB images remains a significant<br>challenge mainly due to the severe occlusions inherent in hand-object<br>interactions. Existing methods do not sufficiently explore global structural<br>perception and reasoning, which limits their effectiveness in handling occluded<br>hand-object interactions. To address this challenge, we propose an<br>occlusion-aware hand-object pose estimation method based on masked<br>autoencoders, termed as HOMAE. Specifically, we propose a target-focused<br>masking strategy that imposes structured occlusion on regions of hand-object<br>interaction, encouraging the model to learn context-aware features and reason<br>about the occluded structures. We further integrate multi-scale features<br>extracted from the decoder to predict a signed distance field (SDF), capturing<br>both global context and fine-grained geometry. To enhance geometric perception,<br>we combine the implicit SDF with an explicit point cloud derived from the SDF,<br>leveraging the complementary strengths of both representations. This fusion<br>enables more robust handling of occluded regions by combining the global<br>context from the SDF with the precise local geometry provided by the point<br>cloud. Extensive experiments on challenging DexYCB and HO3Dv2 benchmarks<br>demonstrate that HOMAE achieves state-of-the-art performance in hand-object<br>pose estimation. We will release our code and model.</p>
</details>


<h3 id="101-VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using"><a href="#101-VideoDeepResearch-Long-Video-Understanding-With-Agentic-Tool-Using" class="headerlink" title="[101] VideoDeepResearch: Long Video Understanding With Agentic Tool Using"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10821">VideoDeepResearch: Long Video Understanding With Agentic Tool Using</a></h3><p><em>Huaying Yuan,Zheng Liu,Junjie Zhou,Ji-Rong Wen,Zhicheng Dou</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了VideoDeepResearch，一种用于长视频理解（LVU）的新型智能代理框架，仅依赖文本推理模型和多模态工具包，显著优于现有MLLM基线。</p>
<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在处理长视频理解（LVU）任务时面临复杂性及上下文窗口限制的挑战，论文旨在通过智能代理系统克服这些限制。

<p>Contribution: 1. 提出了VideoDeepResearch框架，无需扩展上下文窗口的多模态大模型；2. 结合文本推理模型和多模态工具包，实现高效视频内容筛选与利用。</p>
<p>Method: 使用文本推理模型（LRM）和多模态工具包（如检索器和视觉感知器），通过智能代理策略动态选择并处理关键视频内容。</p>
<p>Result: 在MLVU、Video-MME和LVBench等基准测试中，VideoDeepResearch显著超越现有MLLM基线，最高提升9.6%。</p>
<p>Insight: 智能代理系统通过动态工具调用和多模态协作，能够有效解决长视频理解的复杂性和上下文限制问题。</p>
<p>Abstract: Long video understanding (LVU) presents a significant challenge for current<br>multi-modal large language models (MLLMs) due to the task’s inherent complexity<br>and context window constraint. It is widely assumed that addressing LVU tasks<br>requires foundation MLLMs with extended context windows, strong visual<br>perception capabilities, and proficient domain expertise. In this work, we<br>challenge this common belief by introducing VideoDeepResearch, a novel agentic<br>framework for long video understanding. Our approach relies solely on a<br>text-only large reasoning model (LRM) combined with a modular multi-modal<br>toolkit, including multimodal retrievers and visual perceivers, all of which<br>are readily available in practice. For each LVU task, the system formulates a<br>problem-solving strategy through reasoning, while selectively accessing and<br>utilizing essential video content via tool using. We conduct extensive<br>experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.<br>Our results demonstrate that VideoDeepResearch achieves substantial<br>improvements over existing MLLM baselines, surpassing the previous<br>state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and<br>LongVideoBench, respectively. These findings highlight the promise of agentic<br>systems in overcoming key challenges in LVU problems.</p>
</details>


<h3 id="102-Post-Training-Quantization-for-Video-Matting"><a href="#102-Post-Training-Quantization-for-Video-Matting" class="headerlink" title="[102] Post-Training Quantization for Video Matting"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10840">Post-Training Quantization for Video Matting</a></h3><p><em>Tianrui Zhu,Houyuan Chen,Ruihao Gong,Michele Magno,Haotong Qin,Kai Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本篇论文提出了一种专为视频抠图设计的后训练量化（PTQ）框架PTQ4VM，通过两阶段策略和改进的全局校准方法（GAC）以及光流辅助（OFA）组件，显著提升了量化后模型的精度和时序一致性，同时大幅降低了计算开销，达到了4位量化下接近全精度的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 视频抠图任务在计算资源受限的设备上面临部署困难，现有PTQ方法在精度和时序一致性上的不足限制了在这一领域的应用。

<p>Contribution: 1. 提出两阶段PTQ策略；2. 提出基于统计的全局仿射校准方法（GAC）；3. 引入光流辅助组件（OFA）。</p>
<p>Method: 结合块重建优化与全局校准的PTQ策略，利用光流信息指导量化过程。</p>
<p>Result: PTQ4VM在多种位宽下均达到最优性能，4位量化模型接近全精度性能，计算开销降低8倍。</p>
<p>Insight: 通过捕捉局部依赖性和全局统计特性，并结合时序信息，可以显著提升视频抠图模型的量化效果。</p>
<p>Abstract: Video matting is crucial for applications such as film production and virtual<br>reality, yet deploying its computationally intensive models on<br>resource-constrained devices presents challenges. Quantization is a key<br>technique for model compression and acceleration. As an efficient approach,<br>Post-Training Quantization (PTQ) is still in its nascent stages for video<br>matting, facing significant hurdles in maintaining accuracy and temporal<br>coherence. To address these challenges, this paper proposes a novel and general<br>PTQ framework specifically designed for video matting models, marking, to the<br>best of our knowledge, the first systematic attempt in this domain. Our<br>contributions include: (1) A two-stage PTQ strategy that combines<br>block-reconstruction-based optimization for fast, stable initial quantization<br>and local dependency capture, followed by a global calibration of quantization<br>parameters to minimize accuracy loss. (2) A Statistically-Driven Global Affine<br>Calibration (GAC) method that enables the network to compensate for cumulative<br>statistical distortions arising from factors such as neglected BN layer<br>effects, even reducing the error of existing PTQ methods on video matting tasks<br>up to 20%. (3) An Optical Flow Assistance (OFA) component that leverages<br>temporal and semantic priors from frames to guide the PTQ process, enhancing<br>the model’s ability to distinguish moving foregrounds in complex scenes and<br>ultimately achieving near full-precision performance even under ultra-low-bit<br>quantization. Comprehensive quantitative and visual results show that our<br>PTQ4VM achieves the state-of-the-art accuracy performance across different<br>bit-widths compared to the existing quantization methods. We highlight that the<br>4-bit PTQ4VM even achieves performance close to the full-precision counterpart<br>while enjoying 8x FLOP savings.</p>
</details>


<h3 id="103-VRBench-A-Benchmark-for-Multi-Step-Reasoning-in-Long-Narrative-Videos"><a href="#103-VRBench-A-Benchmark-for-Multi-Step-Reasoning-in-Long-Narrative-Videos" class="headerlink" title="[103] VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10857">VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos</a></h3><p><em>Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: VRBench是首个针对长叙事视频多步推理能力的基准测试，包含1,010个长视频和9,468个人工标注的多步问题-答案对，为评估大型模型的时序推理和过程有效性提供标准化工具。</p>
<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视了时序推理和过程的合理性，缺乏对长视频多步推理的系统性测试，VRBench填补了这一空白。

<p>Contribution: 1. 提出首个长叙事视频多步推理基准VRBench；2. 提出人-AI协作框架生成时序逻辑链；3. 设计多阶段评估流程，包括结果和推理链质量的量化评估。</p>
<p>Method: 1. 通过多阶段筛选和专家评审确保视频内容连贯性；2. 开发人-AI协作框架生成7类多步推理问题；3. 使用多阶段评估流程（包括最终结果和LLM引导的推理链评分）。</p>
<p>Result: 对12个LLM和16个VLM的广泛评测显示，VRBench能有效区分模型在多步推理任务中的性能差异。</p>
<p>Insight: 长视频的多步推理需要更复杂的时序建模能力，模型的推理链质量需从多维度综合评估。</p>
<p>Abstract: We present VRBench, the first long narrative video benchmark crafted for<br>evaluating large models’ multi-step reasoning capabilities, addressing<br>limitations in existing evaluations that overlook temporal reasoning and<br>procedural validity. It comprises 1,010 long videos (with an average duration<br>of 1.6 hours), along with 9,468 human-labeled multi-step question-answering<br>pairs and 30,292 reasoning steps with timestamps. These videos are curated via<br>a multi-stage filtering process including expert inter-rater reviewing to<br>prioritize plot coherence. We develop a human-AI collaborative framework that<br>generates coherent reasoning chains, each requiring multiple temporally<br>grounded steps, spanning seven types (e.g., event attribution, implicit<br>inference). VRBench designs a multi-phase evaluation pipeline that assesses<br>models at both the outcome and process levels. Apart from the MCQs for the<br>final results, we propose a progress-level LLM-guided scoring metric to<br>evaluate the quality of the reasoning chain from multiple dimensions<br>comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on<br>VRBench, we undertake a thorough analysis and provide valuable insights that<br>advance the field of multi-step reasoning.</p>
</details>


<h3 id="104-CreatiPoster-Towards-Editable-and-Controllable-Multi-Layer-Graphic-Design-Generation"><a href="#104-CreatiPoster-Towards-Editable-and-Controllable-Multi-Layer-Graphic-Design-Generation" class="headerlink" title="[104] CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10890">CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic Design Generation</a></h3><p><em>Zhao Zhang,Yutao Cheng,Dexiang Hong,Maoke Yang,Gonglei Shi,Lei Ma,Hui Zhang,Jie Shao,Xinglong Wu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: CreatiPoster是一个可编辑、可控的多层次图形设计生成框架，通过结合自然语言指令或用户提供的资产，生成高质量的JSON规范和多层次设计，超越现有开源和商业工具。</p>
<details>
  <summary>Details</summary>
Motivation: 当前AI工具难以准确整合用户提供的资产并保持可编辑性和视觉吸引力，商业系统依赖模板库，限制了创造性和实用性。

<p>Contribution: 提出了CreatiPoster框架，结合协议模型和条件背景模型，生成可编辑的多层次设计，并发布了10万份无版权多图层设计数据集。</p>
<p>Method: 使用协议模型生成JSON规范，描述各层布局、内容和样式；条件背景模型基于前景层生成一致背景。</p>
<p>Result: CreatiPoster超越现有开源方法和商业系统，支持多种应用（如画布编辑、多语言适配）。</p>
<p>Insight: 将多层次设计的生成与背景合成分离，提高了设计的可编辑性和视觉一致性，推动了AI辅助图形设计的民主化。</p>
<p>Abstract: Graphic design plays a crucial role in both commercial and personal contexts,<br>yet creating high-quality, editable, and aesthetically pleasing graphic<br>compositions remains a time-consuming and skill-intensive task, especially for<br>beginners. Current AI tools automate parts of the workflow, but struggle to<br>accurately incorporate user-supplied assets, maintain editability, and achieve<br>professional visual appeal. Commercial systems, like Canva Magic Design, rely<br>on vast template libraries, which are impractical for replicate. In this paper,<br>we introduce CreatiPoster, a framework that generates editable, multi-layer<br>compositions from optional natural-language instructions or assets. A protocol<br>model, an RGBA large multimodal model, first produces a JSON specification<br>detailing every layer (text or asset) with precise layout, hierarchy, content<br>and style, plus a concise background prompt. A conditional background model<br>then synthesizes a coherent background conditioned on this rendered foreground<br>layers. We construct a benchmark with automated metrics for graphic-design<br>generation and show that CreatiPoster surpasses leading open-source approaches<br>and proprietary commercial systems. To catalyze further research, we release a<br>copyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports<br>diverse applications such as canvas editing, text overlay, responsive resizing,<br>multilingual adaptation, and animated posters, advancing the democratization of<br>AI-assisted graphic design. Project homepage:<br><a target="_blank" rel="noopener" href="https://github.com/graphic-design-ai/creatiposter">https://github.com/graphic-design-ai/creatiposter</a></p>
</details>


<h3 id="105-AIR-Zero-shot-Generative-Model-Adaptation-with-Iterative-Refinement"><a href="#105-AIR-Zero-shot-Generative-Model-Adaptation-with-Iterative-Refinement" class="headerlink" title="[105] AIR: Zero-shot Generative Model Adaptation with Iterative Refinement"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10895">AIR: Zero-shot Generative Model Adaptation with Iterative Refinement</a></h3><p><em>Guimeng Liu,Milad Abdollahzadeh,Ngai-Man Cheung</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种零样本生成模型自适应方法AIR，通过迭代优化解决现有方法中图像与文本偏移不对齐的问题，实验证明其在26种实验设置中均达到最优性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有零样本生成模型自适应方法假设图像偏移与文本偏移在CLIP嵌入空间中完全对齐，导致生成图像质量下降。本文通过实证研究发现偏移不对齐与概念距离相关，并提出了改进方法。

<p>Contribution: 1. 通过实证分析发现CLIP嵌入空间中文本偏移与图像偏移的不对齐现象与概念距离相关。2. 提出AIR方法，首次基于偏移不对齐的新见解提升目标域图像质量。</p>
<p>Method: 提出Adaptation with Iterative Refinement (AIR)，通过迭代优化目标域图像生成过程，解决偏移不对齐问题。</p>
<p>Result: 在26种实验设置中，AIR在定性和定量评估及用户研究中均达到最优性能。</p>
<p>Insight: CLIP嵌入空间中偏移不对齐与概念距离相关，接近的概念偏移不对齐较小，这一发现可指导生成模型优化。</p>
<p>Abstract: Zero-shot generative model adaptation (ZSGM) aims to adapt a pre-trained<br>generator to a target domain using only text guidance and without any samples<br>from the target domain. Central to recent ZSGM approaches are directional loss<br>which use the text guidance in the form of aligning the image offset with text<br>offset in the embedding space of a vision-language model like CLIP. This is<br>similar to the analogical reasoning in NLP where the offset between one pair of<br>words is used to identify a missing element in another pair by aligning the<br>offset between these two pairs. However, a major limitation of existing ZSGM<br>methods is that the learning objective assumes the complete alignment between<br>image offset and text offset in the CLIP embedding space, resulting in quality<br>degrade in generated images. Our work makes two main contributions. Inspired by<br>the offset misalignment studies in NLP, as our first contribution, we perform<br>an empirical study to analyze the misalignment between text offset and image<br>offset in CLIP embedding space for various large publicly available datasets.<br>Our important finding is that offset misalignment in CLIP embedding space is<br>correlated with concept distance, i.e., close concepts have a less offset<br>misalignment. To address the limitations of the current approaches, as our<br>second contribution, we propose Adaptation with Iterative Refinement (AIR)<br>which is the first ZSGM approach to focus on improving target domain image<br>quality based on our new insight on offset misalignment.Qualitative,<br>quantitative, and user study in 26 experiment setups consistently demonstrate<br>the proposed AIR approach achieves SOTA performance. Additional experiments are<br>in Supp.</p>
</details>


<h3 id="106-M4V-Multi-Modal-Mamba-for-Text-to-Video-Generation"><a href="#106-M4V-Multi-Modal-Mamba-for-Text-to-Video-Generation" class="headerlink" title="[106] M4V: Multi-Modal Mamba for Text-to-Video Generation"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10915">M4V: Multi-Modal Mamba for Text-to-Video Generation</a></h3><p><em>Jiancheng Huang,Gengwei Zhang,Zequn Jie,Siyu Jiao,Yinlong Qian,Ling Chen,Yunchao Wei,Lin Ma</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: M4V是一个基于Mamba架构的多模态文本到视频生成框架，通过多模态扩散Mamba块和奖励学习策略，显著降低了计算成本并提升了生成视频的质量。</p>
<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的视频生成方法由于计算复杂度高（平方级），限制了实际应用。Mamba架构虽高效，但其设计难以直接适用于多模态和时空建模任务。M4V旨在解决这些问题。

<p>Contribution: 1. 提出多模态扩散Mamba块（MM-DiM），实现多模态信息的无缝集成和时空建模。2. 在768×1280分辨率下，计算量减少45%。3. 引入奖励学习策略，提升长上下文生成中的视觉质量。</p>
<p>Method: 1. 设计MM-DiM块支持多模态token重组。2. 使用Mamba块替代Transformer，降低计算复杂度。3. 通过奖励学习优化单帧视觉真实感。</p>
<p>Result: 在文本到视频基准测试中，M4V能够生成高质量视频，同时显著降低计算成本。</p>
<p>Insight: 1. Mamba架构在多模态任务中经过适配后表现出色。2. 奖励学习是提升长序列生成质量的有效方法。</p>
<p>Abstract: Text-to-video generation has significantly enriched content creation and<br>holds the potential to evolve into powerful world simulators. However, modeling<br>the vast spatiotemporal space remains computationally demanding, particularly<br>when employing Transformers, which incur quadratic complexity in sequence<br>processing and thus limit practical applications. Recent advancements in<br>linear-time sequence modeling, particularly the Mamba architecture, offer a<br>more efficient alternative. Nevertheless, its plain design limits its direct<br>applicability to multi-modal and spatiotemporal video generation tasks. To<br>address these challenges, we introduce M4V, a Multi-Modal Mamba framework for<br>text-to-video generation. Specifically, we propose a multi-modal diffusion<br>Mamba (MM-DiM) block that enables seamless integration of multi-modal<br>information and spatiotemporal modeling through a multi-modal token<br>re-composition design. As a result, the Mamba blocks in M4V reduce FLOPs by 45%<br>compared to the attention-based alternative when generating videos at<br>768$\times$1280 resolution. Additionally, to mitigate the visual quality<br>degradation in long-context autoregressive generation processes, we introduce a<br>reward learning strategy that further enhances per-frame visual realism.<br>Extensive experiments on text-to-video benchmarks demonstrate M4V’s ability to<br>produce high-quality videos while significantly lowering computational costs.<br>Code and models will be publicly available at<br><a target="_blank" rel="noopener" href="https://huangjch526.github.io/M4V_project">https://huangjch526.github.io/M4V_project</a>.</p>
</details>


<h3 id="107-VINCIE-Unlocking-In-context-Image-Editing-from-Video"><a href="#107-VINCIE-Unlocking-In-context-Image-Editing-from-Video" class="headerlink" title="[107] VINCIE: Unlocking In-context Image Editing from Video"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10941">VINCIE: Unlocking In-context Image Editing from Video</a></h3><p><em>Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: VINCIE提出了一种直接从视频中学习上下文图像编辑的方法，通过多模态序列标注和块因果扩散变换器，实现了多任务学习，并在多项任务中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定的流水线和专家模型，而VINCIE探索是否可以直接从视频中学习上下文图像编辑，以简化流程并提升灵活性。

<p>Contribution: 提出了从视频中标注多模态序列的方法，设计了块因果扩散变换器进行多任务学习，并引入了新的多轮图像编辑基准。</p>
<p>Method: 使用块因果扩散变换器学习视频中的多模态序列，通过三个代理任务（下一图像预测、当前分割预测和下一分割预测）进行训练。</p>
<p>Result: 在多项任务中表现优异，包括多概念组合、故事生成和编辑链应用，并在多轮图像编辑基准中达到SOTA。</p>
<p>Insight: 从视频中学习上下文图像编辑是可行的，且多任务学习可以有效提升模型的泛化能力和表现。</p>
<p>Abstract: In-context image editing aims to modify images based on a contextual sequence<br>comprising text and previously generated images. Existing methods typically<br>depend on task-specific pipelines and expert models (e.g., segmentation and<br>inpainting) to curate training data. In this work, we explore whether an<br>in-context image editing model can be learned directly from videos. We<br>introduce a scalable approach to annotate videos as interleaved multimodal<br>sequences. To effectively learn from this data, we design a block-causal<br>diffusion transformer trained on three proxy tasks: next-image prediction,<br>current segmentation prediction, and next-segmentation prediction.<br>Additionally, we propose a novel multi-turn image editing benchmark to advance<br>research in this area. Extensive experiments demonstrate that our model<br>exhibits strong in-context image editing capabilities and achieves<br>state-of-the-art results on two multi-turn image editing benchmarks. Despite<br>being trained exclusively on videos, our model also shows promising abilities<br>in multi-concept composition, story generation, and chain-of-editing<br>applications.</p>
</details>


<h3 id="108-MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning"><a href="#108-MMMG-A-Massive-Multidisciplinary-Multi-Tier-Generation-Benchmark-for-Text-to-Image-Reasoning" class="headerlink" title="[108] MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10963">MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning</a></h3><p><em>Yuxuan Luo,Yuhui Yuan,Junwen Chen,Haonan Cai,Ziyi Yue,Yuwei Yang,Fatima Zohra Daha,Ji Li,Zhouhui Lian</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了知识图像生成这一新任务，并发布了MMMG基准，用于评估图像生成模型的推理能力。MMMG包含4,456对专家验证的知识图像-文本对，涵盖多学科、多教育层次和多种知识形式。通过统一的KG表示和MMMG-Score评估方法，揭示了当前模型的推理不足，并提供了一个开源基线FLUX-Reason。</p>
<details>
  <summary>Details</summary>
Motivation: 知识图像在人类文明和学习中扮演重要角色，但生成此类图像需要多模态推理能力，目前缺乏专门的任务和基准来评估模型的这一能力。

<p>Contribution: 1. 提出知识图像生成任务；2. 发布MMMG基准，包含多样化的知识图像-文本对；3. 设计MMMG-Score评估指标，结合事实保真度和视觉清晰度；4. 提供开源基线FLUX-Reason。</p>
<p>Method: 1. 统一使用知识图（KG）表示图像的核心实体和依赖关系；2. 通过图编辑距离和视觉清晰度评估生成图像的质量；3. 结合推理LLM和扩散模型，提出FLUX-Reason方法。</p>
<p>Result: 评估了16个SOTA文本到图像生成模型，发现普遍存在推理缺陷。GPT-4o的MMMG-Score仅为50.20，提供的基线FLUX-Reason得分为34.45。</p>
<p>Insight: 当前模型在多模态推理能力上仍有显著不足，未来的工作需要更深入地结合事实知识和视觉生成能力。</p>
<p>Abstract: In this paper, we introduce knowledge image generation as a new task,<br>alongside the Massive Multi-Discipline Multi-Tier Knowledge-Image Generation<br>Benchmark (MMMG) to probe the reasoning capability of image generation models.<br>Knowledge images have been central to human civilization and to the mechanisms<br>of human learning–a fact underscored by dual-coding theory and the<br>picture-superiority effect. Generating such images is challenging, demanding<br>multimodal reasoning that fuses world knowledge with pixel-level grounding into<br>clear explanatory visuals. To enable comprehensive evaluation, MMMG offers<br>4,456 expert-validated (knowledge) image-prompt pairs spanning 10 disciplines,<br>6 educational levels, and diverse knowledge formats such as charts, diagrams,<br>and mind maps. To eliminate confounding complexity during evaluation, we adopt<br>a unified Knowledge Graph (KG) representation. Each KG explicitly delineates a<br>target image’s core entities and their dependencies. We further introduce<br>MMMG-Score to evaluate generated knowledge images. This metric combines factual<br>fidelity, measured by graph-edit distance between KGs, with visual clarity<br>assessment. Comprehensive evaluations of 16 state-of-the-art text-to-image<br>generation models expose serious reasoning deficits–low entity fidelity, weak<br>relations, and clutter–with GPT-4o achieving an MMMG-Score of only 50.20,<br>underscoring the benchmark’s difficulty. To spur further progress, we release<br>FLUX-Reason (MMMG-Score of 34.45), an effective and open baseline that combines<br>a reasoning LLM with diffusion models and is trained on 16,000 curated<br>knowledge image-prompt pairs.</p>
</details>


<h3 id="109-Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs"><a href="#109-Beyond-Attention-or-Similarity-Maximizing-Conditional-Diversity-for-Token-Pruning-in-MLLMs" class="headerlink" title="[109] Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10967">Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</a></h3><p><em>Qizhe Zhang,Mengzhen Liu,Lichen Li,Ming Lu,Yuan Zhang,Junwen Pan,Qi She,Shanghang Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种新的视觉token裁剪方法CDPruner，通过最大化条件多样性来优化多模态大语言模型（MLLMs）中的视觉token裁剪问题，显著提升了性能并降低了计算开销。</p>
<details>
  <summary>Details</summary>
Motivation: 在多模态大语言模型中，视觉token的长度通常远大于文本token，导致推理成本高昂。现有的裁剪方法（如基于注意力或相似性的方法）无法同时避免重复token和忽略指令相关性，从而影响模型性能。

<p>Contribution: 1. 提出了CDPruner，一种新的token裁剪方法，最大化保留token的条件多样性；<br>2. 通过条件相似性和行列式点过程（DPP）重新定义裁剪问题；<br>3. 在多种MLLMs上实现了SOTA性能，同时显著降低计算成本。</p>
<p>Method: 1. 定义基于指令的条件相似性；<br>2. 使用DPP重新建模token裁剪问题，以最大化条件多样性；<br>3. CDPruner是无训练的，且与模型无关，可轻松适配不同MLLMs。</p>
<p>Result: 在LLaVA等模型中，CDPruner将FLOPs减少95%，CUDA延迟降低78%，同时保持94%的原始精度。在多个视觉-语言基准上达到SOTA性能。</p>
<p>Insight: 最大化条件多样性不仅能减少冗余token，还能更好保留图像输入的代表性并紧密贴合用户指令，从而在高裁剪率下仍维持高性能。</p>
<p>Abstract: In multimodal large language models (MLLMs), the length of input visual<br>tokens is often significantly greater than that of their textual counterparts,<br>leading to a high inference cost. Many works aim to address this issue by<br>removing redundant visual tokens. However, current approaches either rely on<br>attention-based pruning, which retains numerous duplicate tokens, or use<br>similarity-based pruning, overlooking the instruction relevance, consequently<br>causing suboptimal performance. In this paper, we go beyond attention or<br>similarity by proposing a novel visual token pruning method named CDPruner,<br>which maximizes the conditional diversity of retained tokens. We first define<br>the conditional similarity between visual tokens conditioned on the<br>instruction, and then reformulate the token pruning problem with determinantal<br>point process (DPP) to maximize the conditional diversity of the selected<br>subset. The proposed CDPruner is training-free and model-agnostic, allowing<br>easy application to various MLLMs. Extensive experiments across diverse MLLMs<br>show that CDPruner establishes new state-of-the-art on various vision-language<br>benchmarks. By maximizing conditional diversity through DPP, the selected<br>subset better represents the input images while closely adhering to user<br>instructions, thereby preserving strong performance even with high reduction<br>ratios. When applied to LLaVA, CDPruner reduces FLOPs by 95% and CUDA latency<br>by 78%, while maintaining 94% of the original accuracy. Our code is available<br>at <a target="_blank" rel="noopener" href="https://github.com/Theia-4869/CDPruner">https://github.com/Theia-4869/CDPruner</a>.</p>
</details>


<h3 id="110-GenWorld-Towards-Detecting-AI-generated-Real-world-Simulation-Videos"><a href="#110-GenWorld-Towards-Detecting-AI-generated-Real-world-Simulation-Videos" class="headerlink" title="[110] GenWorld: Towards Detecting AI-generated Real-world Simulation Videos"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10975">GenWorld: Towards Detecting AI-generated Real-world Simulation Videos</a></h3><p><em>Weiliang Chen,Wenzhao Zheng,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu,Yueqi Duan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: GenWorld是一个用于检测AI生成视频的大规模高质量真实世界模拟数据集，并提出SpannDetector模型，利用多视角一致性提升检测性能。</p>
<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的发展，AI生成的视频对真实世界信息的可信度构成威胁，亟需可靠的检测方法。然而，缺乏高质量的真实世界模拟数据集阻碍了检测器的发展。

<p>Contribution: 1) 提出了GenWorld数据集，专注于真实世界模拟场景和高品质AI生成视频；2) 发现现有多数方法无法检测高质视频；3) 提出SpannDetector模型，利用多视角一致性提升检测性能。</p>
<p>Method: 通过构建包含多模态提示生成视频的GenWorld数据集，并提出SpannDetector模型，利用多视角一致性作为检测AI生成视频的标准。</p>
<p>Result: 实验表明SpannDetector在检测高质生成视频上表现优异，为基于物理合理性的可解释检测提供了新方向。</p>
<p>Insight: 真实世界线索对于AI生成视频检测至关重要，多视角一致性是一种有效的检测标准。</p>
<p>Abstract: The flourishing of video generation technologies has endangered the<br>credibility of real-world information and intensified the demand for<br>AI-generated video detectors. Despite some progress, the lack of high-quality<br>real-world datasets hinders the development of trustworthy detectors. In this<br>paper, we propose GenWorld, a large-scale, high-quality, and real-world<br>simulation dataset for AI-generated video detection. GenWorld features the<br>following characteristics: (1) Real-world Simulation: GenWorld focuses on<br>videos that replicate real-world scenarios, which have a significant impact due<br>to their realism and potential influence; (2) High Quality: GenWorld employs<br>multiple state-of-the-art video generation models to provide realistic and<br>high-quality forged videos; (3) Cross-prompt Diversity: GenWorld includes<br>videos generated from diverse generators and various prompt modalities (e.g.,<br>text, image, video), offering the potential to learn more generalizable<br>forensic features. We analyze existing methods and find they fail to detect<br>high-quality videos generated by world models (i.e., Cosmos), revealing<br>potential drawbacks of ignoring real-world clues. To address this, we propose a<br>simple yet effective model, SpannDetector, to leverage multi-view consistency<br>as a strong criterion for real-world AI-generated video detection. Experiments<br>show that our method achieves superior results, highlighting a promising<br>direction for explainable AI-generated video detection based on physical<br>plausibility. We believe that GenWorld will advance the field of AI-generated<br>video detection. Project Page: <a target="_blank" rel="noopener" href="https://chen-wl20.github.io/GenWorld">https://chen-wl20.github.io/GenWorld</a></p>
</details>


<h3 id="111-Fine-Grained-Perturbation-Guidance-via-Attention-Head-Selection"><a href="#111-Fine-Grained-Perturbation-Guidance-via-Attention-Head-Selection" class="headerlink" title="[111] Fine-Grained Perturbation Guidance via Attention Head Selection"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10978">Fine-Grained Perturbation Guidance via Attention Head Selection</a></h3><p><em>Donghoon Ahn,Jiwon Kang,Sanghyun Lee,Minjae Kim,Jaewon Min,Wooseok Jang,Saungwu Lee,Sayak Paul,Susung Hong,Seungryong Kim</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了HeadHunter和SoftPAG方法，通过细粒度选择和扰动注意力头，提升扩散模型中生成图像的视觉质量和可控性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有注意力扰动方法缺乏确定扰动位置的系统性方法，尤其是在DiT架构中，质量相关的计算分散在不同层。

<p>Contribution: 提出了HeadHunter框架和SoftPAG方法，首次实现了对注意力头的细粒度分析和扰动，增强了生成的视觉质量与风格控制。</p>
<p>Method: 分析注意力头的功能特性，迭代选择符合目标的注意力头，并通过SoftPAG线性插值扰动强度。</p>
<p>Result: 在Stable Diffusion 3和FLUX.1上验证了方法的有效性，提升了生成质量并实现了风格特异性控制。</p>
<p>Insight: 注意力头在视觉概念（如结构、风格等）中表现出专业化分工，可被用于精准控制生成过程。</p>
<p>Abstract: Recent guidance methods in diffusion models steer reverse sampling by<br>perturbing the model to construct an implicit weak model and guide generation<br>away from it. Among these approaches, attention perturbation has demonstrated<br>strong empirical performance in unconditional scenarios where classifier-free<br>guidance is not applicable. However, existing attention perturbation methods<br>lack principled approaches for determining where perturbations should be<br>applied, particularly in Diffusion Transformer (DiT) architectures where<br>quality-relevant computations are distributed across layers. In this paper, we<br>investigate the granularity of attention perturbations, ranging from the layer<br>level down to individual attention heads, and discover that specific heads<br>govern distinct visual concepts such as structure, style, and texture quality.<br>Building on this insight, we propose “HeadHunter”, a systematic framework for<br>iteratively selecting attention heads that align with user-centric objectives,<br>enabling fine-grained control over generation quality and visual attributes. In<br>addition, we introduce SoftPAG, which linearly interpolates each selected<br>head’s attention map toward an identity matrix, providing a continuous knob to<br>tune perturbation strength and suppress artifacts. Our approach not only<br>mitigates the oversmoothing issues of existing layer-level perturbation but<br>also enables targeted manipulation of specific visual styles through<br>compositional head selection. We validate our method on modern large-scale<br>DiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,<br>demonstrating superior performance in both general quality enhancement and<br>style-specific guidance. Our work provides the first head-level analysis of<br>attention perturbation in diffusion models, uncovering interpretable<br>specialization within attention layers and enabling practical design of<br>effective perturbation strategies.</p>
</details>


<h3 id="112-InstaInpaint-Instant-3D-Scene-Inpainting-with-Masked-Large-Reconstruction-Model"><a href="#112-InstaInpaint-Instant-3D-Scene-Inpainting-with-Masked-Large-Reconstruction-Model" class="headerlink" title="[112] InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10980">InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model</a></h3><p><em>Junqi You,Chieh Hubert Lin,Weijie Lyu,Zhengbo Zhang,Ming-Hsuan Yang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: InstaInpaint 是一个基于参考的快速前馈框架，能够在0.4秒内从2D修复提案生成3D场景修复。通过自监督的掩码微调策略训练定制的大型重建模型（LRM），实现了1000倍的速度提升，并在两个标准基准测试中保持最新性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的3D场景修复方法依赖耗时且计算密集的优化，不适合实时或在线应用，因此需要一种高效的解决方案来支持交互式操作。

<p>Contribution: 提出了InstaInpaint框架，实现了快速3D场景修复；开发了自监督掩码微调策略；在速度与性能上显著优于现有方法。</p>
<p>Method: 采用基于参考的前馈框架，结合掩码微调策略训练大型重建模型（LRM），从2D修复提案生成3D修复结果。</p>
<p>Result: 在0.4秒内完成修复，速度提升1000倍；在两个标准基准测试中表现优异；适用于对象插入和多区域修复等下游任务。</p>
<p>Insight: 自监督掩码微调策略和大规模数据训练对提升模型的泛化能力、纹理一致性和几何正确性至关重要；快速框架扩展了3D修复的实际应用场景。</p>
<p>Abstract: Recent advances in 3D scene reconstruction enable real-time viewing in<br>virtual and augmented reality. To support interactive operations for better<br>immersiveness, such as moving or editing objects, 3D scene inpainting methods<br>are proposed to repair or complete the altered geometry. However, current<br>approaches rely on lengthy and computationally intensive optimization, making<br>them impractical for real-time or online applications. We propose InstaInpaint,<br>a reference-based feed-forward framework that produces 3D-scene inpainting from<br>a 2D inpainting proposal within 0.4 seconds. We develop a self-supervised<br>masked-finetuning strategy to enable training of our custom large<br>reconstruction model (LRM) on the large-scale dataset. Through extensive<br>experiments, we analyze and identify several key designs that improve<br>generalization, textural consistency, and geometric correctness. InstaInpaint<br>achieves a 1000x speed-up from prior methods while maintaining a<br>state-of-the-art performance across two standard benchmarks. Moreover, we show<br>that InstaInpaint generalizes well to flexible downstream applications such as<br>object insertion and multi-region inpainting. More video results are available<br>at our project page: <a target="_blank" rel="noopener" href="https://dhmbb2.github.io/InstaInpaint_page/">https://dhmbb2.github.io/InstaInpaint_page/</a>.</p>
</details>


<div id='cs.MM'></div>

<h1 id="cs-MM-Back"><a href="#cs-MM-Back" class="headerlink" title="cs.MM [Back]"></a>cs.MM <a href="#toc">[Back]</a></h1><h3 id="113-Multimodal-Large-Language-Models-A-Survey"><a href="#113-Multimodal-Large-Language-Models-A-Survey" class="headerlink" title="[113] Multimodal Large Language Models: A Survey"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10016">Multimodal Large Language Models: A Survey</a></h3><p><em>Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: 这篇综述探讨了多模态大语言模型（MLLMs）的发展，分析了其从单一文本生成扩展到图像、音乐、视频等多样化输出的能力。文章重点研究了自监督学习、专家混合、人类反馈强化学习和思维链提示等核心技术如何推动跨模态能力的实现，并总结了当前研究的架构趋势、跨模态协同效应以及未解决的挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型的迅速发展，其应用范围已远超文本生成，涉及图像、音频等多种模态。为了更好地理解其技术基础和未来方向，需要对现有研究进行系统分类和分析。

<p>Contribution: 1. 将多模态生成分为六种主要模态；2. 分析了自监督学习、专家混合等基础技术在跨模态能力中的作用；3. 总结了模型的架构趋势和跨模态协同效应；4. 提出了未解决的挑战，如评估、模块化和结构化推理。</p>
<p>Method: 通过文献调研和分类，对MLLMs的生成模态、基础技术和架构趋势进行了系统性的分析，重点关注了跨模态能力的技术实现和模型优化。</p>
<p>Result: 总结了当前MLLMs的发展现状，指出了跨模态协同的潜力，并提出了未来研究应关注的方向，如提升模型的通用性和可解释性。</p>
<p>Insight: 跨模态能力的实现依赖于基础技术的结合，如自监督学习和强化学习的协同作用。未来的挑战在于如何进一步提升模型的可扩展性和结构化推理能力。</p>
<p>Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text<br>generation, now spanning diverse output modalities including images, music,<br>video, human motion, and 3D objects, by integrating language with other sensory<br>modalities under unified architectures. This survey categorises six primary<br>generative modalities and examines how foundational techniques, namely<br>Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement<br>Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,<br>enable cross-modal capabilities. We analyze key models, architectural trends,<br>and emergent cross-modal synergies, while highlighting transferable techniques<br>and unresolved challenges. Architectural innovations like transformers and<br>diffusion models underpin this convergence, enabling cross-modal transfer and<br>modular specialization. We highlight emerging patterns of synergy, and identify<br>open challenges in evaluation, modularity, and structured reasoning. This<br>survey offers a unified perspective on MLLM development and identifies critical<br>paths toward more general-purpose, adaptive, and interpretable multimodal<br>systems.</p>
</details>


<h3 id="114-EQ-TAA-Equivariant-Traffic-Accident-Anticipation-via-Diffusion-Based-Accident-Video-Synthesis"><a href="#114-EQ-TAA-Equivariant-Traffic-Accident-Anticipation-via-Diffusion-Based-Accident-Video-Synthesis" class="headerlink" title="[114] EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10002">EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis</a></h3><p><em>Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: EQ-TAA提出了一种基于扩散模型的事故视频合成方法，通过生成因果视频帧来提升事故预测性能，同时避免了数据偏差问题，实现了无需额外标注的训练。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的交通事故事件预测方法依赖于大量标注数据，且容易受数据偏差影响。为了解决这一问题，作者提出了一种通过视频合成生成因果部分的方法。

<p>Contribution: 1. 提出了一种基于注意力的视频扩散模型（AVD），用于合成事故视频片段；<br>2. 设计了等变TAA（EQ-TAA）框架，通过对比学习提升模型鲁棒性；<br>3. 实现了无需额外标注的训练，提升了模型泛化能力。</p>
<p>Method: 1. 使用AVD模型通过文本提示生成因果视频帧；<br>2. 设计等变三元损失，利用合成的事故与非事故片段进行对比学习；<br>3. 在多种场景数据上进行训练，避免标注依赖。</p>
<p>Result: 实验表明，EQ-TAA在性能上优于现有方法，且能够有效缓解数据偏差问题。</p>
<p>Insight: 通过合成视频因果部分的方法，可以显著减少标注需求，同时提升模型对真实事故场景的预测能力。</p>
<p>Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging<br>problem for achieving zero fatalities in the future. Current approaches<br>typically treat TAA as a supervised learning task needing the laborious<br>annotation of accident occurrence duration. However, the inherent long-tailed,<br>uncertain, and fast-evolving nature of traffic scenes has the problem that real<br>causal parts of accidents are difficult to identify and are easily dominated by<br>data bias, resulting in a background confounding issue. Thus, we propose an<br>Attentive Video Diffusion (AVD) model that synthesizes additional accident<br>video clips by generating the causal part in dashcam videos, i.e., from normal<br>clips to accident clips. AVD aims to generate causal video frames based on<br>accident or accident-free text prompts while preserving the style and content<br>of frames for TAA after video generation. This approach can be trained using<br>datasets collected from various driving scenes without any extra annotations.<br>Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant<br>triple loss for an anchor accident-free video clip, along with the generated<br>pair of contrastive pseudo-normal and pseudo-accident clips. Extensive<br>experiments have been conducted to evaluate the performance of AVD and EQ-TAA,<br>and competitive performance compared to state-of-the-art methods has been<br>obtained.</p>
</details>


<h3 id="115-HER2-Expression-Prediction-with-Flexible-Multi-Modal-Inputs-via-Dynamic-Bidirectional-Reconstruction"><a href="#115-HER2-Expression-Prediction-with-Flexible-Multi-Modal-Inputs-via-Dynamic-Bidirectional-Reconstruction" class="headerlink" title="[115] HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10006">HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction</a></h3><p><em>Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: 该论文提出了一种动态双向重建的灵活多模态输入框架，用于HER2表达的预测，显著提升了单模态和双模态输入的准确性和适应性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的HER2评估模型通常单独分析H&E或IHC图像，而临床实践中需要两者的协同解释，但同步获取这两种模态数据存在工作流程复杂性和成本限制的问题。

<p>Contribution: 1) 动态分支选择器根据输入完整性激活单模态重建或双模态联合推理；2) 双向跨模态GAN用于缺失模态的上下文感知特征空间重建；3) 结合对抗学习和多任务优化的混合训练协议。</p>
<p>Method: 通过动态分支选择器、双向跨模态GAN和混合训练协议，实现灵活的单&#x2F;双模态输入预测。</p>
<p>Result: 单模态H&amp;E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，仅IHC输入时的可靠性为90.28%。</p>
<p>Insight: 该框架的“双模态优先、单模态兼容”设计可以在不需要同步采样的条件下实现接近双模态的性能，尤其适合资源有限的医疗环境。</p>
<p>Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&amp;E or<br>IHC images in isolation,despite clinical reliance on their synergistic<br>interpretation. However, concurrent acquisition of both modalities is often<br>hindered by workflow complexity and cost constraints. We propose an adaptive<br>bimodal framework enabling flexible single-&#x2F;dual-modality HER2 prediction<br>through three innovations: 1) A dynamic branch selector that activates either<br>single-modality reconstruction or dual-modality joint inference based on input<br>completeness; 2) A bidirectional cross-modal GAN performing context-aware<br>feature-space reconstruction of missing modalities; 3) A hybrid training<br>protocol integrating adversarial learning and multi-task optimization. This<br>architecture elevates single-modality H&amp;E prediction accuracy from 71.44% to<br>94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%<br>reliability with sole IHC inputs. The framework’s “dual-preferred,<br>single-compatible” design delivers near-bimodal performance without requiring<br>synchronized acquisition, particularly benefiting resource-limited settings<br>through IHC infrastructure cost reduction. Experimental validation confirms<br>22.81%&#x2F;12.90% accuracy improvements over H&amp;E&#x2F;IHC baselines respectively, with<br>cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251<br>(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or<br>native fusion pathways, the system mitigates performance degradation from<br>missing data while preserving computational efficiency (78.55% parameter<br>reduction in lightweight variant). This elastic architecture demonstrates<br>significant potential for democratizing precise HER2 assessment across diverse<br>healthcare settings.</p>
</details>


<h3 id="116-Controllable-Expressive-3D-Facial-Animation-via-Diffusion-in-a-Unified-Multimodal-Space"><a href="#116-Controllable-Expressive-3D-Facial-Animation-via-Diffusion-in-a-Unified-Multimodal-Space" class="headerlink" title="[116] Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10007">Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space</a></h3><p><em>Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: 本文提出了一种基于扩散模型的3D面部动画生成方法，通过多模态信号（文本、音频、情感标签）的统一表示和扩散模型增强情感表达的多样性和可控性。</p>
<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的3D面部动画方法多依赖单模态信号且采用确定性映射，限制了情感表达的多样性和灵活性。

<p>Contribution: 主要贡献包括：1) 通过对比学习实现多模态（文本、音频、情感标签）统一表示的FLAME中心对齐策略；2) 引入注意力机制的潜在扩散模型，增强动画多样性和时序一致性。</p>
<p>Method: 方法包括多模态情感对齐策略和基于注意力的潜在扩散模型，通过对比学习对齐多模态信号，扩散模型生成多样性动画。</p>
<p>Result: 实验表明，该方法在情感相似度上提升21.6%，同时保持生理合理的面部动态。</p>
<p>Insight: 多模态信号的统一表示和扩散模型的引入显著提升了情感表达的多样性和可控性，为3D面部动画提供了新思路。</p>
<p>Abstract: Audio-driven emotional 3D facial animation encounters two significant<br>challenges: (1) reliance on single-modal control signals (videos, text, or<br>emotion labels) without leveraging their complementary strengths for<br>comprehensive emotion manipulation, and (2) deterministic regression-based<br>mapping that constrains the stochastic nature of emotional expressions and<br>non-verbal behaviors, limiting the expressiveness of synthesized animations. To<br>address these challenges, we present a diffusion-based framework for<br>controllable expressive 3D facial animation. Our approach introduces two key<br>innovations: (1) a FLAME-centered multimodal emotion binding strategy that<br>aligns diverse modalities (text, audio, and emotion labels) through contrastive<br>learning, enabling flexible emotion control from multiple signal sources, and<br>(2) an attention-based latent diffusion model with content-aware attention and<br>emotion-guided layers, which enriches motion diversity while maintaining<br>temporal coherence and natural facial dynamics. Extensive experiments<br>demonstrate that our method outperforms existing approaches across most<br>metrics, achieving a 21.6% improvement in emotion similarity while preserving<br>physiologically plausible facial dynamics. Project Page:<br><a target="_blank" rel="noopener" href="https://kangweiiliu.github.io/Control_3D_Animation">https://kangweiiliu.github.io/Control_3D_Animation</a>.</p>
</details>


<h3 id="117-Structured-Graph-Representations-for-Visual-Narrative-Reasoning-A-Hierarchical-Framework-for-Comics"><a href="#117-Structured-Graph-Representations-for-Visual-Narrative-Reasoning-A-Hierarchical-Framework-for-Comics" class="headerlink" title="[117] Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10008">Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics</a></h3><p><em>Yi-Chun Chen</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: 这篇论文提出了一种层次化的知识图谱框架，用于漫画等多模态媒体的结构化理解，通过多层次分解叙事内容并构建集成知识图谱，支持符号化推理任务。</p>
<details>
  <summary>Details</summary>
Motivation: 视觉叙事（如漫画）包含复杂的多模态信息（图像和文本），需要一种结构化的方法来理解其语义、空间和时间关系。

<p>Contribution: 提出了一个层次化的知识图谱框架，能够从宏观故事弧到细粒度事件片段，整合语义、空间和时间关系，支持多种叙事推理任务。</p>
<p>Method: 将叙事内容分解为多层次（如面板级别构建多模态图谱），并结合视觉元素与文本组件，通过集成知识图谱实现跨层次的推理。</p>
<p>Result: 在Manga109数据集上验证了框架的有效性，在动作检索、对话追踪等任务中表现出高精度和高召回率。</p>
<p>Insight: 层次化知识图谱能够有效地建模视觉叙事的复杂性，为基于叙事的分析、交互式叙事和多模态推理提供了可扩展的基础。</p>
<p>Abstract: This paper presents a hierarchical knowledge graph framework for the<br>structured understanding of visual narratives, focusing on multimodal media<br>such as comics. The proposed method decomposes narrative content into multiple<br>levels, from macro-level story arcs to fine-grained event segments. It<br>represents them through integrated knowledge graphs that capture semantic,<br>spatial, and temporal relationships. At the panel level, we construct<br>multimodal graphs that link visual elements such as characters, objects, and<br>actions with corresponding textual components, including dialogue and captions.<br>These graphs are integrated across narrative levels to support reasoning over<br>story structure, character continuity, and event progression.<br>  We apply our approach to a manually annotated subset of the Manga109 dataset<br>and demonstrate its ability to support symbolic reasoning across diverse<br>narrative tasks, including action retrieval, dialogue tracing, character<br>appearance mapping, and panel timeline reconstruction. Evaluation results show<br>high precision and recall across tasks, validating the coherence and<br>interpretability of the framework. This work contributes a scalable foundation<br>for narrative-based content analysis, interactive storytelling, and multimodal<br>reasoning in visual media.</p>
</details>


<h3 id="118-WDMIR-Wavelet-Driven-Multimodal-Intent-Recognition"><a href="#118-WDMIR-Wavelet-Driven-Multimodal-Intent-Recognition" class="headerlink" title="[118] WDMIR: Wavelet-Driven Multimodal Intent Recognition"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10011">WDMIR: Wavelet-Driven Multimodal Intent Recognition</a></h3><p><em>Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu</em></p>
<p>Main category: cs.MM</p>
<p>TL;DR: 论文提出了一种基于小波变换的多模态意图识别框架（WDMIR），通过频域分析提升非语言信息的语义提取能力，显著提高了意图识别的准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖文本信息，忽视了视频和音频等非语言信息中的丰富语义内容，导致意图识别不够全面。

<p>Contribution: 1. 提出小波驱动的融合模块，在频域中同步分解和集成视频-音频特征；2. 设计跨模态交互机制，逐步增强多模态特征整合。</p>
<p>Method: 利用小波变换对视频和音频进行频域分析，结合跨模态交互机制实现从双模态到三模态的渐进式特征增强。</p>
<p>Result: 在MIntRec数据集上取得SOTA性能，准确率提升1.13%；小波融合模块对非语言信息的分析能力显著提升（0.41%）。</p>
<p>Insight: 频域分析为多模态意图识别提供了新的视角，小波变换能有效捕捉非语言信息中的细微动态特征。</p>
<p>Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user<br>intentions by integrating verbal and non-verbal information across video, audio<br>and text modalities. While existing approaches prioritize text analysis, they<br>often overlook the rich semantic content embedded in non-verbal cues. This<br>paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)<br>framework that enhances intent understanding through frequency-domain analysis<br>of non-verbal information. To be more specific, we propose: (1) a<br>wavelet-driven fusion module that performs synchronized decomposition and<br>integration of video-audio features in the frequency domain, enabling<br>fine-grained analysis of temporal dynamics; (2) a cross-modal interaction<br>mechanism that facilitates progressive feature enhancement from bimodal to<br>trimodal integration, effectively bridging the semantic gap between verbal and<br>non-verbal information. Extensive experiments on MIntRec demonstrate that our<br>approach achieves state-of-the-art performance, surpassing previous methods by<br>1.13% on accuracy. Ablation studies further verify that the wavelet-driven<br>fusion module significantly improves the extraction of semantic information<br>from non-verbal sources, with a 0.41% increase in recognition accuracy when<br>analyzing subtle emotional cues.</p>
</details>


<div id='cs.GR'></div>

<h1 id="cs-GR-Back"><a href="#cs-GR-Back" class="headerlink" title="cs.GR [Back]"></a>cs.GR <a href="#toc">[Back]</a></h1><h3 id="119-Learning-based-density-equalizing-map"><a href="#119-Learning-based-density-equalizing-map" class="headerlink" title="[119] Learning-based density-equalizing map"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10027">Learning-based density-equalizing map</a></h3><p><em>Yanwen Huang,Lok Ming Lui,Gary P. T. Choi</em></p>
<p>Main category: cs.GR</p>
<p>TL;DR: 本文提出了一种基于学习的密度等值映射框架（LDEM），利用深度神经网络改进传统的密度等值映射方法，解决了传统方法在精度、重叠和2D到3D扩展方面的限制。</p>
<details>
  <summary>Details</summary>
Motivation: 传统密度等值映射方法依赖数值求解器或手工设计的能量函数，存在精度有限、极端情况下产生重叠以及难以从2D扩展到3D的问题。本文希望通过学习的方法解决这些问题。

<p>Contribution: 1. 提出了基于深度学习的LDEM框架，引入新的损失函数确保密度均匀性和几何规则性。2. 采用分层方法预测粗粒度和密集级别的变换。3. 实现了从2D到3D的无缝扩展，无需调整模型结构或损失函数。</p>
<p>Method: 1. 设计损失函数结合密度均匀性和几何规则性。2. 使用层级神经网络预测不同粒度的变换。3. 通过深度学习直接学习映射关系，避免迭代求解。</p>
<p>Result: LDEM在简单和复杂密度分布上表现出优越的密度等值和双射性，优于传统方法。同时，该方法可直接应用于3D场景，具有更强的扩展性。</p>
<p>Insight: 深度学习可以替代传统数值方法解决几何问题，尤其是在无需显式设计能量函数的情况下，能够实现更高效和鲁棒的解决方案。</p>
<p>Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating<br>shape deformations with the area changes reflecting an underlying density<br>function. In recent decades, DEM has found widespread applications in fields<br>such as data visualization, geometry processing, and medical imaging.<br>Traditional approaches to DEM primarily rely on iterative numerical solvers for<br>diffusion equations or optimization-based methods that minimize handcrafted<br>energy functionals. However, these conventional techniques often face several<br>challenges: they may suffer from limited accuracy, produce overlapping<br>artifacts in extreme cases, and require substantial algorithmic redesign when<br>extended from 2D to 3D, due to the derivative-dependent nature of their energy<br>formulations. In this work, we propose a novel learning-based<br>density-equalizing mapping framework (LDEM) using deep neural networks.<br>Specifically, we introduce a loss function that enforces density uniformity and<br>geometric regularity, and utilize a hierarchical approach to predict the<br>transformations at both the coarse and dense levels. Our method demonstrates<br>superior density-equalizing and bijectivity properties compared to prior<br>methods for a wide range of simple and complex density distributions, and can<br>be easily applied to surface remeshing with different effects. Also, it<br>generalizes seamlessly from 2D to 3D domains without structural changes to the<br>model architecture or loss formulation. Altogether, our work opens up new<br>possibilities for scalable and robust computation of density-equalizing maps<br>for practical applications.</p>
</details>


<h3 id="120-Edit360-2D-Image-Edits-to-3D-Assets-from-Any-Angle"><a href="#120-Edit360-2D-Image-Edits-to-3D-Assets-from-Any-Angle" class="headerlink" title="[120] Edit360: 2D Image Edits to 3D Assets from Any Angle"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10507">Edit360: 2D Image Edits to 3D Assets from Any Angle</a></h3><p><em>Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang</em></p>
<p>Main category: cs.GR</p>
<p>TL;DR: Edit360提出了一种无需调参的框架，将2D图像编辑扩展到多视角一致的3D编辑，解决了现有方法视角受限的问题，通过视频扩散模型和锚定视图编辑传播机制实现高质量的3D内容重建。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角3D编辑中存在视角受限和一致性不足的问题，限制了实际应用的灵活性。

<p>Contribution: 1) 提出Edit360框架，支持从任意视角对3D资产进行编辑；2) 引入锚定视图编辑传播机制，确保多视角一致性；3) 基于视频扩散模型实现高质量3D重建。</p>
<p>Method: 1) 基于视频扩散模型构建框架；2) 选择锚定视图进行2D编辑；3) 通过潜在空间和注意力空间对齐多视角信息；4) 生成多视角序列重建3D资产。</p>
<p>Result: Edit360能够生成多视角一致的3D编辑内容，支持从任意视角查看和修改。</p>
<p>Insight: 视频扩散模型在3D内容生成中具有潜力，锚定视图机制为多视角编辑提供了新思路。</p>
<p>Abstract: Recent advances in diffusion models have significantly improved image<br>generation and editing, but extending these capabilities to 3D assets remains<br>challenging, especially for fine-grained edits that require multi-view<br>consistency. Existing methods typically restrict editing to predetermined<br>viewing angles, severely limiting their flexibility and practical applications.<br>We introduce Edit360, a tuning-free framework that extends 2D modifications to<br>multi-view consistent 3D editing. Built upon video diffusion models, Edit360<br>enables user-specific editing from arbitrary viewpoints while ensuring<br>structural coherence across all views. The framework selects anchor views for<br>2D modifications and propagates edits across the entire 360-degree range. To<br>achieve this, Edit360 introduces a novel Anchor-View Editing Propagation<br>mechanism, which effectively aligns and merges multi-view information within<br>the latent and attention spaces of diffusion models. The resulting edited<br>multi-view sequences facilitate the reconstruction of high-quality 3D assets,<br>enabling customizable 3D content creation.</p>
</details>


<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="121-Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs"><a href="#121-Omni-DPO-A-Dual-Perspective-Paradigm-for-Dynamic-Preference-Learning-of-LLMs" class="headerlink" title="[121] Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10054">Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs</a></h3><p><em>Shangpin Peng,Weinong Wang,Zhuotao Tian,Senqiao Yang,Xing Wu,Haotian Xu,Chengquan Zhang,Takashi Isobe,Baotian Hu,Min Zhang</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: Omni-DPO 是一种双视角优化的动态偏好学习框架，通过考虑偏好对的数据质量和模型学习动态，提升了 DPO 方法的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的 DPO 方法将所有偏好对视为同等重要，忽视了其数据质量与学习效用的差异，导致数据利用不足和性能下降。

<p>Contribution: 提出了 Omni-DPO，结合了偏好对的固有质量和模型学习动态的双视角优化方法，显著提升了性能。</p>
<p>Method: 通过自适应加权样本，同时考虑数据质量和模型的学习状态，实现更高效的训练数据利用。</p>
<p>Result: 在文本理解和数学推理任务中，Omni-DPO 表现优于基准方法，显著超越 Claude 3 Opus 6.7 分。</p>
<p>Insight: 动态调整偏好对的权重可以更有效地利用数据，提升模型性能。</p>
<p>Abstract: Direct Preference Optimization (DPO) has become a cornerstone of<br>reinforcement learning from human feedback (RLHF) due to its simplicity and<br>efficiency. However, existing DPO-based approaches typically treat all<br>preference pairs uniformly, ignoring critical variations in their inherent<br>quality and learning utility, leading to suboptimal data utilization and<br>performance. To address this challenge, we propose Omni-DPO, a dual-perspective<br>optimization framework that jointly accounts for (1) the inherent quality of<br>each preference pair and (2) the model’s evolving performance on those pairs.<br>By adaptively weighting samples according to both data quality and the model’s<br>learning dynamics during training, Omni-DPO enables more effective training<br>data utilization and achieves better performance. Experimental results on<br>various models and benchmarks demonstrate the superiority and generalization<br>capabilities of Omni-DPO. On textual understanding tasks, Gemma-2-9b-it<br>finetuned with Omni-DPO beats the leading LLM, Claude 3 Opus, by a significant<br>margin of 6.7 points on the Arena-Hard benchmark. On mathematical reasoning<br>tasks, Omni-DPO consistently outperforms the baseline methods across all<br>benchmarks, providing strong empirical evidence for the effectiveness and<br>robustness of our approach. Code and models will be available at<br><a target="_blank" rel="noopener" href="https://github.com/pspdada/Omni-DPO">https://github.com/pspdada/Omni-DPO</a>.</p>
</details>


<h3 id="122-Discovering-Hierarchical-Latent-Capabilities-of-Language-Models-via-Causal-Representation-Learning"><a href="#122-Discovering-Hierarchical-Latent-Capabilities-of-Language-Models-via-Causal-Representation-Learning" class="headerlink" title="[122] Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10378">Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning</a></h3><p><em>Jikai Jin,Vasilis Syrgkanis,Sham Kakade,Hanlin Zhang</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 该论文提出了一个因果表示学习框架，通过控制基础模型作为混淆变量，识别出语言模型的潜在能力因素及其因果结构，从而更好地评估和理解模型的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的语言模型评估方法面临复杂混淆效应和高计算成本的挑战，难以揭示模型能力的本质因果关系。

<p>Contribution: 提出了一个因果表示学习框架，能够识别语言模型的潜在能力因素及其因果结构，为模型评估提供了新的科学视角。</p>
<p>Method: 通过线性变换建模基准表现与潜在能力因素的关系，控制基础模型作为混淆变量，识别因果结构并应用于1500多个模型的数据集。</p>
<p>Result: 发现了一个简洁的三节点线性因果结构，揭示了从通用问题解决能力到数学推理能力的因果路径。</p>
<p>Insight: 研究发现基础模型的差异对评估结果有显著影响，控制这些差异有助于揭示潜在能力的真实因果关系。</p>
<p>Abstract: Faithful evaluation of language model capabilities is crucial for deriving<br>actionable insights that can inform model development. However, rigorous causal<br>evaluations in this domain face significant methodological challenges,<br>including complex confounding effects and prohibitive computational costs<br>associated with extensive retraining. To tackle these challenges, we propose a<br>causal representation learning framework wherein observed benchmark performance<br>is modeled as a linear transformation of a few latent capability factors.<br>Crucially, these latent factors are identified as causally interrelated after<br>appropriately controlling for the base model as a common confounder. Applying<br>this approach to a comprehensive dataset encompassing over 1500 models<br>evaluated across six benchmarks from the Open LLM Leaderboard, we identify a<br>concise three-node linear causal structure that reliably explains the observed<br>performance variations. Further interpretation of this causal structure<br>provides substantial scientific insights beyond simple numerical rankings:<br>specifically, we reveal a clear causal direction starting from general<br>problem-solving capabilities, advancing through instruction-following<br>proficiency, and culminating in mathematical reasoning ability. Our results<br>underscore the essential role of carefully controlling base model variations<br>during evaluation, a step critical to accurately uncovering the underlying<br>causal relationships among latent model capabilities.</p>
</details>


<h3 id="123-Time-IMM-A-Dataset-and-Benchmark-for-Irregular-Multimodal-Multivariate-Time-Series"><a href="#123-Time-IMM-A-Dataset-and-Benchmark-for-Irregular-Multimodal-Multivariate-Time-Series" class="headerlink" title="[123] Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10412">Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series</a></h3><p><em>Ching Chang,Jeehyun Hwang,Yidan Shi,Haixin Wang,Wen-Chih Peng,Tien-Fu Chen,Wei Wang</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文介绍了Time-IMM数据集和IMM-TSF基准库，用于处理多模态多变量的不规则时间序列，填补了当前研究中与真实数据之间的差距。</p>
<details>
  <summary>Details</summary>
Motivation: 现实中的时间序列数据（如医疗、气候和金融领域）通常是不规则、多模态和混乱的，而现有基准多假设数据是干净、规则和单模态的。

<p>Contribution: 提出了Time-IMM数据集和IMM-TSF基准库，支持多模态异步融合和时间序列预测的评估。</p>
<p>Method: Time-IMM包含九种不规则时间序列类型，IMM-TSF提供了时间戳到文本的融合模块和多模态融合模块，支持基于注意力和加权平均的策略。</p>
<p>Result: 实验表明，显式建模多模态在不规则时间序列上的预测性能有显著提升。</p>
<p>Insight: 多模态和异步融合策略是提升不规则时间序列预测的关键。</p>
<p>Abstract: Time series data in real-world applications such as healthcare, climate<br>modeling, and finance are often irregular, multimodal, and messy, with varying<br>sampling rates, asynchronous modalities, and pervasive missingness. However,<br>existing benchmarks typically assume clean, regularly sampled, unimodal data,<br>creating a significant gap between research and real-world deployment. We<br>introduce Time-IMM, a dataset specifically designed to capture cause-driven<br>irregularity in multimodal multivariate time series. Time-IMM represents nine<br>distinct types of time series irregularity, categorized into trigger-based,<br>constraint-based, and artifact-based mechanisms. Complementing the dataset, we<br>introduce IMM-TSF, a benchmark library for forecasting on irregular multimodal<br>time series, enabling asynchronous integration and realistic evaluation.<br>IMM-TSF includes specialized fusion modules, including a timestamp-to-text<br>fusion module and a multimodality fusion module, which support both<br>recency-aware averaging and attention-based integration strategies. Empirical<br>results demonstrate that explicitly modeling multimodality on irregular time<br>series data leads to substantial gains in forecasting performance. Time-IMM and<br>IMM-TSF provide a foundation for advancing time series analysis under<br>real-world conditions. The dataset is publicly available at<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/blacksnail789521/time-imm/data">https://www.kaggle.com/datasets/blacksnail789521/time-imm/data</a>, and the<br>benchmark library can be accessed at<br><a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/IMMTSF_NeurIPS2025">https://anonymous.4open.science/r/IMMTSF_NeurIPS2025</a>.</p>
</details>


<h3 id="124-Neural-at-ArchEHR-QA-2025-Agentic-Prompt-Optimization-for-Evidence-Grounded-Clinical-Question-Answering"><a href="#124-Neural-at-ArchEHR-QA-2025-Agentic-Prompt-Optimization-for-Evidence-Grounded-Clinical-Question-Answering" class="headerlink" title="[124] Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10751">Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering</a></h3><p><em>Sai Prasanna Teja Reddy Bogireddy,Abrar Majeedi,Viswanatha Reddy Gajjala,Zhuoyan Xu,Siddhant Rai,Vaishnav Potlapalli</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 本文提出了一种基于DSPy的MIPROv2优化器的提示优化方法，用于提升临床电子健康记录（EHR）问答任务中的证据检索和答案生成能力。</p>
<details>
  <summary>Details</summary>
Motivation: 临床电子健康记录（EHR）问答任务需要高精度的证据检索和可靠的答案生成，但现有方法在有限监督下表现不佳。本文旨在通过提示优化方法解决这一问题。

<p>Contribution: 1. 将任务解耦为句子级证据识别和答案生成两部分；2. 使用DSPy的MIPROv2优化器自动优化提示；3. 引入自一致性投票机制提升证据召回率。</p>
<p>Method: 1. 使用DSPy的MIPROv2优化器联合优化指令和小样本示例；2. 通过自一致性投票机制提升证据召回率。</p>
<p>Result: 在隐藏测试集上，该方法总分达到51.5，排名第二，优于零样本和小样本提示方法20和10分以上。</p>
<p>Insight: 数据驱动的提示优化是模型微调的高效替代方案，可提升高风险临床问答任务的可靠性。</p>
<p>Abstract: Automated question answering (QA) over electronic health records (EHRs) can<br>bridge critical information gaps for clinicians and patients, yet it demands<br>both precise evidence retrieval and faithful answer generation under limited<br>supervision. In this work, we present Neural, the runner-up in the BioNLP 2025<br>ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method<br>decouples the task into (1) sentence-level evidence identification and (2)<br>answer synthesis with explicit citations. For each stage, we automatically<br>explore the prompt space with DSPy’s MIPROv2 optimizer, jointly tuning<br>instructions and few-shot demonstrations on the development set. A<br>self-consistency voting scheme further improves evidence recall without<br>sacrificing precision. On the hidden test set, our method attains an overall<br>score of 51.5, placing second stage while outperforming standard zero-shot and<br>few-shot prompting by over 20 and 10 points, respectively. These results<br>indicate that data-driven prompt optimization is a cost-effective alternative<br>to model fine-tuning for high-stakes clinical QA, advancing the reliability of<br>AI assistants in healthcare.</p>
</details>


<h3 id="125-Robustly-Improving-LLM-Fairness-in-Realistic-Settings-via-Interpretability"><a href="#125-Robustly-Improving-LLM-Fairness-in-Realistic-Settings-via-Interpretability" class="headerlink" title="[125] Robustly Improving LLM Fairness in Realistic Settings via Interpretability"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10922">Robustly Improving LLM Fairness in Realistic Settings via Interpretability</a></h3><p><em>Adam Karvonen,Samuel Marks</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 这篇论文提出了一种通过内部偏置缓解方法，在大语言模型（LLM）的实际应用中有效减少种族和性别偏见的策略。</p>
<details>
  <summary>Details</summary>
Motivation: 当前在受控环境中简单的反偏见提示可以消除LLM的人口统计偏见，但在引入实际上下文后效果不佳。论文旨在解决这种问题，确保LLM在招聘等高风险应用中公平决策。

<p>Contribution: 提出了基于敏感属性方向的中性化方法（internal bias mitigation），在保持模型性能的同时，显著降低了偏见（通常低于1%）。</p>
<p>Method: 通过识别和中和模型激活中的敏感属性方向，应用仿射概念编辑在推理时干预偏见。</p>
<p>Result: 该方法在多种商业和开源模型上一致地将偏见降低至极低水平（通常低于1%，最高2.5%）。</p>
<p>Insight: 实际上下文会显著增强LLM的偏见，而内部偏置缓解是一种有效且泛化性强的解决方案，适用于实际部署场景。</p>
<p>Abstract: Large language models (LLMs) are increasingly deployed in high-stakes hiring<br>applications, making decisions that directly impact people’s careers and<br>livelihoods. While prior studies suggest simple anti-bias prompts can eliminate<br>demographic biases in controlled evaluations, we find these mitigations fail<br>when realistic contextual details are introduced. We address these failures<br>through internal bias mitigation: by identifying and neutralizing sensitive<br>attribute directions within model activations, we achieve robust bias reduction<br>across all tested scenarios. Across leading commercial (GPT-4o, Claude 4<br>Sonnet, Gemini 2.5 Flash) and open-source models (Gemma-2 27B, Gemma-3,<br>Mistral-24B), we find that adding realistic context such as company names,<br>culture descriptions from public careers pages, and selective hiring<br>constraints (e.g.,&#96;&#96;only accept candidates in the top 10%“) induces<br>significant racial and gender biases (up to 12% differences in interview<br>rates). When these biases emerge, they consistently favor Black over White<br>candidates and female over male candidates across all tested models and<br>scenarios. Moreover, models can infer demographics and become biased from<br>subtle cues like college affiliations, with these biases remaining invisible<br>even when inspecting the model’s chain-of-thought reasoning. To address these<br>limitations, our internal bias mitigation identifies race and gender-correlated<br>directions and applies affine concept editing at inference time. Despite using<br>directions from a simple synthetic dataset, the intervention generalizes<br>robustly, consistently reducing bias to very low levels (typically under 1%,<br>always below 2.5%) while largely maintaining model performance. Our findings<br>suggest that practitioners deploying LLMs for hiring should adopt more<br>realistic evaluation methodologies and consider internal mitigation strategies<br>for equitable outcomes.</p>
</details>


<h3 id="126-GUARD-Guided-Unlearning-and-Retention-via-Data-Attribution-for-Large-Language-Models"><a href="#126-GUARD-Guided-Unlearning-and-Retention-via-Data-Attribution-for-Large-Language-Models" class="headerlink" title="[126] GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10946">GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models</a></h3><p><em>Evelyn Ma,Duo Zhou,Peizhi Niu,Huiting Zhou,Huan Zhang,Olgica Milenkovic,S. Rasoul Etesami</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 该论文提出了GUARD框架，通过数据归因方法解决了大语言模型（LLM）在遗忘特定数据时导致的非预期遗忘问题，显著提升了模型在保留重要信息方面的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 随着法规合规、版权保护和隐私问题的日益重要，LLM的遗忘能力变得关键。然而，现有方法在遗忘高影响力数据时往往导致模型保留性能下降，因此需要一种更高效的数据级解决方案。

<p>Contribution: 1. 提出了一种轻量级的代理数据归因指标，专门用于LLM遗忘任务；2. 设计了基于归因分数的自适应非均匀遗忘目标；3. 通过理论证明和实验验证，GUARD在保留重要信息的同时实现了有效遗忘。</p>
<p>Method: GUARD框架的核心包括：1. 设计代理数据归因指标，量化遗忘集与保留集的“对齐”程度；2. 提出基于归因分数的自适应遗忘权重分配方法。通过重新分配遗忘能力，减少对保留集的非预期损失。</p>
<p>Result: 在TOFU基准测试中，GUARD在多种LLM架构上表现优异，遗忘10%训练数据时，保留集的Truth Ratio提升了194.92%，显著优于现有方法。</p>
<p>Insight: 数据级归因方法在模型遗忘任务中具有重要作用，通过自适应权重分配可以平衡遗忘与保留的需求，为LLM的合规性和实用性提供了新思路。</p>
<p>Abstract: Unlearning in large language models (LLMs) is becoming increasingly important<br>due to regulatory compliance, copyright protection, and privacy concerns.<br>However, a key challenge in LLM unlearning is unintended forgetting, where the<br>removal of specific data inadvertently impairs the utility of the model and its<br>retention of valuable, desired information. While prior work has primarily<br>focused on architectural innovations, the influence of data-level factors on<br>unlearning performance remains underexplored. As a result, existing methods<br>often suffer from degraded retention when forgetting high-impact data. To<br>address this, we propose GUARD-a novel framework for Guided Unlearning And<br>Retention via Data attribution. At its core, GUARD introduces a lightweight<br>proxy data attribution metric tailored for LLM unlearning, which quantifies the<br>“alignment” between the forget and retain sets while remaining computationally<br>efficient. Building on this, we design a novel unlearning objective that<br>assigns adaptive, nonuniform unlearning weights to samples, inversely<br>proportional to their proxy attribution scores. Through such a reallocation of<br>unlearning power, GUARD mitigates unintended losses in retention. We provide<br>rigorous theoretical guarantees that GUARD significantly enhances retention<br>while maintaining forgetting metrics comparable to prior methods. Extensive<br>experiments on the TOFU benchmark across multiple LLM architectures demonstrate<br>that GUARD substantially improves utility preservation while ensuring effective<br>unlearning. Notably, GUARD reduces utility sacrifice on the Retain Set by up to<br>194.92% in terms of Truth Ratio when forgetting 10% of the training data.</p>
</details>


<h3 id="127-Build-the-web-for-agents-not-agents-for-the-web"><a href="#127-Build-the-web-for-agents-not-agents-for-the-web" class="headerlink" title="[127] Build the web for agents, not agents for the web"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10953">Build the web for agents, not agents for the web</a></h3><p><em>Xing Han Lù,Gaurav Kamath,Marius Mosbach,Siva Reddy</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 这篇立场论文主张一种新的网络代理研究范式，提出开发专门为代理能力优化的交互界面（AWI），而非让代理适应人类设计的界面。</p>
<details>
  <summary>Details</summary>
Motivation: 当前网络代理方法面临重大挑战，因为人类设计的界面与LLM能力之间存在不匹配问题。为了解决这一问题，论文提出了专门为代理设计的界面理念。

<p>Contribution: 论文引入了Agentic Web Interface（AWI）的概念，并提出了六项设计原则，旨在优化代理在网络环境中的交互能力。</p>
<p>Method: 提出了AWI的设计理念，并围绕安全性、效率和标准化确立了六项设计原则，以促进代理在网络中的高效导航。</p>
<p>Result: 通过AWI的提出，论文展望了更加高效、可靠和透明的网络代理设计可能性。</p>
<p>Insight: 网络代理的研究需要协同努力，重新设计界面以更好地匹配代理的能力，而非试图让代理适应现有的人类界面。</p>
<p>Abstract: Recent advancements in Large Language Models (LLMs) and multimodal<br>counterparts have spurred significant interest in developing web agents – AI<br>systems capable of autonomously navigating and completing tasks within web<br>environments. While holding tremendous promise for automating complex web<br>interactions, current approaches face substantial challenges due to the<br>fundamental mismatch between human-designed interfaces and LLM capabilities.<br>Current methods struggle with the inherent complexity of web inputs, whether<br>processing massive DOM trees, relying on screenshots augmented with additional<br>information, or bypassing the user interface entirely through API interactions.<br>This position paper advocates for a paradigm shift in web agent research:<br>rather than forcing web agents to adapt to interfaces designed for humans, we<br>should develop a new interaction paradigm specifically optimized for agentic<br>capabilities. To this end, we introduce the concept of an Agentic Web Interface<br>(AWI), an interface specifically designed for agents to navigate a website. We<br>establish six guiding principles for AWI design, emphasizing safety,<br>efficiency, and standardization, to account for the interests of all primary<br>stakeholders. This reframing aims to overcome fundamental limitations of<br>existing interfaces, paving the way for more efficient, reliable, and<br>transparent web agent design, which will be a collaborative effort involving<br>the broader ML community.</p>
</details>


<h3 id="128-ReGuidance-A-Simple-Diffusion-Wrapper-for-Boosting-Sample-Quality-on-Hard-Inverse-Problems"><a href="#128-ReGuidance-A-Simple-Diffusion-Wrapper-for-Boosting-Sample-Quality-on-Hard-Inverse-Problems" class="headerlink" title="[128] ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10955">ReGuidance: A Simple Diffusion Wrapper for Boosting Sample Quality on Hard Inverse Problems</a></h3><p><em>Aayush Karan,Kulin Shah,Sitan Chen</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: ReGuidance是一个简单的扩散模型包装器，用于提升困难逆问题中的样本质量，通过逆向操作和重新初始化改进现有方法的表现。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在信号噪声比低的困难逆问题中容易偏离数据流形，导致输出不真实。ReGuidance旨在通过一种简单的方法提升样本质量和奖励一致性。

<p>Contribution: 提出ReGuidance包装器，通过逆向概率流ODE和重新初始化的方法，显著提升现有技术在困难逆问题中的表现。</p>
<p>Method: 首先逆向操作无条件概率流ODE，将候选解回退到潜在空间，再以此为初始化重新运行DPS。</p>
<p>Result: 在大型框内填充和高倍超分辨率等任务中，ReGuidance显著提升了样本质量，且理论证明其在多模态数据分布中能同时提升奖励和接近数据流形。</p>
<p>Insight: ReGuidance首次为DPS提供了严格的算法保证，展示了通过简单操作显著改进样本质量的潜力。</p>
<p>Abstract: There has been a flurry of activity around using pretrained diffusion models<br>as informed data priors for solving inverse problems, and more generally around<br>steering these models using reward models. Training-free methods like diffusion<br>posterior sampling (DPS) and its many variants have offered flexible heuristic<br>algorithms for these tasks, but when the reward is not informative enough,<br>e.g., in hard inverse problems with low signal-to-noise ratio, these techniques<br>veer off the data manifold, failing to produce realistic outputs. In this work,<br>we devise a simple wrapper, ReGuidance, for boosting both the sample realism<br>and reward achieved by these methods. Given a candidate solution $\hat{x}$<br>produced by an algorithm of the user’s choice, we propose inverting the<br>solution by running the unconditional probability flow ODE in reverse starting<br>from $\hat{x}$, and then using the resulting latent as an initialization for<br>DPS. We evaluate our wrapper on hard inverse problems like large box<br>in-painting and super-resolution with high upscaling. Whereas state-of-the-art<br>baselines visibly fail, we find that applying our wrapper on top of these<br>baselines significantly boosts sample quality and measurement consistency. We<br>complement these findings with theory proving that on certain multimodal data<br>distributions, ReGuidance simultaneously boosts the reward and brings the<br>candidate solution closer to the data manifold. To our knowledge, this<br>constitutes the first rigorous algorithmic guarantee for DPS.</p>
</details>


<div id='eess.SY'></div>

<h1 id="eess-SY-Back"><a href="#eess-SY-Back" class="headerlink" title="eess.SY [Back]"></a>eess.SY <a href="#toc">[Back]</a></h1><h3 id="129-Energy-Aware-Camera-Location-Search-Algorithm-for-Increasing-Precision-of-Observation-in-Automated-Manufacturing"><a href="#129-Energy-Aware-Camera-Location-Search-Algorithm-for-Increasing-Precision-of-Observation-in-Automated-Manufacturing" class="headerlink" title="[129] Energy Aware Camera Location Search Algorithm for Increasing Precision of Observation in Automated Manufacturing"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10251">Energy Aware Camera Location Search Algorithm for Increasing Precision of Observation in Automated Manufacturing</a></h3><p><em>Rongfei Li,Francis Assadian</em></p>
<p>Main category: eess.SY</p>
<p>TL;DR: 论文提出了一种针对自动化制造环境中视觉伺服任务的相机位置搜索算法，旨在通过优化相机位置以减少图像噪声，从而提高观测精度。</p>
<details>
  <summary>Details</summary>
Motivation: 现有研究多关注控制与观测架构的设计，很少讨论相机位置对观测质量的影响；而在制造环境中，相机位置会显著影响图像噪声水平，从而影响观测精度。

<p>Contribution: 提出了一种能量感知的相机移动策略算法，能够高效搜索最优或次优观测位置，并在有限能量下最大化观测精度，同时避免了高频信息丢失。</p>
<p>Method: 算法通过自适应探索策略学习环境，结合图像平均技术，动态调整相机移动策略以最小化图像噪声。</p>
<p>Result: 仿真实验表明，该算法能够在有限能量下显著提高观测精度。</p>
<p>Insight: 相机位置的动态优化对提升视觉伺服系统的观测精度至关重要，尤其是在噪声多变的制造环境中。</p>
<p>Abstract: Visual servoing technology has been well developed and applied in many<br>automated manufacturing tasks, especially in tools’ pose alignment. To access a<br>full global view of tools, most applications adopt eye-to-hand configuration or<br>eye-to-hand&#x2F;eye-in-hand cooperation configuration in an automated manufacturing<br>environment. Most research papers mainly put efforts into developing control<br>and observation architectures in various scenarios, but few of them have<br>discussed the importance of the camera’s location in eye-to-hand configuration.<br>In a manufacturing environment, the quality of camera estimations may vary<br>significantly from one observation location to another, as the combined effects<br>of environmental conditions result in different noise levels of a single image<br>shot at different locations. In this paper, we propose an algorithm for the<br>camera’s moving policy so that it explores the camera workspace and searches<br>for the optimal location where the images’ noise level is minimized. Also, this<br>algorithm ensures the camera ends up at a suboptimal (if the optimal one is<br>unreachable) location among the locations already searched, with limited energy<br>available for moving the camera. Unlike a simple brute force approach, the<br>algorithm enables the camera to explore space more efficiently by adapting the<br>search policy from learning the environment. With the aid of an image averaging<br>technique, this algorithm, in use of a solo camera, achieves the observation<br>accuracy in eye-to-hand configurations to a desirable extent without filtering<br>out high-frequency information in the original image. An automated<br>manufacturing application has been simulated and the results show the success<br>of this algorithm’s improvement of observation precision with limited energy.</p>
</details>


<h3 id="130-Semi-Tensor-Product-Based-Convolutional-Neural-Networks"><a href="#130-Semi-Tensor-Product-Based-Convolutional-Neural-Networks" class="headerlink" title="[130] Semi-Tensor-Product Based Convolutional Neural Networks"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10407">Semi-Tensor-Product Based Convolutional Neural Networks</a></h3><p><em>Daizhan Cheng</em></p>
<p>Main category: eess.SY</p>
<p>TL;DR: 该论文提出了一种基于半张量积（STP）的卷积神经网絡（CNN），通过STP的向量卷积积（CP）避免了填充带来的噪音问题，并在图像和三维信号识别中应用。</p>
<details>
  <summary>Details</summary>
Motivation: 传统卷积操作中，填充（padding）可能引入无用信息，影响模型性能。本文通过STP和域基CP的结合，试图解决这一问题。

<p>Contribution: 提出了一种新的基于STP的卷积操作（CP），无需填充，减少了无用信息的干扰，并构建了STP-CNN模型。</p>
<p>Method: 采用半张量积（STP）和域基卷积积（CP），避免了传统卷积中的填充操作，从而设计出STP-CNN。</p>
<p>Result: 所提出的STP-CNN在图像和三维信号识别任务中展示了有效性。</p>
<p>Insight: 通过STP推广向量运算，不仅可以处理不同维度的向量，还能在卷积中避免填充的负面影响，提升模型性能。</p>
<p>Abstract: The semi-tensor product (STP) of vectors is a generalization of conventional<br>inner product of vectors, which allows the factor vectors to of different<br>dimensions. This paper proposes a domain-based convolutional product (CP).<br>Combining domain-based CP with STP of vectors, a new CP is proposed. Since<br>there is no zero or any other padding, it can avoid the junk information caused<br>by padding. Using it, the STP-based convolutional neural network (CNN) is<br>developed. Its application to image and third order signal identifications is<br>considered.</p>
</details>


<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="131-One-Patient-Many-Contexts-Scaling-Medical-AI-Through-Contextual-Intelligence"><a href="#131-One-Patient-Many-Contexts-Scaling-Medical-AI-Through-Contextual-Intelligence" class="headerlink" title="[131] One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10157">One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence</a></h3><p><em>Michelle M. Li,Ben Y. Reis,Adam Rodman,Tianxi Cai,Noa Dagan,Ran D. Balicer,Joseph Loscalzo,Isaac S. Kohane,Marinka Zitnik</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 本文提出需要解决医疗AI在上下文切换中的动态适应问题，以避免因上下文错误导致的预测失效。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的医疗基础模型（如临床文本、医学图像的多模态模型）在处理新环境时需微调或检索知识库，但这种方法不切实际且难以动态适应临床情境。

<p>Contribution: 提出了一种上下文切换的医疗AI愿景，使模型能动态适应不同专科、人群和临床角色，而无需重新训练。</p>
<p>Method: 未明确具体方法，但强调了上下文动态适应的重要性，可能是通过多模态推理或增强模型的上下文意识实现。</p>
<p>Result: 愿景目标是实现更广泛的医疗覆盖，让AI能跨专科和地区诊断、管理和治疗疾病。</p>
<p>Insight: 医疗AI的未来发展需解决动态上下文适应问题，以提高模型的泛化能力和临床应用可靠性。</p>
<p>Abstract: Medical foundation models, including language models trained on clinical<br>notes, vision-language models on medical images, and multimodal models on<br>electronic health records, can summarize clinical notes, answer medical<br>questions, and assist in decision-making. Adapting these models to new<br>populations, specialties, or settings typically requires fine-tuning, careful<br>prompting, or retrieval from knowledge bases. This can be impractical, and<br>limits their ability to interpret unfamiliar inputs and adjust to clinical<br>situations not represented during training. As a result, models are prone to<br>contextual errors, where predictions appear reasonable but fail to account for<br>critical patient-specific or contextual information. These errors stem from a<br>fundamental limitation that current models struggle with: dynamically adjusting<br>their behavior across evolving contexts of medical care. In this Perspective,<br>we outline a vision for context-switching in medical AI: models that<br>dynamically adapt their reasoning without retraining to new specialties,<br>populations, workflows, and clinical roles. We envision context-switching AI to<br>diagnose, manage, and treat a wide range of diseases across specialties and<br>regions, and expand access to medical care.</p>
</details>


<h3 id="132-Scientists’-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning"><a href="#132-Scientists’-First-Exam-Probing-Cognitive-Abilities-of-MLLM-via-Perception-Understanding-and-Reasoning" class="headerlink" title="[132] Scientists’ First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10521">Scientists’ First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning</a></h3><p><em>Yuhao Zhou,Yiheng Wang,Xuming He,Ruoyao Xiao,Zhiwei Li,Qiantai Feng,Zijie Guo,Yuejin Yang,Hao Wu,Wenxuan Huang,Jiaqi Wei,Dan Si,Xiuqi Yao,Jia Bu,Haiwen Huang,Tianfan Fu,Shixiang Tang,Ben Fei,Dongzhan Zhou,Fenghua Ling,Yan Lu,Siqi Sun,Chenhui Li,Guanjie Zheng,Jiancheng Lv,Wenlong Zhang,Lei Bai</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 该论文提出了一个名为‘科学家第一考试’（SFE）的基准测试，旨在评估多模态大语言模型（MLLM）在科学领域的感知、理解和推理能力。实验表明当前先进模型在这些任务上表现仍有显著提升空间。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的科学基准测试主要评估MLLM的知识理解能力，忽视了其感知和推理能力，因此需要一个新的基准来全面评估科学领域的认知能力。

<p>Contribution: 贡献在于提出了SFE基准，包含830个专家验证的视觉问答对，覆盖66个多模态任务和五个高价值学科，填补了科学认知能力评估的空白。</p>
<p>Method: SFE通过三个层次设计评估：科学信号感知、科学属性理解和科学比较推理，利用多模态任务和专家验证的问题对MLLM进行测试。</p>
<p>Result: 实验结果显示，当前最先进的GPT-o3和InternVL-3在SFE上的得分仅为34.08%和26.52%，表明MLLM在科学领域仍有巨大改进空间。</p>
<p>Insight: SFE的提出不仅揭示了MLLM在科学认知能力上的不足，还为未来AI在科学发现中的进一步发展和应用提供了方向。</p>
<p>Abstract: Scientific discoveries increasingly rely on complex multimodal reasoning<br>based on information-intensive scientific data and domain-specific expertise.<br>Empowered by expert-level scientific benchmarks, scientific Multimodal Large<br>Language Models (MLLMs) hold the potential to significantly enhance this<br>discovery process in realistic workflows. However, current scientific<br>benchmarks mostly focus on evaluating the knowledge understanding capabilities<br>of MLLMs, leading to an inadequate assessment of their perception and reasoning<br>abilities. To address this gap, we present the Scientists’ First Exam (SFE)<br>benchmark, designed to evaluate the scientific cognitive capacities of MLLMs<br>through three interconnected levels: scientific signal perception, scientific<br>attribute understanding, scientific comparative reasoning. Specifically, SFE<br>comprises 830 expert-verified VQA pairs across three question types, spanning<br>66 multimodal tasks across five high-value disciplines. Extensive experiments<br>reveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%<br>and 26.52% on SFE, highlighting significant room for MLLMs to improve in<br>scientific realms. We hope the insights obtained in SFE will facilitate further<br>developments in AI-enhanced scientific discoveries.</p>
</details>


<h3 id="133-TeleMath-A-Benchmark-for-Large-Language-Models-in-Telecom-Mathematical-Problem-Solving"><a href="#133-TeleMath-A-Benchmark-for-Large-Language-Models-in-Telecom-Mathematical-Problem-Solving" class="headerlink" title="[133] TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10674">TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving</a></h3><p><em>Vincenzo Colle,Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Fadhel Ayed,Merouane Debbah</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 论文介绍了TeleMath，首个专门评估LLMs在电信领域数学问题解决能力的基准数据集，包含500个问题-答案对，覆盖电信领域多主题。实验表明，专为数学或逻辑推理设计的LLMs表现最佳，通用模型即使参数多也表现不佳。</p>
<details>
  <summary>Details</summary>
Motivation: 当前LLMs在通用数学推理中表现提升，但在电信等专业领域的数学问题解决能力尚未充分探索。因此，作者提出TeleMath填补这一空白。

<p>Contribution: 1. 提出首个电信领域数学问题解决基准数据集TeleMath；2. 设计了问题生成的流程；3. 评估了多种LLMs的表现，揭示了专用模型的优势。</p>
<p>Method: 通过专家选定的种子问题生成500个QnA对，覆盖电信领域多主题。评估了多种开源LLMs在TeleMath上的表现，重点关注数学或逻辑推理专用模型。</p>
<p>Result: 实验显示，专为数学或逻辑推理设计的LLMs在TeleMath上表现最佳，通用模型即使参数多也表现不佳。数据集和评估代码已开源。</p>
<p>Insight: 在特定领域的数学问题解决中，通用LLMs的表现可能不如专用模型，表明领域适配的重要性。</p>
<p>Abstract: The increasing adoption of artificial intelligence in telecommunications has<br>raised interest in the capability of Large Language Models (LLMs) to address<br>domain-specific, mathematically intensive tasks. Although recent advancements<br>have improved the performance of LLMs in general mathematical reasoning, their<br>effectiveness within specialized domains, such as signal processing, network<br>optimization, and performance analysis, remains largely unexplored. To address<br>this gap, we introduce TeleMath, the first benchmark dataset specifically<br>designed to evaluate LLM performance in solving mathematical problems with<br>numerical solutions in the telecommunications domain. Comprising 500<br>question-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the<br>telecommunications field. This paper outlines the proposed QnAs generation<br>pipeline, starting from a selected seed of problems crafted by Subject Matter<br>Experts. The evaluation of a wide range of open-source LLMs reveals that best<br>performance on TeleMath is achieved by recent models explicitly designed for<br>mathematical or logical reasoning. In contrast, general-purpose models, even<br>those with a large number of parameters, often struggle with these challenges.<br>We have released the dataset and the evaluation code to ease result<br>reproducibility and support future research.</p>
</details>


<h3 id="134-Breaking-Bad-Molecules-Are-MLLMs-Ready-for-Structure-Level-Molecular-Detoxification"><a href="#134-Breaking-Bad-Molecules-Are-MLLMs-Ready-for-Structure-Level-Molecular-Detoxification" class="headerlink" title="[134] Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10912">Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?</a></h3><p><em>Fei Lin,Ziyang Gong,Cong Wang,Yonglin Tian,Tengchao Zhang,Xue Yang,Gen Luo,Fei-Yue Wang</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 该论文提出了ToxiMol，首个针对分子毒性修复的通用多模态大语言模型（MLLM）基准任务，并设计了评估框架ToxiEval，系统性评估了近30种主流MLLM，发现其在毒性理解和分子编辑方面表现出潜力。</p>
<details>
  <summary>Details</summary>
Motivation: 早期药物开发中，毒性是导致失败的主要原因，但目前缺乏系统性的分子毒性修复任务定义与基准。

<p>Contribution: 1. 提出首个分子毒性修复的基准任务ToxiMol；2. 设计自动化评估框架ToxiEval；3. 系统性评估了近30种MLLM。</p>
<p>Method: 构建标准化数据集，设计基于专家毒理学知识的提示标注流程，并提出集成毒性预测、合成可及性等指标的评估链。</p>
<p>Result: 当前MLLM在此任务上仍面临挑战，但在毒性理解、语义约束和分子编辑方面展现出潜力。</p>
<p>Insight: MLLM在分子毒性修复任务上的能力尚不成熟，但初步结果表明其在该领域具备进一步开发的潜力。</p>
<p>Abstract: Toxicity remains a leading cause of early-stage drug development failure.<br>Despite advances in molecular design and property prediction, the task of<br>molecular toxicity repair - generating structurally valid molecular<br>alternatives with reduced toxicity - has not yet been systematically defined or<br>benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task<br>for general-purpose Multimodal Large Language Models (MLLMs) focused on<br>molecular toxicity repair. We construct a standardized dataset covering 11<br>primary tasks and 560 representative toxic molecules spanning diverse<br>mechanisms and granularities. We design a prompt annotation pipeline with<br>mechanism-aware and task-adaptive capabilities, informed by expert<br>toxicological knowledge. In parallel, we propose an automated evaluation<br>framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic<br>accessibility, drug-likeness, and structural similarity into a high-throughput<br>evaluation chain for repair success. We systematically assess nearly 30<br>mainstream general-purpose MLLMs and design multiple ablation studies to<br>analyze key factors such as evaluation criteria, candidate diversity, and<br>failure attribution. Experimental results show that although current MLLMs<br>still face significant challenges on this task, they begin to demonstrate<br>promising capabilities in toxicity understanding, semantic constraint<br>adherence, and structure-aware molecule editing.</p>
</details>


<div id='cs.MA'></div>

<h1 id="cs-MA-Back"><a href="#cs-MA-Back" class="headerlink" title="cs.MA [Back]"></a>cs.MA <a href="#toc">[Back]</a></h1><h3 id="135-AniMaker-Automated-Multi-Agent-Animated-Storytelling-with-MCTS-Driven-Clip-Generation"><a href="#135-AniMaker-Automated-Multi-Agent-Animated-Storytelling-with-MCTS-Driven-Clip-Generation" class="headerlink" title="[135] AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10540">AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation</a></h3><p><em>Haoyuan Shi,Yunxin Li,Xinyu Chen,Longyue Wang,Baotian Hu,Min Zhang</em></p>
<p>Main category: cs.MA</p>
<p>TL;DR: AniMaker是一个多智能体框架，通过蒙特卡洛树搜索（MCTS）驱动的视频片段生成和故事感知的片段选择，实现从文本输入生成全局一致且故事连贯的动画。</p>
<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法在生成跨多场景和多角色的连贯故事视频时面临挑战，现有的方法通常只能生成固定长度的片段，导致叙事不连贯和节奏问题，且不稳定。

<p>Contribution: 1. 提出AniMaker框架，包含导演、摄影、评审和后期制作四个智能体；2. 引入MCTS-Gen技术优化片段生成；3. 设计AniEval框架用于多镜头动画评估。</p>
<p>Method: 1. 使用多智能体协作（Director Agent、Photography Agent等）；2. MCTS-Gen在Photography Agent中用于高效生成高质量候选片段；3. AniEval在Reviewer Agent中评估故事一致性、动作完成度等。</p>
<p>Result: 实验表明，AniMaker在VBench和AniEval等指标上表现优异，显著提升了多候选片段生成的效率。</p>
<p>Insight: 通过智能体分工和MCTS优化，AniMaker展示了文本到视频生成中全局一致性和故事连贯性的重要性。</p>
<p>Abstract: Despite rapid advancements in video generation models, generating coherent<br>storytelling videos that span multiple scenes and characters remains<br>challenging. Current methods often rigidly convert pre-generated keyframes into<br>fixed-length clips, resulting in disjointed narratives and pacing issues.<br>Furthermore, the inherent instability of video generation models means that<br>even a single low-quality clip can significantly degrade the entire output<br>animation’s logical coherence and visual continuity. To overcome these<br>obstacles, we introduce AniMaker, a multi-agent framework enabling efficient<br>multi-candidate clip generation and storytelling-aware clip selection, thus<br>creating globally consistent and story-coherent animation solely from text<br>input. The framework is structured around specialized agents, including the<br>Director Agent for storyboard generation, the Photography Agent for video clip<br>generation, the Reviewer Agent for evaluation, and the Post-Production Agent<br>for editing and voiceover. Central to AniMaker’s approach are two key technical<br>components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search<br>(MCTS)-inspired strategy that intelligently navigates the candidate space to<br>generate high-potential clips while optimizing resource usage; and AniEval in<br>Reviewer Agent, the first framework specifically designed for multi-shot<br>animation evaluation, which assesses critical aspects such as story-level<br>consistency, action completion, and animation-specific features by considering<br>each clip in the context of its preceding and succeeding clips. Experiments<br>demonstrate that AniMaker achieves superior quality as measured by popular<br>metrics including VBench and our proposed AniEval framework, while<br>significantly improving the efficiency of multi-candidate generation, pushing<br>AI-generated storytelling animation closer to production standards.</p>
</details>


<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="136-Rethinking-Brain-Tumor-Segmentation-from-the-Frequency-Domain-Perspective"><a href="#136-Rethinking-Brain-Tumor-Segmentation-from-the-Frequency-Domain-Perspective" class="headerlink" title="[136] Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10142">Rethinking Brain Tumor Segmentation from the Frequency Domain Perspective</a></h3><p><em>Minye Shao,Zeyu Wang,Haoran Duan,Yawen Huang,Bing Zhai,Shizheng Wang,Yang Long,Yefeng Zheng</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: 该论文提出了一种从频域视角重新思考脑肿瘤分割的方法，通过HFF-Net网络结合频域分解和自适应拉普拉斯卷积，显著提升了对比增强区域的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前方法在分割增强脑肿瘤区域时性能下降，主要因为未充分考虑MRI图像的复杂纹理和方向变化特征，因此需要一种更全面的方法来捕捉这些特征。

<p>Contribution: 1.提出HFF-Net网络；2.设计频域分解模块（FDD）和自适应拉普拉斯卷积模块（ALC）；3.引入频域交叉注意力模块（FDCA）实现多尺度特征融合。</p>
<p>Method: 1.FDD模块分解MRI图像为低高频成分；2.ALC模块动态增强高频细节；3.FDCA模块融合语义、位置和切片信息。</p>
<p>Result: 在四个公开数据集上，HFF-Net在三个主要子区域的Dice分数平均提升4.48%，对比增强区域分割性能平均提升7.33%。</p>
<p>Insight: 频域视角能够更全面地表征肿瘤区域的特征，尤其是在处理复杂纹理和边界时表现出显著优势。</p>
<p>Abstract: Precise segmentation of brain tumors, particularly contrast-enhancing regions<br>visible in post-contrast MRI (areas highlighted by contrast agent injection),<br>is crucial for accurate clinical diagnosis and treatment planning but remains<br>challenging. However, current methods exhibit notable performance degradation<br>in segmenting these enhancing brain tumor areas, largely due to insufficient<br>consideration of MRI-specific tumor features such as complex textures and<br>directional variations. To address this, we propose the Harmonized Frequency<br>Fusion Network (HFF-Net), which rethinks brain tumor segmentation from a<br>frequency-domain perspective. To comprehensively characterize tumor regions, we<br>develop a Frequency Domain Decomposition (FDD) module that separates MRI images<br>into low-frequency components, capturing smooth tumor contours and<br>high-frequency components, highlighting detailed textures and directional<br>edges. To further enhance sensitivity to tumor boundaries, we introduce an<br>Adaptive Laplacian Convolution (ALC) module that adaptively emphasizes critical<br>high-frequency details using dynamically updated convolution kernels. To<br>effectively fuse tumor features across multiple scales, we design a Frequency<br>Domain Cross-Attention (FDCA) integrating semantic, positional, and<br>slice-specific information. We further validate and interpret frequency-domain<br>improvements through visualization, theoretical reasoning, and experimental<br>analyses. Extensive experiments on four public datasets demonstrate that<br>HFF-Net achieves an average relative improvement of 4.48% (ranging from 2.39%<br>to 7.72%) in the mean Dice scores across the three major subregions, and an<br>average relative improvement of 7.33% (ranging from 5.96% to 8.64%) in the<br>segmentation of contrast-enhancing tumor regions, while maintaining favorable<br>computational efficiency and clinical applicability. Code:<br><a target="_blank" rel="noopener" href="https://github.com/VinyehShaw/HFF">https://github.com/VinyehShaw/HFF</a>.</p>
</details>


<h3 id="137-Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation"><a href="#137-Prompt-Guided-Latent-Diffusion-with-Predictive-Class-Conditioning-for-3D-Prostate-MRI-Generation" class="headerlink" title="[137] Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10230">Prompt-Guided Latent Diffusion with Predictive Class Conditioning for 3D Prostate MRI Generation</a></h3><p><em>Emerson P. Grabke,Masoom A. Haider,Babak Taati</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: 论文提出了一种名为CCELLA的新方法，通过结合大型语言模型和病理分类的双重条件引导，解决了医学LDM训练中的数据稀缺问题，显著提升了合成图像的质量和分类器性能。</p>
<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺限制了机器学习的发展，现有的LDM训练方法依赖于短提示文本编码器或非医学预训练模型，且需要大量数据微调，影响了性能和科学可访问性。

<p>Contribution: 1. 提出CCELLA，一种双重条件引导方法，结合文本特征和病理分类；2. 设计了联合损失函数和数据高效的LDM训练框架；3. 在有限数据下实现高质量医学图像合成，提升了分类器性能。</p>
<p>Method: 采用CCELLA双头条件引导，通过交叉注意力引入文本特征，并通过时间步嵌入引入病理分类。提出联合损失函数和数据高效训练框架，优化LDM在有限数据下的表现。</p>
<p>Result: 在受限的前列腺MRI数据集上，3D FID达到0.025，显著优于其他方法（FID 0.071）。合成图像将前列腺癌分类器准确率从69%提升到74%，且仅用合成图像训练的分类器性能接近真实图像训练结果。</p>
<p>Insight: 通过结合文本和病理条件的双重引导，可以在数据稀缺场景下高效训练LDM，生成高质量医学图像，并为下游任务提供显著性能提升。</p>
<p>Abstract: Latent diffusion models (LDM) could alleviate data scarcity challenges<br>affecting machine learning development for medical imaging. However, medical<br>LDM training typically relies on performance- or scientific<br>accessibility-limiting strategies including a reliance on short-prompt text<br>encoders, the reuse of non-medical LDMs, or a requirement for fine-tuning with<br>large data volumes. We propose a Class-Conditioned Efficient Large Language<br>model Adapter (CCELLA) to address these limitations. CCELLA is a novel<br>dual-head conditioning approach that simultaneously conditions the LDM U-Net<br>with non-medical large language model-encoded text features through<br>cross-attention and with pathology classification through the timestep<br>embedding. We also propose a joint loss function and a data-efficient LDM<br>training framework. In combination, these strategies enable<br>pathology-conditioned LDM training for high-quality medical image synthesis<br>given limited data volume and human data annotation, improving LDM performance<br>and scientific accessibility. Our method achieves a 3D FID score of 0.025 on a<br>size-limited prostate MRI dataset, significantly outperforming a recent<br>foundation model with FID 0.071. When training a classifier for prostate cancer<br>prediction, adding synthetic images generated by our method to the training<br>dataset improves classifier accuracy from 69% to 74%. Training a classifier<br>solely on our method’s synthetic images achieved comparable performance to<br>training on real images alone.</p>
</details>


<h3 id="138-DUN-SRE-Deep-Unrolling-Network-with-Spatiotemporal-Rotation-Equivariance-for-Dynamic-MRI-Reconstruction"><a href="#138-DUN-SRE-Deep-Unrolling-Network-with-Spatiotemporal-Rotation-Equivariance-for-Dynamic-MRI-Reconstruction" class="headerlink" title="[138] DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10309">DUN-SRE: Deep Unrolling Network with Spatiotemporal Rotation Equivariance for Dynamic MRI Reconstruction</a></h3><p><em>Yuliang Zhu,Jing Cheng,Qi Xie,Zhuo-Xu Cui,Qingyong Zhu,Yuanyuan Liu,Xin Liu,Jianfeng Ren,Chengbo Wang,Dong Liang</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: 论文提出了一个名为DUN-SRE的新型深度展开网络，通过时空旋转等变性解决动态MRI重建中的对称性问题，尤其在心脏CINE MRI中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 动态MRI在时间和空间维度上具有变换对称性，但现有方法未能有效建模时间对称性，导致重建质量受限。

<p>Contribution: 提出了DUN-SRE，首次实现了时空旋转等变性的深度展开网络，并开发了高保真群滤波器参数化机制。</p>
<p>Method: 采用(2+1)D等变卷积架构，将数据一致性和近似映射模块整合到统一的深度展开框架中，确保时空对称性约束的严格传播。</p>
<p>Result: 在心脏CINE MRI数据集上实现了最先进的性能，尤其擅长保留旋转对称结构，并表现出强大的泛化能力。</p>
<p>Insight: 时空对称性约束的显式建模显著提升了动态MRI重建的质量，尤其在高度欠采样场景下效果显著。</p>
<p>Abstract: Dynamic Magnetic Resonance Imaging (MRI) exhibits transformation symmetries,<br>including spatial rotation symmetry within individual frames and temporal<br>symmetry along the time dimension. Explicit incorporation of these symmetry<br>priors in the reconstruction model can significantly improve image quality,<br>especially under aggressive undersampling scenarios. Recently, Equivariant<br>convolutional neural network (ECNN) has shown great promise in exploiting<br>spatial symmetry priors. However, existing ECNNs critically fail to model<br>temporal symmetry, arguably the most universal and informative structural prior<br>in dynamic MRI reconstruction. To tackle this issue, we propose a novel Deep<br>Unrolling Network with Spatiotemporal Rotation Equivariance (DUN-SRE) for<br>Dynamic MRI Reconstruction. The DUN-SRE establishes spatiotemporal equivariance<br>through a (2+1)D equivariant convolutional architecture. In particular, it<br>integrates both the data consistency and proximal mapping module into a unified<br>deep unrolling framework. This architecture ensures rigorous propagation of<br>spatiotemporal rotation symmetry constraints throughout the reconstruction<br>process, enabling more physically accurate modeling of cardiac motion dynamics<br>in cine MRI. In addition, a high-fidelity group filter parameterization<br>mechanism is developed to maintain representation precision while enforcing<br>symmetry constraints. Comprehensive experiments on Cardiac CINE MRI datasets<br>demonstrate that DUN-SRE achieves state-of-the-art performance, particularly in<br>preserving rotation-symmetric structures, offering strong generalization<br>capability to a broad range of dynamic MRI reconstruction tasks.</p>
</details>


<h3 id="139-ConStyX-Content-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation"><a href="#139-ConStyX-Content-Style-Augmentation-for-Generalizable-Medical-Image-Segmentation" class="headerlink" title="[139] ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10675">ConStyX: Content Style Augmentation for Generalizable Medical Image Segmentation</a></h3><p><em>Xi Chen,Zhiqiang Shen,Peng Cao,Jinzhu Yang,Osmar R. Zaiane</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: 论文提出了一种名为ConStyX的新型域泛化方法，通过内容和风格增强来提升医学图像分割模型的泛化性能。</p>
<details>
  <summary>Details</summary>
Motivation: 医学图像通常来自多个域，导致域偏移问题，影响分割模型的性能。现有的域随机化方法仅依赖图像风格扰动，效率受限，且忽略了过度增强图像对训练的负面影响。

<p>Contribution: 1）提出了一种内容和风格双重增强的域泛化方法（ConStyX）；2）在训练中有效利用增强特征，同时减轻过度增强的负面影响。</p>
<p>Method: ConStyX通过同时增强训练数据的内容和风格，覆盖更广的数据域，并在模型训练中优化增强特征的利用。</p>
<p>Result: 实验表明，ConStyX在多个域上取得了优越的泛化性能。</p>
<p>Insight: 内容和风格的协同增强对于改善医学图像分割的域泛化能力至关重要。</p>
<p>Abstract: Medical images are usually collected from multiple domains, leading to domain<br>shifts that impair the performance of medical image segmentation models. Domain<br>Generalization (DG) aims to address this issue by training a robust model with<br>strong generalizability. Recently, numerous domain randomization-based DG<br>methods have been proposed. However, these methods suffer from the following<br>limitations: 1) constrained efficiency of domain randomization due to their<br>exclusive dependence on image style perturbation, and 2) neglect of the adverse<br>effects of over-augmented images on model training. To address these issues, we<br>propose a novel domain randomization-based DG method, called content style<br>augmentation (ConStyX), for generalizable medical image segmentation.<br>Specifically, ConStyX 1) augments the content and style of training data,<br>allowing the augmented training data to better cover a wider range of data<br>domains, and 2) leverages well-augmented features while mitigating the negative<br>effects of over-augmented features during model training. Extensive experiments<br>across multiple domains demonstrate that our ConStyX achieves superior<br>generalization performance. The code is available at<br><a target="_blank" rel="noopener" href="https://github.com/jwxsp1/ConStyX">https://github.com/jwxsp1/ConStyX</a>.</p>
</details>


<h3 id="140-Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches"><a href="#140-Generalist-Models-in-Medical-Image-Segmentation-A-Survey-and-Performance-Comparison-with-Task-Specific-Approaches" class="headerlink" title="[140] Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10825">Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches</a></h3><p><em>Andrea Moglia,Matteo Leccardi,Matteo Cavicchioli,Alice Maccarini,Marco Marcon,Luca Mainardi,Pietro Cerveri</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: 本文是对医学图像分割领域的通才模型（如SAM及其变种）的全面调查，比较了它们的性能，并探讨了与任务特定模型的差异。</p>
<details>
  <summary>Details</summary>
Motivation: 受大型语言模型的启发，通才模型在计算机视觉领域崭露头角，特别是在医学图像分割中。本文旨在调查这些模型的性能及其挑战。

<p>Contribution: 提供了通才模型的分类和性能分析，对比了任务特定模型的最新成果，并探讨了未来发展方向。</p>
<p>Method: 分类了通才模型的变种（如零样本、少样本、微调等），并进行了性能比较。</p>
<p>Result: 通才模型在医学图像分割中表现良好，但仍需解决合规性、隐私和预算等挑战。</p>
<p>Insight: 未来的方向包括合成数据、早期融合、代理AI和临床转化，借鉴自然语言处理的经验。</p>
<p>Abstract: Following the successful paradigm shift of large language models, leveraging<br>pre-training on a massive corpus of data and fine-tuning on different<br>downstream tasks, generalist models have made their foray into computer vision.<br>The introduction of Segment Anything Model (SAM) set a milestone on<br>segmentation of natural images, inspiring the design of a multitude of<br>architectures for medical image segmentation. In this survey we offer a<br>comprehensive and in-depth investigation on generalist models for medical image<br>segmentation. We start with an introduction on the fundamentals concepts<br>underpinning their development. Then, we provide a taxonomy on the different<br>declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on<br>the recent SAM 2, on other innovative models trained on images alone, and<br>others trained on both text and images. We thoroughly analyze their<br>performances at the level of both primary research and best-in-literature,<br>followed by a rigorous comparison with the state-of-the-art task-specific<br>models. We emphasize the need to address challenges in terms of compliance with<br>regulatory frameworks, privacy and security laws, budget, and trustworthy<br>artificial intelligence (AI). Finally, we share our perspective on future<br>directions concerning synthetic data, early fusion, lessons learnt from<br>generalist models in natural language processing, agentic AI and physical AI,<br>and clinical translation.</p>
</details>


<h3 id="141-Med-URWKV-Pure-RWKV-With-ImageNet-Pre-training-For-Medical-Image-Segmentation"><a href="#141-Med-URWKV-Pure-RWKV-With-ImageNet-Pre-training-For-Medical-Image-Segmentation" class="headerlink" title="[141] Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10858">Med-URWKV: Pure RWKV With ImageNet Pre-training For Medical Image Segmentation</a></h3><p><em>Zhenhuan Zhou</em></p>
<p>Main category: eess.IV</p>
<p>TL;DR: Med-URWKV提出了一种基于纯RWKV架构的医学图像分割模型，首次利用ImageNet预训练的VRWKV编码器，性能优于从头训练的RWKV模型。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割方法（CNN、Transformer或混合架构）存在局限性，如CNN感受野受限或Transformer计算复杂度高。RWKV具备线性复杂度的远程建模能力，但其在医学领域的预训练潜力未被探索。

<p>Contribution: 1. 提出了首个基于纯RWKV架构的医学图像分割模型Med-URWKV；2. 首次利用ImageNet预训练的VRWKV编码器，验证了预训练对模型性能的提升作用。</p>
<p>Method: 基于U-Net框架，使用纯RWKV架构，并引入ImageNet预训练的VRWKV编码器。实验在七个数据集上进行，对比从头训练的RWKV模型。</p>
<p>Result: Med-URWKV在多个数据集上取得了与从头训练的RWKV模型相当或更优的分割性能，验证了预训练的有效性。</p>
<p>Insight: 利用ImageNet预训练的RWKV编码器可以显著提升医学图像分割任务的性能，为未来研究提供了新的方向。</p>
<p>Abstract: Medical image segmentation is a fundamental and key technology in<br>computer-aided diagnosis and treatment. Previous methods can be broadly<br>classified into three categories: convolutional neural network (CNN) based,<br>Transformer based, and hybrid architectures that combine both. However, each of<br>them has its own limitations, such as restricted receptive fields in CNNs or<br>the computational overhead caused by the quadratic complexity of Transformers.<br>Recently, the Receptance Weighted Key Value (RWKV) model has emerged as a<br>promising alternative for various vision tasks, offering strong long-range<br>modeling capabilities with linear computational complexity. Some studies have<br>also adapted RWKV to medical image segmentation tasks, achieving competitive<br>performance. However, most of these studies focus on modifications to the<br>Vision-RWKV (VRWKV) mechanism and train models from scratch, without exploring<br>the potential advantages of leveraging pre-trained VRWKV models for medical<br>image segmentation tasks. In this paper, we propose Med-URWKV, a pure<br>RWKV-based architecture built upon the U-Net framework, which incorporates<br>ImageNet-based pretraining to further explore the potential of RWKV in medical<br>image segmentation tasks. To the best of our knowledge, Med-URWKV is the first<br>pure RWKV segmentation model in the medical field that can directly reuse a<br>large-scale pre-trained VRWKV encoder. Experimental results on seven datasets<br>demonstrate that Med-URWKV achieves comparable or even superior segmentation<br>performance compared to other carefully optimized RWKV models trained from<br>scratch. This validates the effectiveness of using a pretrained VRWKV encoder<br>in enhancing model performance. The codes will be released.</p>
</details>


<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="142-GenBreak-Red-Teaming-Text-to-Image-Generators-Using-Large-Language-Models"><a href="#142-GenBreak-Red-Teaming-Text-to-Image-Generators-Using-Large-Language-Models" class="headerlink" title="[142] GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10047">GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models</a></h3><p><em>Zilong Wang,Xiang Zheng,Xiaosen Wang,Bo Wang,Xingjun Ma,Yu-Gang Jiang</em></p>
<p>Main category: cs.CR</p>
<p>TL;DR: GenBreak提出了一种通过微调大型语言模型（LLM）来系统探索文本到图像（T2I）生成器潜在漏洞的框架，结合监督微调和强化学习，生成既能绕过安全机制又能产生高毒性图像的对抗性提示。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的T2I模型（如Stable Diffusion）可能被滥用以生成有害内容，但现有研究在对抗性攻击上存在局限性：或容易被检测，或无法生成真正有害的输出。GenBreak旨在填补这一空白，提供一种可靠的工具来评估T2I模型的安全性。

<p>Contribution: GenBreak的主要贡献是提出了一个结合监督微调和强化学习的框架，通过多奖励信号引导LLM生成既能绕过安全机制又具有高毒性的对抗性提示，揭示了商用T2I模型的严重安全漏洞。</p>
<p>Method: 方法包括：1）使用精心策划的数据集对LLM进行监督微调；2）通过与代理T2I模型交互进行强化学习；3）整合多个奖励信号以优化提示的逃避能力和毒性。</p>
<p>Result: 生成的对抗性提示在针对商用T2I模型的黑盒攻击中表现出色，揭示了严重的安全弱点。</p>
<p>Insight: 通过LLM生成对抗性提示是一种高效发现T2I模型漏洞的方法，结合多奖励信号的强化学习可以显著提升提示的有效性和多样性。</p>
<p>Abstract: Text-to-image (T2I) models such as Stable Diffusion have advanced rapidly and<br>are now widely used in content creation. However, these models can be misused<br>to generate harmful content, including nudity or violence, posing significant<br>safety risks. While most platforms employ content moderation systems,<br>underlying vulnerabilities can still be exploited by determined adversaries.<br>Recent research on red-teaming and adversarial attacks against T2I models has<br>notable limitations: some studies successfully generate highly toxic images but<br>use adversarial prompts that are easily detected and blocked by safety filters,<br>while others focus on bypassing safety mechanisms but fail to produce genuinely<br>harmful outputs, neglecting the discovery of truly high-risk prompts.<br>Consequently, there remains a lack of reliable tools for evaluating the safety<br>of defended T2I models. To address this gap, we propose GenBreak, a framework<br>that fine-tunes a red-team large language model (LLM) to systematically explore<br>underlying vulnerabilities in T2I generators. Our approach combines supervised<br>fine-tuning on curated datasets with reinforcement learning via interaction<br>with a surrogate T2I model. By integrating multiple reward signals, we guide<br>the LLM to craft adversarial prompts that enhance both evasion capability and<br>image toxicity, while maintaining semantic coherence and diversity. These<br>prompts demonstrate strong effectiveness in black-box attacks against<br>commercial T2I generators, revealing practical and concerning safety<br>weaknesses.</p>
</details>


<div id='cs.SD'></div>

<h1 id="cs-SD-Back"><a href="#cs-SD-Back" class="headerlink" title="cs.SD [Back]"></a>cs.SD <a href="#toc">[Back]</a></h1><h3 id="143-PAL-Probing-Audio-Encoders-via-LLMs-–-A-Study-of-Information-Transfer-from-Audio-Encoders-to-LLMs"><a href="#143-PAL-Probing-Audio-Encoders-via-LLMs-–-A-Study-of-Information-Transfer-from-Audio-Encoders-to-LLMs" class="headerlink" title="[143] PAL: Probing Audio Encoders via LLMs – A Study of Information Transfer from Audio Encoders to LLMs"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10423">PAL: Probing Audio Encoders via LLMs – A Study of Information Transfer from Audio Encoders to LLMs</a></h3><p><em>Tony Alex,Wish Suharitdamrong,Sara Atito,Armin Mustafa,Philip J. B. Jackson,Imran Razzak,Muhammad Awais</em></p>
<p>Main category: cs.SD</p>
<p>TL;DR: 该论文研究了音频编码器与LLMs的交互机制，提出通过延迟音频整合、仅用注意力子模块和多样化编码器集成，优化跨模态信息传输，显著提升性能。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管音频-LLMs的应用发展迅速，但音频编码器与LLMs间的语义表征传输机制尚不明确。研究旨在优化这种交互，提升LLMs对音频信息的探测能力。

<p>Contribution: 1. 提出延迟音频整合的策略；2. 验证仅通过注意力子模块即可有效探测音频表征；3. 引入多样化音频编码器集成，丰富信息传输。</p>
<p>Method: 基于Pengi&#x2F;LLaVA架构，提出延迟整合、注意力模块优化和编码器集成，并在560万音频-文本对数据集上验证。</p>
<p>Result: 最终架构在基线基础上提升10%-60%，验证了跨模态信息传输优化的有效性。</p>
<p>Insight: 延迟音频整合和注意力模块的简化设计是关键；多样化的编码器集成能显著扩展LLMs的音频信息处理能力。</p>
<p>Abstract: The integration of audio perception capabilities into Large Language Models<br>(LLMs) has enabled significant advances in Audio-LLMs. Although<br>application-focused developments, particularly in curating training data for<br>specific capabilities e.g., audio reasoning, have progressed rapidly, the<br>underlying mechanisms that govern efficient transfer of rich semantic<br>representations from audio encoders to LLMs remain under-explored. We<br>conceptualize effective audio-LLM interaction as the LLM’s ability to<br>proficiently probe the audio encoder representations to satisfy textual<br>queries. This paper presents a systematic investigation on how architectural<br>design choices can affect that. Beginning with a standard Pengi&#x2F;LLaVA-style<br>audio-LLM architecture, we propose and evaluate several modifications guided by<br>hypotheses derived from mechanistic interpretability studies and LLM<br>operational principles. Our experiments demonstrate that: (1) delaying audio<br>integration until the LLM’s initial layers establish textual context that<br>enhances its ability to probe the audio representations for relevant<br>information; (2) the LLM can proficiently probe audio representations<br>exclusively through LLM layer’s attention submodule, without requiring<br>propagation to its Feed-Forward Network (FFN) submodule; (3) an efficiently<br>integrated ensemble of diverse audio encoders provides richer, complementary<br>representations, thereby broadening the LLM’s capacity to probe a wider<br>spectrum of audio information. All hypotheses are evaluated using an identical<br>three-stage training curriculum on a dataset of 5.6 million audio-text pairs,<br>ensuring controlled comparisons. Our final architecture, which incorporates all<br>proposed modifications, achieves relative improvements from 10% to 60% over<br>the baseline, validating our approach to optimizing cross-modal information<br>transfer in audio-LLMs. Project page: <a target="_blank" rel="noopener" href="https://ta012.github.io/PAL/">https://ta012.github.io/PAL/</a></p>
</details>


<div id='physics.med-ph'></div>

<h1 id="physics-med-ph-Back"><a href="#physics-med-ph-Back" class="headerlink" title="physics.med-ph [Back]"></a>physics.med-ph <a href="#toc">[Back]</a></h1><h3 id="144-Modality-AGnostic-Image-Cascade-MAGIC-for-Multi-Modality-Cardiac-Substructure-Segmentation"><a href="#144-Modality-AGnostic-Image-Cascade-MAGIC-for-Multi-Modality-Cardiac-Substructure-Segmentation" class="headerlink" title="[144] Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10797">Modality-AGnostic Image Cascade (MAGIC) for Multi-Modality Cardiac Substructure Segmentation</a></h3><p><em>Nicholas Summerfield,Qisheng He,Alex Kuo,Ahmed I. Ghanem,Simeng Zhu,Chase Ruff,Joshua Pan,Anudeep Kumar,Prashant Nagpal,Jiwei Zhao,Ming Dong,Carri K. Glide-Hurst</em></p>
<p>Main category: physics.med-ph</p>
<p>TL;DR: 论文提出了一种名为MAGIC的多模态心脏亚结构分割方法，通过单一的nnU-Net模型实现多模态数据的分割，并在三种模态下取得了优于对比模型的表现。</p>
<details>
  <summary>Details</summary>
Motivation: 心脏亚结构分割在放射治疗规划中至关重要，现有深度学习方法在多模态数据和重叠结构分割上缺乏通用性。

<p>Contribution: 提出了一种轻量级的、多模态通用的心脏亚结构分割框架MAGIC，能够在单一模型中处理多种模态数据。</p>
<p>Method: 基于nnU-Net的U型网络结构，通过复制编码和解码分支实现多模态分割功能。</p>
<p>Result: 在Sim-CT、MR-Linac和CCTA三种模态上的平均DSC分数分别为0.75、0.68和0.80，优于57%的对比模型。</p>
<p>Insight: MAGIC简化了计算需求，提高了临床应用的灵活性，为多模态心脏亚结构分割提供了一种高效解决方案。</p>
<p>Abstract: Cardiac substructures are essential in thoracic radiation therapy planning to<br>minimize risk of radiation-induced heart disease. Deep learning (DL) offers<br>efficient methods to reduce contouring burden but lacks generalizability across<br>different modalities and overlapping structures. This work introduces and<br>validates a Modality-AGnostic Image Cascade (MAGIC) for comprehensive and<br>multi-modal cardiac substructure segmentation. MAGIC is implemented through<br>replicated encoding and decoding branches of an nnU-Net-based, U-shaped<br>backbone conserving the function of a single model. Twenty cardiac<br>substructures (heart, chambers, great vessels (GVs), valves, coronary arteries<br>(CAs), and conduction nodes) from simulation CT (Sim-CT), low-field MR-Linac,<br>and cardiac CT angiography (CCTA) modalities were manually delineated and used<br>to train (n&#x3D;76), validate (n&#x3D;15), and test (n&#x3D;30) MAGIC. Twelve comparison<br>models (four segmentation subgroups across three modalities) were equivalently<br>trained. All methods were compared for training efficiency and against<br>reference contours using the Dice Similarity Coefficient (DSC) and two-tailed<br>Wilcoxon Signed-Rank test (threshold, p&lt;0.05). Average DSC scores were<br>0.75(0.16) for Sim-CT, 0.68(0.21) for MR-Linac, and 0.80(0.16) for CCTA. MAGIC<br>outperforms the comparison in 57% of cases, with limited statistical<br>differences. MAGIC offers an effective and accurate segmentation solution that<br>is lightweight and capable of segmenting multiple modalities and overlapping<br>structures in a single model. MAGIC further enables clinical implementation by<br>simplifying the computational requirements and offering unparalleled<br>flexibility for clinical settings.</p>
</details>


<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="145-A-Navigation-Framework-Utilizing-Vision-Language-Models"><a href="#145-A-Navigation-Framework-Utilizing-Vision-Language-Models" class="headerlink" title="[145] A Navigation Framework Utilizing Vision-Language Models"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10172">A Navigation Framework Utilizing Vision-Language Models</a></h3><p><em>Yicheng Duan,Kaiyu tang</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: 该论文提出了一种模块化的导航框架，将视觉-语言理解与动作规划解耦，利用冻结的视觉-语言模型和轻量级规划逻辑，旨在实现灵活、快速且适应性强的导航。</p>
<details>
  <summary>Details</summary>
Motivation: 视觉-语言导航（VLN）任务需要智能体解析自然语言指令并在陌生环境中导航，现有大型视觉-语言模型虽然在多模态理解上表现优异，但存在计算成本高和实时部署困难的问题。

<p>Contribution: 提出了一种模块化的导航框架，通过解耦视觉-语言理解和动作规划，结合冻结模型与轻量级规划逻辑，提升导航的效率和灵活性。</p>
<p>Method: 采用冻结的视觉-语言模型 Qwen2.5-VL-7B-Instruct，结合提示工程、结构化历史管理和双帧视觉输入策略，优化导航决策的连续性。</p>
<p>Result: 虽然在新环境泛化能力上仍有挑战，但模块化设计为高效可扩展的导航系统奠定了基础。</p>
<p>Insight: 通过增强环境先验和扩展多模态输入集成，未来有望进一步提升导航系统的性能。</p>
<p>Abstract: Vision-and-Language Navigation (VLN) presents a complex challenge in embodied<br>AI, requiring agents to interpret natural language instructions and navigate<br>through visually rich, unfamiliar environments. Recent advances in large<br>vision-language models (LVLMs), such as CLIP and Flamingo, have significantly<br>improved multimodal understanding but introduced new challenges related to<br>computational cost and real-time deployment. In this project, we propose a<br>modular, plug-and-play navigation framework that decouples vision-language<br>understanding from action planning. By integrating a frozen vision-language<br>model, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to<br>achieve flexible, fast, and adaptable navigation without extensive model<br>fine-tuning. Our framework leverages prompt engineering, structured history<br>management, and a two-frame visual input strategy to enhance decision-making<br>continuity across navigation steps. We evaluate our system on the Room-to-Room<br>benchmark within the VLN-CE setting using the Matterport3D dataset and<br>Habitat-Lab simulation environment. Although our initial results reveal<br>challenges in generalizing to unseen environments under strict evaluation<br>settings, our modular approach lays a foundation for scalable and efficient<br>navigation systems, highlighting promising directions for future improvement<br>through enhanced environmental priors and expanded multimodal input<br>integration.</p>
</details>


<h3 id="146-EmbodiedGen-Towards-a-Generative-3D-World-Engine-for-Embodied-Intelligence"><a href="#146-EmbodiedGen-Towards-a-Generative-3D-World-Engine-for-Embodied-Intelligence" class="headerlink" title="[146] EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10600">EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence</a></h3><p><em>Wang Xinjie,Liu Liu,Cao Yu,Wu Ruiqi,Qin Wenkang,Wang Dehui,Sui Wei,Su Zhizhong</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: EmbodiedGen是一个用于生成高质量、可控且逼真的3D资产的平台，支持具身智能任务。它通过生成式AI解决了传统3D资产的高成本和低真实性问题。</p>
<details>
  <summary>Details</summary>
Motivation: 传统3D资产的高生产成本和有限的真实性问题阻碍了具身智能任务的扩展性需求。EmbodiedGen旨在通过生成式AI提供低成本、多样化的3D世界生成方案。

<p>Contribution: 提出了EmbodiedGen平台，通过六个关键模块（如Image-to-3D、Text-to-3D等）生成多样化的3D资产，支持URDF格式，可直接用于物理仿真引擎。</p>
<p>Method: 利用生成式AI技术，结合图像和文本输入，生成高质量、可控的3D资产。平台包含多个模块，涵盖从纹理生成到场景布局的全流程。</p>
<p>Result: 生成的3D资产具有高真实性和准确的物理属性，可直接用于具身智能任务的训练和评估。代码已开源。</p>
<p>Insight: EmbodiedGen展示了生成式AI在解决3D资产稀缺问题上的潜力，为具身智能研究的扩展性和泛化性提供了新工具。</p>
<p>Abstract: Constructing a physically realistic and accurately scaled simulated 3D world<br>is crucial for the training and evaluation of embodied intelligence tasks. The<br>diversity, realism, low cost accessibility and affordability of 3D data assets<br>are critical for achieving generalization and scalability in embodied AI.<br>However, most current embodied intelligence tasks still rely heavily on<br>traditional 3D computer graphics assets manually created and annotated, which<br>suffer from high production costs and limited realism. These limitations<br>significantly hinder the scalability of data driven approaches. We present<br>EmbodiedGen, a foundational platform for interactive 3D world generation. It<br>enables the scalable generation of high-quality, controllable and<br>photorealistic 3D assets with accurate physical properties and real-world scale<br>in the Unified Robotics Description Format (URDF) at low cost. These assets can<br>be directly imported into various physics simulation engines for fine-grained<br>physical control, supporting downstream tasks in training and evaluation.<br>EmbodiedGen is an easy-to-use, full-featured toolkit composed of six key<br>modules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object<br>Generation, Scene Generation and Layout Generation. EmbodiedGen generates<br>diverse and interactive 3D worlds composed of generative 3D assets, leveraging<br>generative AI to address the challenges of generalization and evaluation to the<br>needs of embodied intelligence related research. Code is available at<br><a target="_blank" rel="noopener" href="https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html">https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html</a>.</p>
</details>


<h3 id="147-Eye-Robot-Learning-to-Look-to-Act-with-a-BC-RL-Perception-Action-Loop"><a href="#147-Eye-Robot-Learning-to-Look-to-Act-with-a-BC-RL-Perception-Action-Loop" class="headerlink" title="[147] Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10968">Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</a></h3><p><em>Justin Kerr,Kush Hari,Ethan Weber,Chung Min Kim,Brent Yi,Tyler Bonnen,Ken Goldberg,Angjoo Kanazawa</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: EyeRobot是一个结合了机械眼球和强化学习的机器人系统，通过联合训练手和眼的动作来完成实际任务。</p>
<details>
  <summary>Details</summary>
Motivation: 人类通过主动观察环境来完成任务，受此启发，研究者开发了EyeRobot，模拟人类的主动注视行为，以提高机器人在大工作空间中的操作能力。

<p>Contribution: EyeRobot的主要贡献包括：1) 引入了一个可自由旋转的机械眼球，支持360度观察；2) 提出了BC-RL联合训练框架，实现手眼协调；3) 开发了高分辨率、低计算开销的注视策略架构。</p>
<p>Method: 方法包括：1) 使用遥操作演示数据训练初始手部动作；2) 在仿真环境中通过强化学习训练注视策略；3) 采用BC-RL循环联合优化手眼动作，其中手的动作为行为克隆，眼的动作为强化学习。</p>
<p>Result: 实验表明，EyeRobot在五个全景工作空间任务中表现优越，能够有效跟踪目标并忽略干扰物，实现了在大工作空间中的高效操作。</p>
<p>Insight: 论文的洞察包括：1) 高分辨率注视策略有助于稳定注视和目标跟踪；2) 手眼协调的自然涌现可以显著提升任务完成效果。</p>
<p>Abstract: Humans do not passively observe the visual world – we actively look in order<br>to act. Motivated by this principle, we introduce EyeRobot, a robotic system<br>with gaze behavior that emerges from the need to complete real-world tasks. We<br>develop a mechanical eyeball that can freely rotate to observe its surroundings<br>and train a gaze policy to control it using reinforcement learning. We<br>accomplish this by first collecting teleoperated demonstrations paired with a<br>360 camera. This data is imported into a simulation environment that supports<br>rendering arbitrary eyeball viewpoints, allowing episode rollouts of eye gaze<br>on top of robot demonstrations. We then introduce a BC-RL loop to train the<br>hand and eye jointly: the hand (BC) agent is trained from rendered eye<br>observations, and the eye (RL) agent is rewarded when the hand produces correct<br>action predictions. In this way, hand-eye coordination emerges as the eye looks<br>towards regions which allow the hand to complete the task. EyeRobot implements<br>a foveal-inspired policy architecture allowing high resolution with a small<br>compute budget, which we find also leads to the emergence of more stable<br>fixation as well as improved ability to track objects and ignore distractors.<br>We evaluate EyeRobot on five panoramic workspace manipulation tasks requiring<br>manipulation in an arc surrounding the robot arm. Our experiments suggest<br>EyeRobot exhibits hand-eye coordination behaviors which effectively facilitate<br>manipulation over large workspaces with a single camera. See project site for<br>videos: <a target="_blank" rel="noopener" href="https://www.eyerobot.net/">https://www.eyerobot.net/</a></p>
</details>


<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="148-Conversational-Search-From-Fundamentals-to-Frontiers-in-the-LLM-Era"><a href="#148-Conversational-Search-From-Fundamentals-to-Frontiers-in-the-LLM-Era" class="headerlink" title="[148] Conversational Search: From Fundamentals to Frontiers in the LLM Era"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.10635">Conversational Search: From Fundamentals to Frontiers in the LLM Era</a></h3><p><em>Fengran Mo,Chuan Meng,Mohammad Aliannejadi,Jian-Yun Nie</em></p>
<p>Main category: cs.IR</p>
<p>TL;DR: 该教程介绍了对话式搜索的基础知识和新兴发展，特别是大型语言模型(LLM)带来的革新，旨在帮助研究人员和实践者掌握核心技术并推动下一代对话式搜索系统的开发。</p>
<details>
  <summary>Details</summary>
Motivation: 随着LLM在指令跟随、内容生成和推理等方面能力的显著提升，为构建智能对话式搜索系统提供了新的机遇和挑战，促使研究者重新审视和推进这一领域的发展。

<p>Contribution: 该教程系统地连接了对话式搜索的基础理论与LLM驱动的先进技术，为学术界和工业界的研究者提供了一个全面的知识框架。</p>
<p>Method: 教程结合核心原则和前沿进展，介绍LLM如何革新对话式搜索系统，包括意图理解、上下文处理和对话生成等关键技术。</p>
<p>Result: 参与者将掌握对话式搜索的基础和LLM带来的前沿技术，具备开发下一代智能对话式搜索系统的能力。</p>
<p>Insight: LLM的强大能力为对话式搜索注入了新的活力，但也带来了如何高效利用和微调模型以满足特定需求的挑战。</p>
<p>Abstract: Conversational search enables multi-turn interactions between users and<br>systems to fulfill users’ complex information needs. During this interaction,<br>the system should understand the users’ search intent within the conversational<br>context and then return the relevant information through a flexible,<br>dialogue-based interface. The recent powerful large language models (LLMs) with<br>capacities of instruction following, content generation, and reasoning, attract<br>significant attention and advancements, providing new opportunities and<br>challenges for building up intelligent conversational search systems. This<br>tutorial aims to introduce the connection between fundamentals and the emerging<br>topics revolutionized by LLMs in the context of conversational search. It is<br>designed for students, researchers, and practitioners from both academia and<br>industry. Participants will gain a comprehensive understanding of both the core<br>principles and cutting-edge developments driven by LLMs in conversational<br>search, equipping them with the knowledge needed to contribute to the<br>development of next-generation conversational search systems.</p>
</details>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-06-16/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-06-14/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
