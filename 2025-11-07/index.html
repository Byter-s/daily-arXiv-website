<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-11-07 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-11-07
            
            
        </div>
        <span class="post-date">
            Nov 7, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 32]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 21]</li>
<li><a href="#q-fin.TR">q-fin.TR</a> [Total: 1]</li>
<li><a href="#cs.GR">cs.GR</a> [Total: 1]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 3]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 2]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 3]</li>
<li><a href="#eess.SP">eess.SP</a> [Total: 2]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Generative-Hints-cs-CV-cs-AIPDF"><a href="#1-Generative-Hints-cs-CV-cs-AIPDF" class="headerlink" title="[1] Generative Hints cs.CV | cs.AIPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02933">Generative Hints</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02933" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Andy Dimnaku, Abdullah Yusuf Kavranoğlu, Yaser Abu-Mostafa</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种称为‘生成提示’的训练方法，旨在通过生成模型生成无标签虚拟样本，以半监督方式学习已知的不变性（即‘提示’），从而改进传统数据增强方法在输入空间全局捕捉不变性的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的数据增强方法仅依赖于有限的训练数据转换来学习不变性，未能全局捕捉输入空间中的不变性。为了解决这一问题，作者提出利用生成模型生成虚拟样本，以半监督方式直接强化模型对不变性的学习。</p>
<p><strong>Result:</strong> 在多个任务中，生成提示方法均优于传统数据增强方法，例如在细粒度视觉分类任务中平均提升0.63%的Top-1准确率，在CheXpert X-ray数据集上平均提升1.286%的性能。</p>
<p><strong>Insight:</strong> 生成模型可以为学习不变性提供丰富的全局信息，弥补数据增强的局限性；半监督学习能够有效利用虚拟样本提升模型性能。</p>
<p><strong>Abstract:</strong> Data augmentation is widely used in vision to introduce variation and mitigate overfitting, through enabling models to learn invariant properties, such as spatial invariance. However, these properties are not fully captured by data augmentation alone, since it attempts to learn the property on transformations of the training data only. We propose generative hints, a training methodology that directly enforces known invariances in the entire input space. Our approach leverages a generative model trained on the training set to approximate the input distribution and generate unlabeled images, which we refer to as virtual examples. These virtual examples are used to enforce functional properties known as hints. In generative hints, although the training dataset is fully labeled, the model is trained in a semi-supervised manner on both the classification and hint objectives, using the unlabeled virtual examples to guide the model in learning the desired hint. Across datasets, architectures, and loss functions, generative hints consistently outperform standard data augmentation when learning the same property. On popular fine-grained visual classification benchmarks, we achieved up to 1.78% top-1 accuracy improvement (0.63% on average) over fine-tuned models with data augmentation and an average performance boost of 1.286% on the CheXpert X-ray dataset.</p>
  </div>
</details>

<hr>
<h3 id="2-ProM3E-Probabilistic-Masked-MultiModal-Embedding-Model-for-Ecology-cs-CVPDF"><a href="#2-ProM3E-Probabilistic-Masked-MultiModal-Embedding-Model-for-Ecology-cs-CVPDF" class="headerlink" title="[2] ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology cs.CVPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02946">ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02946" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher</span></p>
<p><strong>TL;DR:</strong> ProM3E是一种概率掩码多模态嵌入模型，专注于生态学领域的多模态表示生成与重建，支持嵌入空间中的模态反转与模态融合分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生态学研究需要处理多模态数据（如视觉、声音等），而现有方法在多模态表示学习与模态补全方面存在局限性。ProM3E通过概率掩码学习填补这一空白。</p>
<p><strong>Result:</strong> 模型在多模态检索与线性探测任务中表现优越，代码与数据集已开源。</p>
<p><strong>Insight:</strong> 概率掩码学习不仅提升多模态融合的灵活性，还为模态互补性分析提供了新视角。</p>
<p><strong>Abstract:</strong> We introduce ProM3E, a probabilistic masked multimodal embedding model for any-to-any generation of multimodal representations for ecology. ProM3E is based on masked modality reconstruction in the embedding space, learning to infer missing modalities given a few context modalities. By design, our model supports modality inversion in the embedding space. The probabilistic nature of our model allows us to analyse the feasibility of fusing various modalities for given downstream tasks, essentially learning what to fuse. Using these features of our model, we propose a novel cross-modal retrieval approach that mixes inter-modal and intra-modal similarities to achieve superior performance across all retrieval tasks. We further leverage the hidden representation from our model to perform linear probing tasks and demonstrate the superior representation learning capability of our model. All our code, datasets and model will be released at <a target="_blank" rel="noopener" href="https://vishu26.github.io/prom3e">https://vishu26.github.io/prom3e</a>.</p>
  </div>
</details>

<hr>
<h3 id="3-EvtSlowTV-–-A-Large-and-Diverse-Dataset-for-Event-Based-Depth-Estimation-cs-CV-cs-AI-cs-LG-cs-ROPDF"><a href="#3-EvtSlowTV-–-A-Large-and-Diverse-Dataset-for-Event-Based-Depth-Estimation-cs-CV-cs-AI-cs-LG-cs-ROPDF" class="headerlink" title="[3] EvtSlowTV – A Large and Diverse Dataset for Event-Based Depth Estimation cs.CV | cs.AI | cs.LG | cs.ROPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02953">EvtSlowTV – A Large and Diverse Dataset for Event-Based Depth Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02953" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sadiq Layi Macaulay, Nimet Kaygusuz, Simon Hadfield</span></p>
<p><strong>TL;DR:</strong> 本文介绍了一个名为EvtSlowTV的大规模事件相机数据集，用于解决现有事件数据集规模小、泛化能力不足的问题。通过从YouTube公开视频中提取数据，该数据集包含了多种环境条件下的13B事件，支持自监督学习框架，提升了模型在复杂场景下的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有事件相机数据集规模小且受限，限制了基于事件的深度估计方法在实际场景中的泛化能力。</p>
<p><strong>Result:</strong> 实验表明，使用EvtSlowTV训练的模型在复杂场景和运动中表现出更强的泛化能力。</p>
<p><strong>Insight:</strong> 大规模自然数据集对于提升事件相机模型的性能和泛化能力至关重要，同时自监督学习可以充分利用事件的异步特性。</p>
<p><strong>Abstract:</strong> Event cameras, with their high dynamic range (HDR) and low latency, offer a promising alternative for robust depth estimation in challenging environments. However, many event-based depth estimation approaches are constrained by small-scale annotated datasets, limiting their generalizability to real-world scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event camera dataset curated from publicly available YouTube footage, which contains more than 13B events across various environmental conditions and motions, including seasonal hiking, flying, scenic driving, and underwater exploration. EvtSlowTV is an order of magnitude larger than existing event datasets, providing an unconstrained, naturalistic setting for event-based depth learning. This work shows the suitability of EvtSlowTV for a self-supervised learning framework to capitalise on the HDR potential of raw event streams. We further demonstrate that training with EvtSlowTV enhances the model’s ability to generalise to complex scenes and motions. Our approach removes the need for frame-based annotations and preserves the asynchronous nature of event data.</p>
  </div>
</details>

<hr>
<h3 id="4-Hybrid-Convolution-and-Vision-Transformer-NAS-Search-Space-for-TinyML-Image-Classification-cs-CV-cs-LGPDF"><a href="#4-Hybrid-Convolution-and-Vision-Transformer-NAS-Search-Space-for-TinyML-Image-Classification-cs-CV-cs-LGPDF" class="headerlink" title="[4] Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification cs.CV | cs.LGPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02992">Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02992" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mikhael Djajapermana, Moritz Reiber, Daniel Mueller-Gritschneder, Ulf Schlichtmann</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于TinyML图像分类的新型混合CNN-ViT搜索空间，通过NAS找到高效的混合架构，平衡计算成本和模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管混合CNN和ViT架构在图像分类中表现优异，但其高参数量和计算成本使其难以部署在TinyML设备上。本文旨在通过NAS搜索空间找到适合TinyML的高效混合架构。</p>
<p><strong>Result:</strong> 实验表明，所提架构在CIFAR10上的准确性和推理速度优于基于ResNet的TinyML模型。</p>
<p><strong>Insight:</strong> 通过灵活组合CNN和ViT的模块，并优化池化层设计，可以在资源受限的设备上实现高性能的图像分类模型。</p>
<p><strong>Abstract:</strong> Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT) have outperformed pure CNN or ViT architecture. However, since these architectures require large parameters and incur large computational costs, they are unsuitable for tinyML deployment. This paper introduces a new hybrid CNN-ViT search space for Neural Architecture Search (NAS) to find efficient hybrid architectures for image classification. The search space covers hybrid CNN and ViT blocks to learn local and global information, as well as the novel Pooling block of searchable pooling layers for efficient feature map reduction. Experimental results on the CIFAR10 dataset show that our proposed search space can produce hybrid CNN-ViT architectures with superior accuracy and inference speed to ResNet-based tinyML models under tight model size constraints.</p>
  </div>
</details>

<hr>
<h3 id="5-SCALE-VLP-Soft-Weighted-Contrastive-Volumetric-Vision-Language-Pre-training-with-Spatial-Knowledge-Semantics-cs-CVPDF"><a href="#5-SCALE-VLP-Soft-Weighted-Contrastive-Volumetric-Vision-Language-Pre-training-with-Spatial-Knowledge-Semantics-cs-CVPDF" class="headerlink" title="[5] SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics cs.CVPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02996">SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02996" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ailar Mahdizadeh, Puria Azadi Moghadam, Xiangteng He, Shahriar Mirabbasi, Panos Nasiopoulos</span></p>
<p><strong>TL;DR:</strong> SCALE-VLP提出了一种软加权对比视觉语言预训练框架，专注于体数据（如CT），通过整合空间知识语义和领域知识，显著提升了跨任务和跨领域的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型主要针对2D数据，忽视了体数据的连续性和结构化依赖。此外，现有方法通常将体数据视为独立2D切片，破坏了空间一致性，且未能充分利用丰富的临床语义。</p>
<p><strong>Result:</strong> 在CT报告检索中Top-1准确率提升4.3倍，异常分类提升10点，报告生成ROUGE-L达0.44，BERT-F1达0.89。零样本跨域评估也表现稳定。</p>
<p><strong>Insight:</strong> 整合空间和领域知识语义能显著提升体数据任务的性能，同时展现出跨任务和跨领域的泛化潜力，为医学影像分析提供了新思路。</p>
<p><strong>Abstract:</strong> Vision-language models (VLMs) have demonstrated strong cross-modal capabilities, yet most work remains limited to 2D data and assumes binary supervision (i.e., positive vs. negative pairs), overlooking the continuous and structured dependencies present in volumetric data such as CT. Existing approaches often treat volumetric scans as independent 2D slices, compromising spatial coherence and underutilizing rich clinical semantics. We propose SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework that integrates (i) volumetric spatial semantics to preserve anatomical structure and (ii) domain-aware, knowledge-infused semantics (e.g., radiological ontologies) to guide alignment. This yields structurally consistent and semantically grounded representations under limited supervision, demonstrating strong cross-task transferability (retrieval, report generation, and classification), and cross-domain generalizability with consistent gains without further fine-tuning. In particular, compared to the previous state of the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval, improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an out-of-domain external dataset, we observe consistent gains, indicating the cross-task and cross-domain generalization ability of SCALE-VLP.</p>
  </div>
</details>

<hr>
<h3 id="6-Learning-with-less-label-efficient-land-cover-classification-at-very-high-spatial-resolution-using-self-supervised-deep-learning-cs-CVPDF"><a href="#6-Learning-with-less-label-efficient-land-cover-classification-at-very-high-spatial-resolution-using-self-supervised-deep-learning-cs-CVPDF" class="headerlink" title="[6] Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning cs.CVPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03004">Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03004" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dakota Hester, Vitor S. Martins, Lucas B. Ferreira, Thainara M. A. Lima</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于自监督深度学习的高效标签方法，用于1米分辨率的土地覆盖分类，仅需1000个标注样本即可实现州级范围的高精度分类。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 高分辨率土地覆盖分类需要大量标注数据，但标注成本高昂，限制了模型的广泛应用。论文旨在通过自监督学习减少对标注数据的依赖。</p>
<p><strong>Result:</strong> 模型在密西西比州的8类土地覆盖分类中达到87.14%整体准确率和75.58%宏F1分数，展示了自监督学习在小样本任务中的有效性。</p>
<p><strong>Insight:</strong> 自监督学习显著减少了高分辨率土地覆盖分类对标注数据的依赖，但仍存在耕地、草本和裸地分类的挑战。</p>
<p><strong>Abstract:</strong> Deep learning semantic segmentation methods have shown promising performance for very high 1-m resolution land cover classification, but the challenge of collecting large volumes of representative training data creates a significant barrier to widespread adoption of such models for meter-scale land cover mapping over large areas. In this study, we present a novel label-efficient approach for statewide 1-m land cover classification using only 1,000 annotated reference image patches with self-supervised deep learning. We use the “Bootstrap Your Own Latent” pre-training strategy with a large amount of unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to pre-train a ResNet-101 convolutional encoder. The learned encoder weights were subsequently transferred into multiple deep semantic segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then fine-tuned using very small training dataset sizes with cross-validation (250, 500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall accuracy and 75.58% macro F1 score using an ensemble of the best performing U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more than 123 billion pixels over the state of Mississippi, USA. Detailed qualitative and quantitative analysis revealed accurate mapping of open water and forested areas, while highlighting challenges in accurate delineation between cropland, herbaceous, and barren land cover types. These results show that self-supervised learning is an effective strategy for reducing the need for large volumes of manually annotated data, directly addressing a major limitation to high spatial resolution land cover mapping at scale.</p>
  </div>
</details>

<hr>
<h3 id="7-A-Foundation-Model-for-Brain-MRI-with-Dynamic-Modality-Integration-cs-CVPDF"><a href="#7-A-Foundation-Model-for-Brain-MRI-with-Dynamic-Modality-Integration-cs-CVPDF" class="headerlink" title="[7] A Foundation Model for Brain MRI with Dynamic Modality Integration cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03014">A Foundation Model for Brain MRI with Dynamic Modality Integration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03014" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minh Sao Khue Luu, Bair N. Tuchinov</span></p>
<p><strong>TL;DR:</strong> 提出了一种基础模型，用于处理脑部MRI的不同成像序列组合。通过可学习模态嵌入和条件层归一化，结合掩码自编码目标，处理缺失模态，并使用方差-协方差正则化器提升特征学习的稳定性和多样性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统方法需要为每个MRI模态单独训练模型，这不仅计算资源消耗大，且无法灵活处理缺失或未见的模态序列。</p>
<p><strong>Result:</strong> 初步结果显示模型可行，计划进一步评估其在脑肿瘤、多发性硬化分割及病变分类任务中的表现。</p>
<p><strong>Insight:</strong> 统一的编码器设计显著减少了计算资源需求，同时增强了模型对缺失模态的鲁棒性。</p>
<p><strong>Abstract:</strong> We present a foundation model for brain MRI that can work with different combinations of imaging sequences. The model uses one encoder with learnable modality embeddings, conditional layer normalization, and a masked autoencoding objective that accounts for missing modalities. A variance-covariance regularizer is applied to stabilize feature learning and improve representation diversity. This design removes the need for separate models for each modality and allows the network to adapt when some sequences are missing or unseen. It is trained on about 60,000 multi-center MRIs using self-supervised reconstruction and modality imputation to learn flexible representations. A learnable modality embedding guides feature extraction so the encoder can adjust to different inputs. We describe our planned evaluation on brain tumor and multiple sclerosis segmentation, as well as lesion classification, under various modality settings. Preliminary results show that the method works feasibly, and further experiments are planned to study its performance in more detail. All code and pretrained models are available at <a target="_blank" rel="noopener" href="https://github.com/BrainFM/brainfm">https://github.com/BrainFM/brainfm</a></p>
  </div>
</details>

<hr>
<h3 id="8-SLIP-Structural-aware-Language-Image-Pretraining-for-Vision-Language-Alignment-cs-CV-cs-AIPDF"><a href="#8-SLIP-Structural-aware-Language-Image-Pretraining-for-Vision-Language-Alignment-cs-CV-cs-AIPDF" class="headerlink" title="[8] SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment cs.CV | cs.AIPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03019">SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03019" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenbo Lu</span></p>
<p><strong>TL;DR:</strong> SLIP提出了一种结构感知的语言-图像预训练方法，通过引入结构化对比损失和建模实体间关系，显著提升了跨模态对齐性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉-语言预训练方法将图像-文本对视为独立训练样本，忽略了领域内的丰富关系结构（如电商产品共购图）。</p>
<p><strong>Result:</strong> SLIP在零样本和小样本跨模态检索及分类任务上一致优于CLIP。</p>
<p><strong>Insight:</strong> 关系监督可以显著提升跨模态对齐性能，结构化信息是重要的监督信号。</p>
<p><strong>Abstract:</strong> Vision-Language Pretraining (VLP) has achieved remarkable success across various downstream tasks, but such gains are largely driven by scaling up on training data. Yet, literature methods treat image-text pairs as isolated training examples; this neglects the rich relational structure naturally present in many domains, such as e-commerce product co-purchase graphs and social recommendation networks. Inspired by neuroscientific evidence that human encodes knowledge as relationship cognitive maps, we introduce Structure-aware Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive loss to align modalities while also modeling relationships between neighboring entities in a structured graph. To support this paradigm, we construct a large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling structured cross-modality supervision at scale. Experiment results show that SLIP consistently outperforms CLIP on cross-modal retrieval and classification tasks in both zero-shot and few-shot settings, showing the value of relational supervision for cross-modal alignment.</p>
  </div>
</details>

<hr>
<h3 id="9-ISC-Perception-A-Hybrid-Computer-Vision-Dataset-for-Object-Detection-in-Novel-Steel-Assembly-cs-CV-eess-IVPDF"><a href="#9-ISC-Perception-A-Hybrid-Computer-Vision-Dataset-for-Object-Detection-in-Novel-Steel-Assembly-cs-CV-eess-IVPDF" class="headerlink" title="[9] ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly cs.CV | eess.IVPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03098">ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03098" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miftahur Rahman, Samuel Adebayo, Dorian A. Acevedo-Mejia, David Hester, Daniel McPolin</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为ISC-Perception的混合数据集，专门用于检测新型钢结构装配中的对象，解决了建筑机器人感知领域的数据缺失问题。通过结合CAD渲染图像、游戏引擎生成的逼真场景和少量真实照片，实现了高效自动标注，显著减少了人工标注时间。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 建筑机器人（如ISC系统的机器人）需要可靠的感知能力，但当前缺乏专门的数据集，且在施工现场收集图像存在安全和隐私问题。</p>
<p><strong>Result:</strong> 使用ISC-Perception训练的检测器在IoU 0.50下的mAP为0.756，显著优于仅使用合成数据或逼真数据训练的模型。在1,200帧测试中，<a href="mailto:&#109;&#x41;&#x50;&#64;&#x30;&#x2e;&#53;&#x30;">mAP@0.50</a>&#x2F;mAP@[0.50:0.95]分别达到0.943&#x2F;0.823。</p>
<p><strong>Insight:</strong> 混合数据集（合成+真实）在目标检测任务中表现优异，同时大幅降低了人工标注成本，对建筑机器人和工业应用的快速开发具有重要意义。</p>
<p><strong>Abstract:</strong> The Intermeshed Steel Connection (ISC) system, when paired with robotic manipulators, can accelerate steel-frame assembly and improve worker safety by eliminating manual assembly. Dependable perception is one of the initial stages for ISC-aware robots. However, this is hampered by the absence of a dedicated image corpus, as collecting photographs on active construction sites is logistically difficult and raises safety and privacy concerns. In response, we introduce ISC-Perception, the first hybrid dataset expressly designed for ISC component detection. It blends procedurally rendered CAD images, game-engine photorealistic scenes, and a limited, curated set of real photographs, enabling fully automatic labelling of the synthetic portion. We explicitly account for all human effort to produce the dataset, including simulation engine and scene setup, asset preparation, post-processing scripts and quality checks; our total human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for manual labelling at 60,s per image (-81.7%). A manual pilot on a representative image with five instances of ISC members took 60,s (maximum 80,s), anchoring the manual baseline. Detectors trained on ISC-Perception achieved a mean Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we report <a href="mailto:&#x6d;&#65;&#x50;&#64;&#x30;&#46;&#53;&#x30;">mAP@0.50</a>&#x2F;mAP@[0.50:0.95] of 0.943&#x2F;0.823. By bridging the data gap for construction-robotics perception, ISC-Perception facilitates rapid development of custom object detectors and is freely available for research and industrial use upon request.</p>
  </div>
</details>

<hr>
<h3 id="10-DentalSplat-Dental-Occlusion-Novel-View-Synthesis-from-Sparse-Intra-Oral-Photographs-cs-CVPDF"><a href="#10-DentalSplat-Dental-Occlusion-Novel-View-Synthesis-from-Sparse-Intra-Oral-Photographs-cs-CVPDF" class="headerlink" title="[10] DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs cs.CVPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03099">DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03099" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang</span></p>
<p><strong>TL;DR:</strong> 提出了DentalSplat，一种从稀疏口腔照片中进行3D重建和新视角合成的方法，适用于正畸治疗中的远程医疗场景。通过先验引导的立体重建和尺度自适应剪枝策略，显著提升了稀疏输入下的重建质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在正畸治疗和远程医疗中，需要从稀疏的口腔照片（如前视图和双侧颊视图）中重建3D牙齿咬合情况，但传统3DGS方法依赖密集输入和精确相机姿态，无法直接应用。</p>
<p><strong>Result:</strong> 在950个临床案例和195个视频测试集上验证，DentalSplat在稀疏输入下优于现有技术。</p>
<p><strong>Insight:</strong> 通过引入先验和几何约束，可以显著提升稀疏输入下的3D重建质量，为远程正畸治疗提供了实用工具。</p>
<p><strong>Abstract:</strong> In orthodontic treatment, particularly within telemedicine contexts, observing patients’ dental occlusion from multiple viewpoints facilitates timely clinical decision-making. Recent advances in 3D Gaussian Splatting (3DGS) have shown strong potential in 3D reconstruction and novel view synthesis. However, conventional 3DGS pipelines typically rely on densely captured multi-view inputs and precisely initialized camera poses, limiting their practicality. Orthodontic cases, in contrast, often comprise only three sparse images, specifically, the anterior view and bilateral buccal views, rendering the reconstruction task especially challenging. The extreme sparsity of input views severely degrades reconstruction quality, while the absence of camera pose information further complicates the process. To overcome these limitations, we propose DentalSplat, an effective framework for 3D reconstruction from sparse orthodontic imagery. Our method leverages a prior-guided dense stereo reconstruction model to initialize the point cloud, followed by a scale-adaptive pruning strategy to improve the training efficiency and reconstruction quality of 3DGS. In scenarios with extremely sparse viewpoints, we further incorporate optical flow as a geometric constraint, coupled with gradient regularization, to enhance rendering fidelity. We validate our approach on a large-scale dataset comprising 950 clinical cases and an additional video-based test set of 195 cases designed to simulate real-world remote orthodontic imaging conditions. Experimental results demonstrate that our method effectively handles sparse input scenarios and achieves superior novel view synthesis quality for dental occlusion visualization, outperforming state-of-the-art techniques.</p>
  </div>
</details>

<hr>
<h3 id="11-Image-Intrinsic-Priors-for-Integrated-Circuit-Defect-Detection-and-Novel-Class-Discovery-via-Self-Supervised-Learning-cs-CV-cs-AIPDF"><a href="#11-Image-Intrinsic-Priors-for-Integrated-Circuit-Defect-Detection-and-Novel-Class-Discovery-via-Self-Supervised-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[11] Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning cs.CV | cs.AIPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03120">Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03120" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Botong. Zhao, Xubin. Wang, Shujing. Lyu, Yue. Lu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为IC DefectNCD的框架，通过自监督学习利用图像固有先验，实现了集成电路缺陷检测和新类发现，避免了支持集依赖和标注问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 集成电路制造过程中缺陷复杂且多样，监督方法需要大量标注且难以处理新类别，无监督方法性能不稳定。论文旨在解决这些问题。</p>
<p><strong>Result:</strong> 在涵盖15种缺陷类型的真实数据集上验证，展现了对缺陷检测和未见过缺陷分类的鲁棒性能。</p>
<p><strong>Insight:</strong> 通过结合自监督学习和图像固有先验，可以显著提升对复杂制造环境中缺陷的检测和新类别识别的能力，为工业缺陷分析提供了新思路。</p>
<p><strong>Abstract:</strong> Integrated circuit manufacturing is highly complex, comprising hundreds of process steps. Defects can arise at any stage, causing yield loss and ultimately degrading product reliability. Supervised methods require extensive human annotation and struggle with emergent categories and rare, data scarce defects. Clustering-based unsupervised methods often exhibit unstable performance due to missing priors. We propose IC DefectNCD, a support set free framework that leverages Image Intrinsic Priors in IC SEM images for defect detection and novel class discovery. We first develop Self Normal Information Guided IC Defect Detection, aggregating representative normal features via a learnable normal information extractor and using reconstruction residuals to coarsely localize defect regions. To handle saliency variations across defects, we introduce an adaptive binarization strategy that produces stable subimages focused on core defective areas. Finally, we design Self Defect Information Guided IC Defect Classification, which incorporates a soft mask guided attention mechanism to inject spatial defect priors into the teacher student model. This enhances sensitivity to defective regions, suppresses background interference, and enables recognition and classification of unseen defects. We validate the approach on a real world dataset spanning three key fabrication stages and covering 15 defect types. Experiments demonstrate robust performance on both defect detection and unseen defect classification.</p>
  </div>
</details>

<hr>
<h3 id="12-Accelerating-Physical-Property-Reasoning-for-Augmented-Visual-Cognition-cs-CV-cs-HCPDF"><a href="#12-Accelerating-Physical-Property-Reasoning-for-Augmented-Visual-Cognition-cs-CV-cs-HCPDF" class="headerlink" title="[12] Accelerating Physical Property Reasoning for Augmented Visual Cognition cs.CV | cs.HCPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03126">Accelerating Physical Property Reasoning for Augmented Visual Cognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03126" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongbo Lan, Zhenlin An, Haoyu Li, Vaibhav Singh, Longfei Shangguan</span></p>
<p><strong>TL;DR:</strong> <sysname>是一个通过算法和系统优化加速视觉指导的物理属性推理的系统，将延迟从10-20分钟降至6秒以内，并保持或提高了准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉指导的物理属性推理常靠长时间的处理，影响实晋可用性。</p>
<p><strong>Result:</strong> 将延迟从10-20分钟降至6秒以内，并在物体层面的物理属性估计中达到或超过SOTA。</p>
<p><strong>Insight:</strong> 通过绿色计算和并行化，可以在保持性能的同时实现分布式推理。</p>
<p><strong>Abstract:</strong> This paper introduces \sysname, a system that accelerates vision-guided physical property reasoning to enable augmented visual cognition. \sysname minimizes the run-time latency of this reasoning pipeline through a combination of both algorithmic and systematic optimizations, including rapid geometric 3D reconstruction, efficient semantic feature fusion, and parallel view encoding. Through these simple yet effective optimizations, \sysname reduces the end-to-end latency of this reasoning pipeline from 10–20 minutes to less than 6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname achieves this 62.9$\times$–287.2$\times$ speedup while not only reaching on-par (and sometimes slightly better) object-level physical property estimation accuracy(e.g. mass), but also demonstrating superior performance in material segmentation and voxel-level inference than two SOTA baselines. We further combine gaze-tracking with \sysname to localize the object of interest in cluttered, real-world environments, streamlining the physical property reasoning on smart glasses. The case study with Meta Aria Glasses conducted at an IKEA furniture store demonstrates that \sysname achives consistently high performance compared to controlled captures, providing robust property estimations even with fewer views in real-world scenarios.</p>
  </div>
</details>

<hr>
<h3 id="13-Deploying-Rapid-Damage-Assessments-from-sUAS-Imagery-for-Disaster-Response-cs-CV-cs-AI-cs-CYPDF"><a href="#13-Deploying-Rapid-Damage-Assessments-from-sUAS-Imagery-for-Disaster-Response-cs-CV-cs-AI-cs-CYPDF" class="headerlink" title="[13] Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response cs.CV | cs.AI | cs.CYPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03132">Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03132" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thomas Manzini, Priyankari Perali, Robin R. Murphy</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了首个用于无人机（sUAS）图像中建筑损坏自动评估的AI&#x2F;ML系统，并在联邦宣布的飓风灾害中实际部署，显著提升了灾害响应效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 灾害期间，无人机团队每天收集大量图像（47GB至369GB），远超专家手动处理能力，导致响应延迟。亟需自动化的计算机视觉和机器学习技术来解决这一问题。</p>
<p><strong>Result:</strong> 在飓风Debby和Helene响应中，模型在约18分钟内评估了415座建筑，显著提升了评估效率。</p>
<p><strong>Insight:</strong> 本研究证明了AI&#x2F;ML在灾害响应中的实际价值，提供了一套可扩展的自动化评估框架。</p>
<p><strong>Abstract:</strong> This paper presents the first AI&#x2F;ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI&#x2F;ML for damage assessment during a disaster and lessons learned to the benefit of the AI&#x2F;ML research and user communities.</p>
  </div>
</details>

<hr>
<h3 id="14-Finetuning-Free-Personalization-of-Text-to-Image-Generation-via-Hypernetworks-cs-CVPDF"><a href="#14-Finetuning-Free-Personalization-of-Text-to-Image-Generation-via-Hypernetworks-cs-CVPDF" class="headerlink" title="[14] Finetuning-Free Personalization of Text to Image Generation via Hypernetworks cs.CVPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03156">Finetuning-Free Personalization of Text to Image Generation via Hypernetworks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03156" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sagar Shrestha, Gopal Sharma, Luowei Zhou, Suren Kumar</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种无需微调的个性化文本到图像生成方法，通过超网络预测LoRA适应权重，避免了传统方法的高计算成本问题，并通过改进的训练目标和混合模型分类器自由引导（HM-CFG）提升了生成质量和组合泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统文本到图像扩散模型的个性化方法（如DreamBooth）依赖对每个主题的微调，计算成本高昂且推理速度慢。现有基于适配器或编码器的方法虽尝试降低开销，但仍需额外微调或依赖大型骨干模型。为此，论文探索了一种无需微调的超网络方法。</p>
<p><strong>Result:</strong> 在CelebA-HQ、AFHQ-v2和DreamBench上的实验表明，该方法在个性化生成性能上表现优异，同时保持了主题保真度和提示对齐。</p>
<p><strong>Insight:</strong> 超网络是一种可扩展的、高效的开放类别个性化方向，通过结合基础模型和个性化模型的优势，可以在不增加额外计算负担的情况下实现高质量生成。</p>
<p><strong>Abstract:</strong> Personalizing text-to-image diffusion models has traditionally relied on subject-specific fine-tuning approaches such as DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and slow at inference. Recent adapter- and encoder-based methods attempt to reduce this overhead but still depend on additional fine-tuning or large backbone models for satisfactory results. In this work, we revisit an orthogonal direction: fine-tuning-free personalization via Hypernetworks that predict LoRA-adapted weights directly from subject images. Prior hypernetwork-based approaches, however, suffer from costly data generation or unstable attempts to mimic base model optimization trajectories. We address these limitations with an end-to-end training objective, stabilized by a simple output regularization, yielding reliable and effective hypernetworks. Our method removes the need for per-subject optimization at test time while preserving both subject fidelity and prompt alignment. To further enhance compositional generalization at inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG), which combines the compositional strengths of the base diffusion model with the subject fidelity of personalized models during sampling. Extensive experiments on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves strong personalization performance and highlights the promise of hypernetworks as a scalable and effective direction for open-category personalization.</p>
  </div>
</details>

<hr>
<h3 id="15-Subsampled-Randomized-Fourier-GaLore-for-Adapting-Foundation-Models-in-Depth-Driven-Liver-Landmark-Segmentation-cs-CVPDF"><a href="#15-Subsampled-Randomized-Fourier-GaLore-for-Adapting-Foundation-Models-in-Depth-Driven-Liver-Landmark-Segmentation-cs-CVPDF" class="headerlink" title="[15] Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation cs.CVPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03163">Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03163" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yun-Chen Lin, Jiayuan Huang, Hanyuan Zhang, Sergi Kavtaradze, Matthew J. Clarkson</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合RGB和深度信息的肝脏标志分割框架，通过引入SRFT-GaLore方法高效调整大规模视觉模型以适应手术领域，并在跨数据集评估中表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 腹腔镜肝脏手术中2D视频流限制了深度感知，导致标志定位困难。现有方法在RGB和深度特征融合及大模型高效适配方面仍有挑战，需改进以适应实时手术环境。</p>
<p><strong>Result:</strong> 在L3D数据集上Dice相似系数提升4.85%，平均对称表面距离降低11.78点；在LLSD数据集上表现优于基线方法。</p>
<p><strong>Insight:</strong> SRFT-GaLore为高效调整大模型提供了新思路，双编码器融合RGB和深度信息显著提升了手术场景下的分割精度。</p>
<p><strong>Abstract:</strong> Accurate detection and delineation of anatomical structures in medical imaging are critical for computer-assisted interventions, particularly in laparoscopic liver surgery where 2D video streams limit depth perception and complicate landmark localization. While recent works have leveraged monocular depth cues for enhanced landmark detection, challenges remain in fusing RGB and depth features and in efficiently adapting large-scale vision models to surgical domains. We propose a depth-guided liver landmark segmentation framework integrating semantic and geometric cues via vision foundation encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB features and Depth Anything V2 (DA2) encoder to extract depth-aware features. To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient projection method that replaces the computationally expensive SVD with a Subsampled Randomized Fourier Transform (SRFT). This enables efficient fine-tuning of high-dimensional attention layers without sacrificing representational power. A cross-attention fusion module further integrates RGB and depth cues. To assess cross-dataset generalization, we also construct a new Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark. On the public L3D dataset, our method achieves a 4.85% improvement in Dice Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface Distance compared to the D2GPLand. To further assess generalization capability, we evaluate our model on LLSD dataset. Our model maintains competitive performance and significantly outperforms SAM-based baselines, demonstrating strong cross-dataset robustness and adaptability to unseen surgical environments. These results demonstrate that our SRFT-GaLore-enhanced dual-encoder framework enables scalable and precise segmentation under real-time, depth-constrained surgical settings.</p>
  </div>
</details>

<hr>
<h3 id="16-SurgAnt-ViVQA-Learning-to-Anticipate-Surgical-Events-through-GRU-Driven-Temporal-Cross-Attention-cs-CVPDF"><a href="#16-SurgAnt-ViVQA-Learning-to-Anticipate-Surgical-Events-through-GRU-Driven-Temporal-Cross-Attention-cs-CVPDF" class="headerlink" title="[16] SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention cs.CVPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03178">SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03178" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos</span></p>
<p><strong>TL;DR:</strong> 该论文提出了首个用于前瞻性手术推理的视觉问答（VQA）数据集PitVQA-Anticipation，并提出了一种名为SurgAnt-ViVQA的模型，通过GRU驱动的时序交叉注意力模块实现手术事件的预测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在有限的视野和快速变化的手术流程中，提前预测手术事件对于实时辅助至关重要。现有VQA系统仅基于静态帧进行推理，无法有效支持未来事件的预测。</p>
<p><strong>Result:</strong> 在PitVQA-Anticipation和EndoVis数据集上，SurgAnt-ViVQA超越了基于图像和视频的基线模型。时序建模和门控融合显著提升了性能。</p>
<p><strong>Insight:</strong> 时序建模和细粒度的门控交叉注意力是未来手术VQA系统的关键，8帧的输入在流畅性和预测性能之间取得了平衡。</p>
<p><strong>Abstract:</strong> Anticipating forthcoming surgical events is vital for real-time assistance in endonasal transsphenoidal pituitary surgery, where visibility is limited and workflow changes rapidly. Most visual question answering (VQA) systems reason on isolated frames with static vision language alignment, providing little support for forecasting next steps or instrument needs. Existing surgical VQA datasets likewise center on the current scene rather than the near future. We introduce PitVQA-Anticipation, the first VQA dataset designed for forward looking surgical reasoning. It comprises 33.5 hours of operative video and 734,769 question answer pairs built from temporally grouped clips and expert annotations across four tasks: predicting the future phase, next step, upcoming instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video language model that adapts a large language model using a GRU Gated Temporal Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics, while an adaptive gate injects visual context into the language stream at the token level. Parameter efficient fine tuning customizes the language backbone to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and EndoVis datasets, surpassing strong image and video based baselines. Ablations show that temporal recurrence and gated fusion drive most of the gains. A frame budget study indicates a trade-off: 8 frames maximize fluency, whereas 32 frames slightly reduce BLEU but improve numeric time estimation. By pairing a temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA advances surgical VQA from retrospective description to proactive anticipation. PitVQA-Anticipation offers a comprehensive benchmark for this setting and highlights the importance of targeted temporal modeling for reliable, future aware surgical assistance.</p>
  </div>
</details>

<hr>
<h3 id="17-PETWB-REP-A-Multi-Cancer-Whole-Body-FDG-PET-CT-and-Radiology-Report-Dataset-for-Medical-Imaging-Research-cs-CVPDF"><a href="#17-PETWB-REP-A-Multi-Cancer-Whole-Body-FDG-PET-CT-and-Radiology-Report-Dataset-for-Medical-Imaging-Research-cs-CVPDF" class="headerlink" title="[17] PETWB-REP: A Multi-Cancer Whole-Body FDG PET&#x2F;CT and Radiology Report Dataset for Medical Imaging Research cs.CVPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03194">PETWB-REP: A Multi-Cancer Whole-Body FDG PET&#x2F;CT and Radiology Report Dataset for Medical Imaging Research</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03194" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Le Xue, Gang Feng, Wenbo Zhang, Yichi Zhang, Lanlan Li</span></p>
<p><strong>TL;DR:</strong> PETWB-REP是一个公开的多癌种全身FDG PET&#x2F;CT及放射学报告数据集，包含490名患者的影像与报告，旨在支持医学影像、放射组学和多模态学习研究。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有医学影像数据集较少结合功能与解剖影像及详细临床报告，限制了医学影像AI研究的发展。</p>
<p><strong>Result:</strong> 构建了一个包含多种常见癌症的数据集，为医学影像和多模态学习研究提供了资源。</p>
<p><strong>Insight:</strong> 该数据集支持功能与解剖影像的结合分析，有助于推动医学影像AI和多模态学习的进步。</p>
<p><strong>Abstract:</strong> Publicly available, large-scale medical imaging datasets are crucial for developing and validating artificial intelligence models and conducting retrospective clinical research. However, datasets that combine functional and anatomical imaging with detailed clinical reports across multiple cancer types remain scarce. Here, we present PETWB-REP, a curated dataset comprising whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography&#x2F;Computed Tomography (PET&#x2F;CT) scans and corresponding radiology reports from 490 patients diagnosed with various malignancies. The dataset primarily includes common cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and ovarian cancer. This dataset includes paired PET and CT images, de-identified textual reports, and structured clinical metadata. It is designed to support research in medical imaging, radiomics, artificial intelligence, and multi-modal learning.</p>
  </div>
</details>

<hr>
<h3 id="18-QG-CoC-Question-Guided-Chain-of-Captions-for-Large-Multimodal-Models-cs-CV-cs-AI-cs-LGPDF"><a href="#18-QG-CoC-Question-Guided-Chain-of-Captions-for-Large-Multimodal-Models-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[18] QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models cs.CV | cs.AI | cs.LGPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03206">QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03206" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kuei-Chun Kao, Hsu Tzu-Yin, Yunqi Hong, Ruochen Wang, Cho-Jui Hsieh</span></p>
<p><strong>TL;DR:</strong> 论文提出了QG-CoC方法，通过问题引导的链式描述（Chain-of-Captions），解决了多模态大语言模型在处理多图像任务时的细粒度感知和信息整合问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有多模态大语言模型在多图像场景中缺乏细粒度感知和有效推理能力，且现有提示方法多局限于单图像场景，无法解决复杂的多图像任务。</p>
<p><strong>Result:</strong> 实验表明QG-CoC在单图像和多图像基准测试中均表现优异，尤其在现有方法失败的挑战性场景中表现稳健。</p>
<p><strong>Insight:</strong> 问题引导的链式描述可以有效弥补多模态模型在多图像任务中的感知与推理不足。</p>
<p><strong>Abstract:</strong> Recently, Multimodal Large Language Models (MLLMs) encounter two key issues in multi-image contexts: (1) a lack of fine-grained perception across disparate images, and (2) a diminished capability to effectively reason over and synthesize information from multiple visual inputs. However, while various prompting methods aim to describe visual content, many existing studies focus primarily on single-image settings or specific, constrained scenarios. This leaves a critical gap in understanding and addressing how MLLMs tackle more general and complex multi-image reasoning tasks. Thus, we first extensively investigate how current prompting methods perceive fine-grained visual details and process visual information when dealing with multiple images. Our findings reveal that existing prompting methods fall short in attending to needed clues and seamlessly integrating perception and reasoning. Inspired by the findings, we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions (QG-CoC), a generalized prompting approach that effectively handles problems with an arbitrary number of images. We evaluate our method on various open-source and closed-source MLLMs for multi-image and single-image benchmarks. Experimental results indicate that QG-CoC demonstrates competitive performance across tasks and exhibits robust improvements in the challenging scenarios where existing prompting methods fail.</p>
  </div>
</details>

<hr>
<h3 id="19-MvBody-Multi-View-Based-Hybrid-Transformer-Using-Optical-3D-Body-Scan-for-Explainable-Cesarean-Section-Prediction-cs-CV-68T10-68T45PDF"><a href="#19-MvBody-Multi-View-Based-Hybrid-Transformer-Using-Optical-3D-Body-Scan-for-Explainable-Cesarean-Section-Prediction-cs-CV-68T10-68T45PDF" class="headerlink" title="[19] MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction cs.CV | 68T10, 68T45PDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03212">MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | 68T10, 68T45</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03212" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruting Cheng, Boyuan Feng, Yijiang Zheng, Chuhui Qiu, Aizierjiang Aiersilan</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于多视角Transformer网络MvBody的方法，利用3D光学身体扫描数据预测剖宫产风险，结合自报告医疗数据，实现高精度和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在医疗资源有限的地区，剖宫产风险预测对产前护理的早期决策至关重要。现有模型依赖院内参数，难以推广到资源有限的环境，因此探索使用3D体形数据作为替代方案。</p>
<p><strong>Result:</strong> 模型在独立测试集上达到84.62%准确率和0.724 AUC-ROC，优于现有机器学习方法和3D分析技术。</p>
<p><strong>Insight:</strong> 预孕体重、产妇年龄、产科史、既往剖宫产史及头肩部体形是剖宫产风险的关键预测因素，验证了模型的临床相关性。</p>
<p><strong>Abstract:</strong> Accurately assessing the risk of cesarean section (CS) delivery is critical, especially in settings with limited medical resources, where access to healthcare is often restricted. Early and reliable risk prediction allows better-informed prenatal care decisions and can improve maternal and neonatal outcomes. However, most existing predictive models are tailored for in-hospital use during labor and rely on parameters that are often unavailable in resource-limited or home-based settings. In this study, we conduct a pilot investigation to examine the feasibility of using 3D body shape for CS risk assessment for future applications with more affordable general devices. We propose a novel multi-view-based Transformer network, MvBody, which predicts CS risk using only self-reported medical data and 3D optical body scans obtained between the 31st and 38th weeks of gestation. To enhance training efficiency and model generalizability in data-scarce environments, we incorporate a metric learning loss into the network. Compared to widely used machine learning models and the latest advanced 3D analysis methods, our method demonstrates superior performance, achieving an accuracy of 84.62% and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set. To improve transparency and trust in the model’s predictions, we apply the Integrated Gradients algorithm to provide theoretically grounded explanations of the model’s decision-making process. Our results indicate that pre-pregnancy weight, maternal age, obstetric history, previous CS history, and body shape, particularly around the head and shoulders, are key contributors to CS risk prediction.</p>
  </div>
</details>

<hr>
<h3 id="20-Diffusion-Guided-Mask-Consistent-Paired-Mixing-for-Endoscopic-Image-Segmentation-cs-CVPDF"><a href="#20-Diffusion-Guided-Mask-Consistent-Paired-Mixing-for-Endoscopic-Image-Segmentation-cs-CVPDF" class="headerlink" title="[20] Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation cs.CVPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03219">Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03219" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pengyu Jie, Wanquan Liu, Rui He, Yihui Wen, Deyu Meng</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合扩散合成和样本混合的方法（MCPMix），通过生成与真实图像相同掩码的合成图像对，并在混合时保持掩码一致性，同时引入自适应调整策略（RLA）优化训练过程，提升了内窥镜图像分割的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的数据增强方法（如样本混合或扩散合成）分别存在标签模糊和域偏移的问题，无法同时兼顾多样性和语义一致性。论文试图结合两种方法的优势，解决这些挑战。</p>
<p><strong>Result:</strong> 在多个数据集（Kvasir-SEG、PICCOLO等）上实现了SOTA分割性能，证明了方法的有效性和通用性。</p>
<p><strong>Insight:</strong> 结合扩散合成的多样性和样本混合的鲁棒性，同时通过自适应策略保持与真实数据的对齐，是实现高性能医学图像分割的关键。</p>
<p><strong>Abstract:</strong> Augmentation for dense prediction typically relies on either sample mixing or generative synthesis. Mixing improves robustness but misaligned masks yield soft label ambiguity. Diffusion synthesis increases apparent diversity but, when trained as common samples, overlooks the structural benefit of mask conditioning and introduces synthetic-real domain shift. We propose a paired, diffusion-guided paradigm that fuses the strengths of both. For each real image, a synthetic counterpart is generated under the same mask and the pair is used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which mixes only image appearance while supervision always uses the original hard mask. This produces a continuous family of intermediate samples that smoothly bridges synthetic and real appearances under shared geometry, enlarging diversity without compromising pixel-level semantics. To keep learning aligned with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the mixing strength and the loss weight of mixed samples over training, gradually re-anchoring optimization to real data and mitigating distributional bias. Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC 2017, the approach achieves state-of-the-art segmentation performance and consistent gains over baselines. The results show that combining label-preserving mixing with diffusion-driven diversity, together with adaptive re-anchoring, yields robust and generalizable endoscopic segmentation.</p>
  </div>
</details>

<hr>
<h3 id="21-Generative-deep-learning-for-foundational-video-translation-in-ultrasound-cs-CV-cs-AIPDF"><a href="#21-Generative-deep-learning-for-foundational-video-translation-in-ultrasound-cs-CV-cs-AIPDF" class="headerlink" title="[21] Generative deep learning for foundational video translation in ultrasound cs.CV | cs.AIPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03255">Generative deep learning for foundational video translation in ultrasound</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03255" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nikolina Tomic Roshni Bhatnagar, Sarthak Jain, Connor Lau, Tien-Yu Liu, Laura Gambini</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种生成式深度学习方法，用于超声成像中子模态（如灰度与彩色多普勒）之间的视频翻译，解决了数据不平衡和缺失问题。方法结合像素级、对抗性和感知损失，通过两个网络实现高保真合成，结果接近真实数据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 超声成像中子模态数据不平衡和缺失问题限制了深度学习在医学影像中的应用。生成式方法可以平衡数据集，但超声子模态间的翻译仍具挑战性。</p>
<p><strong>Result:</strong> 1. 合成视频与真实视频的结构相似性指数（SSIM）平均为0.91±0.04；2. 合成数据在分类（F1分数0.89）和分割（Dice分数0.97）中表现接近真实数据；3. 临床专家区分合成与真实视频的准确率仅为54±6%。</p>
<p><strong>Insight:</strong> 1. 尽管仅基于心脏视频训练，模型在多个临床领域的超声数据中表现良好，展示了基础能力；2. 生成式方法可扩展回顾性影像数据的实用性，丰富医学影像数据集设计工具箱。</p>
<p><strong>Abstract:</strong> Deep learning (DL) has the potential to revolutionize image acquisition and interpretation across medicine, however, attention to data imbalance and missingness is required. Ultrasound data presents a particular challenge because in addition to different views and structures, it includes several sub-modalities-such as greyscale and color flow doppler (CFD)-that are often imbalanced in clinical studies. Image translation can help balance datasets but is challenging for ultrasound sub-modalities to date. Here, we present a generative method for ultrasound CFD-greyscale video translation, trained on 54,975 videos and tested on 8,368. The method developed leveraged pixel-wise, adversarial, and perceptual loses and utilized two networks: one for reconstructing anatomic structures and one for denoising to achieve realistic ultrasound imaging. Average pairwise SSIM between synthetic videos and ground truth was 0.91+&#x2F;-0.04. Synthetic videos performed indistinguishably from real ones in DL classification and segmentation tasks and when evaluated by blinded clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice score between real and synthetic segmentation was 0.97. Overall clinician accuracy in distinguishing real vs synthetic videos was 54+&#x2F;-6% (42-61%), indicating realistic synthetic videos. Although trained only on heart videos, the model worked well on ultrasound spanning several clinical domains (average SSIM 0.91+&#x2F;-0.05), demonstrating foundational abilities. Together, these data expand the utility of retrospectively collected imaging and augment the dataset design toolbox for medical imaging.</p>
  </div>
</details>

<hr>
<h3 id="22-Enhancing-Medical-Image-Segmentation-via-Heat-Conduction-Equation-cs-CVPDF"><a href="#22-Enhancing-Medical-Image-Segmentation-via-Heat-Conduction-Equation-cs-CVPDF" class="headerlink" title="[22] Enhancing Medical Image Segmentation via Heat Conduction Equation cs.CVPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03260">Enhancing Medical Image Segmentation via Heat Conduction Equation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03260" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rong Wu, Yim-Sang Yu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合U-Mamba和热传导方程的新架构，用于医学图像分割，通过状态空间模块和热传导算子提升长距离依赖建模能力，实验证明其在多模态腹部CT和MRI数据集上优于基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于深度学习的医学图像分割方法（如U-Net变体）在全局上下文建模和长距离依赖推理上效率不足，尤其是在计算资源有限的情况下。因此，需一种更高效且可扩展的解决方案。</p>
<p><strong>Result:</strong> 在多模态腹部CT和MRI数据集上超越基线方法，验证了模型的有效性和泛化能力。</p>
<p><strong>Insight:</strong> 融合状态空间动态和基于热扩散的全局建模为医学图像分割提供了一种可扩展且可解释的解决方案。</p>
<p><strong>Abstract:</strong> Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.</p>
  </div>
</details>

<hr>
<h3 id="23-Unified-Long-Video-Inpainting-and-Outpainting-via-Overlapping-High-Order-Co-Denoising-cs-CVPDF"><a href="#23-Unified-Long-Video-Inpainting-and-Outpainting-via-Overlapping-High-Order-Co-Denoising-cs-CVPDF" class="headerlink" title="[23] Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising cs.CVPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03272">Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03272" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuangquan Lyu, Steven Mao, Yue Ma</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种统一的长视频修复与外延方法，通过重叠高阶共去噪技术实现高质量的长视频编辑。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生成高质量的长视频具有挑战性，尤其是在视频修复和外延的可控性方面。现有方法通常难以处理长视频的一致性问题或会产生拼接痕迹。</p>
<p><strong>Result:</strong> 在长视频修复和外延任务中，表现优于Wan 2.1和VACE基线方法，PSNR&#x2F;SSIM和LPIPS指标均更优。</p>
<p><strong>Insight:</strong> 结合高效参数微调和高阶共去噪策略，能够在不显著增加计算开销的情况下实现高质量的长视频编辑。</p>
<p><strong>Abstract:</strong> Generating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba’s Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting&#x2F;outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR&#x2F;SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.</p>
  </div>
</details>

<hr>
<h3 id="24-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models-cs-CVPDF"><a href="#24-Diffusion-SDPO-Safeguarded-Direct-Preference-Optimization-for-Diffusion-Models-cs-CVPDF" class="headerlink" title="[24] Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models cs.CVPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03317">Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03317" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minghao Fu, Guo-Hua Wang, Tianyu Cui, Qing-Guo Chen, Zhao Xu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为Diffusion-SDPO的新方法，针对扩散模型在直接偏好优化（DPO）中存在的偏好边界扩大问题，通过自适应缩放损失分支的梯度来保护优势分支，从而提升生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 文本到图像扩散模型虽然能生成高质量图像，但在与人类偏好对齐方面仍存在挑战。标准的Diffusion-DPO方法在扩大偏好边界时可能导致生成质量下降，甚至影响优势分支的输出质量。</p>
<p><strong>Result:</strong> 实验表明，Diffusion-SDPO在标准文本到图像基准测试中，在自动化偏好、美学和提示对齐指标上均优于基线方法。</p>
<p><strong>Insight:</strong> 通过保护优势分支并动态调整损失分支梯度，可以在优化偏好对齐的同时避免生成质量下降。</p>
<p><strong>Abstract:</strong> Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/AIDC-AI/Diffusion-SDPO">https://github.com/AIDC-AI/Diffusion-SDPO</a>.</p>
  </div>
</details>

<hr>
<h3 id="25-SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding-cs-CVPDF"><a href="#25-SurgViVQA-Temporally-Grounded-Video-Question-Answering-for-Surgical-Scene-Understanding-cs-CVPDF" class="headerlink" title="[25] SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding cs.CVPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03325">SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03325" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena</span></p>
<p><strong>TL;DR:</strong> SurgViVQA 是一个针对手术场景动态理解的新型视频问答模型，通过结合视频与文本特征，捕捉时间动态信息，显著提升现有基于图像的VQA模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的手术视频问答模型多基于静态图像特征，缺乏对时间动态信息的捕捉，而手术场景的理解高度依赖连续动作和工具-组织交互等动态信息。</p>
<p><strong>Result:</strong> 在REAL-Colon-VQA和EndoVis18-VQA数据集上，SurgViVQA性能优于PitVQA模型，关键词准确率分别提升11%和9%。扰动测试显示模型对问题表达的鲁棒性更强。</p>
<p><strong>Insight:</strong> 时间动态信息对手术场景理解至关重要，SurgViVQA通过结合视频与文本特征，为动态手术视频问答提供了有效框架。</p>
<p><strong>Abstract:</strong> Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video–Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool–tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11% on REAL-Colon-VQA and +9% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at <a target="_blank" rel="noopener" href="https://github.com/madratak/SurgViVQA">https://github.com/madratak/SurgViVQA</a>.</p>
  </div>
</details>

<hr>
<h3 id="26-Multi-Object-Tracking-Retrieval-with-LLaVA-Video-A-Training-Free-Solution-to-MOT25-StAG-Challenge-cs-CVPDF"><a href="#26-Multi-Object-Tracking-Retrieval-with-LLaVA-Video-A-Training-Free-Solution-to-MOT25-StAG-Challenge-cs-CVPDF" class="headerlink" title="[26] Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge cs.CVPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03332">Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03332" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi Yang, Yiming Xu, Timo Kaiser, Hao Cheng, Bodo Rosenhahn</span></p>
<p><strong>TL;DR:</strong> 本文提出一种无需训练的两阶段方法，结合FastTracker和LLaVA-Video，在MOT25-StAG挑战中实现多目标跟踪与检索任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决复杂场景中基于语言查询的多目标跟踪问题，提出一种无需训练的解决方案。</p>
<p><strong>Result:</strong> 在MOT25-StAG测试集上m-HIoU和HOTA分别达到20.68和10.73。</p>
<p><strong>Insight:</strong> 跨模态大语言模型可有效用于视频检索任务，零样本方法在复杂场景中具备潜力。</p>
<p><strong>Abstract:</strong> In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.</p>
  </div>
</details>

<hr>
<h3 id="27-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions-cs-CVPDF"><a href="#27-UniAVGen-Unified-Audio-and-Video-Generation-with-Asymmetric-Cross-Modal-Interactions-cs-CVPDF" class="headerlink" title="[27] UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions cs.CVPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03334">UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03334" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guozhen Zhang, Zixiang Zhou, Teng Hu, Ziqiao Peng, Youliang Zhang</span></p>
<p><strong>TL;DR:</strong> UniAVGen提出了一种统一的音频和视频生成框架，通过非对称跨模态交互机制解决了现有方法在唇部同步和语义一致性上的不足，并在多个任务中表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有开源音频-视频生成方法缺乏有效的跨模态建模，导致唇部同步和语义一致性不足，因此需要一种更高效的统一框架。</p>
<p><strong>Result:</strong> 在1.3M训练样本（远少于30.1M）下，UniAVGen在音频-视频同步、音色一致性和情感一致性上表现优越。</p>
<p><strong>Insight:</strong> 1. 非对称跨模态交互机制可显著提升多模态任务的同步性和语义一致性；2. 动态调制和引导策略是提升生成质量的关键。</p>
<p><strong>Abstract:</strong> Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen’s robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.</p>
  </div>
</details>

<hr>
<h3 id="28-Decoupling-Augmentation-Bias-in-Prompt-Learning-for-Vision-Language-Models-cs-CV-cs-AI-cs-LGPDF"><a href="#28-Decoupling-Augmentation-Bias-in-Prompt-Learning-for-Vision-Language-Models-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[28] Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models cs.CV | cs.AI | cs.LGPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03367">Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03367" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gahyeon Kim, Sohee Kim, Seokju Lee</span></p>
<p><strong>TL;DR:</strong> 论文提出了AAPL方法，通过解耦图像增广引入的视觉变化与类别相关的语义表征，提升提示学习在零样本和少样本任务中的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有提示学习方法（如CoOp和CoCoOp）主要关注文本修改，忽视了图像增广的潜力，且在未见过的类别上泛化能力有限。</p>
<p><strong>Result:</strong> 在11个基准数据集上，AAPL在少样本、零样本、跨数据集和领域泛化任务中均优于现有方法。</p>
<p><strong>Insight:</strong> 图像增广与提示学习的结合可以有效提升模型泛化能力，关键在于解耦增广带来的噪声与类别语义。</p>
<p><strong>Abstract:</strong> Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Gahyeonkim09/AAPL">https://github.com/Gahyeonkim09/AAPL</a></p>
  </div>
</details>

<hr>
<h3 id="29-Robust-Alignment-of-the-Human-Embryo-in-3D-Ultrasound-using-PCA-and-an-Ensemble-of-Heuristic-Atlas-based-and-Learning-based-Classifiers-Evaluated-on-the-Rotterdam-Periconceptional-Cohort-cs-CV-I-4PDF"><a href="#29-Robust-Alignment-of-the-Human-Embryo-in-3D-Ultrasound-using-PCA-and-an-Ensemble-of-Heuristic-Atlas-based-and-Learning-based-Classifiers-Evaluated-on-the-Rotterdam-Periconceptional-Cohort-cs-CV-I-4PDF" class="headerlink" title="[29] Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort cs.CV | I.4PDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03416">Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | I.4</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03416" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nikolai Herrmann, Marcella C. Zijta, Stefan Klein, Régine P. M. Steegers-Theunissen, Rene M. H. Wijnen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种自动化方法，通过PCA提取胚胎主轴并结合启发式、图谱对齐和分类器策略，实现了三维超声图像中胚胎的标准化对齐，显著提高了对齐准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 胚胎在三维超声图像中的标准化对齐有助于临床生长监测和标准化平面检测，但目前缺乏高效、准确的自动化方法。</p>
<p><strong>Result:</strong> 在99.0%的图像中，PCA成功提取主轴；三种策略分别实现97.4%、95.8%和98.4%的正确选择率；多数投票进一步提升至98.5%。</p>
<p><strong>Insight:</strong> 结合多种策略（启发式+图谱+学习）能显著提升对齐任务的鲁棒性和准确性，为临床和研究提供了可扩展的自动化工具。</p>
<p><strong>Abstract:</strong> Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo’s principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson’s correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: <a target="_blank" rel="noopener" href="https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment">https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment</a>.</p>
  </div>
</details>

<hr>
<h3 id="30-A-Lightweight-3D-CNN-for-Event-Based-Human-Action-Recognition-with-Privacy-Preserving-Potential-cs-CVPDF"><a href="#30-A-Lightweight-3D-CNN-for-Event-Based-Human-Action-Recognition-with-Privacy-Preserving-Potential-cs-CVPDF" class="headerlink" title="[30] A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential cs.CVPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03665">A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03665" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mehdi Sefidgar Dilmaghani, Francis Fowley, Peter Corcoran</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种轻量化的3D-CNN，用于基于事件视觉数据的人类动作识别，通过事件相机保护隐私，同时实现了高精度（F1-score 0.9415）和高效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统帧相机在监控中可能泄露个人隐私信息，而事件相机仅记录像素强度变化，具有天然的隐私保护性。因此，研究如何利用事件相机开发高效、隐私保护的人类动作识别系统具有重要意义。</p>
<p><strong>Result:</strong> 模型在测试中表现优异，F1-score达到0.9415，整体准确率为94.17%，超越了C3D、ResNet3D等基准模型。</p>
<p><strong>Insight:</strong> 事件相机结合轻量化深度学习模型，不仅保护隐私，还能实现高效的动作识别，为边缘计算场景提供了新思路。</p>
<p><strong>Abstract:</strong> This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.</p>
  </div>
</details>

<hr>
<h3 id="31-Part-Aware-Bottom-Up-Group-Reasoning-for-Fine-Grained-Social-Interaction-Detection-cs-CVPDF"><a href="#31-Part-Aware-Bottom-Up-Group-Reasoning-for-Fine-Grained-Social-Interaction-Detection-cs-CVPDF" class="headerlink" title="[31] Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection cs.CVPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03666">Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03666" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongkeun Kim, Minsu Cho, Suha Kwak</span></p>
<p><strong>TL;DR:</strong> 提出了一种基于局部感知的从下至上群体推理框架，用于细粒度社交交互检测，通过考虑身体局部特征和人际关系的细微社交线索，显著提升了社交群体推断的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有社交交互检测方法忽略细微的社交线索（如表情、注视和手势），并依赖个体的整体表示，导致无法捕捉局部社交信号和在群体配置推断中引入模糊性。</p>
<p><strong>Result:</strong> 在NVI数据集上表现优于现有方法，达到新的SOTA。</p>
<p><strong>Insight:</strong> 局部特征和细微社交线索对社交交互检测至关重要，从下至上的推理能更准确地建模群体配置。</p>
<p><strong>Abstract:</strong> Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.</p>
  </div>
</details>

<hr>
<h3 id="32-Disentangled-Concepts-Speak-Louder-Than-Words-Explainable-Video-Action-Recognition-cs-CVPDF"><a href="#32-Disentangled-Concepts-Speak-Louder-Than-Words-Explainable-Video-Action-Recognition-cs-CVPDF" class="headerlink" title="[32] Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition cs.CVPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03725">Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03725" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jongseo Lee, Wooil Lee, Gyeong-Moon Park, Seong Tae Kim, Jinwoo Choi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于解耦概念的视频动作识别框架DANCE，通过分离运动动态、物体和场景的概念，提高了模型解释的清晰度，并在多个数据集上验证了其性能和实用性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频动作识别解释方法（如显著性）往往将运动和空间上下文纠缠在一起，导致解释不清晰。语言驱动的方法则难以解释直觉性的运动。论文旨在解决这些问题。</p>
<p><strong>Result:</strong> 在四个数据集上的实验表明，DANCE显著提升了解释清晰度且性能竞争性强。用户研究验证了其优越的可解释性，并展示了其在模型调试、编辑和失败分析中的实用价值。</p>
<p><strong>Insight:</strong> 解耦概念设计不仅提升了模型的可解释性，还为模型的实际应用（如调试和编辑）提供了新的可能性。</p>
<p><strong>Abstract:</strong> Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature – intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets – KTH, Penn Action, HAA500, and UCF-101 – demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="33-Automatic-Machine-Translation-Detection-Using-a-Surrogate-Multilingual-Translation-Model-cs-CL-cs-LGPDF"><a href="#33-Automatic-Machine-Translation-Detection-Using-a-Surrogate-Multilingual-Translation-Model-cs-CL-cs-LGPDF" class="headerlink" title="[33] Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model cs.CL | cs.LGPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02958">Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02958" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cristian García-Romero, Miquel Esplà-Gomis, Felipe Sánchez-Martínez</span></p>
<p><strong>TL;DR:</strong> 提出了一种新的方法，利用多语言机器翻译模型的内部表示来区分人工翻译和机器翻译的句子，显著提升了检测准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目前机器翻译系统依赖大规模平行语料库，但这些语料中可能混杂机器翻译的内容，影响翻译质量。因此需要有效过滤非人工翻译文本。</p>
<p><strong>Result:</strong> 在非英语语言对上，该方法比现有技术至少提升5%的准确率。</p>
<p><strong>Insight:</strong> 机器翻译模型的内部表示蕴含了区分人工与机器翻译的重要信息，可用于优化翻译检测任务。</p>
<p><strong>Abstract:</strong> Modern machine translation (MT) systems depend on large parallel corpora, often collected from the Internet. However, recent evidence indicates that (i) a substantial portion of these texts are machine-generated translations, and (ii) an overreliance on such synthetic content in training data can significantly degrade translation quality. As a result, filtering out non-human translations is becoming an essential pre-processing step in building high-quality MT systems. In this work, we propose a novel approach that directly exploits the internal representations of a surrogate multilingual MT model to distinguish between human and machine-translated sentences. Experimental results show that our method outperforms current state-of-the-art techniques, particularly for non-English language pairs, achieving gains of at least 5 percentage points of accuracy.</p>
  </div>
</details>

<hr>
<h3 id="34-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation-cs-CLPDF"><a href="#34-LEGO-Eval-Towards-Fine-Grained-Evaluation-on-Synthesizing-3D-Embodied-Environments-with-Tool-Augmentation-cs-CLPDF" class="headerlink" title="[34] LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation cs.CLPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03001">LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03001" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gyeom Hwangbo, Hyungjoo Chae, Minseok Kang, Hyeonjong Ju, Soohyun Oh</span></p>
<p><strong>TL;DR:</strong> LEGO-Eval是一个用于评估3D场景合成质量的框架，通过工具增强实现细粒度对齐评估，揭示了当前生成方法的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于LLM的3D场景生成方法因指令粒度不足，导致生成场景与现实环境不符，影响具身智能体的训练效果。</p>
<p><strong>Result:</strong> LEGO-Eval在评估场景-指令对齐任务中F1分数比VLM-as-a-judge高0.41，现有方法在LEGO-Bench上的完全对齐成功率仅为10%。</p>
<p><strong>Insight:</strong> 细粒度指令和显式对齐工具是评估和改进3D场景生成质量的关键。</p>
<p><strong>Abstract:</strong> Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.</p>
  </div>
</details>

<hr>
<h3 id="35-Targeted-Error-Correction-in-Knowledge-Distillation-Small-Language-Models-Surpass-GPT-cs-CLPDF"><a href="#35-Targeted-Error-Correction-in-Knowledge-Distillation-Small-Language-Models-Surpass-GPT-cs-CLPDF" class="headerlink" title="[35] Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT cs.CLPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03005">Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03005" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hee-Jin Lee, Zhen Guo, Luchao Jin, Morteza Moazami Goudarzi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种Analyze-Revise-Finetune（ARF）流程，通过针对性纠错，使得较小的开源语言模型在客服摘要任务中超越GPT-3.5等大型商业模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的知识蒸馏方法通常依赖大型教师模型生成训练数据，但难以纠正其中的错误。ARF流程旨在解决这一问题，提升小型模型性能，同时兼顾成本和数据隐私。</p>
<p><strong>Result:</strong> 学生模型在摘要任务中超越了GPT-3.5，同时显著降低了计算成本和数据隐私风险。</p>
<p><strong>Insight:</strong> 通过针对性修正教师模型的错误，小型模型可以实现超越大型模型的性能，为开源模型的应用提供了可行路径。</p>
<p><strong>Abstract:</strong> We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller open-source language models (LLMs) to surpass substantially larger proprietary models in customer service summarization tasks. The pipeline first analyzes and categorizes common errors in summaries produced by a teacher model (GPT-3.5), then performs a targeted revision using a compact editor model (Llama 3.1 70B) to generate high-quality, refined training data. Fine-tuning a smaller student model (Llama 3.1 8B) on this refined data resulted in superior summarization performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and data privacy while maintaining competitive accuracy, illustrating a generalizable framework for enhancing open-source LLMs across diverse downstream applications.</p>
  </div>
</details>

<hr>
<h3 id="36-Data-Efficient-Adaptation-and-a-Novel-Evaluation-Method-for-Aspect-based-Sentiment-Analysis-cs-CL-cs-LGPDF"><a href="#36-Data-Efficient-Adaptation-and-a-Novel-Evaluation-Method-for-Aspect-based-Sentiment-Analysis-cs-CL-cs-LGPDF" class="headerlink" title="[36] Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis cs.CL | cs.LGPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03034">Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03034" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yan Cathy Hua, Paul Denny, Jörg Wicker, Katerina Taškova</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新的评估方法FTS-OBP，解决了传统ABSA评估中边界变化被过度惩罚的问题，并通过研究小型生成语言模型（SLMs）的数据高效适应方法，展示了在低资源场景下的显著性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前ABSA研究集中在商业领域，而教育和医疗等高需求低资源领域的研究和资源匮乏。此外，传统评估方法对边界变化过于严格，生成模型的性能难以准确评估。</p>
<p><strong>Result:</strong> 1.5-3.8B参数的SLMs性能超过专有大模型，接近基准结果；FTS-OBP与传统指标强相关且提供更灵活的评估。</p>
<p><strong>Insight:</strong> 小型生成模型通过数据高效适应策略可在低资源场景下表现优异；灵活的评估方法更贴合实际需求，推动ABSA在非商业领域的发展。</p>
<p><strong>Abstract:</strong> Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining approach that identifies and classifies opinions associated with specific entities (aspects) or their categories within a sentence. Despite its rapid growth and broad potential, ABSA research and resources remain concentrated in commercial domains, leaving analytical needs unmet in high-demand yet low-resource areas such as education and healthcare. Domain adaptation challenges and most existing methods’ reliance on resource-intensive in-training knowledge injection further hinder progress in these areas. Moreover, traditional evaluation methods based on exact matches are overly rigid for ABSA tasks, penalising any boundary variations which may misrepresent the performance of generative models. This work addresses these gaps through three contributions: 1) We propose a novel evaluation method, Flexible Text Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates realistic extraction boundary variations while maintaining strong correlation with traditional metrics and offering fine-grained diagnostics. 2) We present the first ABSA study of small decoder-only generative language models (SLMs; &lt;7B parameters), examining resource lower bounds via a case study in education review ABSA. We systematically explore data-free (in-context learning and weight merging) and data-light fine-tuning methods, and propose a multitask fine-tuning strategy that significantly enhances SLM performance, enabling 1.5-3.8 B models to surpass proprietary large models and approach benchmark results with only 200-1,000 examples on a single GPU. 3) We release the first public set of education review ABSA resources to support future research in low-resource domains.</p>
  </div>
</details>

<hr>
<h3 id="37-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity-cs-CLPDF"><a href="#37-MME-CC-A-Challenging-Multi-Modal-Evaluation-Benchmark-of-Cognitive-Capacity-cs-CLPDF" class="headerlink" title="[37] MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity cs.CLPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03146">MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03146" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了名为MME-CC的多模态评估基准，用于系统性评估多模态大语言模型（MLLMs）在视觉为中心的认知能力方面的表现，并通过实验揭示了当前模型的局限性和常见错误模式。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有多模态基准大多过度关注文本推理，未能系统性评估视觉为中心的认知行为，导致MLLMs的认知能力评估不足。</p>
<p><strong>Result:</strong> 实验显示闭源模型（如Gemini-2.5-Pro）整体表现领先，但空间和几何推理能力普遍较弱（≤30%）。发现常见的错误模式包括方向错误、跨视图身份识别问题等。</p>
<p><strong>Insight:</strong> Chain-of-Thought（思维链）通常遵循提取→推理→验证的三阶段过程，但严重依赖视觉提取。研究呼吁将MLLMs的认知能力作为评估和模型设计的核心。</p>
<p><strong>Abstract:</strong> As reasoning models scale rapidly, the essential role of multimodality in human cognition has come into sharp relief, driving a growing need to probe vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either overemphasize textual reasoning or fall short of systematically capturing vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs insufficiently assessed. To address this limitation, we introduce MME-CC (Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded benchmark that organizes 11 representative reasoning tasks into three fundamental categories of visual information: spatial, geometric, and knowledge-based reasoning, and provides fine-grained analyses of MLLMs’ cognitive capacity across these dimensions. Based on MME-CC, we conduct extensive experiments over 16 representative MLLMs. Our study reveals that closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs. 30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak (less than or equal to 30%). We further identify common error patterns, including orientation mistakes, fragile cross-view identity persistence, and poor adherence to counterfactual instructions, and observe that Chain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt; verify) with heavy reliance on visual extraction. We hope this work catalyzes a shift toward treating the cognitive capacity of MLLMs as central to both evaluation and model design.</p>
  </div>
</details>

<hr>
<h3 id="38-Who-Sees-the-Risk-Stakeholder-Conflicts-and-Explanatory-Policies-in-LLM-based-Risk-Assessment-cs-CL-cs-AIPDF"><a href="#38-Who-Sees-the-Risk-Stakeholder-Conflicts-and-Explanatory-Policies-in-LLM-based-Risk-Assessment-cs-CL-cs-AIPDF" class="headerlink" title="[38] Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment cs.CL | cs.AIPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03152">Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03152" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Srishti Yadav, Jasmina Gajcin, Erik Miehling, Elizabeth Daly</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一个基于LLM的利益相关者风险评估框架，通过Risk Atlas Nexus和GloVE解释方法生成可解释的政策，揭示不同利益相关者对风险的分歧与共识，并在三个实际用例中验证了方法的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了负责任地部署AI系统，理解不同利益相关者对风险的认知差异至关重要。</p>
<p><strong>Result:</strong> 结果表明，利益相关者的视角显著影响风险感知和冲突模式。</p>
<p><strong>Insight:</strong> 利益相关者感知的透明解释是LLM评估透明化和符合以人为本AI治理目标的关键。</p>
<p><strong>Abstract:</strong> Understanding how different stakeholders perceive risks in AI systems is essential for their responsible deployment. This paper presents a framework for stakeholder-grounded risk assessment by using LLMs, acting as judges to predict and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our framework generates stakeholder-specific, interpretable policies that shows how different stakeholders agree or disagree about the same risks. We demonstrate our method using three real-world AI use cases of medical AI, autonomous vehicles, and fraud detection domain. We further propose an interactive visualization that reveals how and why conflicts emerge across stakeholder perspectives, enhancing transparency in conflict reasoning. Our results show that stakeholder perspectives significantly influence risk perception and conflict patterns. Our work emphasizes the importance of these stakeholder-aware explanations needed to make LLM-based evaluations more transparent, interpretable, and aligned with human-centered AI governance goals.</p>
  </div>
</details>

<hr>
<h3 id="39-Measuring-Aleatoric-and-Epistemic-Uncertainty-in-LLMs-Empirical-Evaluation-on-ID-and-OOD-QA-Tasks-cs-CLPDF"><a href="#39-Measuring-Aleatoric-and-Epistemic-Uncertainty-in-LLMs-Empirical-Evaluation-on-ID-and-OOD-QA-Tasks-cs-CLPDF" class="headerlink" title="[39] Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks cs.CLPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03166">Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03166" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kevin Wang, Subre Abdoul Moktar, Jia Li, Kangshuo Li, Feng Chen</span></p>
<p><strong>TL;DR:</strong> 本文通过实证研究评估了LLMs中的随机性和认知不确定性，比较了12种不确定性估计方法在ID和OOD QA任务中的表现，发现信息基方法在ID设置中表现优异，而密度基方法和P(True)指标在OOD场景中更有效。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 确保LLM输出的可信度是当前研究的关键，不确定性估计（UE）在其中扮演重要角色。本文旨在评估不同UE方法在衡量LLM中随机性和认知不确定性时的效果。</p>
<p><strong>Result:</strong> 信息基方法在ID数据中表现优异；密度基方法和P(True)在OOD场景中更有效；语义一致性方法在不同数据集和指标中表现稳健。</p>
<p><strong>Insight:</strong> 不同UE方法适用于不同场景，信息基方法适合ID数据，而密度基方法和P(True)更适合捕捉OOD中的认知不确定性。语义一致性方法是一种通用但非最优的选择。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have become increasingly pervasive, finding applications across many industries and disciplines. Ensuring the trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE) plays a key role. In this work, a comprehensive empirical study is conducted to examine the robustness and effectiveness of diverse UE measures regarding aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE methods and four generation quality metrics including LLMScore from LLM criticizers to evaluate the uncertainty of LLM-generated answers in Question-Answering (QA) tasks on both in-distribution (ID) and out-of-distribution (OOD) datasets. Our analysis reveals that information-based methods, which leverage token and sequence probabilities, perform exceptionally well in ID settings due to their alignment with the model’s understanding of the data. Conversely, density-based methods and the P(True) metric exhibit superior performance in OOD contexts, highlighting their effectiveness in capturing the model’s epistemic uncertainty. Semantic consistency methods, which assess variability in generated answers, show reliable performance across different datasets and generation metrics. These methods generally perform well but may not be optimal for every situation.</p>
  </div>
</details>

<hr>
<h3 id="40-BengaliMoralBench-A-Benchmark-for-Auditing-Moral-Reasoning-in-Large-Language-Models-within-Bengali-Language-and-Culture-cs-CLPDF"><a href="#40-BengaliMoralBench-A-Benchmark-for-Auditing-Moral-Reasoning-in-Large-Language-Models-within-Bengali-Language-and-Culture-cs-CLPDF" class="headerlink" title="[40] BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture cs.CLPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03180">BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03180" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shahriyar Zaman Ridoy, Azmine Toushik Wasi, Koushik Ahamed Tonmoy</span></p>
<p><strong>TL;DR:</strong> 该论文提出了BengaliMoralBench，这是首个针对孟加拉语言和文化的道德推理基准，填补了多语言大语言模型在本地伦理规范对齐方面的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着多语言大语言模型在南亚的普及，其在孟加拉语言和文化中的道德对齐问题尚未被充分研究。现有的道德基准大多以英语为中心，忽略了本地文化背景的重要性。</p>
<p><strong>Result:</strong> 评估结果显示，模型的准确率在50-91%之间，表现出显著的性能差异，且存在文化基础薄弱和道德公平性问题。</p>
<p><strong>Insight:</strong> 该基准为多语言、低资源环境下的伦理对齐提供了重要工具，支持未来开发更具文化适应性的AI系统。</p>
<p><strong>Abstract:</strong> As multilingual Large Language Models (LLMs) gain traction across South Asia, their alignment with local ethical norms, particularly for Bengali, which is spoken by over 285 million people and ranked 6th globally, remains underexplored. Existing ethics benchmarks are largely English-centric and shaped by Western frameworks, overlooking cultural nuances critical for real-world deployment. To address this, we introduce BengaliMoralBench, the first large-scale ethics benchmark for the Bengali language and socio-cultural contexts. It covers five moral domains, Daily Activities, Habits, Parenting, Family Relationships, and Religious Activities, subdivided into 50 culturally relevant subtopics. Each scenario is annotated via native-speaker consensus using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct systematic zero-shot evaluation of prominent multilingual LLMs, including Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and standard metrics. Performance varies widely (50-91% accuracy), with qualitative analysis revealing consistent weaknesses in cultural grounding, commonsense reasoning, and moral fairness. BengaliMoralBench provides a foundation for responsible localization, enabling culturally aligned evaluation and supporting the deployment of ethically robust AI in diverse, low-resource multilingual settings such as Bangladesh.</p>
  </div>
</details>

<hr>
<h3 id="41-Benchmarking-the-Thinking-Mode-of-Multimodal-Large-Language-Models-in-Clinical-Tasks-cs-CL-cs-AI-cs-CV-cs-LGPDF"><a href="#41-Benchmarking-the-Thinking-Mode-of-Multimodal-Large-Language-Models-in-Clinical-Tasks-cs-CL-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[41] Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks cs.CL | cs.AI | cs.CV | cs.LGPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03328">Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03328" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jindong Hong, Tianjie Chen, Lingjie Luo, Chuanyang Zheng, Ting Xu</span></p>
<p><strong>TL;DR:</strong> 该论文评估了多模态大语言模型（MLLMs）在临床任务中‘思考模式’的表现，发现其与标准‘非思考模式’相比性能提升有限，尤其在复杂医疗任务中表现欠佳。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着支持‘思考模式’的MLLMs快速发展，研究旨在验证这种模式在临床任务中对模型性能和可靠性的实际影响。</p>
<p><strong>Result:</strong> 思考模式在多数任务中的改进有限，尤其是在开放性问答和医学图像解读等复杂任务中表现不佳。</p>
<p><strong>Insight:</strong> 研究强调了医疗领域需要更多领域特定数据和更先进的医学知识整合方法来提升MLLMs的临床适用性。</p>
<p><strong>Abstract:</strong> A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of “reasoning MLLMs” that offer explicit control over their internal thinking processes (normally referred as the “thinking mode”) alongside the standard “non-thinking mode”. This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these “dual-state” MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active “thinking mode” capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.</p>
  </div>
</details>

<hr>
<h3 id="42-EQ-Negotiator-Dynamic-Emotional-Personas-Empower-Small-Language-Models-for-Edge-Deployable-Credit-Negotiation-cs-CLPDF"><a href="#42-EQ-Negotiator-Dynamic-Emotional-Personas-Empower-Small-Language-Models-for-Edge-Deployable-Credit-Negotiation-cs-CLPDF" class="headerlink" title="[42] EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation cs.CLPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03370">EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03370" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yunbo Long, Yuhan Liu, Alexandra Brintrup</span></p>
<p><strong>TL;DR:</strong> EQ-Negotiator通过动态情感角色为小型语言模型赋能，使其在边缘部署的信用谈判中表现出色，甚至超越更大的模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLM）在自动谈判中性能优秀，但其计算成本高且隐私保护不足，不适合移动设备等边缘场景。小型语言模型（SLM）虽然更实用，但在处理情感化复杂角色时表现不佳。EQ-Negotiator旨在填补SLM与LLM之间的性能差距。</p>
<p><strong>Result:</strong> 在多种信用谈判场景中，7B参数的SLM搭载EQ-Negotiator，表现优于规模大十倍的LLM基准模型，尤其在对抗性策略下。</p>
<p><strong>Insight:</strong> 情感智能和动态角色建模是谈判成功的关键。EQ-Negotiator展示了在隐私约束下实现高效谈判的可能性，推动了轻量化AI的发展。</p>
<p><strong>Abstract:</strong> The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.</p>
  </div>
</details>

<hr>
<h3 id="43-LFC-DA-Logical-Formula-Controlled-Data-Augmentation-for-Enhanced-Logical-Reasoning-cs-CL-I-2-7-I-2-6-F-4-1PDF"><a href="#43-LFC-DA-Logical-Formula-Controlled-Data-Augmentation-for-Enhanced-Logical-Reasoning-cs-CL-I-2-7-I-2-6-F-4-1PDF" class="headerlink" title="[43] LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning cs.CL | I.2.7; I.2.6; F.4.1PDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03372">LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | I.2.7; I.2.6; F.4.1</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03372" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shenghao Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于符号逻辑的数据增强方法LFC-DA，通过命题表达式和规则库生成多样化的逻辑问题，显著提升了预训练模型的逻辑推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有数据增强方法在复杂逻辑任务中对人工标注依赖高，或依赖大模型生成但缺乏逻辑多样性和可解释性。</p>
<p><strong>Result:</strong> 在ReClor和LogiQA数据集上的实验表明，LFC-DA显著提升了预训练模型的逻辑推理准确性。</p>
<p><strong>Insight:</strong> 通过符号逻辑控制生成多样化的逻辑问题，可以有效增强模型的逻辑推理能力。</p>
<p><strong>Abstract:</strong> For complex logical data augmentation, heavy reliance on human annotation is costly, whereas direct generation with large language models yields uninterpretable and logically homogeneous examples. To address this, we present LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to propositional expressions, a compact rule library is compiled, and a bounded state-space search systematically discovers valid formulas that are then verbalized back into natural-language questions, ensuring both diversity and logical rigor under propositional logic. Experiments on ReClor and LogiQA show significant improvements in the logical-reasoning accuracy of pretrained models, confirming the effectiveness of LFC-DA for LLM-guided logical data augmentation.</p>
  </div>
</details>

<hr>
<h3 id="44-Segmentation-Beyond-Defaults-Asymmetrical-Byte-Pair-Encoding-for-Optimal-Machine-Translation-Performance-cs-CLPDF"><a href="#44-Segmentation-Beyond-Defaults-Asymmetrical-Byte-Pair-Encoding-for-Optimal-Machine-Translation-Performance-cs-CLPDF" class="headerlink" title="[44] Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance cs.CLPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03383">Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03383" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Saumitra Yadav, Manish Shrivastava</span></p>
<p><strong>TL;DR:</strong> 论文研究了在机器翻译中不对称Byte Pair Encoding（BPE）的效果，发现与对称BPE相比，不对称BPE（源语言和目标语言使用不同的合并操作次数）在低资源语言对中显著提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究通常默认使用对称BPE（源语言和目标语言使用相同的合并操作次数），但作者发现这种统一方法并不能在所有语言对和数据规模下实现最优性能，尤其在低资源场景下。</p>
<p><strong>Result:</strong> 不对称BPE在低资源场景（50K、100K、500K句对）中显著优于对称BPE，例如在英语-印地语任务中平均提升5.32、4.46和0.7 CHRF++分数。</p>
<p><strong>Insight:</strong> 高NMO用于源语言（4K到32K）和低NMO用于目标语言（0.5K到2K）的组合在低资源机器翻译中表现最佳，打破了对对称BPE的默认假设。</p>
<p><strong>Abstract:</strong> Existing Machine Translation (MT) research often suggests a single, fixed set of hyperparameters for word segmentation models, symmetric Byte Pair Encoding (BPE), which applies the same number of merge operations (NMO) to train tokenizers for both source and target languages. However, we demonstrate that this uniform approach doesn’t guarantee optimal MT performance across different language pairs and data sizes. This work investigates BPE segmentation recipes across various data volumes and language pairs to evaluate MT system performance. We find that utilizing asymmetric BPE, where the source and target languages have different NMOs, significantly improves results over the symmetric approach, especially in low-resource settings (50K, 100K, and 500K sentence pairs). Specifically, asymmetric BPE yield statistically significant ($p&lt;0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in low-resource setups. We validated this trend across six additional language pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut), observing statistically significant improvement in 10 out of 12 systems compared to symmetric BPE. Our findings indicate a high NMO for the source (4K to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results, particularly benefiting low-resource MT.</p>
  </div>
</details>

<hr>
<h3 id="45-Overcoming-the-Generalization-Limits-of-SLM-Finetuning-for-Shape-Based-Extraction-of-Datatype-and-Object-Properties-cs-CL-I-2-7-I-2-4PDF"><a href="#45-Overcoming-the-Generalization-Limits-of-SLM-Finetuning-for-Shape-Based-Extraction-of-Datatype-and-Object-Properties-cs-CL-I-2-7-I-2-4PDF" class="headerlink" title="[45] Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties cs.CL | I.2.7; I.2.4PDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03407">Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | I.2.7; I.2.4</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03407" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Célian Ringwald, Fabien Gandon, Catherine Faron, Franck Michel, Hanna Abi Akl</span></p>
<p><strong>TL;DR:</strong> 该论文研究了小型语言模型（SLM）在基于形状的RDF图提取中处理数据类型和对象属性的能力，发现稀有属性的长尾分布是主要瓶颈，并提出了多种策略来解决这一问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前SLM在关系提取（RE）中表现良好，但仅限于常见数据类型属性。作者希望探索SLM在完整RDF图提取（包括数据类型和对象属性）中的能力，尤其是如何处理稀有属性的长尾分布问题。</p>
<p><strong>Result:</strong> 实验结果表明，最佳策略能够平衡模型在稀有属性和常见属性上的表现。作者公开了数据集、代码和实验结果，支持研究的可复现性。</p>
<p><strong>Insight:</strong> 论文指出，SLM在语义关系提取中的性能提升依赖于训练数据的平衡性，未来研究可以进一步探索更高效率的数据增强方法和模型优化策略。</p>
<p><strong>Abstract:</strong> Small language models (SLMs) have shown promises for relation extraction (RE) when extracting RDF triples guided by SHACL shapes focused on common datatype properties. This paper investigates how SLMs handle both datatype and object properties for a complete RDF graph extraction. We show that the key bottleneck is related to long-tail distribution of rare properties. To solve this issue, we evaluate several strategies: stratified sampling, weighted loss, dataset scaling, and template-based synthetic data augmentation. We show that the best strategy to perform equally well over unbalanced target properties is to build a training set where the number of occurrences of each property exceeds a given threshold. To enable reproducibility, we publicly released our datasets, experimental results and code. Our findings offer practical guidance for training shape-aware SLMs and highlight promising directions for future work in semantic RE.</p>
  </div>
</details>

<hr>
<h3 id="46-Efficient-Reasoning-via-Thought-Training-and-Thought-Free-Inference-cs-CL-I-2-7PDF"><a href="#46-Efficient-Reasoning-via-Thought-Training-and-Thought-Free-Inference-cs-CL-I-2-7PDF" class="headerlink" title="[46] Efficient Reasoning via Thought-Training and Thought-Free Inference cs.CL | I.2.7PDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03408">Efficient Reasoning via Thought-Training and Thought-Free Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03408" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为3TF（Thought-Training and Thought-Free inference）的框架，通过短到长的视角提升大规模语言模型的推理效率，将结构化推理内化，同时在推理时启用无需显式推理的模式，显著提高了推理性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前利用显式思维链（CoT）提示的推理方法虽然有效，但依赖冗长的显式推理输出，效率较低。3TF旨在通过训练模型内化推理能力，减少推理时的显式输出需求。</p>
<p><strong>Result:</strong> 实验表明，3TF训练的模型在无需显式推理的情况下显著提升了推理性能，证明了高质量推理可以通过隐式学习实现。</p>
<p><strong>Insight:</strong> 内化推理能力可以减少推理时的显式输出需求，同时提高效率和质量；短到长的训练视角为高效的推理模型设计提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explicit reasoning during inference. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.</p>
  </div>
</details>

<hr>
<h3 id="47-Knowledge-Augmented-Question-Error-Correction-for-Chinese-Question-Answer-System-with-QuestionRAG-cs-CLPDF"><a href="#47-Knowledge-Augmented-Question-Error-Correction-for-Chinese-Question-Answer-System-with-QuestionRAG-cs-CLPDF" class="headerlink" title="[47] Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG cs.CLPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03410">Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03410" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Longpeng Qiu, Ting Li, Shuai Mao, Nan Yang, Xiaohui Yan</span></p>
<p><strong>TL;DR:</strong> 论文提出了QuestionRAG框架，通过知识增强和强化学习解决问答系统中的输入错误问题，提升错误问题的理解和校正能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 问答系统中的输入错误常导致错误回答，现有大语言模型在理解用户意图和避免过度校正方面表现不佳。</p>
<p><strong>Result:</strong> 实验表明，知识增强对理解错误问题至关重要，RL比传统监督微调更有效。</p>
<p><strong>Insight:</strong> 结合知识增强和强化学习可以充分释放大语言模型在问题校正任务中的潜力。</p>
<p><strong>Abstract:</strong> Input errors in question-answering (QA) systems often lead to incorrect responses. Large language models (LLMs) struggle with this task, frequently failing to interpret user intent (misinterpretation) or unnecessarily altering the original question’s structure (over-correction). We propose QuestionRAG, a framework that tackles these problems. To address misinterpretation, it enriches the input with external knowledge (e.g., search results, related entities). To prevent over-correction, it uses reinforcement learning (RL) to align the model’s objective with precise correction, not just paraphrasing. Our results demonstrate that knowledge augmentation is critical for understanding faulty questions. Furthermore, RL-based alignment proves significantly more effective than traditional supervised fine-tuning (SFT), boosting the model’s ability to follow instructions and generalize. By integrating these two strategies, QuestionRAG unlocks the full potential of LLMs for the question correction task.</p>
  </div>
</details>

<hr>
<h3 id="48-CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field-cs-CL-cs-AIPDF"><a href="#48-CareMedEval-dataset-Evaluating-Critical-Appraisal-and-Reasoning-in-the-Biomedical-Field-cs-CL-cs-AIPDF" class="headerlink" title="[48] CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field cs.CL | cs.AIPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03441">CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03441" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Doria Bonzi, Alexandre Guiggi, Frédéric Béchet, Carlos Ramisch, Benoit Favre</span></p>
<p><strong>TL;DR:</strong> CareMedEval是一个新的数据集，专为评估大型语言模型（LLM）在生物医学领域的批判性评价和推理能力而设计，包含534个问题，基于37篇科学论文。结果显示现有LLM在此任务上表现不佳，尤其是在研究局限性和统计分析问题上。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生物医学领域的批判性评价是一个关键技能，但现有LLM在专业领域的可靠性有限。CareMedEval旨在填补这一空白，提供一个专门的数据集来评估和改进LLM的表现。</p>
<p><strong>Result:</strong> 现有LLM在Exact Match Rate上未超过0.5，生成中间推理虽能提升表现，但在研究局限性和统计分析问题上仍表现不佳。</p>
<p><strong>Insight:</strong> CareMedEval揭示了当前LLM在专业领域批判性推理上的局限性，为未来自动化支持工具的开发提供了重要基准。</p>
<p><strong>Abstract:</strong> Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.</p>
  </div>
</details>

<hr>
<h3 id="49-MultiZebraLogic-A-Multilingual-Logical-Reasoning-Benchmark-cs-CL-cs-AIPDF"><a href="#49-MultiZebraLogic-A-Multilingual-Logical-Reasoning-Benchmark-cs-CL-cs-AIPDF" class="headerlink" title="[49] MultiZebraLogic: A Multilingual Logical Reasoning Benchmark cs.CL | cs.AIPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03553">MultiZebraLogic: A Multilingual Logical Reasoning Benchmark</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03553" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sofie Helene Bruun, Dan Saattrup Smart</span></p>
<p><strong>TL;DR:</strong> 论文提出了MultiZebraLogic，一个多语言的逻辑推理基准，旨在评估大型语言模型在不同语言和不同难度下的逻辑推理能力。通过生成不同主题、大小和干扰类型的斑马谜题，作者展示了模型的性能表现，并发布了相关数据集和生成代码。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大型语言模型（LLM）评测基准难以全面评估其逻辑推理能力，尤其是在多语言和不同难度任务上的表现。因此，作者旨在创建一个高质量、多语言的数据集，以填补这一空白。</p>
<p><strong>Result:</strong> 研究发现：1) GPT-4o mini（非推理模型）和o3-mini（推理模型）分别在2x3和4x5规模的谜题上表现较好；2) 加入5个干扰线索会使o3-mini在4x5谜题上的准确率下降15%；3) 语言和主题对模型表现无显著影响；4) 线索类型与难度无相关性。</p>
<p><strong>Insight:</strong> 1) 干扰信息显著影响模型表现，表明模型在处理无关信息时存在不足；2) 多语言和主题的可扩展性表明基准具有普适性；3) 谜题大小是一个有效的难度调节因素。</p>
<p><strong>Abstract:</strong> Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.</p>
  </div>
</details>

<hr>
<h3 id="50-AILA–First-Experiments-with-Localist-Language-Models-cs-CL-cs-AIPDF"><a href="#50-AILA–First-Experiments-with-Localist-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[50] AILA–First Experiments with Localist Language Models cs.CL | cs.AIPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03559">AILA–First Experiments with Localist Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03559" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joachim Diederich</span></p>
<p><strong>TL;DR:</strong> 该论文首次展示了可控局部性在Transformer语言模型中的实证，通过可调局部性参数实现了介于完全局部化与分布式表示之间的动态控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统语言模型完全依赖分布式表示，缺乏透明性和解释性，而该论文通过引入局部化表示参数，探索如何在保持性能的同时提高模型的解释性。</p>
<p><strong>Result:</strong> 完全局部化配置显著降低了注意力熵（5.36 bits vs. 7.18 bits），同时保持了较高的指针保真度。中间局部性值（如{\lambda}&#x3D;0.6）在解释性和性能之间取得了最佳平衡，测试困惑度为4.65，准确率为84.7%。</p>
<p><strong>Insight:</strong> 局部化语言模型通过显式控制参数，为需要在透明性和性能之间取得平衡的应用提供了灵活的工具，尤其是在受监管领域。</p>
<p><strong>Abstract:</strong> This paper presents the first empirical demonstration of controllable locality in transformer language models, a novel architectural framework that enables continuous control over the degree of representation localization through a tunable locality dial parameter. Unlike traditional language models that rely exclusively on distributed representations, our approach allows dynamic interpolation between highly interpretable localist encodings and efficient distributed representations without requiring model retraining. We conducted experiments on the WikiText corpus using a two-layer transformer architecture, systematically varying the locality parameter {\lambda} across the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our results demonstrate that localist configurations achieve dramatically lower attention entropy, with {\lambda} &#x3D; 1.0 yielding 5.36 bits compared to 7.18 bits at {\lambda} &#x3D; 0.0, while maintaining substantially higher pointer fidelity scores reflecting stronger alignment with rule-specified targets. Prediction experiments reveal that intermediate locality values optimize the tradeoff between interpretability and performance, with {\lambda} &#x3D; 0.6 achieving test perplexity of 4.65 and accuracy of 84.7%. These findings establish that localist language models provide a practical framework for applications in regulated domains requiring both transparency and capability, offering precise mathematical control over the interpretability-performance spectrum through explicit penalty thresholds and information-theoretic design principles.</p>
  </div>
</details>

<hr>
<h3 id="51-ASVRI-Legal-Fine-Tuning-LLMs-with-Retrieval-Augmented-Generation-for-Enhanced-Legal-Regulation-cs-CLPDF"><a href="#51-ASVRI-Legal-Fine-Tuning-LLMs-with-Retrieval-Augmented-Generation-for-Enhanced-Legal-Regulation-cs-CLPDF" class="headerlink" title="[51] ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation cs.CLPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03563">ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03563" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">One Octadion, Bondan Sapta Prakoso, Nanang Yudi Setiawan, Novanto Yudistira</span></p>
<p><strong>TL;DR:</strong> 本文通过结合微调大型语言模型（LLMs）与检索增强生成（RAG）方法，提出了一种提升法律文本理解和法规制定的工具。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 法律领域的信息量大且动态变化，传统方法难以满足政策制定者对法规理解和制定的需求。通过结合微调和RAG，可以更好地支持政策制定者工作。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提升了法律研究和法规制定的效率，为政策制定者提供了有力支持。</p>
<p><strong>Insight:</strong> 将RAG与LLMs结合在法律领域具有潜力，能够动态适应法律变化并提升文本生成的相关性和准确性。</p>
<p><strong>Abstract:</strong> In this study, we explore the fine-tuning of Large Language Models (LLMs) to better support policymakers in their crucial work of understanding, analyzing, and crafting legal regulations. To equip the model with a deep understanding of legal texts, we curated a supervised dataset tailored to the specific needs of the legal domain. Additionally, we integrated the Retrieval-Augmented Generation (RAG) method, enabling the LLM to access and incorporate up-to-date legal knowledge from external sources. This combination of fine-tuning and RAG-based augmentation results in a tool that not only processes legal information but actively assists policymakers in interpreting regulations and drafting new ones that align with current needs. The results demonstrate that this approach can significantly enhance the effectiveness of legal research and regulation development, offering a valuable resource in the ever-evolving field of law.</p>
  </div>
</details>

<hr>
<h3 id="52-Step-Audio-EditX-Technical-Report-cs-CL-cs-AI-cs-HC-cs-SD-eess-ASPDF"><a href="#52-Step-Audio-EditX-Technical-Report-cs-CL-cs-AI-cs-HC-cs-SD-eess-ASPDF" class="headerlink" title="[52] Step-Audio-EditX Technical Report cs.CL | cs.AI | cs.HC | cs.SD | eess.ASPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03601">Step-Audio-EditX Technical Report</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.HC | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03601" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chao Yan, Boyong Wu, Peng Yang, Pengfei Tan, Guoqiang Hu</span></p>
<p><strong>TL;DR:</strong> Step-Audio-EditX是基于LLM的开源音频模型，首次在情感、说话风格和副语言等方面实现迭代式音频编辑，并具备零样本文本转语音能力，通过大间隔合成数据替代嵌入先验或辅助模块。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统音频编辑方法依赖于表示级解耦和辅助模块，限制了模型的表达能力和迭代控制。Step-Audio-EditX旨在通过大间隔学习突破这些限制。</p>
<p><strong>Result:</strong> 在情感编辑和细粒度控制任务中，性能优于MiniMax-2.6-hd和Doubao-Seed-TTS-2.0等模型。</p>
<p><strong>Insight:</strong> 大间隔学习方法为音频编辑提供了一种新范式，避免了复杂的表示解耦，同时实现了更高的表达力和灵活性。</p>
<p><strong>Abstract:</strong> We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.</p>
  </div>
</details>

<hr>
<h3 id="53-Towards-Transparent-Stance-Detection-A-Zero-Shot-Approach-Using-Implicit-and-Explicit-Interpretability-cs-CL-cs-LGPDF"><a href="#53-Towards-Transparent-Stance-Detection-A-Zero-Shot-Approach-Using-Implicit-and-Explicit-Interpretability-cs-CL-cs-LGPDF" class="headerlink" title="[53] Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability cs.CL | cs.LGPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03635">Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03635" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为IRIS的零样本立场检测框架，结合隐式和显式解释性，解决了现有方法在泛化性和解释性上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的零样本立场检测方法过度依赖显式推理，解释粗糙，且未显式建模推理过程。IRIS旨在提供更透明的立场检测，结合隐式和显式解释性模型。</p>
<p><strong>Result:</strong> 在VAST、EZ-STANCE、P-Stance和RFD数据集上，即使仅用10%训练数据，IRIS也表现出优异的泛化能力。</p>
<p><strong>Insight:</strong> 结合隐式和显式解释性方法可以显著提升零样本立场检测的透明度和泛化性。</p>
<p><strong>Abstract:</strong> Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the model’s predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the author’s attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design.</p>
  </div>
</details>

<hr>
<div id='q-fin.TR'></div>

<h1 id="q-fin-TR-Back"><a href="#q-fin-TR-Back" class="headerlink" title="q-fin.TR [Back]"></a>q-fin.TR <a href="#toc">[Back]</a></h1><h3 id="54-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models-q-fin-TR-cs-AI-cs-CE-cs-CLPDF"><a href="#54-LiveTradeBench-Seeking-Real-World-Alpha-with-Large-Language-Models-q-fin-TR-cs-AI-cs-CE-cs-CLPDF" class="headerlink" title="[54] LiveTradeBench: Seeking Real-World Alpha with Large Language Models q-fin.TR | cs.AI | cs.CE | cs.CLPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03628">LiveTradeBench: Seeking Real-World Alpha with Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">q-fin.TR | cs.AI | cs.CE | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03628" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haofei Yu, Fenghai Li, Jiaxuan You</span></p>
<p><strong>TL;DR:</strong> LiveTradeBench是一个实时交易环境，用于在大语言模型（LLMs）中评估现实和动态市场中的表现，揭示了静态评估与真实世界能力之间的差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型在静态环境中表现优异，但缺乏对动态和不确定性的评估。LiveTradeBench旨在填补这一空白，测试LLMs在实时市场中的决策能力。</p>
<p><strong>Result:</strong> 结果显示：（1）高LMArena分数不一定对应优越的交易结果；（2）模型表现出不同的投资风格；（3）部分LLM能有效利用实时信号调整决策。</p>
<p><strong>Insight:</strong> 静态评估与真实世界的动态决策能力存在显著差距，未来的基准测试需更关注实时不确定环境下的决策一致性。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) achieve strong performance across benchmarks–from knowledge quizzes and math reasoning to web-agent tasks–but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments–U.S. stocks and Polymarket prediction markets–differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.</p>
  </div>
</details>

<hr>
<div id='cs.GR'></div>

<h1 id="cs-GR-Back"><a href="#cs-GR-Back" class="headerlink" title="cs.GR [Back]"></a>cs.GR <a href="#toc">[Back]</a></h1><h3 id="55-Scheduling-the-Off-Diagonal-Weingarten-Loss-of-Neural-SDFs-for-CAD-Models-cs-GR-cs-CV-cs-LGPDF"><a href="#55-Scheduling-the-Off-Diagonal-Weingarten-Loss-of-Neural-SDFs-for-CAD-Models-cs-GR-cs-CV-cs-LGPDF" class="headerlink" title="[55] Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models cs.GR | cs.CV | cs.LGPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03147">Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.GR | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03147" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haotian Yin, Przemyslaw Musialski</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对神经符号距离函数（SDFs）的Off-Diagonal Weingarten（ODW）损失权重调度策略，通过动态调整权重在不同训练阶段的强度，优化CAD模型的几何重建效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> FlatCAD的ODW损失虽然是一种高效的第二阶先验，但其固定权重策略在训练早期和后期表现不一：早期需要强正则化以稳定优化，后期则需要弱化正则化以恢复细节。因此，需要一种动态调度策略来平衡不同阶段的需求。</p>
<p><strong>Result:</strong> 实验结果表明，动态调度策略比固定权重基线在Chamfer Distance上提升高达35%，证明了该方法在CAD重建中的有效性。</p>
<p><strong>Insight:</strong> 1. 动态调节正则化强度是优化神经SDFs表现的关键；<br>2. 不同的调度策略对结果有显著影响，需要根据任务需求选择合适的调度方式。</p>
<p><strong>Abstract:</strong> Neural signed distance functions (SDFs) have become a powerful representation for geometric reconstruction from point clouds, yet they often require both gradient- and curvature-based regularization to suppress spurious warp and preserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten (ODW) loss as an efficient second-order prior for CAD surfaces, approximating full-Hessian regularization at roughly half the computational cost. However, FlatCAD applies a fixed ODW weight throughout training, which is suboptimal: strong regularization stabilizes early optimization but suppresses detail recovery in later stages. We present scheduling strategies for the ODW loss that assign a high initial weight to stabilize optimization and progressively decay it to permit fine-scale refinement. We investigate constant, linear, quintic, and step interpolation schedules, as well as an increasing warm-up variant. Experiments on the ABC CAD dataset demonstrate that time-varying schedules consistently outperform fixed weights. Our method achieves up to a 35% improvement in Chamfer Distance over the FlatCAD baseline, establishing scheduling as a simple yet effective extension of curvature regularization for robust CAD reconstruction.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="56-Optimizing-the-nnU-Net-model-for-brain-tumor-Glioma-segmentation-Using-a-BraTS-Sub-Saharan-Africa-SSA-dataset-eess-IV-cs-CVPDF"><a href="#56-Optimizing-the-nnU-Net-model-for-brain-tumor-Glioma-segmentation-Using-a-BraTS-Sub-Saharan-Africa-SSA-dataset-eess-IV-cs-CVPDF" class="headerlink" title="[56] Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset eess.IV | cs.CVPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02893">Optimizing the nnU-Net model for brain tumor (Glioma) segmentation Using a BraTS Sub-Saharan Africa (SSA) dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02893" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chukwuemeka Arua Kalu, Adaobi Chiazor Emegoakor, Fortune Okafor, Augustine Okoh Uchenna, Chijioke Kelvin Ukpai</span></p>
<p><strong>TL;DR:</strong> 该研究使用BraTS撒哈拉以南非洲数据集优化了nnU-Net模型，用于脑瘤分割。研究发现，原始数据结合nnU-Net的在线增强优于离线增强数据，强调了数据质量和增强方法的重要性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学图像分割对现代医疗至关重要，自动分割技术能帮助医生专注于诊断和治疗规划。本研究旨在优化nnU-Net模型，提高在撒哈拉以南非洲地区脑瘤分割的准确性。</p>
<p><strong>Result:</strong> 模型在原始数据结合在线增强下，全肿瘤分割的Dice分数达到0.84，表现优于离线增强数据。</p>
<p><strong>Insight:</strong> 研究表明数据质量和增强方法对模型泛化能力至关重要，尤其是在数据稀缺地区，原始数据的自然变异性可能优于人为增强数据。</p>
<p><strong>Abstract:</strong> Medical image segmentation is a critical achievement in modern medical science, developed over decades of research. It allows for the exact delineation of anatomical and pathological features in two- or three-dimensional pictures by utilizing notions like pixel intensity, texture, and anatomical context. With the advent of automated segmentation, physicians and radiologists may now concentrate on diagnosis and treatment planning while intelligent computers perform routine image processing tasks.   This study used the BraTS Sub-Saharan Africa dataset, a selected subset of the BraTS dataset that included 60 multimodal MRI cases from patients with glioma. Surprisingly, the nnU Net model trained on the initial 60 instances performed better than the network trained on an offline-augmented dataset of 360 cases. Hypothetically, the offline augmentations introduced artificial anatomical variances or intensity distributions, reducing generalization. In contrast, the original dataset, when paired with nnU Net’s robust online augmentation procedures, maintained realistic variability and produced better results. The study achieved a Dice score of 0.84 for whole tumor segmentation. These findings highlight the significance of data quality and proper augmentation approaches in constructing accurate, generalizable medical picture segmentation models, particularly for under-represented locations.</p>
  </div>
</details>

<hr>
<h3 id="57-Domain-Adaptive-Transformer-for-Data-Efficient-Glioma-Segmentation-in-Sub-Saharan-MRI-eess-IV-cs-CV-I-2-10-I-4-8-J-3PDF"><a href="#57-Domain-Adaptive-Transformer-for-Data-Efficient-Glioma-Segmentation-in-Sub-Saharan-MRI-eess-IV-cs-CV-I-2-10-I-4-8-J-3PDF" class="headerlink" title="[57] Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI eess.IV | cs.CV | I.2.10; I.4.8; J.3PDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02928">Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | I.2.10; I.4.8; J.3</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02928" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ilerioluwakiiye Abolade, Aniekan Udo, Augustine Ojo, Abdulbasit Oyetunji, Hammed Ajigbotosho</span></p>
<p><strong>TL;DR:</strong> 提出 SegFormer3D-plus，一种放射组学引导的 Transformer 架构，用于在资源有限的撒哈拉以南非洲地区解决胶质瘤分割中的域偏移问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 撒哈拉以南非洲地区由于 MRI 基础设施有限和采集协议异质性，导致严重的域偏移问题，使得胶质瘤分割极具挑战性。</p>
<p><strong>Result:</strong> 在 BraTS-Africa 数据上微调后，模型在异质性临床扫描中表现出更好的肿瘤子区域分割和边界定位能力。</p>
<p><strong>Insight:</strong> 放射组学引导的域适应方法在资源有限的环境中具有显著价值。</p>
<p><strong>Abstract:</strong> Glioma segmentation is critical for diagnosis and treatment planning, yet remains challenging in Sub-Saharan Africa due to limited MRI infrastructure and heterogeneous acquisition protocols that induce severe domain shift. We propose SegFormer3D-plus, a radiomics-guided transformer architecture designed for robust segmentation under domain variability. Our method combines: (1) histogram matching for intensity harmonization across scanners, (2) radiomic feature extraction with PCA-reduced k-means for domain-aware stratified sampling, (3) a dual-pathway encoder with frequency-aware feature extraction and spatial-channel attention, and (4) composite Dice-Cross-Entropy loss for boundary refinement. Pretrained on BraTS 2023 and fine-tuned on BraTS-Africa data, SegFormer3D-plus demonstrates improved tumor subregion delineation and boundary localization across heterogeneous African clinical scans, highlighting the value of radiomics-guided domain adaptation for resource-limited settings.</p>
  </div>
</details>

<hr>
<h3 id="58-Morpho-Genomic-Deep-Learning-for-Ovarian-Cancer-Subtype-and-Gene-Mutation-Prediction-from-Histopathology-eess-IV-cs-CV-q-bio-QMPDF"><a href="#58-Morpho-Genomic-Deep-Learning-for-Ovarian-Cancer-Subtype-and-Gene-Mutation-Prediction-from-Histopathology-eess-IV-cs-CV-q-bio-QMPDF" class="headerlink" title="[58] Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology eess.IV | cs.CV | q-bio.QMPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03365">Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene Mutation Prediction from Histopathology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | q-bio.QM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03365" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gabriela Fernandes</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合核形态测量和深度学习的方法，从H&amp;E病理图像预测卵巢癌亚型和基因突变，实现了较高的分类和推断准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 卵巢癌因晚期诊断和高度异质性导致高死亡率，当前诊断方法难以揭示对精准肿瘤学至关重要的基因组变异。</p>
<p><strong>Result:</strong> 亚型分类准确率达84.2%，TP53、BRCA1、ARID1A基因突变的AUC分别为0.82、0.76、0.73。特征分析揭示了核形态与基因突变的直接联系。</p>
<p><strong>Insight:</strong> 可量化的组织学表型编码了可测量的基因组信号，为低成本精准病理学提供了新途径。</p>
<p><strong>Abstract:</strong> Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2%$ (Macro AUC of $0.87 \pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} &#x3D; 0.82 \pm 0.02$, $AUC_{BRCA1} &#x3D; 0.76 \pm 0.04$, and $AUC_{ARID1A} &#x3D; 0.73 \pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="59-Watermarking-Large-Language-Models-in-Europe-Interpreting-the-AI-Act-in-Light-of-Technology-cs-CR-cs-AI-cs-CL-cs-CY-68T01-68727-68T30-68T35-68T37-68T50PDF"><a href="#59-Watermarking-Large-Language-Models-in-Europe-Interpreting-the-AI-Act-in-Light-of-Technology-cs-CR-cs-AI-cs-CL-cs-CY-68T01-68727-68T30-68T35-68T37-68T50PDF" class="headerlink" title="[59] Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology cs.CR | cs.AI | cs.CL | cs.CY | 68T01, 68727, 68T30, 68T35, 68T37, 68T50PDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03641">Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.AI | cs.CL | cs.CY | 68T01, 68727, 68T30, 68T35, 68T37, 68T50</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03641" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thomas Souverain</span></p>
<p><strong>TL;DR:</strong> 本文探讨了欧盟AI法案中对大型语言模型（LLM）水印的要求，提出了一种分类框架和评估方法，并指出当前技术尚未完全满足标准，建议未来研究嵌入底层架构的水印技术。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 欧盟AI法案要求提供商对其通用模型输出进行标记和检测，但目前水印技术多样且快速演变，缺乏统一评估标准。</p>
<p><strong>Result:</strong> 研究发现当前水印技术尚未完全满足欧盟标准，建议进一步研究嵌入底层架构的水印方法。</p>
<p><strong>Insight:</strong> 水印技术的评估需结合具体应用场景和标准，未来研究应关注底层架构的嵌入以实现更高可靠性。</p>
<p><strong>Abstract:</strong> To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ‘’sufficiently reliable, interoperable, effective and robust’’. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act’s requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="60-Comprehensive-Assessment-of-LiDAR-Evaluation-Metrics-A-Comparative-Study-Using-Simulated-and-Real-Data-cs-RO-cs-CVPDF"><a href="#60-Comprehensive-Assessment-of-LiDAR-Evaluation-Metrics-A-Comparative-Study-Using-Simulated-and-Real-Data-cs-RO-cs-CVPDF" class="headerlink" title="[60] Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data cs.RO | cs.CVPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02994">Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02994" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Syed Mostaquim Ali, Taufiq Rahman, Ghazal Farhani, Mohamed H. Zaki, Benoit Anctil</span></p>
<p><strong>TL;DR:</strong> 该论文通过模拟和真实LiDAR数据的对比研究，综合评估了LiDAR评估指标的适用性，发现Density Aware Chamfer Distance (DCD)在多种情况下表现最佳。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为自动驾驶系统（ADS）的安全性开发需要严格的测试，但由于成本和安全性问题，实际物理测试不可行，因此虚拟测试环境（VTE）成为替代方案。研究旨在找到适合比较真实和模拟LiDAR扫描的评估指标。</p>
<p><strong>Result:</strong> 模拟和真实LiDAR扫描在语义分割上的mIoU为21%，几何相似性的DCD为0.63，表明两者在几何特性上存在轻微差异，但在感知输出上有显著差异。DCD是与感知方法相关性最强的指标。</p>
<p><strong>Insight:</strong> DCD是评估LiDAR数据模拟与真实数据差异的有效指标，为虚拟测试环境的开发和自动驾驶系统的安全性验证提供了重要参考。</p>
<p><strong>Abstract:</strong> For developing safe Autonomous Driving Systems (ADS), rigorous testing is required before they are deemed safe for road deployments. Since comprehensive conventional physical testing is impractical due to cost and safety concerns, Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing VTE-generated sensor outputs against their real-world analogues can be a strong indication that the VTE accurately represents reality. Correspondingly, this work explores a comprehensive experimental approach to finding evaluation metrics suitable for comparing real-world and simulated LiDAR scans. The metrics were tested in terms of sensitivity and accuracy with different noise, density, distortion, sensor orientation, and channel settings. From comparing the metrics, we found that Density Aware Chamfer Distance (DCD) works best across all cases. In the second step of the research, a Virtual Testing Environment was generated using real LiDAR scan data. The data was collected in a controlled environment with only static objects using an instrumented vehicle equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans were compared in terms of model perception and geometric similarity. Actual and simulated LiDAR scans have a similar semantic segmentation output with a mIoU of 21% with corrected intensity and an average density aware chamfer distance (DCD) of 0.63. This indicates a slight difference in the geometric properties of simulated and real LiDAR scans and a significant difference between model outputs. During the comparison, density-aware chamfer distance was found to be the most correlated among the metrics with perception methods.</p>
  </div>
</details>

<hr>
<h3 id="61-OneOcc-Semantic-Occupancy-Prediction-for-Legged-Robots-with-a-Single-Panoramic-Camera-cs-RO-cs-CV-eess-IVPDF"><a href="#61-OneOcc-Semantic-Occupancy-Prediction-for-Legged-Robots-with-a-Single-Panoramic-Camera-cs-RO-cs-CV-eess-IVPDF" class="headerlink" title="[61] OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera cs.RO | cs.CV | eess.IVPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03571">OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03571" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hao Shi, Ze Wang, Shangwei Guo, Mengfei Duan, Song Wang</span></p>
<p><strong>TL;DR:</strong> OneOcc是一个针对腿部&#x2F;人形机器人设计的全景语义占据预测框架，通过单目全景相机实现高效3D语义占据预测，克服了步态引起的抖动和360度连续性挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有语义场景补全（SSC）系统多针对轮式平台和前置传感器，缺乏对腿部机器人步态抖动和360度连续性的支持。作者提出OneOcc，填补这一空白并提升实用性。</p>
<p><strong>Result:</strong> 在QuadOcc上超越视觉和LiDAR基线，在H3O数据集中分别提升3.83 mIoU（城市场景内）和8.08（跨城市）。</p>
<p><strong>Insight:</strong> 全景视觉与动态融合的结合能够有效解决腿部机器人在步态下的感知问题，轻量化设计使其具备实际部署潜力。</p>
<p><strong>Abstract:</strong> Robust 3D semantic occupancy is crucial for legged&#x2F;humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free&#x2F;occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range&#x2F;occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-&#x2F;cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged&#x2F;humanoid robots. Datasets and code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/MasterHow/OneOcc">https://github.com/MasterHow/OneOcc</a>.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="62-Data-Efficient-Realized-Volatility-Forecasting-with-Vision-Transformers-cs-LG-cs-CV-I-4PDF"><a href="#62-Data-Efficient-Realized-Volatility-Forecasting-with-Vision-Transformers-cs-LG-cs-CV-I-4PDF" class="headerlink" title="[62] Data-Efficient Realized Volatility Forecasting with Vision Transformers cs.LG | cs.CV | I.4PDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03046">Data-Efficient Realized Volatility Forecasting with Vision Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV | I.4</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03046" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Emi Soroka, Artem Arzyn</span></p>
<p><strong>TL;DR:</strong> 这篇论文探索了使用Vision Transformer（ViT）模型从隐含波动率曲面预测资产的未来30天实现波动率，证明了其在金融时序数据中的潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度学习方法在金融预测中表现出色，但Transformer模型在期权数据上的应用尚未充分探索。论文试图填补这一空白。</p>
<p><strong>Result:</strong> ViT能够从隐含波动率曲面中学习季节性模式和复杂非线性关系。</p>
<p><strong>Insight:</strong> ViT在金融时序数据中的表现表明其有望成为波动率预测的新工具，为模型开发提供了新方向。</p>
<p><strong>Abstract:</strong> Recent work in financial machine learning has shown the virtue of complexity: the phenomenon by which deep learning methods capable of learning highly nonlinear relationships outperform simpler approaches in financial forecasting. While transformer architectures like Informer have shown promise for financial time series forecasting, the application of transformer models for options data remains largely unexplored. We conduct preliminary studies towards the development of a transformer model for options data by training the Vision Transformer (ViT) architecture, typically used in modern image recognition and classification systems, to predict the realized volatility of an asset over the next 30 days from its implied volatility surface (augmented with date information) for a single day. We show that the ViT can learn seasonal patterns and nonlinear features from the IV surface, suggesting a promising direction for model development.</p>
  </div>
</details>

<hr>
<h3 id="63-Test-Time-Adaptation-Using-Adaptive-Quantile-Recalibration-cs-LG-cs-CVPDF"><a href="#63-Test-Time-Adaptation-Using-Adaptive-Quantile-Recalibration-cs-LG-cs-CVPDF" class="headerlink" title="[63] Test Time Adaptation Using Adaptive Quantile Recalibration cs.LG | cs.CVPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03148">Test Time Adaptation Using Adaptive Quantile Recalibration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03148" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Paria Mehrbod, Pedro Vianna, Geraldin Nanfack, Guy Wolf, Eugene Belilovsky</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种称为自适应分位数重校准（AQR）的测试时适应方法，通过通道级别的分位数对齐来调整预激活分布，无需重新训练模型，同时改进了分布尾部的估计稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统领域适应方法依赖目标领域先验知识或需要重新训练的问题，以及现有测试时适应方法在处理复杂激活分布和不同归一化层时的局限性。</p>
<p><strong>Result:</strong> 在CIFAR-10-C、CIFAR-100-C和ImageNet-C数据集上的实验表明，AQR能够稳定适应多样化的设置，性能优于现有测试时适应基线方法。</p>
<p><strong>Insight:</strong> AQR展现了在处理动态和不可预测的现实数据分布时的潜力，适用于资源受限或动态环境中部署的深度学习模型。</p>
<p><strong>Abstract:</strong> Domain adaptation is a key strategy for enhancing the generalizability of deep learning models in real-world scenarios, where test distributions often diverge significantly from the training domain. However, conventional approaches typically rely on prior knowledge of the target domain or require model retraining, limiting their practicality in dynamic or resource-constrained environments. Recent test-time adaptation methods based on batch normalization statistic updates allow for unsupervised adaptation, but they often fail to capture complex activation distributions and are constrained to specific normalization layers. We propose Adaptive Quantile Recalibration (AQR), a test-time adaptation technique that modifies pre-activation distributions by aligning quantiles on a channel-wise basis. AQR captures the full shape of activation distributions and generalizes across architectures employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of estimating distribution tails under varying batch sizes, AQR incorporates a robust tail calibration strategy that improves stability and precision. Our method leverages source-domain statistics computed at training time, enabling unsupervised adaptation without retraining models. Experiments on CIFAR-10-C, CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR achieves robust adaptation across diverse settings, outperforming existing test-time adaptation baselines. These results highlight AQR’s potential for deployment in real-world scenarios with dynamic and unpredictable data distributions.</p>
  </div>
</details>

<hr>
<h3 id="64-A-Probabilistic-U-Net-Approach-to-Downscaling-Climate-Simulations-cs-LG-cs-CV-physics-ao-phPDF"><a href="#64-A-Probabilistic-U-Net-Approach-to-Downscaling-Climate-Simulations-cs-LG-cs-CV-physics-ao-phPDF" class="headerlink" title="[64] A Probabilistic U-Net Approach to Downscaling Climate Simulations cs.LG | cs.CV | physics.ao-phPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03197">A Probabilistic U-Net Approach to Downscaling Climate Simulations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV | physics.ao-ph</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03197" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maryam Alipourhajiagha, Pierre-Louis Lemaire, Youssef Diouane, Julie Carreau</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于概率U-Net的方法，用于将气候模拟结果从粗分辨率降尺度到细分辨率，并通过变分潜在空间捕捉不确定性。评估了四种训练目标，发现WMSE-MS-SSIM在极端事件表现优异，而afCRPS在跨尺度空间变异性上表现更好。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 气候模型的计算成本高昂，通常只能生成粗分辨率输出，而许多气候变化影响研究需要更精细的分辨率。统计降尺度方法可以填补这一空白，因此作者提出了一种概率U-Net方法来解决这一问题。</p>
<p><strong>Result:</strong> 结果显示，WMSE-MS-SSIM在某些设置下对极端事件表现良好，而afCRPS在跨尺度空间变异性上表现更优。</p>
<p><strong>Insight:</strong> 通过概率性建模可以有效捕捉气候模拟中的不确定性，并且不同训练目标在不同任务场景下各有优势，为实际应用提供了灵活性。</p>
<p><strong>Abstract:</strong> Climate models are limited by heavy computational costs, often producing outputs at coarse spatial resolutions, while many climate change impact studies require finer scales. Statistical downscaling bridges this gap, and we adapt the probabilistic U-Net for this task, combining a deterministic U-Net backbone with a variational latent space to capture aleatoric uncertainty. We evaluate four training objectives, afCRPS and WMSE-MS-SSIM with three settings for downscaling precipitation and temperature from $16\times$ coarser resolution. Our main finding is that WMSE-MS-SSIM performs well for extremes under certain settings, whereas afCRPS better captures spatial variability across scales.</p>
  </div>
</details>

<hr>
<div id='eess.SP'></div>

<h1 id="eess-SP-Back"><a href="#eess-SP-Back" class="headerlink" title="eess.SP [Back]"></a>eess.SP <a href="#toc">[Back]</a></h1><h3 id="65-Benchmarking-ResNet-for-Short-Term-Hypoglycemia-Classification-with-DiaData-eess-SP-cs-CV-eess-IVPDF"><a href="#65-Benchmarking-ResNet-for-Short-Term-Hypoglycemia-Classification-with-DiaData-eess-SP-cs-CV-eess-IVPDF" class="headerlink" title="[65] Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData eess.SP | cs.CV | eess.IVPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02849">Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.SP | cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02849" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Beyza Cinar, Maria Maleshkova</span></p>
<p><strong>TL;DR:</strong> 该研究通过改进DiaData数据质量，利用ResNet模型为短期低血糖分类提供基准，数据清理和插值方法提升了模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对1型糖尿病（T1D）数据分析中存在的噪声、缺失值和数据量小的问题，研究旨在提升数据质量，并利用高质量数据为低血糖分类提供可靠基准。</p>
<p><strong>Result:</strong> 数据质量改进使模型性能提升2-3%，更多数据训练提升7%。</p>
<p><strong>Insight:</strong> 高质量数据对模型性能至关重要，Stineman插值在较大数据间隙中表现优于线性插值。</p>
<p><strong>Abstract:</strong> Individualized therapy is driven forward by medical data analysis, which provides insight into the patient’s context. In particular, for Type 1 Diabetes (T1D), which is an autoimmune disease, relationships between demographics, sensor data, and context can be analyzed. However, outliers, noisy data, and small data volumes cannot provide a reliable analysis. Hence, the research domain requires large volumes of high-quality data. Moreover, missing values can lead to information loss. To address this limitation, this study improves the data quality of DiaData, an integration of 15 separate datasets containing glucose values from 2510 subjects with T1D. Notably, we make the following contributions: 1) Outliers are identified with the interquartile range (IQR) approach and treated by replacing them with missing values. 2) Small gaps ($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30 and $&lt;$ 120 min) with Stineman interpolation. Based on a visual comparison, Stineman interpolation provides more realistic glucose estimates than linear interpolation for larger gaps. 3) After data cleaning, the correlation between glucose and heart rate is analyzed, yielding a moderate relation between 15 and 60 minutes before hypoglycemia ($\le$ 70 mg&#x2F;dL). 4) Finally, a benchmark for hypoglycemia classification is provided with a state-of-the-art ResNet model. The model is trained with the Maindatabase and Subdatabase II of DiaData to classify hypoglycemia onset up to 2 hours in advance. Training with more data improves performance by 7% while using quality-refined data yields a 2-3% gain compared to raw data.</p>
  </div>
</details>

<hr>
<h3 id="66-NEF-NET-Adapting-Electrocardio-panorama-in-the-wild-eess-SP-cs-AI-cs-CV-eess-IVPDF"><a href="#66-NEF-NET-Adapting-Electrocardio-panorama-in-the-wild-eess-SP-cs-AI-cs-CV-eess-IVPDF" class="headerlink" title="[66] NEF-NET+: Adapting Electrocardio panorama in the wild eess.SP | cs.AI | cs.CV | eess.IVPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02880">NEF-NET+: Adapting Electrocardio panorama in the wild</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.SP | cs.AI | cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.02880" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zehui Zhan, Yaojun Hu, Jiajing Zhan, Wanchen Lian, Wanqing Wu</span></p>
<p><strong>TL;DR:</strong> NEF-NET+ 是一个改进的心电全景合成框架，解决了传统心电图系统的局限性，能够在真实环境下支持任意长度和视角的信号合成，并适应不同设备和操作偏差。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统心电图系统只能从固定的解剖视角捕捉信号，某些心脏疾病需要非标准视角才能检测关键模式。Nef-Net 虽然能够重建连续心电场，但在实际应用中面临长时程建模、设备噪声和电极放置偏差等挑战。</p>
<p><strong>Result:</strong> 在真实环境中，NEF-NET+ 比 Nef-Net 提升了约 6 dB 的 PSNR。</p>
<p><strong>Insight:</strong> 通过引入实际环境中的适应性校准步骤，可以显著提高心电全景合成的鲁棒性和精确性，为心脏病的诊断提供了更灵活的工具。</p>
<p><strong>Abstract:</strong> Conventional multi-lead electrocardiogram (ECG) systems capture cardiac signals from a fixed set of anatomical viewpoints defined by lead placement. However, certain cardiac conditions (e.g., Brugada syndrome) require additional, non-standard viewpoints to reveal diagnostically critical patterns that may be absent in standard leads. To systematically overcome this limitation, Nef-Net was recently introduced to reconstruct a continuous electrocardiac field, enabling virtual observation of ECG signals from arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net operates under idealized assumptions and faces in-the-wild challenges, such as long-duration ECG modeling, robustness to device-specific signal artifacts, and suboptimal lead placement calibration. This paper presents NEF-NET+, an enhanced framework for realistic panoramic ECG synthesis that supports arbitrary-length signal synthesis from any desired view, generalizes across ECG devices, and compensates for operator-induced deviations in electrode placement. These capabilities are enabled by a newly designed model architecture that performs direct view transformation, incorporating a workflow comprising offline pretraining, device calibration tuning steps as well as an on-the-fly calibration step for patient-specific adaptation. To rigorously evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama benchmark, called Panobench, comprising 5367 recordings with 48-view per subject, capturing the full spatial variability of cardiac electrical activity. Experimental results show that NEF-NET+ delivers substantial improvements over Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The code and Panobench will be released in a subsequent publication.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-11-08/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-11-06/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
