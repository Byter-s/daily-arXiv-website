<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2026-01-14 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2026-01-14
            
            
        </div>
        <span class="post-date">
            Jan 14, 2026
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 69]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 83]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 2]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 2]</li>
<li><a href="#cs.CY">cs.CY</a> [Total: 2]</li>
<li><a href="#cs.SE">cs.SE</a> [Total: 1]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 3]</li>
<li><a href="#q-bio.NC">q-bio.NC</a> [Total: 1]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 2]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 9]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 5]</li>
<li><a href="#cs.AR">cs.AR</a> [Total: 1]</li>
<li><a href="#cs.MA">cs.MA</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 2]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-TeleMem-Building-Long-Term-and-Multimodal-Memory-for-Agentic-AI-cs-CL-cs-AI-cs-CVPDF"><a href="#1-TeleMem-Building-Long-Term-and-Multimodal-Memory-for-Agentic-AI-cs-CL-cs-AI-cs-CVPDF" class="headerlink" title="[1] TeleMem: Building Long-Term and Multimodal Memory for Agentic AI cs.CL | cs.AI | cs.CVPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06037">TeleMem: Building Long-Term and Multimodal Memory for Agentic AI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06037" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chunliang Chen, Ming Guan, Xiao Lin, Jiaxu Li, Qiyi Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了TeleMem，一个统一的长时多模态记忆系统，旨在解决大型语言模型在长时交互中因注意力有限而难以维持连贯对话的问题。该系统通过叙事动态提取构建一致的用户画像，采用结构化写入管道优化记忆存储与操作，并结合多模态记忆模块与ReAct推理实现复杂视频内容的准确理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有检索增强生成方法在长时交互中存在记忆更新机制不可靠、易产生模式驱动的幻觉、写入效率低以及多模态推理支持不足等问题，因此需要设计一个能够高效管理长时多模态记忆的系统。</p>
<p><strong>Result:</strong> 在ZH-4O长时角色扮演游戏基准测试中，TeleMem相比当前最优的Mem0基线，准确率提升19%，令牌使用减少43%，速度加快2.1倍，达到了新的SOTA水平。</p>
<p><strong>Insight:</strong> 创新点包括：基于对话的叙事动态提取确保记忆的可靠性；批量检索、聚类与合并的结构化写入管道提升存储效率；结合多模态记忆与ReAct推理的闭环观察-思考-行动过程，增强对复杂视频内容的理解能力。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.</p>
  </div>
</details>

<hr>
<h3 id="2-Operation-Veja-Fixing-Fundamental-Concepts-Missing-from-Modern-Roleplaying-Training-Paradigms-cs-CLPDF"><a href="#2-Operation-Veja-Fixing-Fundamental-Concepts-Missing-from-Modern-Roleplaying-Training-Paradigms-cs-CLPDF" class="headerlink" title="[2] Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms cs.CLPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06039">Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06039" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yueze Liu, Ajay Nagi Reddy Kumdam, Ronit Kanjilal, Hao Yang, Yichi Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了VEJA框架，旨在解决现代角色扮演模型在塑造可信、引人入胜角色方面的核心缺陷。作者认为现有训练范式忽略了角色的内在动态世界，因此定义了价值观、经验、判断和能力四个核心概念，并基于此提出新的数据构建范式。一项初步研究通过LLM作为评判者的评估表明，基于VEJA手动构建的数据集在质量上显著优于最先进的合成基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代角色扮演模型虽然日益复杂，但始终难以捕捉可信、引人入胜角色的本质。作者认为这一失败源于现有训练范式（如RAG、基于事实的提示、基于文献的学习和合成数据生成）忽视了角色内在世界的动态交互，特别是无法建模定义人类互动的深思熟虑、价值冲突的推理过程。</p>
<p><strong>Result:</strong> 一项初步研究将手动构建的、基于VEJA框架的数据集与一个最先进的合成基线进行了比较。使用LLM作为评判者的评估结果显示，两者存在显著的质量差距，表明VEJA框架在提升角色深度和叙事连贯性方面具有潜力。</p>
<p><strong>Insight:</strong> 论文的核心创新点是提出了VEJA（价值观、经验、判断、能力）框架，将其作为一种新的数据构建范式，以解决现有方法在建模角色内在动态和复杂推理方面的系统性局限。从客观角度看，该框架将角色塑造从表面特征转向深层的、概念驱动的数据构建，为提升角色扮演AI的真实性和连贯性提供了一个结构化的理论和方法基础。</p>
<p><strong>Abstract:</strong> Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character’s internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at <a target="_blank" rel="noopener" href="https://github.com/HyouinKyoumaIRL/Operation-Veja">https://github.com/HyouinKyoumaIRL/Operation-Veja</a></p>
  </div>
</details>

<hr>
<h3 id="3-Reinforcement-Learning-for-Chain-of-Thought-Compression-with-One-Domain-to-All-Generalization-cs-CL-cs-AI-cs-LGPDF"><a href="#3-Reinforcement-Learning-for-Chain-of-Thought-Compression-with-One-Domain-to-All-Generalization-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[3] Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization cs.CL | cs.AI | cs.LGPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06052">Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06052" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hanyu Li, Jiangshan Duo, Bofei Gao, Hailin Zhang, Sujian Li</span></p>
<p><strong>TL;DR:</strong> 本文提出一种基于强化学习的思维链压缩方法，通过样本级软惩罚机制减少大语言模型推理过程中的冗余计算，在保持或提升准确率的同时将平均响应长度降低20-40%，并展现出跨数学、代码、指令遵循等领域的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对大语言模型思维链推理中存在的’过度思考陷阱’问题——即冗长的推理步骤带来高计算成本与延迟却未提升准确率，现有全局静态控制方法可能误伤必要推理步骤，需要更精细的压缩策略。</p>
<p><strong>Result:</strong> 在数学推理任务上训练后，模型在代码生成、指令遵循和常识问答等未见任务中自发压缩响应长度20-40%，准确率保持稳定或提升，展现出跨领域泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于提出’仅在模型已掌握且能生成更简洁推理的问题上惩罚冗长推理’的样本级强化学习框架，并设计’准确率-压缩-准确率’的稳定训练课程，为开发高效推理模型提供了可复用的压缩阶段范式。</p>
<p><strong>Abstract:</strong> Chain-of-thought reasoning in large language models often creates an “overthinking trap,” leading to excessive computational cost and latency for unreliable accuracy gains. Prior work has typically relied on global, static controls that risk penalizing necessary reasoning. We introduce a sample-level, soft reinforcement learning compression method that penalizes inefficiently long rollouts, but only on problems where the model has already mastered and already produced a more concise rollout. Our experiments show that this method reduces average response length by 20-40% with comparable or higher accuracy. Crucially, the compression exhibits strong cross-domain generalization; a model trained on math spontaneously shortens responses on unseen tasks like code, instruction following, and general knowledge QA, with stable or improved accuracy. We demonstrate a stable post-training curriculum (accuracy-compression-accuracy) that can ultimately produce models that are more accurate and reason more concisely, arguing that such compression method should be a standard phase in developing efficient reasoning models.</p>
  </div>
</details>

<hr>
<h3 id="4-A-Multi-Stage-Workflow-for-the-Review-of-Marketing-Content-with-Reasoning-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#4-A-Multi-Stage-Workflow-for-the-Review-of-Marketing-Content-with-Reasoning-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[4] A Multi-Stage Workflow for the Review of Marketing Content with Reasoning Large Language Models cs.CL | cs.AIPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06054">A Multi-Stage Workflow for the Review of Marketing Content with Reasoning Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06054" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alberto Purpura, Emily Chen, Swapnil Shinde</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于微调推理大语言模型的多阶段工作流，用于自动化审查营销内容是否符合给定要求。该方法不依赖外部知识表示，通过比较不同微调策略（如SFT和GRPO）以及评估推理令牌生成和奖励函数组合的影响，优化模型在合规性检查任务中的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决营销内容人工审查效率低、易出错的问题，利用推理大语言模型的复杂问题解决能力，自动化识别文本内容中的合规性问题。</p>
<p><strong>Result:</strong> 论文评估了不同微调策略（SFT和GRPO）在训练模型解决此问题上的有效性，并分析了推理令牌生成和奖励函数选择对GRPO模型性能的影响，但未提及具体基准测试或定量结果（如准确率、F1分数）。</p>
<p><strong>Insight:</strong> 创新点包括：不依赖外部知识表示的自动合规问题识别方法；系统比较SFT与GRPO等微调策略；探索小规模LLM生成推理令牌以提升决策可解释性；研究奖励函数组合对强化学习训练效果的影响，为领域特定任务优化LLM提供了实用框架。</p>
<p><strong>Abstract:</strong> Reasoning Large Language Models (LLMs) have shown promising results when tasked with solving complex problems. In this paper, we propose and evaluate a multi-stage workflow that leverages the capabilities of fine-tuned reasoning LLMs to assist in the review process of marketing content, making sure they comply with a given list of requirements. The contributions of this paper are the following: (i) we present a novel approach – that does not rely on any external knowledge representation – for the automatic identification of compliance issues in textual content; (ii) compare the effectiveness of different fine-tuning strategies like Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) in training models to solve this problem; (iii) we evaluate the effectiveness of training small LLMs to generate reasoning tokens before providing their final response; (iv) we evaluate how the choice and combinations of different reward functions affects the performance of a model trained with GRPO.</p>
  </div>
</details>

<hr>
<h3 id="5-AzeroS-Extending-LLM-to-Speech-with-Self-Generated-Instruction-Free-Tuning-cs-CL-cs-SD-eess-ASPDF"><a href="#5-AzeroS-Extending-LLM-to-Speech-with-Self-Generated-Instruction-Free-Tuning-cs-CL-cs-SD-eess-ASPDF" class="headerlink" title="[5] AzeroS: Extending LLM to Speech with Self-Generated Instruction-Free Tuning cs.CL | cs.SD | eess.ASPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06086">AzeroS: Extending LLM to Speech with Self-Generated Instruction-Free Tuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06086" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiwen Shao, Wei Liu, Jiahong Li, Tianzi Wang, Kun Wei</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为AZeroS的语音增强大语言模型，通过自生成无指令调优范式，利用冻结的LLM从语音文本表示生成监督信号，无需收集任务特定的问答对，在约28,000小时的公开语音数据上训练，仅更新两个轻量级投影模块，在语义和副语言基准测试中达到SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有语音LLM依赖大规模任务特定指令调优数据、耗时且泛化能力差的问题，旨在实现无需人工标注指令数据的最佳泛化能力。</p>
<p><strong>Result:</strong> 在VoiceBench、AIR-Bench Foundation (Speech)和AIR-Bench Chat (Speech)等基准测试中，AZeroS在语义和副语言任务上均取得了最先进的性能。</p>
<p><strong>Insight:</strong> 创新点在于提出自生成无指令调优范式，通过冻结LLM生成监督信号，避免了人工标注指令数据的成本；模型设计上仅微调轻量投影模块，大幅降低了训练开销，同时保持了强大的泛化能力。</p>
<p><strong>Abstract:</strong> Extending large language models (LLMs) to the speech domain has recently gained significant attention. A typical approach connects a pretrained LLM with an audio encoder through a projection module and trains the resulting model on large-scale, task-specific instruction-tuning datasets. However, curating such instruction-tuning data for specific requirements is time-consuming, and models trained in this manner often generalize poorly to unseen tasks. In this work, we first formulate that the strongest generalization of a speech-LLM is achieved when it is trained with Self-Generated Instruction-Free Tuning (SIFT), in which supervision signals are generated by a frozen LLM using textual representations of speech as input. Our proposed SIFT paradigm eliminates the need for collecting task-specific question-answer pairs and yields the theoretically best generalization to unseen tasks. Building upon this paradigm, we introduce AZeroS (Auden Zero-instruction-tuned Speech-LLM), which is trained on speech-text pairs derived from publicly available corpora, including approximately 25,000 hours of speech with ASR transcripts and 3,000 hours of speech with paralinguistic labels. Built upon Qwen2.5-7B-Instruct, the model updates only two lightweight projection modules (23.8 million parameters each), while keeping both the LLM and audio encoders frozen. Despite the minimal training cost and modest data scale, AZeroS achieves state-of-the-art performance on both semantic and paralinguistic benchmarks, including VoiceBench, AIR-Bench Foundation (Speech), and AIR-Bench Chat (Speech).</p>
  </div>
</details>

<hr>
<h3 id="6-Amory-Building-Coherent-Narrative-Driven-Agent-Memory-through-Agentic-Reasoning-cs-CL-cs-AIPDF"><a href="#6-Amory-Building-Coherent-Narrative-Driven-Agent-Memory-through-Agentic-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[6] Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning cs.CL | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06282">Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06282" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Zhou, Xiaobo Guo, Belhassen Bayar, Srinivasan H. Sengamedu</span></p>
<p><strong>TL;DR:</strong> 本文提出了Amory，一种通过增强离线时智能体推理来主动构建结构化记忆表示的工作记忆框架，旨在解决长期对话智能体在处理完整对话历史时面临的计算可扩展性挑战。Amory将对话片段组织成情景叙事，通过动量机制巩固记忆，并将外围事实语义化存入语义记忆，在检索时采用基于叙事结构的连贯性驱动推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决长期对话智能体因重复处理整个对话历史而导致的计算成本过高问题，并克服现有方法（如基于嵌入或图表示的RAG风格检索）在记忆形成上过于简化、无法捕捉人类记忆的微妙性和连贯性的缺陷。</p>
<p><strong>Result:</strong> 在长期推理基准LOCOMO上评估，Amory相比之前的最先进方法（SOTA）取得了显著提升，其性能与完整上下文推理相当，同时将响应时间减少了50%。分析表明，动量感知的巩固显著提高了响应质量，而连贯性驱动的检索相比基于嵌入的方法提供了更优的记忆覆盖。</p>
<p><strong>Insight:</strong> 创新点在于将记忆构建视为一个主动的、基于智能体推理的过程，通过组织情景叙事、动量巩固和语义化来构建结构化记忆，并采用连贯性驱动检索，这超越了传统被动、碎片化的记忆表示方法，旨在更好地模拟人类记忆的连贯性和动态性。</p>
<p><strong>Abstract:</strong> Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.</p>
  </div>
</details>

<hr>
<h3 id="7-How-well-can-off-the-shelf-LLMs-elucidate-molecular-structures-from-mass-spectra-using-chain-of-thought-reasoning-cs-CL-cs-LGPDF"><a href="#7-How-well-can-off-the-shelf-LLMs-elucidate-molecular-structures-from-mass-spectra-using-chain-of-thought-reasoning-cs-CL-cs-LGPDF" class="headerlink" title="[7] How well can off-the-shelf LLMs elucidate molecular structures from mass spectra using chain-of-thought reasoning? cs.CL | cs.LGPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06289">How well can off-the-shelf LLMs elucidate molecular structures from mass spectra using chain-of-thought reasoning?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06289" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yufeng Wang, Lu Wei, Lin Liu, Hao Xu, Haibin Ling</span></p>
<p><strong>TL;DR:</strong> 本文评估了现成大型语言模型（LLMs）通过思维链（CoT）推理从串联质谱（MS&#x2F;MS）数据中阐明分子结构的能力。研究引入了一个CoT提示框架和基准，在MassSpecGym数据集上以零样本方式测试了多个先进LLMs（如Claude-3.5-Sonnet、GPT-4o-mini和Llama-3系列），发现LLMs能生成语法有效且部分合理的结构，但无法达到化学准确性或正确关联推理与分子预测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 质谱（MS）是识别小分子的强大分析技术，但由于复杂的碎片模式和化学空间的巨大多样性，直接从串联质谱（MS&#x2F;MS）确定完整分子结构仍是一个长期挑战。近期LLMs在推理密集型科学任务中显示出潜力，但其化学解释能力尚不明确。</p>
<p><strong>Result:</strong> 在MassSpecGym数据集上，使用SMILES有效性、分子式一致性和结构相似性等指标评估，LLMs能生成语法有效且部分合理的结构，但未能达到化学准确性或正确关联推理与分子预测，表明当前LLMs在该任务上性能有限。</p>
<p><strong>Insight:</strong> 论文的创新点在于将化学专家推理步骤（如双键等价分析、中性丢失识别和碎片组装）形式化为结构化提示，构建了一个评估LLMs化学推理能力的CoT框架；客观来看，这为结合领域知识和强化学习实现化学基础的AI推理提供了基础，并揭示了LLMs在科学解释任务中的潜力与局限。</p>
<p><strong>Abstract:</strong> Mass spectrometry (MS) is a powerful analytical technique for identifying small molecules, yet determining complete molecular structures directly from tandem mass spectra (MS&#x2F;MS) remains a long-standing challenge due to complex fragmentation patterns and the vast diversity of chemical space. Recent progress in large language models (LLMs) has shown promise for reasoning-intensive scientific tasks, but their capability for chemical interpretation is still unclear. In this work, we introduce a Chain-of-Thought (CoT) prompting framework and benchmark that evaluate how LLMs reason about mass spectral data to predict molecular structures. We formalize expert chemists’ reasoning steps-such as double bond equivalent (DBE) analysis, neutral loss identification, and fragment assembly-into structured prompts and assess multiple state-of-the-art LLMs (Claude-3.5-Sonnet, GPT-4o-mini, and Llama-3 series) in a zero-shot setting using the MassSpecGym dataset. Our evaluation across metrics of SMILES validity, formula consistency, and structural similarity reveals that while LLMs can produce syntactically valid and partially plausible structures, they fail to achieve chemical accuracy or link reasoning to correct molecular predictions. These findings highlight both the interpretive potential and the current limitations of LLM-based reasoning for molecular elucidation, providing a foundation for future work that combines domain knowledge and reinforcement learning to achieve chemically grounded AI reasoning.</p>
  </div>
</details>

<hr>
<h3 id="8-texttt-AMEND-Benchmarking-Eligibility-Criteria-Amendments-in-Clinical-Trials-cs-CL-cs-AI-cs-LGPDF"><a href="#8-texttt-AMEND-Benchmarking-Eligibility-Criteria-Amendments-in-Clinical-Trials-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[8] $\texttt{AMEND++}$: Benchmarking Eligibility Criteria Amendments in Clinical Trials cs.CL | cs.AI | cs.LGPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06300">$\texttt{AMEND++}$: Benchmarking Eligibility Criteria Amendments in Clinical Trials</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06300" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Trisha Das, Mandis Beigi, Jacob Aptekar, Jimeng Sun</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个新的NLP任务——临床试验资格标准修订预测，旨在预测初始试验方案的资格标准是否会在未来被修订。为此，作者发布了AMEND++基准套件，包含AMEND和AMEND_LLM两个数据集，并提出了Change-Aware Masked Language Modeling (CAMLM)预训练策略，该策略利用历史编辑信息来学习对修订敏感的表示。实验表明，CAMLM能持续改进修订预测性能，有助于设计更稳健、更具成本效益的临床试验。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 临床试验修订经常导致延误、成本增加和行政负担，而资格标准是最常被修订的部分。为了解决这个问题，论文旨在开发一种能够预测资格标准未来是否会被修订的方法，以优化试验设计。</p>
<p><strong>Result:</strong> 在多个基线模型上的实验表明，所提出的CAMLM方法持续改进了修订预测性能，使得临床实验设计更加稳健和具有成本效益。</p>
<p><strong>Insight:</strong> 论文的创新点包括：1) 提出了“资格标准修订预测”这一新的NLP任务；2) 构建并发布了包含原始和LLM去噪版本的AMEND++基准数据集；3) 提出了CAMLM预训练策略，通过利用历史编辑信息来学习对修订敏感的表示，这是一种结合文档修订历史的上下文感知学习方法。</p>
<p><strong>Abstract:</strong> Clinical trial amendments frequently introduce delays, increased costs, and administrative burden, with eligibility criteria being the most commonly amended component. We introduce \textit{eligibility criteria amendment prediction}, a novel NLP task that aims to forecast whether the eligibility criteria of an initial trial protocol will undergo future amendments. To support this task, we release $\texttt{AMEND++}$, a benchmark suite comprising two datasets: $\texttt{AMEND}$, which captures eligibility-criteria version histories and amendment labels from public clinical trials, and $\verb|AMEND_LLM|$, a refined subset curated using an LLM-based denoising pipeline to isolate substantive changes. We further propose $\textit{Change-Aware Masked Language Modeling}$ (CAMLM), a revision-aware pretraining strategy that leverages historical edits to learn amendment-sensitive representations. Experiments across diverse baselines show that CAMLM consistently improves amendment prediction, enabling more robust and cost-effective clinical trial design.</p>
  </div>
</details>

<hr>
<h3 id="9-AfriqueLLM-How-Data-Mixing-and-Model-Architecture-Impact-Continued-Pre-training-for-African-Languages-cs-CLPDF"><a href="#9-AfriqueLLM-How-Data-Mixing-and-Model-Architecture-Impact-Continued-Pre-training-for-African-Languages-cs-CLPDF" class="headerlink" title="[9] AfriqueLLM: How Data Mixing and Model Architecture Impact Continued Pre-training for African Languages cs.CLPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06395">AfriqueLLM: How Data Mixing and Model Architecture Impact Continued Pre-training for African Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06395" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hao Yu, Tianyi Xu, Michael A. Hedderich, Wassim Hamidouche, Syed Waqas Zamir</span></p>
<p><strong>TL;DR:</strong> 本文提出了AfriqueLLM，一套通过持续预训练（CPT）适应20种非洲语言的开源大语言模型（LLMs）。研究基于包括Llama 3.1、Gemma 3和Qwen 3在内的五种不同规模和架构的基础模型，在260亿token上进行了CPT，并系统分析了CPT数据组成（包括数学、代码和合成翻译数据）对下游性能的影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有开源多语言大模型在非洲语言上表现显著落后于专有系统，而针对低资源语言的持续预训练在数学推理等高要求能力上改进有限，这主要源于低资源语料库领域覆盖不均和任务相关知识缺失。</p>
<p><strong>Result:</strong> 在多个多语言基准测试上的评估结果表明，数据组成是CPT性能提升的主要驱动力，添加数学、代码和合成翻译数据能带来一致改进，包括推理导向的评估。在固定架构下，更大模型通常性能更好，但在跨模型族比较时，架构选择比规模更重要。最佳模型在长上下文性能（包括文档级翻译）上有所提升。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于通过系统性的实证研究，揭示了CPT中数据组成（而非仅仅是数据量）对模型性能的关键作用，并指出基础模型的多语言性能并不能可靠预测CPT后的结果，而稳健的架构与任务对齐的数据相结合才是更可靠的方案。这为低资源语言的高效模型适配提供了重要指导。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly multilingual, yet open models continue to underperform relative to proprietary systems, with the gap most pronounced for African languages. Continued pre-training (CPT) offers a practical route to language adaptation, but improvements on demanding capabilities such as mathematical reasoning often remain limited. This limitation is driven in part by the uneven domain coverage and missing task-relevant knowledge that characterize many low-resource language corpora. We present \texttt{AfriqueLLM}, a suite of open LLMs adapted to 20 African languages through CPT on 26B tokens. We perform a comprehensive empirical study across five base models spanning sizes and architectures, including Llama 3.1, Gemma 3, and Qwen 3, and systematically analyze how CPT data composition shapes downstream performance. In particular, we vary mixtures that include math, code, and synthetic translated data, and evaluate the resulting models on a range of multilingual benchmarks. Our results identify data composition as the primary driver of CPT gains. Adding math, code, and synthetic translated data yields consistent improvements, including on reasoning-oriented evaluations. Within a fixed architecture, larger models typically improve performance, but architectural choices dominate scale when comparing across model families. Moreover, strong multilingual performance in the base model does not reliably predict post-CPT outcomes; robust architectures coupled with task-aligned data provide a more dependable recipe. Finally, our best models improve long-context performance, including document-level translation. Models have been released on <a target="_blank" rel="noopener" href="https://huggingface.co/collections/McGill-NLP/afriquellm">Huggingface</a>.</p>
  </div>
</details>

<hr>
<h3 id="10-Structured-Episodic-Event-Memory-cs-CLPDF"><a href="#10-Structured-Episodic-Event-Memory-cs-CLPDF" class="headerlink" title="[10] Structured Episodic Event Memory cs.CLPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06411">Structured Episodic Event Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06411" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhengxuan Lu, Dongfang Li, Yukun Shi, Beilun Wang, Longyue Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为结构化情景事件记忆（SEEM）的分层框架，旨在解决大型语言模型（LLM）中现有静态检索增强生成（RAG）方法在复杂推理时存在的检索分散和结构依赖捕获不足的问题。SEEM结合了用于关系事实的图记忆层和用于叙事进展的动态情景记忆层，通过认知框架理论将交互流转化为结构化的情景事件框架（EEF），并引入了关联融合与反向来源扩展（RPE）机制来从碎片化证据中重建连贯的叙事上下文。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLM中的记忆方法主要依赖静态RAG，这导致检索分散且难以捕捉复杂推理所需的结构依赖关系；对于自主智能体而言，这些被动且扁平化的架构缺乏对长期交互动态性和关联性进行建模所需的认知组织能力。</p>
<p><strong>Result:</strong> 在LoCoMo和LongMemEval基准测试上的实验结果表明，SEEM显著优于基线方法，使智能体能够保持更优的叙事连贯性和逻辑一致性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个结合图记忆与动态情景记忆的分层框架，并基于认知框架理论构建了结构化的情景事件框架（EEF），以及引入了关联融合与反向来源扩展（RPE）机制来增强从碎片化信息中重建连贯上下文的能力，这为智能体的长期记忆和复杂推理提供了更接近人类认知的组织结构。</p>
<p><strong>Abstract:</strong> Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.</p>
  </div>
</details>

<hr>
<h3 id="11-Can-a-Unimodal-Language-Agent-Provide-Preferences-to-Tune-a-Multimodal-Vision-Language-Model-cs-CLPDF"><a href="#11-Can-a-Unimodal-Language-Agent-Provide-Preferences-to-Tune-a-Multimodal-Vision-Language-Model-cs-CLPDF" class="headerlink" title="[11] Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model? cs.CLPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06424">Can a Unimodal Language Agent Provide Preferences to Tune a Multimodal Vision-Language Model?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06424" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sazia Tabasum Mim, Jack Morris, Manish Dhakal, Yanming Xiu, Maria Gorlatova</span></p>
<p><strong>TL;DR:</strong> 本文探讨了单模态语言模型能否通过纯文本推理自身信息需求，并为多模态视觉语言模型提供有效反馈以优化其生成。研究提出了一种方法，使语言智能体能够向视觉语言模型提供偏好反馈，从而调整文本生成以符合智能体偏好。实验结果表明，该方法能显著提升VLM的多模态场景描述质量，帮助LLM更好地理解多模态上下文。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索一种可扩展的路径，为现有LLMs添加多模态能力，核心问题是验证单模态LLM能否仅通过文本推理自身信息需求，并为多模态模型提供有效反馈以实现优化。</p>
<p><strong>Result:</strong> 在实验中，LLM偏好反馈显著提升了VLM描述质量，相比基线多模态方法，绝对准确率最大提升13%；人类研究验证了AI驱动反馈的有效性，LLM选择与人类判断的偏好对齐率达到64.6%。</p>
<p><strong>Insight:</strong> 创新点在于利用单模态LLM的文本推理能力指导多模态VLM的优化，实现了跨模态的偏好对齐；客观分析表明，该方法为多模态模型调优提供了一种无需人工标注的高效反馈机制，但可能存在对LLM推理能力的依赖限制。</p>
<p><strong>Abstract:</strong> To explore a more scalable path for adding multimodal capabilities to existing LLMs, this paper addresses a fundamental question: Can a unimodal LLM, relying solely on text, reason about its own informational needs and provide effective feedback to optimize a multimodal model? To answer this, we propose a method that enables a language agent to give feedback to a vision-language model (VLM) to adapt text generation to the agent’s preferences. Our results from different experiments affirm this hypothesis, showing that LLM preference feedback significantly enhances VLM descriptions. Using our proposed method, we find that the VLM can generate multimodal scene descriptions to help the LLM better understand multimodal context, leading to improvements of maximum 13% in absolute accuracy compared to the baseline multimodal approach. Furthermore, a human study validated our AI-driven feedback, showing a 64.6% preference alignment rate between the LLM’s choices and human judgments. Extensive experiments provide insights on how and why the method works and its limitations.</p>
  </div>
</details>

<hr>
<h3 id="12-Time-Travel-Engine-A-Shared-Latent-Chronological-Manifold-Enables-Historical-Navigation-in-Large-Language-Models-cs-CLPDF"><a href="#12-Time-Travel-Engine-A-Shared-Latent-Chronological-Manifold-Enables-Historical-Navigation-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[12] Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models cs.CLPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06437">Time Travel Engine: A Shared Latent Chronological Manifold Enables Historical Navigation in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06437" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jingmin An, Wei Liu, Qian Wang, Fang Fang</span></p>
<p><strong>TL;DR:</strong> 本文提出Time Travel Engine（TTE）框架，通过将历时语言模式投影到共享的时序流形上，揭示了大型语言模型（LLMs）潜在空间中时间信息以连续、可遍历的几何结构组织，而非离散聚类。TTE直接调制潜在表示以诱导与目标时代一致的风格、词汇和概念转变，实现跨时期的流畅导航，同时限制未来知识的访问。实验表明，中文和英文的时序子空间存在拓扑同构，暗示不同语言共享历史演化的通用几何逻辑。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 时间是人类认知的基本维度，但LLMs编码时间进展的机制尚不明确，本文旨在揭示其潜在空间中时间信息的组织方式，并开发能够控制时序推理的框架。</p>
<p><strong>Result:</strong> 实验在多种模型架构上进行，结果表明TTE能够实现连贯的历时风格转变，并验证了中英文时序子空间之间的拓扑同构性，为时序控制提供了新范式。</p>
<p><strong>Insight:</strong> 创新点在于将历时演化参数化为残差流中的连续流形，实现了对模型潜在表示的直接调制以控制时序属性，并发现了跨语言时序表示的几何普适性，连接了历史语言学与机制可解释性。</p>
<p><strong>Abstract:</strong> Time functions as a fundamental dimension of human cognition, yet the mechanisms by which Large Language Models (LLMs) encode chronological progression remain opaque. We demonstrate that temporal information in their latent space is organized not as discrete clusters but as a continuous, traversable geometry. We introduce the Time Travel Engine (TTE), an interpretability-driven framework that projects diachronic linguistic patterns onto a shared chronological manifold. Unlike surface-level prompting, TTE directly modulates latent representations to induce coherent stylistic, lexical, and conceptual shifts aligned with target eras. By parameterizing diachronic evolution as a continuous manifold within the residual stream, TTE enables fluid navigation through period-specific “zeitgeists” while restricting access to future knowledge. Furthermore, experiments across diverse architectures reveal topological isomorphism between the temporal subspaces of Chinese and English-indicating that distinct languages share a universal geometric logic of historical evolution. These findings bridge historical linguistics with mechanistic interpretability, offering a novel paradigm for controlling temporal reasoning in neural networks.</p>
  </div>
</details>

<hr>
<h3 id="13-IndRegBias-A-Dataset-for-Studying-Indian-Regional-Biases-in-English-and-Code-Mixed-Social-Media-Comments-cs-CL-cs-CY-cs-SIPDF"><a href="#13-IndRegBias-A-Dataset-for-Studying-Indian-Regional-Biases-in-English-and-Code-Mixed-Social-Media-Comments-cs-CL-cs-CY-cs-SIPDF" class="headerlink" title="[13] IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments cs.CL | cs.CY | cs.SIPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06477">IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY | cs.SI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06477" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Debasmita Panda, Akash Anil, Neelesh Kumar Shukla</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为IndRegBias的数据集，用于研究印度社交媒体评论中存在的区域偏见，该数据集包含来自Reddit和YouTube的25,000条英文和混合语言评论，并采用多级标注策略评估偏见严重性。作者评估了开源大语言模型和印度语言模型在零样本、少样本和微调设置下检测区域偏见的能力，发现微调能显著提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有自然语言处理研究主要关注性别、种族等社会偏见，而区域偏见因数据集提取困难、标注不一致及常与其他偏见混合而研究不足，本文旨在填补印度语境下区域偏见数据集的空白。</p>
<p><strong>Result:</strong> 在IndRegBias数据集上，大多数LLMs和ILMs的零样本和少样本方法检测区域偏见及其严重性的准确率较低，但微调方法显著提升了LLM在检测印度区域偏见及其严重性方面的性能。</p>
<p><strong>Insight:</strong> 创新点包括构建首个专注于印度区域偏见的社交媒体数据集，提出多级标注策略以量化偏见严重性，并系统评估了LLMs和ILMs在不同学习范式下的偏见检测能力，为区域偏见研究提供了数据和方法基础。</p>
<p><strong>Abstract:</strong> Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users’ comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.</p>
  </div>
</details>

<hr>
<h3 id="14-Spec-o3-A-Tool-Augmented-Vision-Language-Agent-for-Rare-Celestial-Object-Candidate-Vetting-via-Automated-Spectral-Inspection-cs-CL-astro-ph-IMPDF"><a href="#14-Spec-o3-A-Tool-Augmented-Vision-Language-Agent-for-Rare-Celestial-Object-Candidate-Vetting-via-Automated-Spectral-Inspection-cs-CL-astro-ph-IMPDF" class="headerlink" title="[14] Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection cs.CL | astro-ph.IMPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06498">Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | astro-ph.IM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06498" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minghui Jia, Qichao Zhang, Ali Luo, Linjing Li, Shuo Ye</span></p>
<p><strong>TL;DR:</strong> 本文提出Spec-o3，一个工具增强的视觉语言智能体，用于通过自动化光谱检查来审核罕见天体候选对象。它通过多模态思维链推理模拟天文学家的检查过程，以解决深度学习分类器泛化性和可解释性不足、以及人工检查无法应对现代光谱巡天数据量激增的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前罕见天体候选对象的最终审核仍依赖专家人工视觉检查，这是一个劳动密集型过程，且无法随现代光谱巡天数据量的激增而扩展。深度学习分类器则存在泛化性和可解释性不足的问题，因此需要一种自动化、可扩展且可靠的解决方案。</p>
<p><strong>Result:</strong> 在LAMOST的五个罕见天体识别任务上，Spec-o3取得了新的SOTA结果，将macro-F1分数从28.3提升至76.5（使用7B参数的基础模型），性能优于专有的视觉语言大模型和专门的深度模型。同时，该智能体在跨巡天数据（从LAMOST到SDSS&#x2F;DESI）的未见检查任务上表现出很强的泛化能力，专家评估确认其推理轨迹连贯且物理一致。</p>
<p><strong>Insight:</strong> 论文的创新点在于设计了一个工具增强的视觉语言智能体，通过模拟天文学家使用专业工具进行光谱分析的多模态思维链推理过程，实现了自动化、可解释且可泛化的罕见天体审核。其两阶段后训练方法（基于专家轨迹的监督微调，结合基于结果的强化学习）有效地将领域专家知识注入模型，提升了任务性能与决策透明度。</p>
<p><strong>Abstract:</strong> Due to the limited generalization and interpretability of deep learning classifiers, The final vetting of rare celestial object candidates still relies on expert visual inspection–a manually intensive process. In this process, astronomers leverage specialized tools to analyze spectra and construct reliable catalogs. However, this practice has become the primary bottleneck, as it is fundamentally incapable of scaling with the data deluge from modern spectroscopic surveys. To bridge this gap, we propose Spec-o3, a tool-augmented vision-language agent that performs astronomer-aligned spectral inspection via interleaved multimodal chain-of-thought reasoning. Spec-o3 is trained with a two-stage post-training recipe: cold-start supervised fine-tuning on expert inspection trajectories followed by outcome-based reinforcement learning on rare-type verification tasks. Evaluated on five rare-object identification tasks from LAMOST, Spec-o3 establishes a new State-of-the-Art, boosting the macro-F1 score from 28.3 to 76.5 with a 7B parameter base model and outperforming both proprietary VLMs and specialized deep models. Crucially, the agent demonstrates strong generalization to unseen inspection tasks across survey shifts (from LAMOST to SDSS&#x2F;DESI). Expert evaluations confirm that its reasoning traces are coherent and physically consistent, supporting transparent and trustworthy decision-making. Code, data, and models are available at \href{<a target="_blank" rel="noopener" href="https://github.com/Maxwell-Jia/spec-o3%7D%7BProject">https://github.com/Maxwell-Jia/spec-o3}{Project</a> HomePage}.</p>
  </div>
</details>

<hr>
<h3 id="15-Atomic-SNLI-Fine-Grained-Natural-Language-Inference-through-Atomic-Fact-Decomposition-cs-CL-cs-AIPDF"><a href="#15-Atomic-SNLI-Fine-Grained-Natural-Language-Inference-through-Atomic-Fact-Decomposition-cs-CL-cs-AIPDF" class="headerlink" title="[15] Atomic-SNLI: Fine-Grained Natural Language Inference through Atomic Fact Decomposition cs.CL | cs.AIPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06528">Atomic-SNLI: Fine-Grained Natural Language Inference through Atomic Fact Decomposition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06528" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minghui Huang</span></p>
<p><strong>TL;DR:</strong> 本文针对现有自然语言推理（NLI）系统在句子层面缺乏解释性的问题，提出了通过原子事实分解进行细粒度推理的方法。研究发现，传统假设（即仅当所有原子事实都被蕴含时假设才成立）在实践中因模型细粒度推理能力差而失效。为此，作者构建了Atomic-SNLI数据集，通过语言学的生成策略分解并丰富了SNLI的原子级示例。实验表明，在该数据集上微调的模型在原子推理能力上显著提升，同时保持句子级性能，实现了事实层面的准确判断和可解释结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有NLI系统在句子层面提供黑盒决策、缺乏解释性的问题，并指出传统原子级NLI假设（所有原子事实蕴含则假设成立）因模型细粒度推理能力不足而失败，需要提升模型的原子级推理性能。</p>
<p><strong>Result:</strong> 在构建的Atomic-SNLI数据集上微调的模型在原子推理能力上取得显著改进，同时保持了强句子级性能，实现了事实层面的准确和可解释推理。</p>
<p><strong>Insight:</strong> 创新点在于通过原子事实分解构建细粒度NLI数据集（Atomic-SNLI），并采用语言学驱动的生成策略来增强原子级示例，从而提升模型的细粒度推理和可解释性；从客观角度看，该方法将NLI从句子级黑盒决策转向事实级透明推理，为可解释AI提供了实用路径。</p>
<p><strong>Abstract:</strong> Current Natural Language Inference (NLI) systems primarily operate at the sentence level, providing black-box decisions that lack explanatory power. While atomic-level NLI offers a promising alternative by decomposing hypotheses into individual facts, we demonstrate that the conventional assumption that a hypothesis is entailed only when all its atomic facts are entailed fails in practice due to models’ poor performance on fine-grained reasoning. Our analysis reveals that existing models perform substantially worse on atomic level inference compared to sentence level tasks. To address this limitation, we introduce Atomic-SNLI, a novel dataset constructed by decomposing SNLI and enriching it with carefully curated atomic level examples through linguistically informed generation strategies. Experimental results demonstrate that models fine-tuned on Atomic-SNLI achieve significant improvements in atomic reasoning capabilities while maintaining strong sentence level performance, enabling both accurate judgements and transparent, explainable results at the fact level.</p>
  </div>
</details>

<hr>
<h3 id="16-SimLLM-Fine-Tuning-Code-LLMs-for-SimPy-Based-Queueing-System-Simulation-cs-CL-cs-AI-cs-LGPDF"><a href="#16-SimLLM-Fine-Tuning-Code-LLMs-for-SimPy-Based-Queueing-System-Simulation-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[16] SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation cs.CL | cs.AI | cs.LGPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06543">SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06543" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jun-Qi Chen, Kun Zhang, Rui Zheng, Ying Zhong</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为SimLLM的方法，通过多阶段微调（包括两阶段监督微调和一阶段直接偏好优化）来增强开源代码大语言模型（如Qwen-Coder-7B和DeepSeek-Coder-6.7B）在生成基于SimPy的排队系统仿真代码方面的能力，以解决使用闭源模型（如GPT-4o）时的高计算成本和数据隐私问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> SimPy作为广泛使用的排队系统建模Python包，结合LLM生成可执行代码的潜力，但直接使用闭源LLM存在高成本和隐私风险，因此需要探索通过微调开源模型来生成可靠仿真代码的替代方案。</p>
<p><strong>Result:</strong> 经过微调后，两个模型在可执行性、输出格式合规性和指令-代码一致性方面均取得显著提升，表明领域特定微调能将紧凑的开源代码模型转化为可靠的SimPy仿真生成器，为教育、研究和运营决策支持提供了实用替代方案。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个多阶段微调框架（SFT+DPO）来渐进式提升模型在特定领域（SimPy排队仿真）的代码生成性能，这为将通用开源代码LLM定制化为领域专用工具提供了可借鉴的微调策略，特别是在平衡性能与隐私&#x2F;成本方面。</p>
<p><strong>Abstract:</strong> The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model’s ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.</p>
  </div>
</details>

<hr>
<h3 id="17-Probing-Multimodal-Large-Language-Models-on-Cognitive-Biases-in-Chinese-Short-Video-Misinformation-cs-CLPDF"><a href="#17-Probing-Multimodal-Large-Language-Models-on-Cognitive-Biases-in-Chinese-Short-Video-Misinformation-cs-CLPDF" class="headerlink" title="[17] Probing Multimodal Large Language Models on Cognitive Biases in Chinese Short-Video Misinformation cs.CLPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06600">Probing Multimodal Large Language Models on Cognitive Biases in Chinese Short-Video Misinformation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06600" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jen-tse Huang, Chang Chen, Shiyang Lai, Wenxuan Wang, Michelle R. Kaufman</span></p>
<p><strong>TL;DR:</strong> 本文提出一个评估框架，用于探究多模态大语言模型在中文短视频健康领域错误信息中的认知偏见鲁棒性。研究基于一个包含200个短视频的高质量人工标注数据集，涵盖三种欺骗模式，并评估了八个前沿MLLM在五种模态设置下的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 短视频平台已成为错误信息传播的主要渠道，而现有研究对MLLM在涉及认知偏见的错误信息中的鲁棒性探索不足。</p>
<p><strong>Result:</strong> 实验表明，在多模态设置下，Gemini-2.5-Pro表现最佳，信念得分为71.5&#x2F;100，而o3表现最差，为35.2。模型容易受到权威频道ID等社会线索诱导的偏见影响。</p>
<p><strong>Insight:</strong> 研究创新点在于构建了一个细粒度标注的短视频错误信息数据集，并系统评估了MLLM对认知偏见的敏感性，揭示了模型在社会线索影响下的脆弱性，为提升MLLM的可靠性和抗误导能力提供了重要洞见。</p>
<p><strong>Abstract:</strong> Short-video platforms have become major channels for misinformation, where deceptive claims frequently leverage visual experiments and social cues. While Multimodal Large Language Models (MLLMs) have demonstrated impressive reasoning capabilities, their robustness against misinformation entangled with cognitive biases remains under-explored. In this paper, we introduce a comprehensive evaluation framework using a high-quality, manually annotated dataset of 200 short videos spanning four health domains. This dataset provides fine-grained annotations for three deceptive patterns, experimental errors, logical fallacies, and fabricated claims, each verified by evidence such as national standards and academic literature. We evaluate eight frontier MLLMs across five modality settings. Experimental results demonstrate that Gemini-2.5-Pro achieves the highest performance in the multimodal setting with a belief score of 71.5&#x2F;100, while o3 performs the worst at 35.2. Furthermore, we investigate social cues that induce false beliefs in videos and find that models are susceptible to biases like authoritative channel IDs.</p>
  </div>
</details>

<hr>
<h3 id="18-N2N-GQA-Noise-to-Narrative-for-Graph-Based-Table-Text-Question-Answering-Using-LLMs-cs-CLPDF"><a href="#18-N2N-GQA-Noise-to-Narrative-for-Graph-Based-Table-Text-Question-Answering-Using-LLMs-cs-CLPDF" class="headerlink" title="[18] N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs cs.CLPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06603">N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06603" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohamed Sharafath, Aravindh Annamalai, Ganesh Murugan, Aravindakumar Venugopalan</span></p>
<p><strong>TL;DR:</strong> N2N-GQA是一个零样本框架，用于开放域混合表格-文本问答，它通过将检索到的文档建模为图节点、语义关系为边，构建动态证据图来提升多跳推理能力，在OTT-QA基准上实现了显著性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决标准检索增强生成（RAG）管道将文档处理为扁平排序列表时，检索噪声会模糊推理链的问题，特别是在混合表格-文本数据的多跳问答中。</p>
<p><strong>Result:</strong> 在OTT-QA基准上，基于图的证据整理比强基线提高了19.9个点的EM分数，达到48.80 EM，匹配了微调检索模型（CORE: 49.0 EM）并接近高度优化的系统（COS: 56.9 EM），且无需任务特定训练。</p>
<p><strong>Insight:</strong> 创新点在于首次提出零样本框架，通过构建动态证据图来识别连接推理步骤的桥接文档，这弥补了基于列表检索的不足；客观分析表明，将检索结果组织为结构化图对于可扩展的多跳问答系统至关重要，且简单可解释的图构建方法能与复杂的微调方法相媲美。</p>
<p><strong>Abstract:</strong> Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.</p>
  </div>
</details>

<hr>
<h3 id="19-MedEinst-Benchmarking-the-Einstellung-Effect-in-Medical-LLMs-through-Counterfactual-Differential-Diagnosis-cs-CL-cs-AIPDF"><a href="#19-MedEinst-Benchmarking-the-Einstellung-Effect-in-Medical-LLMs-through-Counterfactual-Differential-Diagnosis-cs-CL-cs-AIPDF" class="headerlink" title="[19] MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis cs.CL | cs.AIPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06636">MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06636" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenting Chen, Zhongrui Zhu, Guolin Huang, Wenxuan Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了MedEinst基准，用于评估医学大语言模型（LLMs）在临床诊断中的Einstellung效应（即依赖统计捷径而非患者特异性证据，导致非典型病例误诊），并开发了ECR-Agent方法，通过动态因果推理和批评驱动的图与记忆演化来对齐基于证据的医学标准，以缓解该效应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLMs在医学基准测试中取得高准确率，但在临床诊断中表现出Einstellung效应，即依赖统计捷径而非患者特异性证据，导致非典型病例误诊，而现有基准无法检测这一关键失败模式。</p>
<p><strong>Result:</strong> 在涵盖49种疾病的5,383对临床病例（包括对照病例和“陷阱”病例）的MedEinst基准上，评估了17个LLMs，结果显示前沿模型具有高基线准确率但严重的偏差陷阱率（Bias Trap Rate）。</p>
<p><strong>Insight:</strong> 创新点包括：1) 引入反事实基准MedEinst来量化Einstellung效应；2) 提出ECR-Agent，通过动态因果推理（DCI）进行结构化推理和证据审计，以及批评驱动的图与记忆演化（CGME）迭代优化系统，以对齐基于证据的医学标准。</p>
<p><strong>Abstract:</strong> Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis–relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a “trap” case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate–probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.</p>
  </div>
</details>

<hr>
<h3 id="20-Do-Language-Models-Reason-Across-Languages-cs-CL-cs-AIPDF"><a href="#20-Do-Language-Models-Reason-Across-Languages-cs-CL-cs-AIPDF" class="headerlink" title="[20] Do Language Models Reason Across Languages? cs.CL | cs.AIPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06644">Do Language Models Reason Across Languages?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06644" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yan Meng, Wafaa Mohammed, Christof Monz</span></p>
<p><strong>TL;DR:</strong> 本文研究了语言模型在多语言环境下的推理能力，特别是能否跨语言合成信息。通过一个简单的两跳问答任务，发现模型对答案跨度文档的语言变化更敏感，且推理过程缺乏忠实的分步分解，导致组合失败。作者提出了一种三阶段的SUBQ提示方法，显著提升了推理准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实世界的信息源本质上是多语言的，因此需要探究语言模型是否能跨语言合成信息，并评估其在多语言文档上的推理能力。</p>
<p><strong>Result:</strong> 在提出的两跳问答任务中，模型在高达33%的多语言案例中未能正确推断桥接信息，但最终答案正确；组合失败率约为18%。使用SUBQ提示方法后，准确率从10.1%提升至66.5%。</p>
<p><strong>Insight:</strong> 论文揭示了语言模型在多语言推理中缺乏忠实的分步分解，这是其性能瓶颈；提出的SUBQ提示方法通过引导分步推理有效缓解了这一问题，为提升模型跨语言推理能力提供了简单有效的策略。</p>
<p><strong>Abstract:</strong> The real-world information sources are inherently multilingual, which naturally raises a question about whether language models can synthesize information across languages. In this paper, we introduce a simple two-hop question answering setting, where answering a question requires making inferences over two multilingual documents. We find that language models are more sensitive to language variation in answer-span documents than in those providing bridging information, despite the equal importance of both documents for answering a question. Under a step-by-step sub-question evaluation, we further show that in up to 33% of multilingual cases, models fail to infer the bridging information in the first step yet still answer the overall question correctly. This indicates that reasoning in language models, especially in multilingual settings, does not follow a faithful step-by-step decomposition. Subsequently, we show that the absence of reasoning decomposition leads to around 18% composition failure, where both sub-questions are answered correctly but fail for the final two-hop questions. To mitigate this, we propose a simple three-stage SUBQ prompting method to guide the multi-step reasoning with sub-questions, which boosts accuracy from 10.1% to 66.5%.</p>
  </div>
</details>

<hr>
<h3 id="21-IDRBench-Interactive-Deep-Research-Benchmark-cs-CL-cs-AI-cs-HCPDF"><a href="#21-IDRBench-Interactive-Deep-Research-Benchmark-cs-CL-cs-AI-cs-HCPDF" class="headerlink" title="[21] IDRBench: Interactive Deep Research Benchmark cs.CL | cs.AI | cs.HCPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06676">IDRBench: Interactive Deep Research Benchmark</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06676" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yingchaojie Feng, Qiang Huang, Xiaoya Xie, Zhaorui Yang, Jun Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出了IDRBench，这是首个用于系统评估交互式深度研究的基准测试。它结合了模块化多智能体研究框架、按需交互机制、可扩展的基于参考的用户模拟器，以及一个能同时衡量交互收益（质量和对齐度）与成本（交互轮次和令牌数）的交互感知评估套件。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于大语言模型的深度研究智能体大多以自主方式运行，假设用户意图完全明确且仅评估最终输出，而实际研究中目标往往不明确且会在探索过程中演变，因此持续交互对于实现稳健对齐至关重要，但现有基准测试未能有效建模动态用户反馈或量化其成本。</p>
<p><strong>Result:</strong> 在七个最先进的大语言模型上的实验表明，交互能持续提升研究质量和鲁棒性，其收益往往超过模型能力差异带来的影响，但同时也揭示了交互效率方面存在显著的权衡。</p>
<p><strong>Insight:</strong> 论文的主要创新点在于首次构建了一个系统评估交互式深度研究的基准，其核心是引入了交互感知的评估框架，能够联合量化交互的收益与成本，这为未来开发更高效、更对齐用户动态意图的研究智能体提供了重要的评估工具和方向指引。</p>
<p><strong>Abstract:</strong> Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.</p>
  </div>
</details>

<hr>
<h3 id="22-Characterising-Toxicity-in-Generative-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#22-Characterising-Toxicity-in-Generative-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[22] Characterising Toxicity in Generative Large Language Models cs.CL | cs.AIPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06700">Characterising Toxicity in Generative Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06700" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiyao Zhang, Yazan Mash’Al, Yuhan Wu</span></p>
<p><strong>TL;DR:</strong> 本文研究了生成式大语言模型（LLM）在特定提示下产生有害（毒性）内容的程度，并分析了影响毒性生成的词汇和句法等语言因素。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管基于Transformer的仅解码器架构在文本处理与生成上取得突破，且RLHF等方法被用于对齐人类价值观，但模型仍可能生成不当、冒犯或有害的回应（统称为“毒性”输出），且现有防护措施可通过精心设计的提示绕过，因此需要深入探究LLM生成毒性内容的机制。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体的定量结果或基准测试，但表明通过分析提示下的毒性生成程度及相关语言因素来展开研究。</p>
<p><strong>Insight:</strong> 创新点在于系统性地从词汇和句法层面分析影响LLM毒性生成的语言因素，为理解和缓解模型的有害输出提供了更细致的视角，而非仅依赖对齐技术。</p>
<p><strong>Abstract:</strong> In recent years, the advent of the attention mechanism has significantly advanced the field of natural language processing (NLP), revolutionizing text processing and text generation. This has come about through transformer-based decoder-only architectures, which have become ubiquitous in NLP due to their impressive text processing and generation capabilities. Despite these breakthroughs, language models (LMs) remain susceptible to generating undesired outputs: inappropriate, offensive, or otherwise harmful responses. We will collectively refer to these as &#96;&#96;toxic’’ outputs. Although methods like reinforcement learning from human feedback (RLHF) have been developed to align model outputs with human values, these safeguards can often be circumvented through carefully crafted prompts. Therefore, this paper examines the extent to which LLMs generate toxic content when prompted, as well as the linguistic factors – both lexical and syntactic – that influence the production of such outputs in generative models.</p>
  </div>
</details>

<hr>
<h3 id="23-Evaluating-Accounting-Reasoning-Capabilities-of-Large-Language-Models-cs-CLPDF"><a href="#23-Evaluating-Accounting-Reasoning-Capabilities-of-Large-Language-Models-cs-CLPDF" class="headerlink" title="[23] Evaluating Accounting Reasoning Capabilities of Large Language Models cs.CLPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06707">Evaluating Accounting Reasoning Capabilities of Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06707" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie Zhou, Xin Chen, Jie Zhang, Hai Li, Jie Wang</span></p>
<p><strong>TL;DR:</strong> 本文定义了垂直领域会计推理任务，并基于对代表性GLM模型训练数据特征的分析，提出了相应的评估标准。作者利用该框架评估了GLM-6B、GLM-130B、GLM-4和OpenAI GPT-4在会计推理任务上的表现，发现提示设计对性能有显著影响，其中GPT-4能力最强，但现有模型仍不足以满足现实企业会计需求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决如何将大语言模型有效集成到会计等专业领域，以支持企业数字化转型的关键挑战。</p>
<p><strong>Result:</strong> 在提出的会计推理评估框架下，GPT-4展示了最强的能力，但所有模型（包括GLM系列和GPT-4）的性能仍不足以满足实际企业会计应用，表明需要进一步优化。</p>
<p><strong>Insight:</strong> 创新点在于为垂直领域（会计）定义了专门的推理任务并建立了系统性的评估标准；客观来看，研究强调了提示工程在专业领域评估中的重要性，并指出了当前大模型在复杂专业任务上的局限性，为未来面向专业应用的模型优化指明了方向。</p>
<p><strong>Abstract:</strong> Large language models are transforming learning, cognition, and research across many fields. Effectively integrating them into professional domains, such as accounting, is a key challenge for enterprise digital transformation. To address this, we define vertical domain accounting reasoning and propose evaluation criteria derived from an analysis of the training data characteristics of representative GLM models. These criteria support systematic study of accounting reasoning and provide benchmarks for performance improvement. Using this framework, we evaluate GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4 on accounting reasoning tasks. Results show that prompt design significantly affects performance, with GPT-4 demonstrating the strongest capability. Despite these gains, current models remain insufficient for real-world enterprise accounting, indicating the need for further optimization to unlock their full practical value.</p>
  </div>
</details>

<hr>
<h3 id="24-Towards-Computational-Chinese-Paleography-cs-CLPDF"><a href="#24-Towards-Computational-Chinese-Paleography-cs-CLPDF" class="headerlink" title="[24] Towards Computational Chinese Paleography cs.CLPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06753">Towards Computational Chinese Paleography</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06753" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiran Rex Ma</span></p>
<p><strong>TL;DR:</strong> 这篇立场论文探讨了计算古文字学这一新兴领域的发展轨迹，指出其正从自动化孤立的视觉任务转向构建用于学术研究的集成化数字生态系统。论文首先梳理了甲骨文、金文和竹简等关键数字资源，然后沿着该领域的方法论流程——从基础的视觉处理（图像修复、文字识别）、上下文分析（器物缀合、断代）到自动释读和人机协作所需的高级推理——展开分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 论文的动机是描绘和推动人工智能驱动的计算古文字学这一新兴领域的发展，旨在解决当前该领域从孤立任务自动化向支持整体性人文研究的集成系统转型过程中所面临的挑战。</p>
<p><strong>Result:</strong> 作为一篇立场论文，本文未报告具体的定量实验结果，而是对领域现状、技术演变（从经典计算机视觉到现代深度学习范式，包括Transformer和大规模多模态模型）和核心挑战进行了系统性分析和综述。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于提出了该领域从任务自动化到构建数字生态系统的演进框架，并倡导未来的研究方向应聚焦于创建多模态、少样本和以人为中心的系统，以增强而非取代学者的专业知识。从客观角度看，其将古文字学研究流程进行系统性计算化分解，并强调人机协作以弥合AI能力与人文研究整体性需求之间鸿沟的视角，具有前瞻性和借鉴意义。</p>
<p><strong>Abstract:</strong> Chinese paleography, the study of ancient Chinese writing, is undergoing a computational turn powered by artificial intelligence. This position paper charts the trajectory of this emerging field, arguing that it is evolving from automating isolated visual tasks to creating integrated digital ecosystems for scholarly research. We first map the landscape of digital resources, analyzing critical datasets for oracle bone, bronze, and bamboo slip scripts. The core of our analysis follows the field’s methodological pipeline: from foundational visual processing (image restoration, character recognition), through contextual analysis (artifact rejoining, dating), to the advanced reasoning required for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including transformers and large multimodal models. Finally, we synthesize the field’s core challenges – notably data scarcity and a disconnect between current AI capabilities and the holistic nature of humanistic inquiry – and advocate for a future research agenda focused on creating multimodal, few-shot, and human-centric systems to augment scholarly expertise.</p>
  </div>
</details>

<hr>
<h3 id="25-MTMCS-Bench-Evaluating-Contextual-Safety-of-Multimodal-Large-Language-Models-in-Multi-Turn-Dialogues-cs-CL-cs-AIPDF"><a href="#25-MTMCS-Bench-Evaluating-Contextual-Safety-of-Multimodal-Large-Language-Models-in-Multi-Turn-Dialogues-cs-CL-cs-AIPDF" class="headerlink" title="[25] MTMCS-Bench: Evaluating Contextual Safety of Multimodal Large Language Models in Multi-Turn Dialogues cs.CL | cs.AIPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06757">MTMCS-Bench: Evaluating Contextual Safety of Multimodal Large Language Models in Multi-Turn Dialogues</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06757" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zheyuan Liu, Dongwhi Kim, Yixin Wan, Xiangchi Yuan, Zhaoxuan Tan</span></p>
<p><strong>TL;DR:</strong> 本文介绍了MTMCS-Bench，一个用于评估多模态大语言模型在多轮对话中上下文安全性的新基准。该基准包含基于风险升级和上下文切换两种互补设置，提供了超过3万个多模态和单模态样本，并设计了结构化评估指标。通过对15个开源和专有模型的测试，研究发现模型在上下文安全性和实用性之间存在权衡，且现有防护措施无法完全解决多轮上下文风险。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有上下文安全基准多为单轮对话，无法捕捉恶意意图的逐步演变或同一场景下良性&#x2F;恶意目标的动态切换，因此需要构建一个更贴近现实的多轮多模态对话安全评估基准。</p>
<p><strong>Result:</strong> 在八个开源和七个专有MLLMs上的评估表明，模型普遍存在上下文安全性与实用性的权衡，要么容易忽略逐步升级的风险，要么对良性对话过度拒绝。对五种现有防护措施的评估发现，它们能缓解部分失败案例，但无法完全解决多轮上下文风险。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个专注于多轮对话中上下文安全性的多模态基准，并设计了风险升级和上下文切换两种新颖评估场景。客观来看，其提出的结构化评估指标（分别衡量上下文意图识别、不安全案例的安全意识以及良性案例的帮助性）为全面评估模型安全行为提供了新框架。</p>
<p><strong>Abstract:</strong> Multimodal large language models (MLLMs) are increasingly deployed as assistants that interact through text and images, making it crucial to evaluate contextual safety when risk depends on both the visual scene and the evolving dialogue. Existing contextual safety benchmarks are mostly single-turn and often miss how malicious intent can emerge gradually or how the same scene can support both benign and exploitative goals. We introduce the Multi-Turn Multimodal Contextual Safety Benchmark (MTMCS-Bench), a benchmark of realistic images and multi-turn conversations that evaluates contextual safety in MLLMs under two complementary settings, escalation-based risk and context-switch risk. MTMCS-Bench offers paired safe and unsafe dialogues with structured evaluation. It contains over 30 thousand multimodal (image+text) and unimodal (text-only) samples, with metrics that separately measure contextual intent recognition, safety-awareness on unsafe cases, and helpfulness on benign ones. Across eight open-source and seven proprietary MLLMs, we observe persistent trade-offs between contextual safety and utility, with models tending to either miss gradual risks or over-refuse benign dialogues. Finally, we evaluate five current guardrails and find that they mitigate some failures but do not fully resolve multi-turn contextual risks.</p>
  </div>
</details>

<hr>
<h3 id="26-GanitLLM-Difficulty-Aware-Bengali-Mathematical-Reasoning-through-Curriculum-GRPO-cs-CL-cs-AI-cs-LGPDF"><a href="#26-GanitLLM-Difficulty-Aware-Bengali-Mathematical-Reasoning-through-Curriculum-GRPO-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[26] GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO cs.CL | cs.AI | cs.LGPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06767">GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06767" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shubhashis Roy Dipta, Khairul Mahbub, Nadia Najjar</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为GanitLLM的孟加拉语数学推理模型，以及一个新的难度感知孟加拉语数学语料库和基于课程学习的GRPO训练流程。该模型旨在解决低资源语言（孟加拉语）中多步数学推理的挑战，通过构建高质量、难度标注的数据集，并结合课程强化学习优化训练过程。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大语言模型在孟加拉语数学推理上表现不佳，通常依赖英语推理后翻译或直接失败，部分原因是强化学习方法在高资源语言上有效，但在低资源环境下因奖励稀疏而失效。</p>
<p><strong>Result:</strong> 在Bn-MGSM和Bn-MSVAMP基准测试上，GanitLLM-4B相比其基础模型Qwen3-4B分别提升了8%和7%的准确率，同时将孟加拉语推理token比例从14%提高到88%以上，并将平均解答长度从943词减少到193词。</p>
<p><strong>Insight:</strong> 创新点包括构建自动难度标注的孟加拉语数学数据集，以及提出课程GRPO方法，结合多阶段训练、难度感知采样和可验证奖励（格式、数值正确性、孟加拉语推理），有效提升低资源语言下的数学推理能力。</p>
<p><strong>Abstract:</strong> We present a Bengali mathematical reasoning model called GanitLLM (named after the Bangla word for mathematics, “Ganit”), together with a new difficulty-aware Bengali math corpus and a curriculum-based GRPO pipeline. Bengali is one of the world’s most widely spoken languages, yet existing LLMs either reason in English and then translate, or simply fail on multi-step Bengali math, in part because reinforcement learning recipes are tuned for high-resource languages and collapse under reward sparsity in low-resource settings. To address this, we construct Ganit, a rigorously filtered and decontaminated Bengali math dataset with automatic difficulty tags derived from the pass@k of a strong evaluator model. Building on this dataset, we propose Curriculum-GRPO, which combines multi-stage training (SFT + GRPO) with difficulty-aware sampling and verifiable rewards for format, numerical correctness, and Bengali reasoning. On Bn-MGSM and Bn-MSVAMP, GanitLLM-4B improves over its Qwen3-4B base by +8 and +7 accuracy points, respectively, while increasing the percentage of Bengali reasoning tokens from 14% to over 88% and reducing average solution length from 943 to 193 words.</p>
  </div>
</details>

<hr>
<h3 id="27-EpiCaR-Knowing-What-You-Don’t-Know-Matters-for-Better-Reasoning-in-LLMs-cs-CLPDF"><a href="#27-EpiCaR-Knowing-What-You-Don’t-Know-Matters-for-Better-Reasoning-in-LLMs-cs-CLPDF" class="headerlink" title="[27] EpiCaR: Knowing What You Don’t Know Matters for Better Reasoning in LLMs cs.CLPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06786">EpiCaR: Knowing What You Don’t Know Matters for Better Reasoning in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06786" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jewon Yeom, Jaewon Sok, Seonghyeon Park, Jeongjae Park, Taesup Kim</span></p>
<p><strong>TL;DR:</strong> 本文提出EpiCaR（Epistemically-Calibrated Reasoning）方法，通过将推理训练重新定义为认知学习问题，联合优化大型语言模型的推理性能和校准能力，以解决现有方法导致模型过度自信和不确定性表征能力丧失的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有提升LLM推理能力的方法主要依赖模型生成数据的迭代自训练，虽然能提高准确性，但会强化成功的推理路径，导致模型校准成本高昂，表现为模型过度自信和不确定性表征能力退化，这是一种对齐中的模型崩溃形式。</p>
<p><strong>Result:</strong> 在Llama-3和Qwen-3模型系列上的实验表明，该方法在准确性和校准方面均优于标准基线，实现了帕累托最优，特别是在具备足够推理能力的模型（如3B+参数）上。该方法能有效泛化到OOD数学推理（GSM8K）和代码生成（MBPP）任务，并能在能力足够的模型中，仅用K&#x3D;10个样本就匹配STaR方法K&#x3D;30个样本的性能，实现3倍的推理计算量减少。</p>
<p><strong>Insight:</strong> 核心创新在于将推理训练视为认知学习问题，强调模型不仅要学习如何推理，还要学习何时信任自己的推理。通过提出EpiCaR训练目标，在迭代监督微调框架中利用显式的自我评估信号，联合优化推理性能和校准，从而缓解模型过度自信问题，提升不确定性表征能力，并实现计算效率的显著提升。</p>
<p><strong>Abstract:</strong> Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K&#x3D;30 performance of STaR with only K&#x3D;10 samples in capable models.</p>
  </div>
</details>

<hr>
<h3 id="28-CIRAG-Construction-Integration-Retrieval-and-Adaptive-Generation-for-Multi-hop-Question-Answering-cs-CL-cs-AIPDF"><a href="#28-CIRAG-Construction-Integration-Retrieval-and-Adaptive-Generation-for-Multi-hop-Question-Answering-cs-CL-cs-AIPDF" class="headerlink" title="[28] CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering cs.CL | cs.AIPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06799">CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06799" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zili Wei, Xiaocui Yang, Yilin Wang, Zihan Wang, Weidong Bao</span></p>
<p><strong>TL;DR:</strong> 本文提出CIRAG模型，用于多跳问答任务，通过迭代构建-集成模块避免贪婪单路径扩展错误，并采用自适应级联多粒度生成模块动态调整证据粒度，同时引入轨迹蒸馏技术提升长程推理效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于三元组的迭代检索增强生成方法存在贪婪单路径扩展导致错误传播，以及单一证据表示难以平衡噪声控制与上下文充分性的问题。</p>
<p><strong>Result:</strong> 在多个基准测试上的广泛实验表明，CIRAG相比现有iRAG方法取得了更优的性能。</p>
<p><strong>Insight:</strong> 创新点包括迭代构建-集成模块保留多证据链、自适应级联多粒度生成实现动态上下文扩展，以及轨迹蒸馏技术将教师模型的集成策略蒸馏到轻量学生模型中，提升推理效率。</p>
<p><strong>Abstract:</strong> Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model’s integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.</p>
  </div>
</details>

<hr>
<h3 id="29-Forest-Before-Trees-Latent-Superposition-for-Efficient-Visual-Reasoning-cs-CL-cs-CVPDF"><a href="#29-Forest-Before-Trees-Latent-Superposition-for-Efficient-Visual-Reasoning-cs-CL-cs-CVPDF" class="headerlink" title="[29] Forest Before Trees: Latent Superposition for Efficient Visual Reasoning cs.CL | cs.CVPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06803">Forest Before Trees: Latent Superposition for Efficient Visual Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06803" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yubo Wang, Juntian Zhang, Yichen Wu, Yankai Lin, Nils Lukas</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Laser的新型视觉推理范式，通过动态窗口对齐学习（DWAL）重新构建视觉推理过程。该方法旨在解决现有潜在推理方法中因自回归目标导致的语义过早坍缩问题，通过强制模型在聚焦局部细节前保持全局特征的叠加状态，实现了高效且可解释的推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决大型视觉语言模型中链式思维推理的瓶颈：显式文本推理会因离散化标记而丢失连续视觉细节，而现有潜在推理方法又常因僵化的自回归目标导致语义过早坍缩。</p>
<p><strong>Result:</strong> 在6个基准测试上的广泛实验表明，Laser在潜在推理方法中达到了最先进的性能，平均比强基线Monet高出5.03%。同时，它实现了极高的效率，推理标记减少了97%以上，并展现出对分布外领域的鲁棒泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于提出了动态窗口对齐学习机制，强制模型遵循“先森林后树木”的认知层次，在潜在状态中保持未来语义的动态有效窗口，从而稳定无约束学习并保持可解码轨迹的可解释性。客观来看，该方法在效率与性能的权衡上提供了新的思路。</p>
<p><strong>Abstract:</strong> While Chain-of-Thought empowers Large Vision-Language Models with multi-step reasoning, explicit textual rationales suffer from an information bandwidth bottleneck, where continuous visual details are discarded during discrete tokenization. Recent latent reasoning methods attempt to address this challenge, but often fall prey to premature semantic collapse due to rigid autoregressive objectives. In this paper, we propose Laser, a novel paradigm that reformulates visual deduction via Dynamic Windowed Alignment Learning (DWAL). Instead of forcing a point-wise prediction, Laser aligns the latent state with a dynamic validity window of future semantics. This mechanism enforces a “Forest-before-Trees” cognitive hierarchy, enabling the model to maintain a probabilistic superposition of global features before narrowing down to local details. Crucially, Laser maintains interpretability via decodable trajectories while stabilizing unconstrained learning via Self-Refined Superposition. Extensive experiments on 6 benchmarks demonstrate that Laser achieves state-of-the-art performance among latent reasoning methods, surpassing the strong baseline Monet by 5.03% on average. Notably, it achieves these gains with extreme efficiency, reducing inference tokens by more than 97%, while demonstrating robust generalization to out-of-distribution domains.</p>
  </div>
</details>

<hr>
<h3 id="30-AgentHallu-Benchmarking-Automated-Hallucination-Attribution-of-LLM-based-Agents-cs-CLPDF"><a href="#30-AgentHallu-Benchmarking-Automated-Hallucination-Attribution-of-LLM-based-Agents-cs-CLPDF" class="headerlink" title="[30] AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents cs.CLPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06818">AgentHallu: Benchmarking Automated Hallucination Attribution of LLM-based Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06818" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuannan Liu, Xiao Yang, Zekun Li, Peipei Li, Ran He</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个新的研究任务——基于LLM的智能体自动幻觉归因，旨在识别多步推理中导致幻觉的步骤并解释原因。为此，研究者构建了名为AgentHallu的综合基准，包含693条高质量轨迹、一个涵盖5大类14子类的幻觉分类法以及多级人工标注。评估了13个领先模型，结果表明该任务极具挑战性，即使顶级模型表现也有限。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决LLM智能体在多步推理工作流中，中间步骤产生的幻觉会沿轨迹传播，从而降低整体可靠性的问题。现有研究主要关注单轮响应的幻觉检测，而诊断多步工作流中的幻觉需要定位导致初始偏差的步骤，这是一个研究空白。</p>
<p><strong>Result:</strong> 在AgentHallu基准上评估了13个领先模型（包括GPT-5和Gemini-2.5-Pro等顶级模型）。结果表明任务极具挑战性：最佳模型在步骤定位准确率上仅达到41.1%，其中工具使用类幻觉最为困难，准确率仅为11.6%。</p>
<p><strong>Insight:</strong> 论文的创新点在于首次系统性地定义了“自动幻觉归因”这一新任务，并构建了首个针对该任务的综合基准AgentHallu，其包含高质量轨迹、细粒度幻觉分类法和多级人工标注。这为未来开发鲁棒、透明、可靠的智能体系统提供了关键的评估框架和研究方向。</p>
<p><strong>Abstract:</strong> As LLM-based agents operate over sequential multi-step reasoning, hallucinations arising at intermediate steps risk propagating along the trajectory, thus degrading overall reliability. Unlike hallucination detection in single-turn responses, diagnosing hallucinations in multi-step workflows requires identifying which step causes the initial divergence. To fill this gap, we propose a new research task, automated hallucination attribution of LLM-based agents, aiming to identify the step responsible for the hallucination and explain why. To support this task, we introduce AgentHallu, a comprehensive benchmark with: (1) 693 high-quality trajectories spanning 7 agent frameworks and 5 domains, (2) a hallucination taxonomy organized into 5 categories (Planning, Retrieval, Reasoning, Human-Interaction, and Tool-Use) and 14 sub-categories, and (3) multi-level annotations curated by humans, covering binary labels, hallucination-responsible steps, and causal explanations. We evaluate 13 leading models, and results show the task is challenging even for top-tier models (like GPT-5, Gemini-2.5-Pro). The best-performing model achieves only 41.1% step localization accuracy, where tool-use hallucinations are the most challenging at just 11.6%. We believe AgentHallu will catalyze future research into developing robust, transparent, and reliable agentic systems.</p>
  </div>
</details>

<hr>
<h3 id="31-Explainable-Multimodal-Aspect-Based-Sentiment-Analysis-with-Dependency-guided-Large-Language-Model-cs-CLPDF"><a href="#31-Explainable-Multimodal-Aspect-Based-Sentiment-Analysis-with-Dependency-guided-Large-Language-Model-cs-CLPDF" class="headerlink" title="[31] Explainable Multimodal Aspect-Based Sentiment Analysis with Dependency-guided Large Language Model cs.CLPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06848">Explainable Multimodal Aspect-Based Sentiment Analysis with Dependency-guided Large Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06848" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhongzheng Wang, Yuanhe Tian, Hongzhi Wang, Yan Song</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种可解释的多模态方面级情感分析框架，将MABSA任务重新定义为生成式任务，利用多模态大语言模型同时预测情感并生成自然语言解释，并通过依赖句法引导的情感线索策略增强方面导向的推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MABSA方法主要依赖复杂的多模态融合进行判别式分类，缺乏明确的情感可解释性，因此需要一种能同时提供情感预测和解释的生成式方法。</p>
<p><strong>Result:</strong> 实验表明，该方法在情感分类准确率上取得了一致性提升，并能生成忠实、基于方面的解释。</p>
<p><strong>Insight:</strong> 创新点在于将MABSA重构为生成式可解释任务，并提出了依赖句法引导的情感线索策略，通过剪枝和文本化方面中心的依赖句法树来增强模型区分不同情感方面的能力，同时利用MLLMs构建带解释的数据集进行微调以实现可解释性。</p>
<p><strong>Abstract:</strong> Multimodal aspect-based sentiment analysis (MABSA) aims to identify aspect-level sentiments by jointly modeling textual and visual information, which is essential for fine-grained opinion understanding in social media. Existing approaches mainly rely on discriminative classification with complex multimodal fusion, yet lacking explicit sentiment explainability. In this paper, we reformulate MABSA as a generative and explainable task, proposing a unified framework that simultaneously predicts aspect-level sentiment and generates natural language explanations. Based on multimodal large language models (MLLMs), our approach employs a prompt-based generative paradigm, jointly producing sentiment and explanation. To further enhance aspect-oriented reasoning capabilities, we propose a dependency-syntax-guided sentiment cue strategy. This strategy prunes and textualizes the aspect-centered dependency syntax tree, guiding the model to distinguish different sentiment aspects and enhancing its explainability. To enable explainability, we use MLLMs to construct new datasets with sentiment explanations to fine-tune. Experiments show that our approach not only achieves consistent gains in sentiment classification accuracy, but also produces faithful, aspect-grounded explanations.</p>
  </div>
</details>

<hr>
<h3 id="32-†DAGGER-Distractor-Aware-Graph-Generation-for-Executable-Reasoning-in-Math-Problems-cs-CL-cs-LGPDF"><a href="#32-†DAGGER-Distractor-Aware-Graph-Generation-for-Executable-Reasoning-in-Math-Problems-cs-CL-cs-LGPDF" class="headerlink" title="[32] †DAGGER: Distractor-Aware Graph Generation for Executable Reasoning in Math Problems cs.CL | cs.LGPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06853">†DAGGER: Distractor-Aware Graph Generation for Executable Reasoning in Math Problems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06853" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zabir Al Nazi, Shubhashis Roy Dipta, Sudipta Kar</span></p>
<p><strong>TL;DR:</strong> 本文针对数学问题求解中CoT提示在无关上下文下的鲁棒性问题，提出了DAGGER方法，将数学问题求解重新定义为可执行计算图的生成，并显式建模干扰节点。通过构建Bangla基准DISTRACTMATH-BN评估模型性能，发现标准模型和推理专用模型在干扰信息下均出现显著性能下降。DAGGER方法通过监督微调和Group Relative Policy Optimization对Gemma-3模型进行微调，在增强基准上实现了相当的加权准确率，同时大幅减少了推理令牌消耗。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是探索CoT提示在数学问题求解中面对语义相关但计算无关的干扰信息时的行为，这一问题在低资源语言中尚未得到充分研究。</p>
<p><strong>Result:</strong> 在DISTRACTMATH-BN基准上评估了7个参数量从3B到12B的模型，标准模型性能下降高达41个百分点，推理专用模型下降14-20个百分点且消耗令牌数增加五倍。DAGGER方法微调的Gemma-3模型在增强基准上达到可比准确率，同时减少89%的令牌消耗，且无需在干扰增强示例上显式训练。</p>
<p><strong>Insight:</strong> 论文的创新点在于将数学推理重新定义为可执行计算图生成，并显式建模干扰节点。从客观角度看，强制使用结构化中间表示相比自由形式方法，在噪声和低资源环境下能提高推理的鲁棒性和效率，这一方法具有借鉴意义。</p>
<p><strong>Abstract:</strong> Chain-of-Thought (CoT) prompting is widely adopted for mathematical problem solving, including in low-resource languages, yet its behavior under irrelevant context remains underexplored. To systematically study this challenge, we introduce DISTRACTMATH-BN, a Bangla benchmark that augments MGSM and MSVAMP with semantically coherent but computationally irrelevant information. Evaluating seven models ranging from 3B to 12B parameters, we observe substantial performance degradation under distractors: standard models drop by up to 41 points, while reasoning-specialized models decline by 14 to 20 points despite consuming five times more tokens. We propose †DAGGER, which reformulates mathematical problem solving as executable computational graph generation with explicit modeling of distractor nodes. Fine-tuning Gemma-3 models using supervised fine-tuning followed by Group Relative Policy Optimization achieves comparable weighted accuracy on augmented benchmarks while using 89 percent fewer tokens than reasoning models. Importantly, this robustness emerges without explicit training on distractor-augmented examples. Our results suggest that enforcing structured intermediate representations improves robustness and inference efficiency in mathematical reasoning compared to free-form approaches, particularly in noisy, low-resource settings.</p>
  </div>
</details>

<hr>
<h3 id="33-Distributional-Clarity-The-Hidden-Driver-of-RL-Friendliness-in-Large-Language-Models-cs-CL-cs-AI-cs-LGPDF"><a href="#33-Distributional-Clarity-The-Hidden-Driver-of-RL-Friendliness-in-Large-Language-Models-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[33] Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models cs.CL | cs.AI | cs.LGPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06911">Distributional Clarity: The Hidden Driver of RL-Friendliness in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06911" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaoning Sun, Mingzhu Cai, Huang He, Bingjin Chen, Siqi Bao</span></p>
<p><strong>TL;DR:</strong> 本文揭示了大型语言模型在强化学习（RL）中表现差异的根本原因在于其概率分布的清晰度（distributional clarity），即模型对正确与错误答案的概率分配具有类内紧凑性和类间分离性。作者通过量化指标轮廓系数（Silhouette Coefficient）验证了该属性与RL性能强相关，并提出了一种轮廓感知重加权训练策略来提升模型的RL友好性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 不同语言模型家族在相同强化学习训练下表现差异显著，本文旨在探究这种RL友好性差异背后的结构性原因，而非仅从数据角度解释。</p>
<p><strong>Result:</strong> 在六个数学基准测试上的实验表明，所提出的轮廓感知重加权策略能一致提升所有模型家族的RL性能，在AIME24基准上最高提升5.9个百分点。</p>
<p><strong>Insight:</strong> 创新点在于将强化学习友好性归因于概率分布的结构性属性——分布清晰度，并引入轮廓系数作为可量化的衡量指标；提出的重加权策略通过关注低清晰度样本来有效提升模型训练效果，为优化语言模型的RL适应性提供了新视角。</p>
<p><strong>Abstract:</strong> Language model families exhibit striking disparity in their capacity to benefit from reinforcement learning: under identical training, models like Qwen achieve substantial gains, while others like Llama yield limited improvements. Complementing data-centric approaches, we reveal that this disparity reflects a hidden structural property: \textbf{distributional clarity} in probability space. Through a three-stage analysis-from phenomenon to mechanism to interpretation-we uncover that RL-friendly models exhibit intra-class compactness and inter-class separation in their probability assignments to correct vs. incorrect responses. We quantify this clarity using the \textbf{Silhouette Coefficient} ($S$) and demonstrate that (1) high $S$ correlates strongly with RL performance; (2) low $S$ is associated with severe logic errors and reasoning instability. To confirm this property, we introduce a Silhouette-Aware Reweighting strategy that prioritizes low-$S$ samples during training. Experiments across six mathematical benchmarks show consistent improvements across all model families, with gains up to 5.9 points on AIME24. Our work establishes distributional clarity as a fundamental, trainable property underlying RL-Friendliness.</p>
  </div>
</details>

<hr>
<h3 id="34-TreePS-RAG-Tree-based-Process-Supervision-for-Reinforcement-Learning-in-Agentic-RAG-cs-CLPDF"><a href="#34-TreePS-RAG-Tree-based-Process-Supervision-for-Reinforcement-Learning-in-Agentic-RAG-cs-CLPDF" class="headerlink" title="[34] TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG cs.CLPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06922">TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06922" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianhua Zhang, Kun Li, Junan Li, Yunxiang Li, Hongyin Luo</span></p>
<p><strong>TL;DR:</strong> 本文提出了TreePS-RAG，一种用于智能体RAG的在线、基于树的强化学习框架。该框架通过将推理过程建模为树形结构，利用蒙特卡洛估计从仅有的最终结果奖励中推导出细粒度的过程优势，从而实现了无需中间标注的逐步信用分配。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于结果监督的强化学习方法在智能体RAG中仅依赖稀疏的最终奖励，限制了逐步信用分配，并为中间推理和行动提供了弱指导。虽然已有研究探索过程级监督，但通常依赖于离线构建的训练数据（存在分布偏移风险）或需要昂贵的中间标注。</p>
<p><strong>Result:</strong> 在七个多跳和通用问答基准测试上，使用多个模型规模的实验表明，TreePS-RAG在计算成本与Search-R1等强基线相当的情况下，持续且显著地优于基于结果监督和领先的基于过程监督的强化学习方法。</p>
<p><strong>Insight:</strong> 核心创新在于将智能体RAG的推理过程建模为展开树，其中每个推理步骤自然映射到一个节点，从而允许通过对其后代结果的蒙特卡洛估计来评估步骤效用。此外，提出了一种高效的在线树构建策略，在有限计算预算下保持了探索的多样性。</p>
<p><strong>Abstract:</strong> Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.</p>
  </div>
</details>

<hr>
<h3 id="35-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests-cs-CL-cs-LGPDF"><a href="#35-X-Coder-Advancing-Competitive-Programming-with-Fully-Synthetic-Tasks-Solutions-and-Tests-cs-CL-cs-LGPDF" class="headerlink" title="[35] X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests cs.CL | cs.LGPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06953">X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06953" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie Wu, Haoling Li, Xin Zhang, Jiani Guo, Jane Luo</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种完全基于合成数据训练代码大语言模型的方法，通过SynthSmith数据合成管道生成多样且具有挑战性的编程任务、解决方案和测试用例，并基于此训练了X-Coder模型系列。该模型在LiveCodeBench基准测试上取得了优于更大参数模型的性能，证明了合成数据在提升代码推理能力方面的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前代码大语言模型严重依赖真实世界数据，这限制了其可扩展性。竞争性编程对代码推理要求高、逻辑复杂，现有模型面临挑战。本文旨在探索完全基于合成数据训练模型，以增强代码推理能力，减少对真实数据的依赖。</p>
<p><strong>Result:</strong> X-Coder模型系列在LiveCodeBench v5上平均通过率为62.9（avg@8），在v6上为55.8，优于DeepCoder-14B-Preview和AReal-boba2-14B等模型，尽管其参数仅为7B。</p>
<p><strong>Insight:</strong> 创新点在于提出了完全合成数据的方法（SynthSmith管道）来训练代码大语言模型，包括任务、解决方案和测试用例的生成。客观分析认为，该方法通过特征合成实现了数据多样性和挑战性，并验证了合成数据上的缩放定律，以及分阶段训练对代码强化学习的关键作用，为减少对真实数据依赖提供了新途径。</p>
<p><strong>Abstract:</strong> Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.</p>
  </div>
</details>

<hr>
<h3 id="36-UETQuintet-at-BioCreative-IX-MedHopQA-Enhancing-Biomedical-QA-with-Selective-Multi-hop-Reasoning-and-Contextual-Retrieval-cs-CLPDF"><a href="#36-UETQuintet-at-BioCreative-IX-MedHopQA-Enhancing-Biomedical-QA-with-Selective-Multi-hop-Reasoning-and-Contextual-Retrieval-cs-CLPDF" class="headerlink" title="[36] UETQuintet at BioCreative IX - MedHopQA: Enhancing Biomedical QA with Selective Multi-hop Reasoning and Contextual Retrieval cs.CLPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06974">UETQuintet at BioCreative IX - MedHopQA: Enhancing Biomedical QA with Selective Multi-hop Reasoning and Contextual Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06974" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Quoc-An Nguyen, Thi-Minh-Thu Vu, Bich-Dat Nguyen, Dinh-Quang-Minh Tran, Hoang-Quynh Le</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对生物医学问答（BioQA）的模型，旨在有效处理直接问题和序列问题，通过将序列问题分解为子问题链进行多跳推理，并利用多源信息检索和上下文学习提供丰富上下文以生成答案。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决生物医学问答系统在处理复杂医学查询时面临的挑战，特别是医学数据的复杂性和对多跳推理的需求。</p>
<p><strong>Result:</strong> 在BioCreative IX - MedHopQA共享任务数据集上评估，模型取得了0.84的精确匹配分数，在当前排行榜上排名第二。</p>
<p><strong>Insight:</strong> 创新点包括选择性多跳推理机制（区分直接与序列问题处理）以及结合多源检索与上下文学习以增强答案生成；客观分析其模块化设计在平衡效率与准确性方面具有借鉴价值。</p>
<p><strong>Abstract:</strong> Biomedical Question Answering systems play a critical role in processing complex medical queries, yet they often struggle with the intricate nature of medical data and the demand for multi-hop reasoning. In this paper, we propose a model designed to effectively address both direct and sequential questions. While sequential questions are decomposed into a chain of sub-questions to perform reasoning across a chain of steps, direct questions are processed directly to ensure efficiency and minimise processing overhead. Additionally, we leverage multi-source information retrieval and in-context learning to provide rich, relevant context for generating answers. We evaluated our model on the BioCreative IX - MedHopQA Shared Task datasets. Our approach achieves an Exact Match score of 0.84, ranking second on the current leaderboard. These results highlight the model’s capability to meet the challenges of Biomedical Question Answering, offering a versatile solution for advancing medical research and practice.</p>
  </div>
</details>

<hr>
<h3 id="37-TurkBench-A-Benchmark-for-Evaluating-Turkish-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#37-TurkBench-A-Benchmark-for-Evaluating-Turkish-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[37] TurkBench: A Benchmark for Evaluating Turkish Large Language Models cs.CL | cs.AIPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07020">TurkBench: A Benchmark for Evaluating Turkish Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07020" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Çağrı Toraman, Ahmet Kaan Sever, Ayse Aysu Cengiz, Elif Ecem Arslan, Görkem Sevinç</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了TurkBench，一个专门用于评估土耳其语大语言模型的综合性基准测试。该基准包含8,151个数据样本，涵盖知识、语言理解、推理、内容审核、土耳其语法与词汇以及指令遵循等六大类共21个子任务，旨在为研究人员和开发者提供一个评估和改进土耳其语模型性能的工具。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大语言模型的发展，针对特定语言的全面评估基准变得至关重要。尽管英语模型的评估已取得显著进展，但针对土耳其语等具有独特语言特性的语言的基准测试仍较为缺乏，因此需要开发专门的评估工具。</p>
<p><strong>Result:</strong> 论文提出了TurkBench基准，包含8,151个数据样本和21个子任务，覆盖六大评估类别。该基准已在Hugging Face平台发布，支持在线提交，为土耳其语大语言模型的评估提供了标准化工具，但目前摘要中未提及具体模型在该基准上的定量结果或SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于首次为土耳其语大语言模型设计了一个全面且文化相关的评估基准，结合了语言特定任务（如语法和词汇）和通用能力（如推理和知识），有助于推动非英语语言模型的发展。从客观角度看，该基准的多样性和实用性为多语言NLP研究提供了重要参考。</p>
<p><strong>Abstract:</strong> With the recent surge in the development of large language models, the need for comprehensive and language-specific evaluation benchmarks has become critical. While significant progress has been made in evaluating English language models, benchmarks for other languages, particularly those with unique linguistic characteristics such as Turkish, remain less developed. Our study introduces TurkBench, a comprehensive benchmark designed to assess the capabilities of generative large language models in the Turkish language. TurkBench involves 8,151 data samples across 21 distinct subtasks. These are organized under six main categories of evaluation: Knowledge, Language Understanding, Reasoning, Content Moderation, Turkish Grammar and Vocabulary, and Instruction Following. The diverse range of tasks and the culturally relevant data would provide researchers and developers with a valuable tool for evaluating their models and identifying areas for improvement. We further publish our benchmark for online submissions at <a target="_blank" rel="noopener" href="https://huggingface.co/turkbench">https://huggingface.co/turkbench</a></p>
  </div>
</details>

<hr>
<h3 id="38-Solar-Open-Technical-Report-cs-CLPDF"><a href="#38-Solar-Open-Technical-Report-cs-CLPDF" class="headerlink" title="[38] Solar Open Technical Report cs.CLPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07022">Solar Open Technical Report</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07022" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sungrae Park, Sanghoon Kim, Jungho Cho, Gyoungjin Gim, Dawoon Jung</span></p>
<p><strong>TL;DR:</strong> 本文介绍了Solar Open，一个1020亿参数的双语专家混合模型，专为资源匮乏语言设计。该研究通过解决数据稀缺、课程学习协调和可扩展强化学习优化三大挑战，提出了一套系统化构建高性能大语言模型的方法论。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在解决资源匮乏语言在AI开发中面临的数据稀缺、训练效率低下和推理能力不足等核心挑战，推动多语言AI的公平发展。</p>
<p><strong>Result:</strong> 在英语和韩语的基准测试中，Solar Open取得了具有竞争力的性能，验证了其方法论在资源匮乏语言AI开发中的有效性。</p>
<p><strong>Insight:</strong> 创新点包括：1) 合成大规模高质量、领域特定且面向强化学习的数据以应对数据稀缺；2) 通过渐进式课程学习联合优化数据构成、质量阈值和领域覆盖；3) 提出SnapPO框架以实现可扩展强化学习下的高效优化。该方法为系统化构建多语言大模型提供了可借鉴的工程框架。</p>
<p><strong>Abstract:</strong> We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.</p>
  </div>
</details>

<hr>
<h3 id="39-Codified-Foreshadowing-Payoff-Text-Generation-cs-CLPDF"><a href="#39-Codified-Foreshadowing-Payoff-Text-Generation-cs-CLPDF" class="headerlink" title="[39] Codified Foreshadowing-Payoff Text Generation cs.CLPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07033">Codified Foreshadowing-Payoff Text Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07033" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Longfei Yun, Kun Zhou, Yupeng Hou, Letian Peng, Jingbo Shang</span></p>
<p><strong>TL;DR:</strong> 本文提出了Codified Foreshadowing-Payoff Generation (CFPG)框架，旨在解决大语言模型在故事生成中难以处理长程叙事依赖（如伏笔与呼应）的问题。该框架通过从BookSum语料库中挖掘并编码‘伏笔-触发-呼应’三元组，将叙事连续性转化为可执行的因果谓词，从而为模型提供结构化监督。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大语言模型在故事生成中经常无法有效连接伏笔与呼应，导致叙事结构上的逻辑断裂，而现有评估方法多关注表面连贯性，忽视了这种结构性失败。</p>
<p><strong>Result:</strong> 实验表明，CFPG在‘呼应准确性’和‘叙事一致性’方面显著优于标准提示基线方法。</p>
<p><strong>Insight:</strong> 核心创新在于将叙事机制（伏笔与呼应）显式地编码为结构化、可执行的因果谓词，为模型提供明确的监督信号，这为提升大语言模型的深层叙事能力（而非仅表面流畅性）提供了新思路。</p>
<p><strong>Abstract:</strong> Foreshadowing and payoff are ubiquitous narrative devices through which authors introduce commitments early in a story and resolve them through concrete, observable outcomes. However, despite advances in story generation, large language models (LLMs) frequently fail to bridge these long-range narrative dependencies, often leaving “Chekhov’s guns” unfired even when the necessary context is present. Existing evaluations largely overlook this structural failure, focusing on surface-level coherence rather than the logical fulfillment of narrative setups. In this paper, we introduce Codified Foreshadowing-Payoff Generation (CFPG), a novel framework that reframes narrative quality through the lens of payoff realization. Recognizing that LLMs struggle to intuitively grasp the “triggering mechanism” of a foreshadowed event, CFPG transforms narrative continuity into a set of executable causal predicates. By mining and encoding Foreshadow-Trigger-Payoff triples from the BookSum corpus, we provide structured supervision that ensures foreshadowed commitments are not only mentioned but also temporally and logically fulfilled. Experiments demonstrate that CFPG significantly outperforms standard prompting baselines in payoff accuracy and narrative alignment. Our findings suggest that explicitly codifying narrative mechanics is essential for moving LLMs from surface-level fluency to genuine narrative competence.</p>
  </div>
</details>

<hr>
<h3 id="40-Mid-Think-Training-Free-Intermediate-Budget-Reasoning-via-Token-Level-Triggers-cs-CL-cs-AI-cs-LGPDF"><a href="#40-Mid-Think-Training-Free-Intermediate-Budget-Reasoning-via-Token-Level-Triggers-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[40] Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers cs.CL | cs.AI | cs.LGPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07036">Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07036" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wang Yang, Debargha Ganguly, Xinpeng Li, Chaoda Song, Shouren Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Mid-Think的无训练提示方法，通过结合特定的触发词（如’Okay’和换行模式）来控制语言模型的推理行为，实现中等计算预算的推理。该方法在推理时能优化准确率与生成长度的权衡，并在强化学习训练中提升效率与性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有混合推理语言模型通常通过高级Think&#x2F;No-think指令控制推理行为，但研究发现这种模式切换主要由少量触发词驱动，而非指令本身。因此，本文旨在探索更精细的token级控制机制，以实现更高效的推理调节。</p>
<p><strong>Result:</strong> 在准确率-长度权衡方面，Mid-Think consistently outperforms fixed-token and prompt-based baselines。应用于SFT后的RL训练时，将Qwen3-8B在AIME数据集上的性能从69.8%提升至72.4%，在GPQA上从58.5%提升至61.1%，同时减少约15%的训练时间。</p>
<p><strong>Insight:</strong> 创新点在于揭示了推理行为由特定触发词（如’Okay’和新行模式）驱动，而非高层指令，并据此设计了无训练的token级提示方法。客观来看，该方法为模型推理控制提供了轻量级、可解释的干预手段，有助于优化计算资源分配和训练效率。</p>
<p><strong>Abstract:</strong> Hybrid reasoning language models are commonly controlled through high-level Think&#x2F;No-think instructions to regulate reasoning behavior, yet we found that such mode switching is largely driven by a small set of trigger tokens rather than the instructions themselves. Through attention analysis and controlled prompting experiments, we show that a leading <code>Okay&#39;&#39; token induces reasoning behavior, while the newline pattern following </code></think>‘’ suppresses it. Based on this observation, we propose Mid-Think, a simple training-free prompting format that combines these triggers to achieve intermediate-budget reasoning, consistently outperforming fixed-token and prompt-based baselines in terms of the accuracy-length trade-off. Furthermore, applying Mid-Think to RL training after SFT reduces training time by approximately 15% while improving final performance of Qwen3-8B on AIME from 69.8% to 72.4% and on GPQA from 58.5% to 61.1%, demonstrating its effectiveness for both inference-time control and RL-based reasoning training.</p>
  </div>
</details>

<hr>
<h3 id="41-When-Abundance-Conceals-Weakness-Knowledge-Conflict-in-Multilingual-Models-cs-CLPDF"><a href="#41-When-Abundance-Conceals-Weakness-Knowledge-Conflict-in-Multilingual-Models-cs-CLPDF" class="headerlink" title="[41] When Abundance Conceals Weakness: Knowledge Conflict in Multilingual Models cs.CLPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07041">When Abundance Conceals Weakness: Knowledge Conflict in Multilingual Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07041" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaqi Zhao, Qiang Huang, Haodong Chen, Xiaoxing You, Jun Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出了CLEAR框架，用于系统评估多语言大语言模型在面临跨语言知识冲突时的行为，即当外部证据与模型内部语言依赖的记忆相矛盾时的情况。研究通过构建涵盖10种类型多样语言的多语言版本ConflictQA和ConflictingQA基准，评估了六种代表性模型，发现任务类型决定了冲突解决策略：在推理密集型任务中，语言资源丰富度起主导作用；而在以实体为中心的事实冲突中，语言亲和力更为关键。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探索多语言大语言模型中存在的跨语言知识冲突现象，即模型内部信念在不同语言间分布不均，当外部证据与这些语言依赖的记忆矛盾时，模型如何协调，这一现象在非英语中心设置中尚未充分研究。</p>
<p><strong>Result:</strong> 在CLEAR框架下，通过两个互补的问答基准（多语言ConflictQA和ConflictingQA）评估六种LLMs，结果显示任务依赖性决策二分：推理任务中高资源语言说服力更强，实体事实冲突中低资源但语言亲和的语言可能优于高资源语言。</p>
<p><strong>Insight:</strong> 创新点在于首次系统研究跨语言知识冲突，提出CLEAR评估框架将冲突分解为四种渐进场景，并揭示任务类型对冲突解决策略的影响，即资源丰富度与语言亲和力在不同任务中的决定性作用，为多语言模型的知识一致性评估提供了新视角。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) encode vast world knowledge across multiple languages, yet their internal beliefs are often unevenly distributed across linguistic spaces. When external evidence contradicts these language-dependent memories, models encounter \emph{cross-lingual knowledge conflict}, a phenomenon largely unexplored beyond English-centric settings. We introduce \textbf{CLEAR}, a \textbf{C}ross-\textbf{L}ingual knowl\textbf{E}dge conflict ev\textbf{A}luation f\textbf{R}amework that systematically examines how multilingual LLMs reconcile conflicting internal beliefs and multilingual external evidence. CLEAR decomposes conflict resolution into four progressive scenarios, from multilingual parametric elicitation to competitive multi-source cross-lingual induction, and systematically evaluates model behavior across two complementary QA benchmarks with distinct task characteristics. We construct multilingual versions of ConflictQA and ConflictingQA covering 10 typologically diverse languages and evaluate six representative LLMs. Our experiments reveal a task-dependent decision dichotomy. In reasoning-intensive tasks, conflict resolution is dominated by language resource abundance, with high-resource languages exerting stronger persuasive power. In contrast, for entity-centric factual conflicts, linguistic affinity, not resource scale, becomes decisive, allowing low-resource but linguistically aligned languages to outperform distant high-resource ones.</p>
  </div>
</details>

<hr>
<h3 id="42-Engineering-of-Hallucination-in-Generative-AI-It’s-not-a-Bug-it’s-a-Feature-cs-CLPDF"><a href="#42-Engineering-of-Hallucination-in-Generative-AI-It’s-not-a-Bug-it’s-a-Feature-cs-CLPDF" class="headerlink" title="[42] Engineering of Hallucination in Generative AI: It’s not a Bug, it’s a Feature cs.CLPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07046">Engineering of Hallucination in Generative AI: It’s not a Bug, it’s a Feature</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07046" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tim Fingscheidt, Patrick Blumenberg, Björn Möller</span></p>
<p><strong>TL;DR:</strong> 本文探讨了生成式AI中幻觉现象的双重性，认为适度的幻觉可能是实现满意输出的必要特征而非缺陷，并回顾了通过概率工程方法可控地诱导有限幻觉以提升模型表现的技术。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对生成式AI（如ChatGPT、GAIA-1）在严格遵循训练数据时输出效果不佳的现象，研究旨在重新审视幻觉的负面含义，探索其作为可控特征以改善模型生成质量的潜力。</p>
<p><strong>Result:</strong> 未提及具体定量结果或基准测试，但通过理论分析表明，通过概率工程方法可控地引入有限幻觉能引导模型产生更符合期望的输出。</p>
<p><strong>Insight:</strong> 创新点在于将幻觉重新定义为可工程化的特征而非纯粹缺陷，提出了通过概率调整可控地诱导幻觉以优化生成结果的方法论，为生成模型的设计提供了新视角。</p>
<p><strong>Abstract:</strong> Generative artificial intelligence (AI) is conquering our lives at lightning speed. Large language models such as ChatGPT answer our questions or write texts for us, large computer vision models such as GAIA-1 generate videos on the basis of text descriptions or continue prompted videos. These neural network models are trained using large amounts of text or video data, strictly according to the real data employed in training. However, there is a surprising observation: When we use these models, they only function satisfactorily when they are allowed a certain degree of fantasy (hallucination). While hallucination usually has a negative connotation in generative AI - after all, ChatGPT is expected to give a fact-based answer! - this article recapitulates some simple means of probability engineering that can be used to encourage generative AI to hallucinate to a limited extent and thus lead to the desired results. We have to ask ourselves: Is hallucination in gen-erative AI probably not a bug, but rather a feature?</p>
  </div>
</details>

<hr>
<h3 id="43-Fine-Tuning-vs-RAG-for-Multi-Hop-Question-Answering-with-Novel-Knowledge-cs-CL-cs-LGPDF"><a href="#43-Fine-Tuning-vs-RAG-for-Multi-Hop-Question-Answering-with-Novel-Knowledge-cs-CL-cs-LGPDF" class="headerlink" title="[43] Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge cs.CL | cs.LGPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07054">Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07054" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhuoyi Yang, Yurun Song, Iftekhar Ahmed, Ian Harris</span></p>
<p><strong>TL;DR:</strong> 本文系统比较了参数化（微调）与非参数化（检索增强生成，RAG）知识注入方法在开放域多跳问答任务中的效果，特别是在所需知识具有时间新颖性时。实验基于三个7B参数的开源大语言模型，在标准科学问答数据集QASC和基于2024年维基百科事件构建的新数据集上进行评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多跳问答中，当所需知识具有时间新颖性时，不同知识注入方法（如微调与RAG）的相对有效性尚不明确的问题。</p>
<p><strong>Result:</strong> 在QASC和包含2024年新颖知识的新建数据集上，无监督微调相比基础模型提升有限；检索增强生成（RAG）带来显著且一致的改进，尤其在依赖时间新颖信息的问题上；监督微调在模型和数据集上达到最高总体准确率。</p>
<p><strong>Insight:</strong> 研究揭示了不同知识注入机制支持多跳问答的根本差异，强调了当需要外部或组合知识时，基于检索的方法的重要性；监督微调在整体准确性上最优，而RAG在处理新颖知识时更具优势。</p>
<p><strong>Abstract:</strong> Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.   In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models’ pretraining cutoff.   Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.</p>
  </div>
</details>

<hr>
<h3 id="44-Measuring-Iterative-Temporal-Reasoning-with-TimePuzzles-cs-CL-cs-AIPDF"><a href="#44-Measuring-Iterative-Temporal-Reasoning-with-TimePuzzles-cs-CL-cs-AIPDF" class="headerlink" title="[44] Measuring Iterative Temporal Reasoning with TimePuzzles cs.CL | cs.AIPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07148">Measuring Iterative Temporal Reasoning with TimePuzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07148" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhengxiang Wang, Zeyu Dong</span></p>
<p><strong>TL;DR:</strong> 本文介绍了TimePuzzles，这是一个基于约束的日期推断任务，用于评估迭代时间推理能力。该任务通过算法生成结合事实时间锚点和跨文化日历关系的谜题，支持单解或多解，旨在提供可控、动态和持续的评估。在13个不同的大语言模型上，TimePuzzles有效区分了它们的迭代时间推理能力，即使数据集简单，GPT-5仅达到49.3%的准确率，其他模型均低于31%，表明任务具有挑战性。使用网络搜索能显著提升性能，而代码解释器的效果不一；当约束被重写为明确日期时，所有模型表现大幅改善，揭示了可靠工具使用的差距。总体而言，TimePuzzles为工具增强的迭代时间推理提供了一个简单且成本效益高的诊断工具。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了解决现有评估方法在迭代时间推理能力上的不足，特别是缺乏可控、动态和持续的测试任务，本文旨在开发一个基于约束的日期推断任务，以更准确地评估大语言模型在复杂时间推理中的表现。</p>
<p><strong>Result:</strong> 在13个不同的大语言模型上，TimePuzzles任务表现出挑战性：GPT-5的准确率仅为49.3%，其他模型均低于31%。使用网络搜索能带来显著提升，代码解释器效果不一；当约束被重写为明确日期时，所有模型性能大幅改善，揭示了工具使用的可靠性差距。</p>
<p><strong>Insight:</strong> 创新点在于设计了一个算法生成的、基于约束的日期推断任务，结合事实时间锚点和跨文化日历关系，支持多解，提供了可控、动态和持续的评估框架。从客观角度看，该任务能有效诊断大语言模型的迭代时间推理能力和工具使用差距，为模型评估和优化提供了新视角。</p>
<p><strong>Abstract:</strong> We introduce TimePuzzles, a constraint-based date inference task for evaluating iterative temporal reasoning. Each puzzle combines factual temporal anchors with (cross-cultural) calendar relations, admits one or multiple valid solution dates, and is algorithmically generated for controlled, dynamic, and continual evaluation. Across 13 diverse LLMs, TimePuzzles well distinguishes their iterative temporal reasoning capabilities and remains challenging without tools: GPT-5 reaches only 49.3% accuracy and all other models stay below 31%, despite the dataset’s simplicity. Web search consistently yields substantial gains and using code interpreter shows mixed effects, but all models perform much better when constraints are rewritten with explicit dates, revealing a gap in reliable tool use. Overall, TimePuzzles presents a simple, cost-effective diagnostic for tool-augmented iterative temporal reasoning.</p>
  </div>
</details>

<hr>
<h3 id="45-Can-Large-Language-Models-Understand-Reason-About-and-Generate-Code-Switched-Text-cs-CL-cs-AIPDF"><a href="#45-Can-Large-Language-Models-Understand-Reason-About-and-Generate-Code-Switched-Text-cs-CL-cs-AIPDF" class="headerlink" title="[45] Can Large Language Models Understand, Reason About, and Generate Code-Switched Text? cs.CL | cs.AIPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07153">Can Large Language Models Understand, Reason About, and Generate Code-Switched Text?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07153" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Genta Indra Winata, David Anugraha, Patrick Amadeus Irawan, Anirban Das, Haneul Yoo</span></p>
<p><strong>TL;DR:</strong> 本文全面评估了大语言模型在理解和生成代码转换文本方面的能力，提出了CodeMixQA基准数据集，包含16种语言对的代码转换变体，并分析了模型在代码转换问答任务中的推理行为以及生成文本的自然度和语义保真度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大语言模型在多语言混合环境下的鲁棒性问题，特别是在代码转换场景中的理解和生成能力不足。</p>
<p><strong>Result:</strong> 在CodeMixQA基准上的评估揭示了模型在代码转换条件下推理和生成方面的持续挑战，生成文本的自然度和语义保真度存在关键限制。</p>
<p><strong>Insight:</strong> 创新点在于构建了高质量的代码转换基准数据集，并系统分析了模型处理混合语言输入的推理行为；可借鉴之处包括对多语言模型鲁棒性的评估方法和代码转换生成任务的局限性分析。</p>
<p><strong>Abstract:</strong> Code-switching is a pervasive phenomenon in multilingual communication, yet the robustness of large language models (LLMs) in mixed-language settings remains insufficiently understood. In this work, we present a comprehensive evaluation of LLM capabilities in understanding, reasoning over, and generating code-switched text. We introduce CodeMixQA a novel benchmark with high-quality human annotations, comprising 16 diverse parallel code-switched language-pair variants that span multiple geographic regions and code-switching patterns, and include both original scripts and their transliterated forms. Using this benchmark, we analyze the reasoning behavior of LLMs on code-switched question-answering tasks, shedding light on how models process and reason over mixed-language inputs. We further conduct a systematic evaluation of LLM-generated synthetic code-switched text, focusing on both naturalness and semantic fidelity, and uncover key limitations in current generation capabilities. Our findings reveal persistent challenges in both reasoning and generation under code-switching conditions and provide actionable insights for building more robust multilingual LLMs. We release the dataset and code as open source.</p>
  </div>
</details>

<hr>
<h3 id="46-Structured-Reasoning-for-Large-Language-Models-cs-CLPDF"><a href="#46-Structured-Reasoning-for-Large-Language-Models-cs-CLPDF" class="headerlink" title="[46] Structured Reasoning for Large Language Models cs.CLPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07180">Structured Reasoning for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07180" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinyi Han, Zixiang Di, Zishang Jiang, Ying Liao, Jiaqing Liang</span></p>
<p><strong>TL;DR:</strong> 本文提出结构化推理（SCR）框架，通过将推理轨迹解耦为可评估、可训练的组件，采用生成-验证-修订范式，结合结构化训练数据和动态终止监督，以提升大语言模型的推理效率和自验证能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大语言模型在长链推理中产生冗余或无效步骤（如不必要的验证和修订）的问题，源于推理轨迹的非结构化和缺乏针对关键推理能力的监督。</p>
<p><strong>Result:</strong> 在三个骨干模型上的实验表明，SCR显著提高了推理效率和自验证能力，相比现有推理范式，输出令牌长度减少高达50%。</p>
<p><strong>Insight:</strong> 创新点包括将推理轨迹结构化分解为明确组件、引入动态终止监督指导模型决定何时终止推理，以及采用渐进式两阶段强化学习策略以避免不同推理能力学习信号间的干扰。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) achieve strong performance by generating long chains of thought, but longer traces always introduce redundant or ineffective reasoning steps. One typical behavior is that they often perform unnecessary verification and revisions even if they have reached the correct answers. This limitation stems from the unstructured nature of reasoning trajectories and the lack of targeted supervision for critical reasoning abilities. To address this, we propose Structured Reasoning (SCR), a framework that decouples reasoning trajectories into explicit, evaluable, and trainable components. We mainly implement SCR using a Generate-Verify-Revise paradigm. Specifically, we construct structured training data and apply Dynamic Termination Supervision to guide the model in deciding when to terminate reasoning. To avoid interference between learning signals for different reasoning abilities, we adopt a progressive two-stage reinforcement learning strategy: the first stage targets initial generation and self-verification, and the second stage focuses on revision. Extensive experiments on three backbone models show that SCR substantially improves reasoning efficiency and self-verification. Besides, compared with existing reasoning paradigms, it reduces output token length by up to 50%.</p>
  </div>
</details>

<hr>
<h3 id="47-Relink-Constructing-Query-Driven-Evidence-Graph-On-the-Fly-for-GraphRAG-cs-CL-cs-AIPDF"><a href="#47-Relink-Constructing-Query-Driven-Evidence-Graph-On-the-Fly-for-GraphRAG-cs-CL-cs-AIPDF" class="headerlink" title="[47] Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG cs.CL | cs.AIPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07192">Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07192" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Manzong Huang, Chenyang Bu, Yi He, Xingrui Zhuo, Xindong Wu</span></p>
<p><strong>TL;DR:</strong> 本文提出Relink框架，采用’推理即构建’的新范式，通过动态构建查询特定的证据图来解决传统GraphRAG方法中静态知识图谱的路径不完整和噪声干扰问题，显著提升了开放域问答任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对当前基于图的检索增强生成方法依赖静态预构建知识图谱导致的推理路径断裂和噪声事实干扰两大核心挑战，提出需要从’先构建后推理’转向’推理即构建’的新范式。</p>
<p><strong>Result:</strong> 在五个开放域问答基准测试上，Relink相比领先的GraphRAG基线方法在EM指标上平均提升5.4%，在F1指标上平均提升5.2%，实现了显著性能改进。</p>
<p><strong>Insight:</strong> 创新点在于提出动态证据图构建范式，通过从文本语料库的潜在关系池中实例化所需事实来修复断裂路径，并采用统一的查询感知评估策略联合筛选知识图谱和潜在关系中的候选事实，主动过滤干扰信息，为每个查询构建最忠实精确的证据路径。</p>
<p><strong>Abstract:</strong> Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG’s inherent incompleteness often breaks reasoning paths. Second, the graph’s low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process.   To address these challenges, we argue for a \textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query.   Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4% in EM and 5.2% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.</p>
  </div>
</details>

<hr>
<h3 id="48-ActiShade-Activating-Overshadowed-Knowledge-to-Guide-Multi-Hop-Reasoning-in-Large-Language-Models-cs-CLPDF"><a href="#48-ActiShade-Activating-Overshadowed-Knowledge-to-Guide-Multi-Hop-Reasoning-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[48] ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models cs.CLPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07260">ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07260" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Huipeng Ma, Luan Zhang, Dandan Song, Linmei Hu, Yuhang Tian</span></p>
<p><strong>TL;DR:</strong> 本文提出了ActiShade方法，用于解决多跳推理中检索增强生成（RAG）面临的知识遮蔽问题。该方法通过迭代检测查询中被遮蔽的关键短语，检索相关文档，并生成新的查询来引导下一轮迭代，从而减少错误累积。实验表明，ActiShade在多个数据集和LLM上优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多轮检索增强生成（RAG）方法中，由于LLM生成内容可能不完整或不准确（即知识遮蔽现象），导致检索不相关和迭代过程中错误累积的问题。</p>
<p><strong>Result:</strong> 在多个数据集和不同大语言模型（LLMs）上的广泛实验表明，ActiShade的性能优于现有方法。</p>
<p><strong>Insight:</strong> 核心创新在于主动检测和激活被遮蔽的关键知识，并将其整合到下一轮查询的制定中，同时最小化无关噪声的引入。这为缓解多跳推理中因知识不完整导致的错误传播提供了一种新思路。</p>
<p><strong>Abstract:</strong> In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.</p>
  </div>
</details>

<hr>
<h3 id="49-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents-cs-CLPDF"><a href="#49-The-Confidence-Dichotomy-Analyzing-and-Mitigating-Miscalibration-in-Tool-Use-Agents-cs-CLPDF" class="headerlink" title="[49] The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents cs.CLPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07264">The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07264" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weihao Xuan, Qingcheng Zeng, Heli Qi, Yunze Xiao, Junjue Wang</span></p>
<p><strong>TL;DR:</strong> 本文系统研究了基于大语言模型的工具使用智能体在任务执行过程中的校准问题，发现工具类型会导致一种根本性的置信度二分现象：证据类工具（如网络搜索）会因检索信息中的固有噪声而引发严重过度自信，而验证类工具（如代码解释器）则能通过确定性反馈来锚定推理并缓解校准错误。为改善校准，论文提出了一个联合优化任务准确性和校准的强化学习微调框架，并在多个领域验证了其有效性和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 确保基于大语言模型的自主智能体的可信度是关键挑战，而校准（即智能体表达的置信度能可靠反映其实际性能）是可信度的基石。然而，在集成工具的智能体工作流中，校准的动态特性尚未得到充分探索。</p>
<p><strong>Result:</strong> 实验表明，所提出的强化学习微调框架训练的智能体不仅实现了更优的校准性能，而且能够从局部训练环境稳健地泛化到嘈杂的网络环境以及数学推理等不同领域。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于揭示了工具类型对智能体校准的二分影响，并提出了一个联合优化任务性能和校准的强化学习框架。从客观角度看，这项工作为构建能够可靠传达不确定性的、具有自我意识的智能体奠定了基础，并强调了针对工具使用智能体设计领域特定校准策略的必要性。</p>
<p><strong>Abstract:</strong> Autonomous agents based on large language models (LLMs) are rapidly evolving to handle multi-turn tasks, but ensuring their trustworthiness remains a critical challenge. A fundamental pillar of this trustworthiness is calibration, which refers to an agent’s ability to express confidence that reliably reflects its actual performance. While calibration is well-established for static models, its dynamics in tool-integrated agentic workflows remain underexplored. In this work, we systematically investigate verbalized calibration in tool-use agents, revealing a fundamental confidence dichotomy driven by tool type. Specifically, our pilot study identifies that evidence tools (e.g., web search) systematically induce severe overconfidence due to inherent noise in retrieved information, while verification tools (e.g., code interpreters) can ground reasoning through deterministic feedback and mitigate miscalibration. To robustly improve calibration across tool types, we propose a reinforcement learning (RL) fine-tuning framework that jointly optimizes task accuracy and calibration, supported by a holistic benchmark of reward designs. We demonstrate that our trained agents not only achieve superior calibration but also exhibit robust generalization from local training environments to noisy web settings and to distinct domains such as mathematical reasoning. Our results highlight the necessity of domain-specific calibration strategies for tool-use agents. More broadly, this work establishes a foundation for building self-aware agents that can reliably communicate uncertainty in high-stakes, real-world deployments.</p>
  </div>
</details>

<hr>
<h3 id="50-ReasonTabQA-A-Comprehensive-Benchmark-for-Table-Question-Answering-from-Real-World-Industrial-Scenarios-cs-CLPDF"><a href="#50-ReasonTabQA-A-Comprehensive-Benchmark-for-Table-Question-Answering-from-Real-World-Industrial-Scenarios-cs-CLPDF" class="headerlink" title="[50] ReasonTabQA: A Comprehensive Benchmark for Table Question Answering from Real World Industrial Scenarios cs.CLPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07280">ReasonTabQA: A Comprehensive Benchmark for Table Question Answering from Real World Industrial Scenarios</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07280" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changzai Pan, Jie Zhang, Kaiwen Wei, Chenshuo Pan, Yu Zhao</span></p>
<p><strong>TL;DR:</strong> 论文提出了ReasonTabQA，一个面向真实工业场景的大规模双语表格问答基准，包含30个行业领域的1932张表格，并提供了最终答案和显式推理链的高质量标注。同时，论文还提出了TabCodeRL，一种利用表格感知可验证奖励来引导逻辑推理路径生成的强化学习方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有表格问答基准往往忽略了工业场景的复杂性（如多表结构、嵌套表头和大规模数据），这些场景需要通过深度结构化推理进行鲁棒的表格理解，而当前方法未能充分解决这一挑战。</p>
<p><strong>Result:</strong> 在ReasonTabQA和4个TableQA数据集上的大量实验表明，TabCodeRL在开源大语言模型上带来了显著的性能提升，但在ReasonTabQA上的持续性能差距突显了真实世界工业表格问答的固有复杂性。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了一个更贴近工业实践、具有挑战性的表格问答基准，并提出了一个结合强化学习和表格感知奖励的推理路径生成方法，以应对复杂表格结构下的深度推理需求。</p>
<p><strong>Abstract:</strong> Recent advancements in Large Language Models (LLMs) have significantly catalyzed table-based question answering (TableQA). However, existing TableQA benchmarks often overlook the intricacies of industrial scenarios, which are characterized by multi-table structures, nested headers, and massive scales. These environments demand robust table reasoning through deep structured inference, presenting a significant challenge that remains inadequately addressed by current methodologies. To bridge this gap, we present ReasonTabQA, a large-scale bilingual benchmark encompassing 1,932 tables across 30 industry domains such as energy and automotive. ReasonTabQA provides high-quality annotations for both final answers and explicit reasoning chains, supporting both thinking and no-thinking paradigms. Furthermore, we introduce TabCodeRL, a reinforcement learning method that leverages table-aware verifiable rewards to guide the generation of logical reasoning paths. Extensive experiments on ReasonTabQA and 4 TableQA datasets demonstrate that while TabCodeRL yields substantial performance gains on open-source LLMs, the persistent performance gap on ReasonTabQA underscores the inherent complexity of real-world industrial TableQA.</p>
  </div>
</details>

<hr>
<h3 id="51-BayesRAG-Probabilistic-Mutual-Evidence-Corroboration-for-Multimodal-Retrieval-Augmented-Generation-cs-CLPDF"><a href="#51-BayesRAG-Probabilistic-Mutual-Evidence-Corroboration-for-Multimodal-Retrieval-Augmented-Generation-cs-CLPDF" class="headerlink" title="[51] BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation cs.CLPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07329">BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07329" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuan Li, Yining Wang, Haocai Luo, Shengping Liu, Jerry Liang</span></p>
<p><strong>TL;DR:</strong> 本文提出了BayesRAG，一个基于贝叶斯推理和Dempster-Shafer证据理论的新型多模态检索增强生成框架。它通过建模跨模态检索结果的内部一致性作为概率证据，来优化检索置信度，优先选择在语义和布局上相互佐证的文本-图像对，从而解决现有方法在处理视觉丰富文档时，将文本和图像视为孤立检索目标的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前检索增强生成方法在处理视觉丰富文档时，通常将文本和图像作为孤立的检索目标，且仅依赖余弦相似度，难以捕捉跨模态对齐和布局诱导连贯性所提供的语义增强。</p>
<p><strong>Result:</strong> 在具有挑战性的多模态基准测试上，BayesRAG显著优于最先进的方法。</p>
<p><strong>Insight:</strong> 创新点在于将跨模态检索结果的内在一致性建模为概率证据，并利用贝叶斯推理和证据理论进行融合，以解决异构模态的隔离问题，提升检索结果的鲁棒性。这为多模态检索融合建立了一个新的范式。</p>
<p><strong>Abstract:</strong> Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/TioeAre/BayesRAG">https://github.com/TioeAre/BayesRAG</a>.</p>
  </div>
</details>

<hr>
<h3 id="52-Reward-Modeling-from-Natural-Language-Human-Feedback-cs-CLPDF"><a href="#52-Reward-Modeling-from-Natural-Language-Human-Feedback-cs-CLPDF" class="headerlink" title="[52] Reward Modeling from Natural Language Human Feedback cs.CLPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07349">Reward Modeling from Natural Language Human Feedback</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07349" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zongqi Wang, Rui Wang, Yuchuan Wu, Yiyao Yu, Pinyi Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为RM-NLHF的新方法，用于改进生成式奖励模型的训练。该方法利用自然语言反馈（如人类评论）来获取过程奖励信号，以替代传统基于二元偏好标签的奖励，从而解决二元分类任务中模型可能通过猜测获得正确结果而缺乏可靠推理的问题。此外，还引入了元奖励模型（MetaRM）来扩展人类评论的适用范围。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统基于二元偏好标签的生成式奖励模型训练中，模型容易通过猜测正确结果而非基于扎实的推理来完成任务，这会导致奖励信号中存在大量噪声，从而损害强化学习的有效性。</p>
<p><strong>Result:</strong> 在多个基准测试上的实验表明，该方法持续优于仅使用结果监督训练的最先进生成式奖励模型，证实了使用自然语言反馈相对于二元人类反馈的优越性。</p>
<p><strong>Insight:</strong> 创新点在于将自然语言反馈（如人类评论）整合为过程奖励信号，以提供比仅基于结果的监督更准确的奖励；同时，通过元奖励模型（MetaRM）来泛化人类评论，解决了人类评论难以大规模获取的问题，从而扩展了方法的实用性。</p>
<p><strong>Abstract:</strong> Reinforcement Learning with Verifiable reward (RLVR) on preference data has become the mainstream approach for training Generative Reward Models (GRMs). Typically in pairwise rewarding tasks, GRMs generate reasoning chains ending with critiques and preference labels, and RLVR then relies on the correctness of the preference labels as the training reward. However, in this paper, we demonstrate that such binary classification tasks make GRMs susceptible to guessing correct outcomes without sound critiques. Consequently, these spurious successes introduce substantial noise into the reward signal, thereby impairing the effectiveness of reinforcement learning. To address this issue, we propose Reward Modeling from Natural Language Human Feedback (RM-NLHF), which leverages natural language feedback to obtain process reward signals, thereby mitigating the problem of limited solution space inherent in binary tasks. Specifically, we compute the similarity between GRM-generated and human critiques as the training reward, which provides more accurate reward signals than outcome-only supervision. Additionally, considering that human critiques are difficult to scale up, we introduce Meta Reward Model (MetaRM) which learns to predict process reward from datasets with human critiques and then generalizes to data without human critiques. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art GRMs trained with outcome-only reward, confirming the superiority of integrating natural language over binary human feedback as supervision.</p>
  </div>
</details>

<hr>
<h3 id="53-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models-cs-CL-cs-AIPDF"><a href="#53-Beyond-Hard-Masks-Progressive-Token-Evolution-for-Diffusion-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[53] Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models cs.CL | cs.AIPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07351">Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07351" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Linhao Zhong, Linyu Wu, Bozhen Fang, Tianjian Feng, Chenchen Jing</span></p>
<p><strong>TL;DR:</strong> 本文提出EvoToken-DLM，一种新颖的基于扩散的语言建模方法，通过使用演化的软令牌分布替代硬二进制掩码，实现了从掩码状态到离散输出的渐进过渡，支持可修订的解码，并在多个基准测试中表现出优越性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有扩散语言模型（DLMs）依赖硬二进制掩码和离散令牌分配，导致早期决策难以修订且未充分利用中间概率表示的问题。</p>
<p><strong>Result:</strong> 在多个基准测试上的广泛实验表明，EvoToken-DLM始终实现优越性能，优于基于扩散和掩码的DLM基线，达到SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于引入演化的软令牌分布和连续轨迹监督，以软分布替代硬掩码，支持渐进可修订解码，并通过对齐训练目标与迭代概率更新来优化模型训练。</p>
<p><strong>Abstract:</strong> Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: <a target="_blank" rel="noopener" href="https://aim-uofa.github.io/EvoTokenDLM">https://aim-uofa.github.io/EvoTokenDLM</a>.</p>
  </div>
</details>

<hr>
<h3 id="54-Conditional-Memory-via-Scalable-Lookup-A-New-Axis-of-Sparsity-for-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#54-Conditional-Memory-via-Scalable-Lookup-A-New-Axis-of-Sparsity-for-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[54] Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models cs.CL | cs.AIPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07372">Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07372" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Cheng, Wangding Zeng, Damai Dai, Qinyu Chen, Bingxuan Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为’条件记忆’的新型稀疏化轴，通过一个名为Engram的模块实现，该模块基于经典的N-gram嵌入进行现代化改造，支持O(1)复杂度的查找。论文通过形式化’稀疏性分配’问题，发现了一种U型扩展定律来优化神经网络计算（如MoE）与静态记忆（如Engram）之间的权衡。基于此定律，将Engram扩展到270亿参数，在多个基准测试中超越了同等参数和计算量的MoE基线模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决Transformer模型缺乏原生知识查找原语的问题，它们被迫通过计算低效地模拟检索。为了弥补这一缺陷，作者引入了’条件记忆’作为混合专家（MoE）条件计算之外的补充稀疏化维度。</p>
<p><strong>Result:</strong> 在多个基准测试上取得了显著提升：在知识检索任务上（如MMLU +3.4， CMMLU +4.0），在通用推理任务上（如BBH +5.0， ARC-Challenge +3.7），以及在代码和数学领域（HumanEval +3.0， MATH +2.4）均观察到更大增益。长上下文检索能力也大幅提升（如Multi-Query NIAH从84.2提升至97.0）。与严格的等参数和等FLOPs的MoE基线相比，性能更优。</p>
<p><strong>Insight:</strong> 宣称的创新点在于提出了’条件记忆’这一新的建模原语，并通过Engram模块具体实现，为大型语言模型引入了基于可扩展查找的静态记忆轴。客观分析，其核心创新在于：1) 形式化了稀疏性分配问题并发现了U型扩展定律，为模型设计提供了理论指导；2) Engram模块将局部依赖关系委托给查找操作，从而释放了注意力机制处理全局上下文的能力；3) 其确定性寻址机制支持运行时从主机内存预取，实现了基础设施感知的高效率。这为下一代稀疏模型的设计提供了新的方向。</p>
<p><strong>Abstract:</strong> While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code&#x2F;math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone’s early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.</p>
  </div>
</details>

<hr>
<h3 id="55-GROKE-Vision-Free-Navigation-Instruction-Evaluation-via-Graph-Reasoning-on-OpenStreetMap-cs-CLPDF"><a href="#55-GROKE-Vision-Free-Navigation-Instruction-Evaluation-via-Graph-Reasoning-on-OpenStreetMap-cs-CLPDF" class="headerlink" title="[55] GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap cs.CLPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07375">GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07375" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Farzad Shami, Subhrasankha Dey, Nico Van de Weghe, Henrikki Tenkanen</span></p>
<p><strong>TL;DR:</strong> 本文提出GROKE框架，一种无需视觉输入和训练的基于LLM的分层评估方法，利用OpenStreetMap数据评估导航指令的功能性效用，通过子指令规划和拓扑图导航显著降低导航错误。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统基于参考的指标（如BLEU和ROUGE）无法评估导航指令功能性效用的问题，以及现有VLN智能体作为评估器时对高保真视觉模拟器的依赖、许可限制、计算成本和感知误差带来的挑战。</p>
<p><strong>Result:</strong> 在Map2Seq数据集上，相比启发式和采样基线，导航错误减少了68.5%；结构化JSON和文本格式的空间信息表示显著优于基于网格和视觉图的表示。</p>
<p><strong>Insight:</strong> 创新点包括使用OpenStreetMap数据实现无视觉依赖的评估，通过分层架构结合子指令规划和拓扑图导航提高评估准确性和可解释性，为导航指令评估提供了可扩展且无需视觉输入的范式。</p>
<p><strong>Abstract:</strong> The evaluation of navigation instructions remains a persistent challenge in Vision-and-Language Navigation (VLN) research. Traditional reference-based metrics such as BLEU and ROUGE fail to capture the functional utility of spatial directives, specifically whether an instruction successfully guides a navigator to the intended destination. Although existing VLN agents could serve as evaluators, their reliance on high-fidelity visual simulators introduces licensing constraints and computational costs, and perception errors further confound linguistic quality assessment. This paper introduces GROKE(Graph-based Reasoning over OSM Knowledge for instruction Evaluation), a vision-free training-free hierarchical LLM-based framework for evaluating navigation instructions using OpenStreetMap data. Through systematic ablation studies, we demonstrate that structured JSON and textual formats for spatial information substantially outperform grid-based and visual graph representations. Our hierarchical architecture combines sub-instruction planning with topological graph navigation, reducing navigation error by 68.5% compared to heuristic and sampling baselines on the Map2Seq dataset. The agent’s execution success, trajectory fidelity, and decision patterns serve as proxy metrics for functional navigability given OSM-visible landmarks and topology, establishing a scalable and interpretable evaluation paradigm without visual dependencies. Code and data are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/groke">https://anonymous.4open.science/r/groke</a>.</p>
  </div>
</details>

<hr>
<h3 id="56-Outcome-Grounded-Advantage-Reshaping-for-Fine-Grained-Credit-Assignment-in-Mathematical-Reasoning-cs-CL-cs-LGPDF"><a href="#56-Outcome-Grounded-Advantage-Reshaping-for-Fine-Grained-Credit-Assignment-in-Mathematical-Reasoning-cs-CL-cs-LGPDF" class="headerlink" title="[56] Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning cs.CL | cs.LGPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07408">Outcome-Grounded Advantage Reshaping for Fine-Grained Credit Assignment in Mathematical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07408" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziheng Li, Liu Kang, Feng Xiao, Luxi Xing, Qingyi Si</span></p>
<p><strong>TL;DR:</strong> 本文针对Group Relative Policy Optimization (GRPO)在推理任务中存在的粗粒度信用分配问题，提出了Outcome-grounded Advantage Reshaping (OAR)方法，通过两种策略（OAR-P和OAR-G）对序列中不同token的影响进行细粒度评估和优势重塑，从而在数学推理基准上显著提升了模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准GRPO采用粗粒度的信用分配机制，将组级奖励均匀分配给序列中的每个token，忽略了各个推理步骤的不同贡献，本文旨在解决这一局限性。</p>
<p><strong>Result:</strong> 在广泛的数学推理基准测试中，OAR-P设定了性能上限，而OAR-G以可忽略的计算开销实现了可比的性能提升，两者均显著优于强大的GRPO基线，推动了无评论家LLM推理的边界。</p>
<p><strong>Insight:</strong> 创新点在于提出了基于结果细粒度信用分配的OAR机制，通过反事实token扰动（OAR-P）和输入梯度敏感性代理（OAR-G）两种互补策略估计token影响力，并结合保守的双层优势重塑方案，在保持整体优势质量的同时抑制低影响token并提升关键token。</p>
<p><strong>Abstract:</strong> Group Relative Policy Optimization (GRPO) has emerged as a promising critic-free reinforcement learning paradigm for reasoning tasks. However, standard GRPO employs a coarse-grained credit assignment mechanism that propagates group-level rewards uniformly to to every token in a sequence, neglecting the varying contribution of individual reasoning steps. We address this limitation by introducing Outcome-grounded Advantage Reshaping (OAR), a fine-grained credit assignment mechanism that redistributes advantages based on how much each token influences the model’s final answer. We instantiate OAR via two complementary strategies: (1) OAR-P, which estimates outcome sensitivity through counterfactual token perturbations, serving as a high-fidelity attribution signal; (2) OAR-G, which uses an input-gradient sensitivity proxy to approximate the influence signal with a single backward pass. These importance signals are integrated with a conservative Bi-Level advantage reshaping scheme that suppresses low-impact tokens and boosts pivotal ones while preserving the overall advantage mass. Empirical results on extensive mathematical reasoning benchmarks demonstrate that while OAR-P sets the performance upper bound, OAR-G achieves comparable gains with negligible computational overhead, both significantly outperforming a strong GRPO baseline, pushing the boundaries of critic-free LLM reasoning.</p>
  </div>
</details>

<hr>
<h3 id="57-SAD-A-Large-Scale-Strategic-Argumentative-Dialogue-Dataset-cs-CLPDF"><a href="#57-SAD-A-Large-Scale-Strategic-Argumentative-Dialogue-Dataset-cs-CLPDF" class="headerlink" title="[57] SAD: A Large-Scale Strategic Argumentative Dialogue Dataset cs.CLPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07423">SAD: A Large-Scale Strategic Argumentative Dialogue Dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07423" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongkang Liu, Jiayang Yu, Mingyang Wang, Yiqun Zhang, Ercong Nie</span></p>
<p><strong>TL;DR:</strong> 本文介绍了SAD，一个大规模战略性论证对话数据集，包含392,822个示例，每个话语标注了五种论证策略类型，支持多策略标注。该数据集旨在推动论证对话的深度建模，要求模型基于对话历史、指定立场和目标策略生成上下文合适的论证。作者还评估了多种预训练生成模型在SAD上的表现，并分析了论证中的策略使用模式。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有论证语料库大多关注非交互式单轮设置，而实际论证常以多轮对话形式进行，说话者通过多样化论证策略增强说服力。为支持论证对话的深度建模，需要大规模、策略标注的对话数据集。</p>
<p><strong>Result:</strong> 在SAD数据集上对多种预训练生成模型进行了基准测试，并深入分析了论证中的策略使用模式，但摘要未提及具体定量结果（如SOTA比较）。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个大规模战略性论证对话数据集，基于论证理论对每个话语进行多策略标注，要求模型综合对话历史、立场和指定策略生成论证，为交互式论证生成研究提供了新基准。</p>
<p><strong>Abstract:</strong> Argumentation generation has attracted substantial research interest due to its central role in human reasoning and decision-making. However, most existing argumentative corpora focus on non-interactive, single-turn settings, either generating arguments from a given topic or refuting an existing argument. In practice, however, argumentation is often realized as multi-turn dialogue, where speakers defend their stances and employ diverse argumentative strategies to strengthen persuasiveness. To support deeper modeling of argumentation dialogue, we present the first large-scale \textbf{S}trategic \textbf{A}rgumentative \textbf{D}ialogue dataset, SAD, consisting of 392,822 examples. Grounded in argumentation theories, we annotate each utterance with five strategy types, allowing multiple strategies per utterance. Unlike prior datasets, SAD requires models to generate contextually appropriate arguments conditioned on the dialogue history, a specified stance on the topic, and targeted argumentation strategies. We further benchmark a range of pretrained generative models on SAD and present in-depth analysis of strategy usage patterns in argumentation.</p>
  </div>
</details>

<hr>
<h3 id="58-KALE-Enhancing-Knowledge-Manipulation-in-Large-Language-Models-via-Knowledge-aware-Learning-cs-CL-cs-AIPDF"><a href="#58-KALE-Enhancing-Knowledge-Manipulation-in-Large-Language-Models-via-Knowledge-aware-Learning-cs-CL-cs-AIPDF" class="headerlink" title="[58] KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning cs.CL | cs.AIPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07430">KALE: Enhancing Knowledge Manipulation in Large Language Models via Knowledge-aware Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07430" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qitan Lv, Tianyu Liu, Qiaosheng Zhang, Xingcheng Xu, Chaochao Lu</span></p>
<p><strong>TL;DR:</strong> 本文提出了KALE（知识感知学习）框架，旨在增强大语言模型的知识操纵能力，即有效回忆、推理和迁移相关知识的能力。该框架利用知识图谱生成高质量推理链，并通过一种知识感知的微调范式，最小化有&#x2F;无推理链预测之间的KL散度，来内化推理过程，从而缓解模型’知道但答错’的现象。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法主要依赖监督微调来提升大语言模型的知识操纵能力，但模型仍存在’知道但答错’的问题，即模型拥有相关知识却无法利用其得出正确答案。本文旨在解决这一挑战。</p>
<p><strong>Result:</strong> 在六个不同大语言模型和八个流行基准测试上的广泛实验表明，KALE有效提升了模型性能，最高实现了11.72%的准确率提升，平均提升为4.18%。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个结合知识图谱的后训练框架，其核心是知识引导的数据合成方法（从知识图谱高效提取多跳推理路径生成高质量推理链）和知识感知的微调范式（通过KL散度目标内化推理过程）。这为利用结构化知识增强LLM的推理能力提供了一种系统性的方法。</p>
<p><strong>Abstract:</strong> Despite the impressive performance of large language models (LLMs) pretrained on vast knowledge corpora, advancing their knowledge manipulation-the ability to effectively recall, reason, and transfer relevant knowledge-remains challenging. Existing methods mainly leverage Supervised Fine-Tuning (SFT) on labeled datasets to enhance LLMs’ knowledge manipulation ability. However, we observe that SFT models still exhibit the known&amp;incorrect phenomenon, where they explicitly possess relevant knowledge for a given question but fail to leverage it for correct answers. To address this challenge, we propose KALE (Knowledge-Aware LEarning)-a post-training framework that leverages knowledge graphs (KGs) to generate high-quality rationales and enhance LLMs’ knowledge manipulation ability. Specifically, KALE first introduces a Knowledge-Induced (KI) data synthesis method that efficiently extracts multi-hop reasoning paths from KGs to generate high-quality rationales for question-answer pairs. Then, KALE employs a Knowledge-Aware (KA) fine-tuning paradigm that enhances knowledge manipulation by internalizing rationale-guided reasoning through minimizing the KL divergence between predictions with and without rationales. Extensive experiments on eight popular benchmarks across six different LLMs demonstrate the effectiveness of KALE, achieving accuracy improvements of up to 11.72% and an average of 4.18%.</p>
  </div>
</details>

<hr>
<h3 id="59-Controlling-Multimodal-Conversational-Agents-with-Coverage-Enhanced-Latent-Actions-cs-CL-cs-AI-cs-LGPDF"><a href="#59-Controlling-Multimodal-Conversational-Agents-with-Coverage-Enhanced-Latent-Actions-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[59] Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions cs.CL | cs.AI | cs.LGPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07516">Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07516" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongqi Li, Hao Lang, Tieyun Qian, Yongbin Li</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于覆盖增强潜在动作的方法，用于控制多模态对话代理。通过构建紧凑的潜在动作空间进行强化学习微调，以应对大规模文本标记空间的挑战，并利用配对图像-文本数据和纯文本数据提升潜在动作空间的覆盖范围。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决多模态对话代理在强化学习微调中面临的大规模文本标记空间处理难题，通过构建潜在动作空间来提升泛化性能。</p>
<p><strong>Result:</strong> 在两个对话任务上，该方法在多种强化学习算法中均优于竞争基线，展示了其有效性。</p>
<p><strong>Insight:</strong> 创新点包括利用观察学习机制构建潜在动作空间，以及通过跨模态投影器和循环一致性损失结合配对与纯文本数据增强覆盖，从而提升模型的鲁棒性和性能。</p>
<p><strong>Abstract:</strong> Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.</p>
  </div>
</details>

<hr>
<h3 id="60-Thinking-Before-Constraining-A-Unified-Decoding-Framework-for-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#60-Thinking-Before-Constraining-A-Unified-Decoding-Framework-for-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[60] Thinking Before Constraining: A Unified Decoding Framework for Large Language Models cs.CL | cs.AIPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07525">Thinking Before Constraining: A Unified Decoding Framework for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07525" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ngoc Trinh Hung Nguyen, Alonso Silva, Laith Zumot, Liubov Tupikina, Armen Aghasaryan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种结合自然生成与结构化生成优势的统一解码框架，允许大语言模型在生成触发词前自由推理，随后切换到结构化输出，从而在保持自然语言表达力的同时确保输出格式的可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决自然生成缺乏结构化输出导致解析困难，而结构化生成又可能限制模型推理能力的问题，旨在平衡两者的优势。</p>
<p><strong>Result:</strong> 在多个分类和推理任务数据集上的评估显示，该方法相比自然生成准确率提升高达27%，仅需10-20个额外token的开销。</p>
<p><strong>Insight:</strong> 通过触发词机制动态切换生成模式，实现了推理自由与输出结构化的统一，为LLM解码提供了灵活且可靠的解决方案。</p>
<p><strong>Abstract:</strong> Natural generation allows Language Models (LMs) to produce free-form responses with rich reasoning, but the lack of guaranteed structure makes outputs difficult to parse or verify. Structured generation, or constrained decoding, addresses this drawback by producing content in standardized formats such as JSON, ensuring consistency and guaranteed-parsable outputs, but it can inadvertently restrict the model’s reasoning capabilities. In this work, we propose a simple approach that combines the advantages of both natural and structured generation. By allowing LLMs to reason freely until specific trigger tokens are generated, and then switching to structured generation, our method preserves the expressive power of natural language reasoning while ensuring the reliability of structured outputs. We further evaluate our approach on several datasets, covering both classification and reasoning tasks, to demonstrate its effectiveness, achieving a substantial gain of up to 27% in accuracy compared to natural generation, while requiring only a small overhead of 10-20 extra tokens.</p>
  </div>
</details>

<hr>
<h3 id="61-From-RAG-to-Agentic-RAG-for-Faithful-Islamic-Question-Answering-cs-CL-cs-AIPDF"><a href="#61-From-RAG-to-Agentic-RAG-for-Faithful-Islamic-Question-Answering-cs-CL-cs-AIPDF" class="headerlink" title="[61] From RAG to Agentic RAG for Faithful Islamic Question Answering cs.CL | cs.AIPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07528">From RAG to Agentic RAG for Faithful Islamic Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07528" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gagan Bhatia, Hamdy Mubarak, Mustafa Jarrar, George Mikros, Fadi Zaraket</span></p>
<p><strong>TL;DR:</strong> 这篇论文针对伊斯兰问答任务中LLMs可能产生无依据回答的问题，提出了ISLAMICFAITHQA双语生成基准和一套端到端的伊斯兰建模资源，并在此基础上开发了一种基于结构化工具调用的代理式RAG框架，以迭代寻求证据和修订答案，实验表明该方法在阿拉伯语和多语言LLMs上均能提升正确性并达到SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决LLMs在伊斯兰问答中因缺乏依据而产生幻觉或未能恰当弃答的问题，现有MCQ&#x2F;MRC式评估无法捕捉这些关键的现实失败模式。</p>
<p><strong>Result:</strong> 在阿拉伯语中心和多语言LLMs上的实验表明，检索提高了正确性，代理式RAG相比标准RAG带来了最大增益，即使使用小模型（如Qwen3 4B）也实现了SOTA性能，并具有更强的阿拉伯语-英语鲁棒性。</p>
<p><strong>Insight:</strong> 创新点包括引入专注于幻觉和弃答测量的生成式基准ISLAMICFAITHQA、构建包含SFT推理对、偏好样本和古兰经检索语料库的建模套件，以及提出使用结构化工具调用进行迭代证据寻求的代理式RAG框架，增强了问答的忠实性和鲁棒性。</p>
<p><strong>Abstract:</strong> LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ&#x2F;MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic&#x2F;English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur’an retrieval corpus of $\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.</p>
  </div>
</details>

<hr>
<h3 id="62-A-Unified-Framework-for-Emotion-Recognition-and-Sentiment-Analysis-via-Expert-Guided-Multimodal-Fusion-with-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#62-A-Unified-Framework-for-Emotion-Recognition-and-Sentiment-Analysis-via-Expert-Guided-Multimodal-Fusion-with-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[62] A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models cs.CL | cs.AIPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07565">A Unified Framework for Emotion Recognition and Sentiment Analysis via Expert-Guided Multimodal Fusion with Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07565" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaqi Qiao, Xiujuan Xu, Xinran Li, Yu Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了EGMF框架，通过专家引导的多模态融合与大型语言模型相结合，统一处理离散情感识别和连续情感分析任务。该框架包含三个专家网络，分别负责细粒度局部特征、跨模态语义关联和全局上下文依赖，并通过分层动态门控进行自适应融合，再通过伪令牌注入和提示工程与LLM集成，实现基于自然语言生成的统一生成式框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态情感理解中文本、音频和视觉模态的有效融合问题，旨在构建一个统一的框架，同时处理离散情感分类和连续情感回归任务，并提高跨语言的鲁棒性。</p>
<p><strong>Result:</strong> 在MELD、CHERMA、MOSEI、SIMS-V2等双语基准测试上，该方法相比现有最先进方法取得了持续的性能提升，并展现出优越的跨语言鲁棒性，揭示了中英文多模态情感表达的通用模式。</p>
<p><strong>Insight:</strong> 创新点在于将专家网络（细粒度局部、语义关联、全局上下文）与分层动态门控融合机制相结合，并通过伪令牌注入和提示工程将增强的多模态表征与LLM集成，实现了单一生成式框架处理分类和回归任务，同时采用LoRA微调保证了计算效率。</p>
<p><strong>Abstract:</strong> Multimodal emotion understanding requires effective integration of text, audio, and visual modalities for both discrete emotion recognition and continuous sentiment analysis. We present EGMF, a unified framework combining expert-guided multimodal fusion with large language models. Our approach features three specialized expert networks–a fine-grained local expert for subtle emotional nuances, a semantic correlation expert for cross-modal relationships, and a global context expert for long-range dependencies–adaptively integrated through hierarchical dynamic gating for context-aware feature selection. Enhanced multimodal representations are integrated with LLMs via pseudo token injection and prompt-based conditioning, enabling a single generative framework to handle both classification and regression through natural language generation. We employ LoRA fine-tuning for computational efficiency. Experiments on bilingual benchmarks (MELD, CHERMA, MOSEI, SIMS-V2) demonstrate consistent improvements over state-of-the-art methods, with superior cross-lingual robustness revealing universal patterns in multimodal emotional expressions across English and Chinese. We will release the source code publicly.</p>
  </div>
</details>

<hr>
<h3 id="63-PlaM-Training-Free-Plateau-Guided-Model-Merging-for-Better-Visual-Grounding-in-MLLMs-cs-CLPDF"><a href="#63-PlaM-Training-Free-Plateau-Guided-Model-Merging-for-Better-Visual-Grounding-in-MLLMs-cs-CLPDF" class="headerlink" title="[63] PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs cs.CLPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07645">PlaM: Training-Free Plateau-Guided Model Merging for Better Visual Grounding in MLLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07645" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zijing Wang, Yongkang Liu, Mingyang Wang, Ercong Nie, Deyuan Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PlaM的无训练框架，旨在缓解多模态大语言模型（MLLMs）在指令微调过程中出现的文本推理能力退化问题。该方法通过层级的视觉令牌掩码揭示了MLLMs中存在的早期模态分离、中期模态对齐和晚期模态退化的三阶段模式，并基于此提出了一种平台引导的模型融合方法，选择性地将基础语言模型的参数注入到MLLMs中。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型在指令微调过程中，其继承自基础语言模型的强大语言推理能力会意外地退化，从而损害多模态性能。本文旨在解决这一退化问题。</p>
<p><strong>Result:</strong> 在基于五个MLLMs和九个基准测试的实验结果表明，该方法有效。注意力分析进一步表明，融合方法能将注意力从分散的模式转移到任务相关的视觉区域上，实现更聚焦的定位。</p>
<p><strong>Insight:</strong> 创新点在于揭示了MLLMs内部的三阶段模式，并据此设计了一种无需额外训练的平台引导模型融合策略，以选择性地保留和注入基础语言模型的参数，从而提升视觉定位能力。从客观角度看，该方法提供了一种新颖的、基于模型内部行为分析的参数融合视角，以缓解多模态微调中的能力退化问题。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) rely on strong linguistic reasoning inherited from their base language models. However, multimodal instruction fine-tuning paradoxically degrades this text’s reasoning capability, undermining multimodal performance. To address this issue, we propose a training-free framework to mitigate this degradation. Through layer-wise vision token masking, we reveal a common three-stage pattern in multimodal large language models: early-modal separation, mid-modal alignment, and late-modal degradation. By analyzing the behavior of MLLMs at different stages, we propose a plateau-guided model merging method that selectively injects base language model parameters into MLLMs. Experimental results based on five MLLMs on nine benchmarks demonstrate the effectiveness of our method. Attention-based analysis further reveals that merging shifts attention from diffuse, scattered patterns to focused localization on task-relevant visual regions. Our repository is on <a target="_blank" rel="noopener" href="https://github.com/wzj1718/PlaM">https://github.com/wzj1718/PlaM</a>.</p>
  </div>
</details>

<hr>
<h3 id="64-Exploring-the-Meta-level-Reasoning-of-Large-Language-Models-via-a-Tool-based-Multi-hop-Tabular-Question-Answering-Task-cs-CLPDF"><a href="#64-Exploring-the-Meta-level-Reasoning-of-Large-Language-Models-via-a-Tool-based-Multi-hop-Tabular-Question-Answering-Task-cs-CLPDF" class="headerlink" title="[64] Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task cs.CLPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07696">Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07696" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nick Ferguson, Alan Bundy, Kwabena Nuamah</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于工具的多跳表格问答任务，以探索大型语言模型（LLMs）的元级推理能力。该任务要求模型将问题分解为中间步骤、检索数据并执行数学运算，通过分析模型选择合适工具的能力来评估其元级推理。研究发现LLMs在该任务上表现出良好的元级推理，但在任务理解和数值计算方面存在缺陷，且少样本提示对准确性影响不大。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了更结构化地研究LLMs的推理能力，区分元级推理（关于解决任务所需中间步骤的推理过程）和对象级推理（低层步骤的执行），并设计一个需要多步分解和工具使用的问答任务来深入分析LLMs的元级推理能力。</p>
<p><strong>Result:</strong> 在基于地缘政治指标的多跳表格问答任务中，LLMs表现出良好的元级推理能力，但存在任务理解不足和数值计算能力差的问题；少样本提示对准确性影响有限，错误信息通常不会导致性能显著下降。</p>
<p><strong>Insight:</strong> 创新点在于通过工具选择行为来量化评估LLMs的元级推理，并引入’基本动作’作为细粒度分析指标；客观来看，该方法为理解LLMs的推理机制提供了结构化框架，但结果也揭示了模型在复杂任务分解和数值处理上的局限性。</p>
<p><strong>Abstract:</strong> Recent advancements in Large Language Models (LLMs) are increasingly focused on “reasoning” ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.) We design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years. Questions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions. To bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains ‘essential actions’ against which we can compare the tool call output of LLMs to infer the strength of reasoning ability. We find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding. We find that n-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs. Finally, we discuss the generalisation and limitation of our findings to other task domains.</p>
  </div>
</details>

<hr>
<h3 id="65-Structure-First-Reason-Next-Enhancing-a-Large-Language-Model-using-Knowledge-Graph-for-Numerical-Reasoning-in-Financial-Documents-cs-CLPDF"><a href="#65-Structure-First-Reason-Next-Enhancing-a-Large-Language-Model-using-Knowledge-Graph-for-Numerical-Reasoning-in-Financial-Documents-cs-CLPDF" class="headerlink" title="[65] Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents cs.CLPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07754">Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07754" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aryan Mishra, Akash Anil</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种结合知识图谱（KG）与大语言模型（LLM）的框架，以提升LLM在金融文档数值推理任务中的性能。该框架首先从文档中提取结构化信息构建知识图谱，再引导LLM进行推理，旨在解决LLM在处理金融报告中复杂数字和计算时的瓶颈问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大语言模型在处理金融文档（如长文本和半结构化表格）时，难以准确提取数值数据并进行可靠计算的瓶颈问题，通过利用文档固有的结构化信息来增强LLM的数值推理能力。</p>
<p><strong>Result:</strong> 在FinQA基准数据集上，使用开源LLM Llama 3.1 8B Instruct进行评估，所提框架相比原始LLM将执行准确率相对提升了约12%。</p>
<p><strong>Insight:</strong> 创新点在于提出了“结构优先，推理在后”的范式，即先利用文档固有模式提取知识图谱来构建结构化表示，再将其与大语言模型结合进行推理。这为增强LLM在领域特定（如金融）复杂任务中的性能提供了一种可借鉴的结构化数据增强方法。</p>
<p><strong>Abstract:</strong> Numerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts. Recently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents related to finance often consist of long and complex financial contexts, LLMs appear well-suited for building high-quality automated financial question-answering systems. However, LLMs often face challenges in accurately processing the various numbers within financial reports. Extracting numerical data from unstructured text and semi-structured tables, and reliably performing accurate calculations, remains a significant bottleneck for numerical reasoning in most state-of-the-art LLMs. Recent studies have shown that structured data augmentations, such as Knowledge Graphs (KGs), have notably improved the predictions of LLMs along with logical explanations. Thus, it is an important requirement to consider inherent structured information in financial reports while using LLMs for various financial analytics. This paper proposes a framework to incorporate structured information using KGs along with LLM predictions for numerical reasoning tasks. The KGs are extracted using a proposed schema inherently from the document under processing. We evaluated our proposed framework over the benchmark data FinQA, using an open-source LLM, namely Llama 3.1 8B Instruct. We observed that the proposed framework improved execution accuracy by approximately 12% relative to the vanilla LLM.</p>
  </div>
</details>

<hr>
<h3 id="66-Enhancing-Self-Correction-in-Large-Language-Models-through-Multi-Perspective-Reflection-cs-CLPDF"><a href="#66-Enhancing-Self-Correction-in-Large-Language-Models-through-Multi-Perspective-Reflection-cs-CLPDF" class="headerlink" title="[66] Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection cs.CLPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07780">Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07780" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mariana Costa, Alberlucia Rafael Soarez, Daniel Kim, Camila Ferreira</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MyGO多视角反思思维链（PR-CoT）的新方法，旨在通过结构化、多角度的自我反思来增强大型语言模型的自我校正能力。该方法在初始思维链推理后，引导模型从逻辑一致性、信息完整性、偏见&#x2F;伦理以及替代方案等多个预设角度进行自我评估，从而在不重新训练模型的情况下，通过提示工程将初始推理优化为更鲁棒和准确的最终答案。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管思维链提示推动了LLM推理，但在复杂或伦理敏感任务中，其一致性、准确性和自我校正能力仍存在不足，现有的单维度反思方法改进有限。</p>
<p><strong>Result:</strong> 在算术、常识、伦理决策和逻辑谜题等任务上，使用GPT-3.5和GPT-4模型的实验表明，PR-CoT在逻辑一致性和错误校正方面显著优于传统思维链和现有反思方法，尤其在伦理决策等微妙领域取得显著提升，消融研究、人工评估和定性分析进一步验证了各反思视角的贡献及整体范式的有效性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个结构化的多视角反思框架，将自我校正从单一维度扩展到逻辑、信息、伦理和方案等多个互补维度，并通过纯提示工程实现，为提升LLM推理的可靠性和鲁棒性提供了一种无需模型微调的新范式。</p>
<p><strong>Abstract:</strong> While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases&#x2F;ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT’s superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.</p>
  </div>
</details>

<hr>
<h3 id="67-Beyond-Single-Shot-Multi-step-Tool-Retrieval-via-Query-Planning-cs-CL-cs-AI-cs-IRPDF"><a href="#67-Beyond-Single-Shot-Multi-step-Tool-Retrieval-via-Query-Planning-cs-CL-cs-AI-cs-IRPDF" class="headerlink" title="[67] Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning cs.CL | cs.AI | cs.IRPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07782">Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07782" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wei Fang, James Glass</span></p>
<p><strong>TL;DR:</strong> 本文提出TOOLQP框架，将工具检索建模为迭代查询规划过程，通过将复杂指令分解为子任务并动态生成查询来弥补用户目标与技术文档之间的语义鸿沟，从而提升大语言模型代理在动态工具库中的检索效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决标准单次密集检索器在处理复杂请求时因抽象用户目标与技术文档脱节、固定大小嵌入难以建模组合工具结构而导致的检索失败问题。</p>
<p><strong>Result:</strong> 实验表明TOOLQP在多个基准测试中达到最先进性能，表现出优异的零样本泛化能力、对不同检索器的鲁棒性，并显著提升下游代理执行效果。</p>
<p><strong>Insight:</strong> 创新点在于将检索过程从单次匹配转变为基于查询规划的迭代分解，通过合成查询轨迹训练并结合可验证奖励的强化学习进行优化，有效建模组合工具需求。</p>
<p><strong>Abstract:</strong> LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.</p>
  </div>
</details>

<hr>
<h3 id="68-Kinship-Data-Benchmark-for-Multi-hop-Reasoning-cs-CL-cs-AIPDF"><a href="#68-Kinship-Data-Benchmark-for-Multi-hop-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[68] Kinship Data Benchmark for Multi-hop Reasoning cs.CL | cs.AIPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07794">Kinship Data Benchmark for Multi-hop Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07794" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianda Sun, Dimitar Kazakov</span></p>
<p><strong>TL;DR:</strong> 该论文提出了KinshipQA基准测试，旨在通过亲属关系推理来评估大语言模型的多跳推理能力。作者开发了一个生成式流水线，能够按需生成大规模、真实且文化特定的家谱数据，并从中构建需要隐式关系链推理的文本推理任务。在零样本协议下评估了六个最先进的大语言模型，结果显示该基准能有效揭示不同模型和文化设置下多跳推理的系统性差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是评估大语言模型的多跳推理能力，即整合多条信息进行连贯推理的能力，现有基准可能缺乏对文化特定性和关系深度的系统控制。</p>
<p><strong>Result:</strong> 在KinshipQA基准上，使用精确匹配和基于集合的指标评估了六个开源和闭源的最先进大语言模型，结果显示性能差异显著，暴露了模型和文化设置对多跳推理的系统性影响。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个可生成大规模、文化特定家谱数据的流水线，从而能系统控制任务难度和文化假设，构建出需要隐式关系链推理的文本任务，为评估模型的多跳推理能力提供了可控且多样化的基准。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.</p>
  </div>
</details>

<hr>
<h3 id="69-Reference-Games-as-a-Testbed-for-the-Alignment-of-Model-Uncertainty-and-Clarification-Requests-cs-CLPDF"><a href="#69-Reference-Games-as-a-Testbed-for-the-Alignment-of-Model-Uncertainty-and-Clarification-Requests-cs-CLPDF" class="headerlink" title="[69] Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests cs.CLPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07820">Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07820" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Manar Ali, Judith Sieker, Sina Zarrieß, Hendrik Buschmeier</span></p>
<p><strong>TL;DR:</strong> 本文探讨了语言模型是否能够像人类对话者一样，在不确定时主动请求澄清。作者提出将指代游戏作为测试平台，评估了三个视觉语言模型在基线指代解析任务和不确定性澄清请求实验中的表现。结果表明，即使在这种简单任务中，模型也难以识别内部不确定性并转化为适当的澄清行为。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决语言模型在对话中是否能够主动识别和表达自身不确定性的问题，以促进模型与人类更自然的交互。</p>
<p><strong>Result:</strong> 在指代游戏测试中，三个视觉语言模型在基线任务和澄清请求实验中的表现表明，模型难以有效识别内部不确定性并进行适当澄清，凸显了当前模型的局限性。</p>
<p><strong>Insight:</strong> 指代游戏可作为评估（视觉）语言模型交互能力的可控测试平台；模型在不确定性识别和澄清请求方面存在显著差距，这为提升模型对话对齐性提供了研究方向。</p>
<p><strong>Abstract:</strong> In human conversation, both interlocutors play an active role in maintaining mutual understanding. When addressees are uncertain about what speakers mean, for example, they can request clarification. It is an open question for language models whether they can assume a similar addressee role, recognizing and expressing their own uncertainty through clarification. We argue that reference games are a good testbed to approach this question as they are controlled, self-contained, and make clarification needs explicit and measurable. To test this, we evaluate three vision-language models comparing a baseline reference resolution task to an experiment where the models are instructed to request clarification when uncertain. The results suggest that even in such simple tasks, models often struggle to recognize internal uncertainty and translate it into adequate clarification behavior. This demonstrates the value of reference games as testbeds for interaction qualities of (vision and) language models.</p>
  </div>
</details>

<hr>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="70-HyperTopo-Adapters-Geometry-and-Topology-Aware-Segmentation-of-Leaf-Lesions-on-Frozen-Encoders-cs-CVPDF"><a href="#70-HyperTopo-Adapters-Geometry-and-Topology-Aware-Segmentation-of-Leaf-Lesions-on-Frozen-Encoders-cs-CVPDF" class="headerlink" title="[70] HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders cs.CVPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06067">HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06067" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chimdi Walter Ndubuisi, Toni Kazic</span></p>
<p><strong>TL;DR:</strong> 本文提出HyperTopo-Adapters，一种轻量级、参数高效的适配器头，用于在冻结的视觉编码器上进行叶片病斑分割。该方法将特征嵌入到双曲+欧几里得+球面（H+E+S）的乘积流形中，以促进层次分离、局部线性细节和全局闭合。通过引入拓扑先验（包括持久同调距离评估和可微替代损失）来补充标准像素级损失，旨在提升分割的边界和拓扑精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 叶片病斑分割对拓扑敏感，标准欧几里得潜在空间中的像素级损失对微小的合并、分裂或假孔洞等具有生物学意义的结构变化惩罚较弱，因此需要几何和拓扑感知的方法来更好地捕捉这些细节。</p>
<p><strong>Result:</strong> 在Kaggle叶片病斑数据集（N&#x3D;2,940）上的初步实验显示，该方法在边界和拓扑指标上持续提升（如将Delta beta_1孔洞误差降低9%），同时Dice&#x2F;IoU指标保持竞争力。研究还进行了控制消融实验，测试了不同编码器、输入分辨率等变量。</p>
<p><strong>Insight:</strong> 创新点包括：1）在乘积流形（H+E+S）中嵌入特征以结合不同几何特性；2）引入拓扑先验（持久同调距离和可微替代损失）作为损失函数的一部分；3）设计了针对双曲对比项和拓扑先验的热身策略，以及基于最小持久图距离的检查点选择规则。该方法提供了一个可复现的训练&#x2F;评估框架，用于隔离几何&#x2F;拓扑先验并揭示失败模式。</p>
<p><strong>Abstract:</strong> Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold – hyperbolic + Euclidean + spherical (H + E + S) – to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice&#x2F;BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N&#x3D;2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice&#x2F;IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2&#x2F;v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train&#x2F;eval suite (available at <a target="_blank" rel="noopener" href="https://github.com/ChimdiWalter/HyperTopo-Adapters">https://github.com/ChimdiWalter/HyperTopo-Adapters</a>) that isolates geometric&#x2F;topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.</p>
  </div>
</details>

<hr>
<h3 id="71-Semantic-Event-Graphs-for-Long-Form-Video-Question-Answering-cs-CV-cs-AIPDF"><a href="#71-Semantic-Event-Graphs-for-Long-Form-Video-Question-Answering-cs-CV-cs-AIPDF" class="headerlink" title="[71] Semantic Event Graphs for Long-Form Video Question Answering cs.CV | cs.AIPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06097">Semantic Event Graphs for Long-Form Video Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06097" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aradhya Dixit, Tianxi Liang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为语义事件图（SEG）的轻量级符号接口，用于解决长视频问答任务中视觉语言模型因计算和标记限制而难以处理小时级视频的问题。该方法通过检测和跟踪对象，将邻近模式转换为START&#x2F;END人-物事件，并组织成时间场景图（TSG），在推理时通过查询感知剪枝模块提取相关子图，再传递给Gemini 2.5 Flash生成答案，显著降低了标记使用量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有系统在处理长视频时通常通过降采样帧或密集视觉嵌入来平衡时间覆盖与成本，但存在推理能力不足的问题，本文旨在设计一种紧凑的符号表示来替代原始帧，以在保持长距离推理能力的同时提高标记和计算效率。</p>
<p><strong>Result:</strong> 在五个YouTube视频（每个包含300-500个交互）和120个自动生成的长时域问题上，SEG实现了65.0%的准确率，每个查询仅使用3.47k标记，与使用40.39k标记的全日志基线（62.5%准确率）相当，同时将标记使用量减少了91.4%；而仅使用最后30秒的短上下文基线准确率降至2.5%，突显了显式时间记忆的必要性。</p>
<p><strong>Insight:</strong> 创新点在于将视频内容抽象为符号时间图作为可插拔的记忆层，通过查询感知剪枝动态提取相关信息，这为现成的视觉语言模型提供了有效的长距离推理支持，同时大幅提升了长视频问答的标记和成本效率，是一种新颖的轻量级接口设计。</p>
<p><strong>Abstract:</strong> Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START&#x2F;END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.</p>
  </div>
</details>

<hr>
<h3 id="72-COVR-Collaborative-Optimization-of-VLMs-and-RL-Agent-for-Visual-Based-Control-cs-CV-cs-AI-cs-LGPDF"><a href="#72-COVR-Collaborative-Optimization-of-VLMs-and-RL-Agent-for-Visual-Based-Control-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[72] COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control cs.CV | cs.AI | cs.LGPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06122">COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06122" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Canming Xia, Peixi Peng, Guang Tan, Zhan Su, Haoran Xu</span></p>
<p><strong>TL;DR:</strong> 本文提出COVR框架，通过协同优化视觉语言模型（VLM）和强化学习（RL）策略来解决视觉强化学习中样本效率低下的问题。COVR利用RL生成的数据微调VLM以增强其与目标任务一致的语义推理能力，并利用增强后的VLM通过动作先验指导策略学习。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有工作通常仅从VLM向RL进行知识蒸馏，忽略了RL生成的交互数据对增强VLM的潜力，导致视觉RL在复杂任务中因高维观测而样本效率低下。</p>
<p><strong>Result:</strong> 在多种具有挑战性的视觉控制任务上进行广泛实验，COVR取得了强劲的性能表现。</p>
<p><strong>Insight:</strong> 创新点在于提出了VLM与RL策略的协同优化框架，并引入了探索驱动的动态过滤模块和回报感知的自适应损失权重模块以提高微调效率和训练稳定性，同时设计了渐进式微调策略以减少资源消耗。</p>
<p><strong>Abstract:</strong> Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.</p>
  </div>
</details>

<hr>
<h3 id="73-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models-cs-CV-cs-AIPDF"><a href="#73-What-Users-Leave-Unsaid-Under-Specified-Queries-Limit-Vision-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[73] What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models cs.CV | cs.AIPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06165">What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06165" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dasol Choi, Guijin Son, Hanwool Lee, Minhyuk Kim, Hyunwoo Ko</span></p>
<p><strong>TL;DR:</strong> 该论文通过构建HAERAE-Vision基准，揭示了当前视觉语言模型在处理真实世界中用户提出的非正式、不完整查询时存在严重性能下降的问题。研究发现，即使是最先进的模型在原始查询上的准确率也不足50%，而将查询明确化后性能可提升8至22个百分点。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决当前视觉语言基准测试中结构化、明确的问题与真实用户查询（通常是非正式且不完整的）之间的差距，评估模型在实际部署中的真实能力。</p>
<p><strong>Result:</strong> 在HAERAE-Vision基准（包含653个来自韩国在线社区的真实视觉问题及其明确改写版本，共1,306个查询变体）上评估了39个VLMs，发现即使GPT-5和Gemini 2.5 Pro等SOTA模型在原始查询上的准确率也低于50%；查询明确化后性能提升显著（8-22点），且较小模型受益最大；即使结合网络搜索，不完整查询的性能仍低于不使用搜索的明确查询。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了一个基于真实用户查询的基准，揭示了查询不完整性是VLM性能瓶颈的关键因素，而非单纯模型能力不足；这强调了基准测试与实际应用之间的关键差距，并指出提升模型对隐含上下文的理解能力是未来重要方向。</p>
<p><strong>Abstract:</strong> Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.</p>
  </div>
</details>

<hr>
<h3 id="74-TIR-Flow-Active-Video-Search-and-Reasoning-with-Frozen-VLMs-cs-CVPDF"><a href="#74-TIR-Flow-Active-Video-Search-and-Reasoning-with-Frozen-VLMs-cs-CVPDF" class="headerlink" title="[74] TIR-Flow: Active Video Search and Reasoning with Frozen VLMs cs.CVPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06176">TIR-Flow: Active Video Search and Reasoning with Frozen VLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06176" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongbo Jin, Siyi Xie, Jiayu Ding, Kuanwei Lin, Ge Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了TIR-Flow框架，旨在解决大型视频语言模型在推理能力上的瓶颈。该框架通过三个协同模块（HDD、HAP、EBA），将模型从被动处理转变为主动的视频搜索与推理，无需额外数据或参数更新。在七个基准测试上的实验表明，该方法显著优于现有基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大型视频语言模型在感知方面进步显著，但其推理能力仍是瓶颈。当前主流方案依赖于大规模合成思维链数据集并进行监督微调和强化学习，这种方法主要优化概率采样效率和对齐输出分布，但未能激活动态视觉探索所需的内在智能。</p>
<p><strong>Result:</strong> 在七个基准测试上的广泛实验表明，TIR-Flow显著优于近期强基线，平均性能提升5.9%，在Egoschema基准上提升达到10.5%。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于将范式从被动处理转变为主动的视频搜索与推理，无需额外训练数据或更新模型参数。其核心是通过三个模块（查询分解、主动感知、证据积累）赋予冻结的视觉语言模型类似系统2的主动感知能力，这为长视野视频推理提供了一个可扩展的路径。</p>
<p><strong>Abstract:</strong> While Large Video-Language Models (Video-LLMs) have achieved remarkable progress in perception, their reasoning capabilities remain a bottleneck. Existing solutions typically resort to a heavy “data engineering” paradigm-synthesizing large-scale Chain-of-Thought (CoT) datasets followed by Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). This pipeline primarily optimizes probability sampling efficiency and aligns output distributions, but fails to activate the intrinsic intelligence required for dynamic visual exploration. In this work, we propose TIR-Flow, a novel framework that shifts the paradigm from passive processing to active video searching and reasoning without additional data or parameter updating. Concretely, our framework operates through three synergistic modules: HDD decomposes complex queries into a set of verifiable sub-tasks; HAP actively directs visual attention to gather high-resolution evidence for hypothesis validation; EBA maintains a persistent workspace to accumulate and update the discovered clues for logical reasoning. Extensive experiments on seven benchmarks demonstrate that TIR-Flow significantly outperforms recent strong baselines, delivering an average performance boost of 5.9%, with gains reaching 10.5% on Egoschema. Our analysis confirms that empowering frozen VLMs with System-2-like active perception is a scalable path toward solving long-horizon video reasoning.</p>
  </div>
</details>

<hr>
<h3 id="75-How-Does-India-Cook-Biryani-cs-CVPDF"><a href="#75-How-Does-India-Cook-Biryani-cs-CVPDF" class="headerlink" title="[75] How Does India Cook Biryani? cs.CVPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06198">How Does India Cook Biryani?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06198" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shubham Goel, Farzana S, C V Rishi, Aditya Arun, C V Jawahar</span></p>
<p><strong>TL;DR:</strong> 该论文构建了一个关于印度比尔亚尼菜烹饪视频的大规模数据集，并提出了一种利用视觉语言模型（VLMs）的多阶段框架，用于对视频进行细粒度程序分割、多模态对齐，并自动比较不同地区烹饪方法的差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频理解方法难以捕捉烹饪视频中细粒度、多模态且基于文化的程序性差异，因此需要新的计算工具来系统性地研究烹饪文化的多样性。</p>
<p><strong>Result:</strong> 研究构建了一个包含12种地区风格、120个高质量YouTube视频的数据集，并创建了一个全面的问答（QA）基准来评估VLMs的程序理解能力，在零样本和微调设置下对多个SOTA模型进行了基准测试。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个结合多模态对齐和视频比较的框架，并引入人机协同验证以提高精度，为通过烹饪视频进行文化遗产计算分析开辟了新方向，并为评估VLMs在结构化多模态推理任务上提供了新测试平台。</p>
<p><strong>Abstract:</strong> Biryani, one of India’s most celebrated dishes, exhibits remarkable regional diversity in its preparation, ingredients, and presentation. With the growing availability of online cooking videos, there is unprecedented potential to study such culinary variations using computational tools systematically. However, existing video understanding methods fail to capture the fine-grained, multimodal, and culturally grounded differences in procedural cooking videos. This work presents the first large-scale, curated dataset of biryani preparation videos, comprising 120 high-quality YouTube recordings across 12 distinct regional styles. We propose a multi-stage framework leveraging recent advances in vision-language models (VLMs) to segment videos into fine-grained procedural units and align them with audio transcripts and canonical recipe text. Building on these aligned representations, we introduce a video comparison pipeline that automatically identifies and explains procedural differences between regional variants. We construct a comprehensive question-answer (QA) benchmark spanning multiple reasoning levels to evaluate procedural understanding in VLMs. Our approach employs multiple VLMs in complementary roles, incorporates human-in-the-loop verification for high-precision tasks, and benchmarks several state-of-the-art models under zero-shot and fine-tuned settings. The resulting dataset, comparison methodology, and QA benchmark provide a new testbed for evaluating VLMs on structured, multimodal reasoning tasks and open new directions for computational analysis of cultural heritage through cooking videos. We release all data, code, and the project website at <a target="_blank" rel="noopener" href="https://farzanashaju.github.io/how-does-india-cook-biryani/">https://farzanashaju.github.io/how-does-india-cook-biryani/</a>.</p>
  </div>
</details>

<hr>
<h3 id="76-Cascading-multi-agent-anomaly-detection-in-surveillance-systems-via-vision-language-models-and-embedding-based-classification-cs-CV-cs-MAPDF"><a href="#76-Cascading-multi-agent-anomaly-detection-in-surveillance-systems-via-vision-language-models-and-embedding-based-classification-cs-CV-cs-MAPDF" class="headerlink" title="[76] Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification cs.CV | cs.MAPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06204">Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06204" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tayyab Rehman, Giovanni De Gasperis, Aly Shmahell</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于监控系统的级联多智能体异常检测框架，该框架通过结合视觉语言模型和基于嵌入的分类，旨在统一实时性能与语义可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决动态视觉环境中智能异常检测需兼顾实时性能和语义可解释性的挑战，传统方法如基于重建的模型、目标检测器和大型视觉语言系统各有局限，无法全面应对。</p>
<p><strong>Result:</strong> 在大规模监控数据上的评估表明，该级联框架相比直接视觉语言推理延迟减少三倍，同时保持高感知保真度（PSNR &#x3D; 38.3 dB, SSIM &#x3D; 0.965）和一致的语义标注。</p>
<p><strong>Insight:</strong> 创新点包括级联多智能体架构、自适应升级阈值和发布-订阅通信骨干，结合了早期退出效率、自适应多智能体推理和可解释异常归因，为可扩展智能视觉监控提供了可复现且节能的基础。</p>
<p><strong>Abstract:</strong> Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR &#x3D; 38.3 dB, SSIM &#x3D; 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.</p>
  </div>
</details>

<hr>
<h3 id="77-When-Imbalance-Comes-Twice-Active-Learning-under-Simulated-Class-Imbalance-and-Label-Shift-in-Binary-Semantic-Segmentation-cs-CVPDF"><a href="#77-When-Imbalance-Comes-Twice-Active-Learning-under-Simulated-Class-Imbalance-and-Label-Shift-in-Binary-Semantic-Segmentation-cs-CVPDF" class="headerlink" title="[77] When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation cs.CVPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06209">When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06209" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Julien Combes, Alexandre Derville, Jean-François Coeurjolly</span></p>
<p><strong>TL;DR:</strong> 本文通过模拟研究探讨了在二元语义分割任务中，类别不平衡和标签偏移对主动学习算法的影响。研究基于开源数据集，人工控制类别不平衡和标签偏移水平，比较了随机采样、基于熵的选择和核心集选择三种主动学习策略。结果表明，即使在高度不平衡的数据集上，基于熵和核心集的选择策略仍保持高效，但强标签偏移会导致效率损失。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决在机器视觉或医学成像中，由于数据量大、标注成本高，且缺陷图像稀少（导致类别不平衡）以及存储限制（可能引发标签偏移）时，主动学习算法的表现问题。</p>
<p><strong>Result:</strong> 在模拟的类别不平衡和标签偏移数据集上，基于熵的选择和核心集选择策略优于随机采样，但强标签偏移会降低这些策略的效率。</p>
<p><strong>Insight:</strong> 创新点在于首次系统模拟了类别不平衡和标签偏移同时存在对主动学习的影响，客观分析表明，即使在不平衡数据中，基于不确定性和多样性的主动学习策略仍有效，但需注意标签偏移的负面影响，这为实际应用中的算法设计提供了重要参考。</p>
<p><strong>Abstract:</strong> The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.</p>
  </div>
</details>

<hr>
<h3 id="78-Akasha-2-Hamiltonian-State-Space-Duality-and-Visual-Language-Joint-Embedding-Predictive-Architectur-cs-CV-cs-AIPDF"><a href="#78-Akasha-2-Hamiltonian-State-Space-Duality-and-Visual-Language-Joint-Embedding-Predictive-Architectur-cs-CV-cs-AIPDF" class="headerlink" title="[78] Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur cs.CV | cs.AIPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06212">Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06212" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yani Meziani</span></p>
<p><strong>TL;DR:</strong> Akasha 2是一种先进的多模态架构，它结合了哈密顿状态空间对偶性（H-SSD）和视觉-语言联合嵌入预测架构（VL-JEPA）。该系统利用Mamba-3选择性状态空间模型（SSM），并通过辛积分增强了一个稀疏哈密顿专家混合（SMoE-HE）来强制潜在的物理守恒定律。对于视觉合成，引入了哈密顿流匹配（HFM）和持久性3D高斯溅射（3DGS），在移动硬件上实现了超低延迟（&lt;50毫秒）。这项工作在潜在世界模型中建立了一个新范式，通过全息记忆架构实现了前所未有的时空一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 将受物理学启发的归纳偏置整合到神经架构中，以改进多模态建模，特别是在视频预测和视觉合成任务中，追求更高的效率、更快的推理速度和更好的时空一致性。</p>
<p><strong>Result:</strong> 在视频预测任务上达到了最先进水平（FVD: 287），视觉合成速度比扩散模型快4倍，推理速度比Transformer基线快3-18倍，同时在长时间范围内保持了能量守恒。</p>
<p><strong>Insight:</strong> 核心创新点在于将哈密顿力学原理（通过H-SSD和SMoE-HE）与先进的序列建模（Mamba-3 SSM）和视觉-语言联合学习（VL-JEPA）相结合，构建了一个具有物理约束和高效推理能力的潜在世界模型。这为构建更高效、更具物理一致性的生成模型和世界模型提供了新思路。</p>
<p><strong>Abstract:</strong> We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.</p>
  </div>
</details>

<hr>
<h3 id="79-SAPL-Semantic-Agnostic-Prompt-Learning-in-CLIP-for-Weakly-Supervised-Image-Manipulation-Localization-cs-CV-cs-AIPDF"><a href="#79-SAPL-Semantic-Agnostic-Prompt-Learning-in-CLIP-for-Weakly-Supervised-Image-Manipulation-Localization-cs-CV-cs-AIPDF" class="headerlink" title="[79] SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization cs.CV | cs.AIPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06222">SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06222" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinghao Wang, Changtao Miao, Dianmo Sheng, Tao Gong, Qi Chu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为SAPL（语义无关提示学习）的方法，用于弱监督图像篡改定位。该方法基于CLIP模型，通过学习编码非语义、边界中心线索的文本提示，使CLIP的多模态相似性关注篡改边缘而非高层语义，结合边缘感知上下文提示学习和分层边缘对比学习模块，在多个公开基准测试中实现了最先进的定位性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 恶意图像篡改威胁公共安全，现有方法依赖昂贵的像素级标注或仅使用图像级二值标签的弱监督方法，后者往往忽视对精确定位至关重要的局部边缘线索，而篡改边界处的特征变化远大于内部区域，因此需要一种能有效利用边缘信息的弱监督定位方法。</p>
<p><strong>Result:</strong> 在多个公开基准测试上的广泛实验表明，SAPL显著优于现有方法，实现了最先进的（SOTA）定位性能。</p>
<p><strong>Insight:</strong> 创新点在于提出语义无关的提示学习，将CLIP的关注点从高层语义引导至篡改边缘；通过边缘感知上下文提示学习将边缘增强的图像特征转化为可学习的文本提示，以及通过分层边缘对比学习增强真实与篡改边缘块之间的区分度，这种在文本和视觉空间双重利用边缘信息的方法具有借鉴意义。</p>
<p><strong>Abstract:</strong> Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that feature variations at manipulated boundaries are substantially larger than in interior regions. To address this gap, we propose Semantic-Agnostic Prompt Learning (SAPL) in CLIP, which learns text prompts that intentionally encode non-semantic, boundary-centric cues so that CLIPs multimodal similarity highlights manipulation edges rather than high-level object semantics. SAPL combines two complementary modules Edge-aware Contextual Prompt Learning (ECPL) and Hierarchical Edge Contrastive Learning (HECL) to exploit edge information in both textual and visual spaces. The proposed ECPL leverages edge-enhanced image features to generate learnable textual prompts via an attention mechanism, embedding semantic-irrelevant information into text features, to guide CLIP focusing on manipulation edges. The proposed HECL extract genuine and manipulated edge patches, and utilize contrastive learning to boost the discrimination between genuine edge patches and manipulated edge patches. Finally, we predict the manipulated regions from the similarity map after processing. Extensive experiments on multiple public benchmarks demonstrate that SAPL significantly outperforms existing approaches, achieving state-of-the-art localization performance.</p>
  </div>
</details>

<hr>
<h3 id="80-Ground-What-You-See-Hallucination-Resistant-MLLMs-via-Caption-Feedback-Diversity-Aware-Sampling-and-Conflict-Regularization-cs-CVPDF"><a href="#80-Ground-What-You-See-Hallucination-Resistant-MLLMs-via-Caption-Feedback-Diversity-Aware-Sampling-and-Conflict-Regularization-cs-CVPDF" class="headerlink" title="[80] Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization cs.CVPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06224">Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06224" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miao Pan, Wangjie Gan, Jintao Chen, Wenqi Zhang, Bing Sun</span></p>
<p><strong>TL;DR:</strong> 本文针对多模态大语言模型在强化学习优化中出现的幻觉问题，提出了一种包含三个核心模块的综合性框架。该框架通过引入规划与描述阶段、基于奖励分布多样性的采样策略以及神经正切核相似性调控，有效减少了幻觉现象并提升了模型推理准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态大语言模型在强化学习训练中因过度依赖链式视觉推理、探索多样性不足以及训练样本间破坏性冲突而导致的严重幻觉问题，以推动其实用化部署。</p>
<p><strong>Result:</strong> 实验结果表明，所提方法显著降低了幻觉率，并有效提升了多模态大语言模型的推理准确率。</p>
<p><strong>Insight:</strong> 创新点在于系统性地识别并针对幻觉的三个根源因素（链式推理锚定错误、探索多样性不足、NTK相似性冲突）设计了对应的模块化解决方案，特别是将NTK相似性分析与InfoNCE损失结合用于调控样本干扰，以及基于奖励分布统计特性（均值与方差）的多样性感知采样策略。</p>
<p><strong>Abstract:</strong> While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization. This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates. To address these challenges, we propose a comprehensive framework comprising three core modules. First, we enhance visual localization by introducing dedicated planning and captioning stages before the reasoning phase, employing a quality-based caption reward to ensure accurate initial anchoring. Second, to improve exploration, we categorize samples based on the mean and variance of their reward distributions, prioritizing samples with high variance to focus the model on diverse and informative data. Finally, to mitigate sample interference, we regulate NTK similarity by grouping sample pairs and applying an InfoNCE loss to push overly similar pairs apart and pull dissimilar ones closer, thereby guiding gradient interactions toward a balanced range. Experimental results demonstrate that our proposed method significantly reduces hallucination rates and effectively enhances the inference accuracy of MLLMs.</p>
  </div>
</details>

<hr>
<h3 id="81-A-survey-of-facial-recognition-techniques-cs-CV-cs-GRPDF"><a href="#81-A-survey-of-facial-recognition-techniques-cs-CV-cs-GRPDF" class="headerlink" title="[81] A survey of facial recognition techniques cs.CV | cs.GRPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06239">A survey of facial recognition techniques</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06239" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aya Kaysan Bahjat</span></p>
<p><strong>TL;DR:</strong> 这篇论文是一篇关于人脸识别技术的综述，系统回顾了该领域的主要方法，包括隐马尔可夫模型、主成分分析、支持向量机、神经网络等，并分析了多个标准人脸数据库上的应用与挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着多媒体内容的快速增长，人脸识别已成为一个重要的研究领域，但人脸作为复杂对象，在光照、年龄、姿态、遮挡和表情等方面存在诸多挑战，需要有效的识别机制。</p>
<p><strong>Result:</strong> 论文未提出新方法，而是综述了现有技术，并基于JAFEE、FEI、Yale、LFW、AT&amp;T和AR等数据库进行了分析，提供了实验结果的讨论。</p>
<p><strong>Insight:</strong> 论文的创新点在于全面梳理了人脸识别中的关键挑战和主流方法，为研究者提供了系统的技术概览和数据库评估参考，有助于理解该领域的进展和未来方向。</p>
<p><strong>Abstract:</strong> As multimedia content is quickly growing, the field of facial recognition has become one of the major research fields, particularly in the recent years. The most problematic area to researchers in image processing and computer vision is the human face which is a complex object with myriads of distinctive features that can be used to identify the face. The survey of this survey is particularly focused on most challenging facial characteristics, including differences in the light, ageing, variation in poses, partial occlusion, and facial expression and presents methodological solutions. The factors, therefore, are inevitable in the creation of effective facial recognition mechanisms used on facial images. This paper reviews the most sophisticated methods of facial detection which are Hidden Markov Models, Principal Component Analysis (PCA), Elastic Cluster Plot Matching, Support Vector Machine (SVM), Gabor Waves, Artificial Neural Networks (ANN), Eigenfaces, Independent Component Analysis (ICA), and 3D Morphable Model. Alongside the works mentioned above, we have also analyzed the images of a number of facial databases, namely JAFEE, FEI, Yale, LFW, AT&amp;T (then called ORL), and AR (created by Martinez and Benavente), to analyze the results. However, this survey is aimed at giving a thorough literature review of face recognition, and its applications, and some experimental results are provided at the end after a detailed discussion.</p>
  </div>
</details>

<hr>
<h3 id="82-Perception-Test-2025-Challenge-Summary-and-a-Unified-VQA-Extension-cs-CVPDF"><a href="#82-Perception-Test-2025-Challenge-Summary-and-a-Unified-VQA-Extension-cs-CVPDF" class="headerlink" title="[82] Perception Test 2025: Challenge Summary and a Unified VQA Extension cs.CVPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06287">Perception Test 2025: Challenge Summary and a Unified VQA Extension</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06287" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joseph Heyward, Nikhil Pathasarathy, Tyler Zhu, Aravindh Mahendran, João Carreira</span></p>
<p><strong>TL;DR:</strong> 本文总结了ICCV 2025 Perception Test挑战赛的成果，该挑战赛旨在评估最先进的视频模型和多模态感知进展，并引入了任务统一化的新框架，将传统感知任务（如目标跟踪、动作定位）重新构建为视频问答形式，以测试模型的统一处理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 挑战赛的主要动机是衡量多模态感知领域的进展，并通过强调任务统一化，为当前最先进的多模态模型提供一个更具挑战性的测试平台，以评估它们通过统一接口处理多样化感知任务的能力。</p>
<p><strong>Result:</strong> 挑战赛包含了五个统一化赛道（如统一视频问答、统一目标与点跟踪等）以及一个分析与可解释性赛道，结果突显了现有模型在通过统一接口处理多样化感知任务时面临的显著困难。</p>
<p><strong>Insight:</strong> 创新点在于将传统感知任务（如点跟踪、时序动作定位）重新构建为多项选择视频问答问题，使视频-语言模型能够原生处理，并通过统一化赛道要求参赛者使用统一方法而非针对特定任务的工程化流程，推动了模型通用性的评估。</p>
<p><strong>Abstract:</strong> The Third Perception Test challenge was organised as a full-day workshop alongside the IEEE&#x2F;CVF International Conference on Computer Vision (ICCV) 2025. Its primary goal is to benchmark state-of-the-art video models and measure the progress in multimodal perception. This year, the workshop featured 2 guest tracks as well: KiVA (an image understanding challenge) and Physic-IQ (a video generation challenge). In this report, we summarise the results from the main Perception Test challenge, detailing both the existing tasks as well as novel additions to the benchmark. In this iteration, we placed an emphasis on task unification, as this poses a more challenging test for current SOTA multimodal models. The challenge included five consolidated tracks: unified video QA, unified object and point tracking, unified action and sound localisation, grounded video QA, and hour-long video QA, alongside an analysis and interpretability track that is still open for submissions. Notably, the unified video QA track introduced a novel subset that reformulates traditional perception tasks (such as point tracking and temporal action localisation) as multiple-choice video QA questions that video-language models can natively tackle. The unified object and point tracking merged the original object tracking and point tracking tasks, whereas the unified action and sound localisation merged the original temporal action localisation and temporal sound localisation tracks. Accordingly, we required competitors to use unified approaches rather than engineered pipelines with task-specific models. By proposing such a unified challenge, Perception Test 2025 highlights the significant difficulties existing models face when tackling diverse perception tasks through unified interfaces.</p>
  </div>
</details>

<hr>
<h3 id="83-VideoWeave-A-Data-Centric-Approach-for-Efficient-Video-Understanding-cs-CV-cs-AIPDF"><a href="#83-VideoWeave-A-Data-Centric-Approach-for-Efficient-Video-Understanding-cs-CV-cs-AIPDF" class="headerlink" title="[83] VideoWeave: A Data-Centric Approach for Efficient Video Understanding cs.CV | cs.AIPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06309">VideoWeave: A Data-Centric Approach for Efficient Video Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06309" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zane Durante, Silky Singh, Arpandeep Khatua, Shobhit Agarwal, Reuben Tan</span></p>
<p><strong>TL;DR:</strong> 本文提出VideoWeave，一种以数据为中心的方法，通过将现有数据集中带字幕的短视频片段拼接成合成的长上下文训练样本，来提高视频语言模型训练的数据效率。该方法在不改变模型架构或优化目标的情况下，通过重组视频-文本对来扩展固定计算预算下的时序多样性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 训练视频语言模型通常成本高昂，因为处理长帧序列的开销大，且带标注的长视频数据稀缺。本文旨在通过数据重组而非模型修改，来更高效地利用现有数据，缓解长视频理解任务的数据瓶颈。</p>
<p><strong>Result:</strong> 在相同的计算约束下，使用VideoWeave训练的模型在视频问答下游任务上比传统的视频微调方法获得了更高的准确率。论文系统研究了随机拼接、视觉聚类拼接和字幕增强等不同数据组合策略对性能的影响。</p>
<p><strong>Insight:</strong> 核心创新点在于提出了一种简单、可扩展的数据中心化训练范式，通过合成拼接短视频来高效模拟长视频上下文，从而提升模型性能。这为训练视频语言模型提供了一条不依赖复杂架构修改的新路径。</p>
<p><strong>Abstract:</strong> Training video-language models is often prohibitively expensive due to the high cost of processing long frame sequences and the limited availability of annotated long videos. We present VideoWeave, a simple yet effective approach to improve data efficiency by constructing synthetic long-context training samples that splice together short, captioned videos from existing datasets. Rather than modifying model architectures or optimization objectives, VideoWeave reorganizes available video-text pairs to expand temporal diversity within fixed compute. We systematically study how different data composition strategies like random versus visually clustered splicing and caption enrichment affect downstream performance on downstream video question answering. Under identical compute constraints, models trained with VideoWeave achieve higher accuracy than conventional video finetuning. Our results highlight that reorganizing training data, rather than altering architectures, may offer a simple and scalable path for training video-language models. We link our code for all experiments here.</p>
  </div>
</details>

<hr>
<h3 id="84-Object-WIPER-Training-Free-Object-and-Associated-Effect-Removal-in-Videos-cs-CVPDF"><a href="#84-Object-WIPER-Training-Free-Object-and-Associated-Effect-Removal-in-Videos-cs-CVPDF" class="headerlink" title="[84] Object-WIPER : Training-Free Object and Associated Effect Removal in Videos cs.CVPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06391">Object-WIPER : Training-Free Object and Associated Effect Removal in Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06391" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Saksham Singh Kushwaha, Sayan Nag, Yapeng Tian, Kuldeep Kulkarni</span></p>
<p><strong>TL;DR:</strong> 本文提出了Object-WIPER，一种无需训练的视频动态物体及其关联视觉效应移除框架，利用预训练的文本到视频扩散变换器（DiT），通过视觉-文本交叉注意力和视觉自注意力定位相关视觉标记，结合用户提供的物体掩码，在去噪过程中替换前景标记并保持背景一致性，以实现语义一致且时序连贯的视频修复。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决从视频中移除动态物体及其关联视觉效应（如阴影、反射）并实现高质量、时序一致修复的挑战，现有方法在关联效应处理或无需训练方面存在不足。</p>
<p><strong>Result:</strong> 在DAVIS数据集和新构建的真实世界关联效应基准（WIPER-Bench）上，Object-WIPER在提出的新评估指标上超越了基于训练和无训练的基线方法，实现了干净的移除和时序稳定的重建，无需任何重新训练。</p>
<p><strong>Insight:</strong> 创新点包括：利用预训练DiT的注意机制定位物体和效应标记的无训练框架；提出融合用户掩码与注意力生成掩码的策略；引入强调时序一致性、前景-背景连贯性及输入-输出差异性的新评估指标；构建了专门针对关联效应移除的基准数据集。</p>
<p><strong>Abstract:</strong> In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.</p>
  </div>
</details>

<hr>
<h3 id="85-Context-Matters-Peer-Aware-Student-Behavioral-Engagement-Measurement-via-VLM-Action-Parsing-and-LLM-Sequence-Classification-cs-CV-cs-AIPDF"><a href="#85-Context-Matters-Peer-Aware-Student-Behavioral-Engagement-Measurement-via-VLM-Action-Parsing-and-LLM-Sequence-Classification-cs-CV-cs-AIPDF" class="headerlink" title="[85] Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification cs.CV | cs.AIPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06394">Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06394" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmed Abdelkawy, Ahmed Elsayed, Asem Ali, Aly Farag, Thomas Tretter</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于视频的学生行为参与度测量框架，通过视觉语言模型进行少样本动作识别，结合滑动时间窗口生成动作序列，并利用大语言模型结合课堂同伴上下文对序列进行分类，以判断学生是否投入学习。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有学生参与度预测方法通常需要大量标注数据且忽略课堂同伴行为上下文，本文旨在解决数据隐私限制下的少样本学习问题，并引入课堂上下文信息以提高测量准确性。</p>
<p><strong>Result:</strong> 实验结果表明，所提方法在识别学生参与度方面有效，但未提及具体基准测试或与SOTA模型的定量比较。</p>
<p><strong>Insight:</strong> 创新点在于将VLM少样本适应与LLM序列分类结合，并融入同伴行为上下文，为隐私敏感场景下的行为分析提供了可扩展的解决方案。</p>
<p><strong>Abstract:</strong> Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers’ actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student’s 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.</p>
  </div>
</details>

<hr>
<h3 id="86-GlobalPaint-Spatiotemporal-Coherent-Video-Outpainting-with-Global-Feature-Guidance-cs-CVPDF"><a href="#86-GlobalPaint-Spatiotemporal-Coherent-Video-Outpainting-with-Global-Feature-Guidance-cs-CVPDF" class="headerlink" title="[86] GlobalPaint: Spatiotemporal Coherent Video Outpainting with Global Feature Guidance cs.CVPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06413">GlobalPaint: Spatiotemporal Coherent Video Outpainting with Global Feature Guidance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06413" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yueming Pan, Ruoyu Feng, Jianmin Bao, Chong Luo, Nanning Zheng</span></p>
<p><strong>TL;DR:</strong> 本文提出GlobalPaint，一种基于扩散模型的视频外绘框架，旨在通过合成缺失的边界内容来扩展视频范围。该方法采用分层流程，先外绘关键帧，再通过插值模型完成中间帧，以减少顺序处理中的误差累积。模型层面，通过增强时空模块和全局特征引导机制，提升时空一致性和重建质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视频外绘不仅需要单帧空间合理性，还需保证长时间跨度的时序一致性，尤其是在相机或物体运动导致外绘内容随时间可见时，现有方法难以同时满足这些要求。</p>
<p><strong>Result:</strong> 在基准数据集上的综合评估表明，相比先前方法，该方法在重建质量和运动自然度方面均有提升，达到了更优的时空一致性效果。</p>
<p><strong>Insight:</strong> 创新点包括：分层处理流程减少误差累积；增强时空模块采用3D窗口注意力强化时空交互；全局特征引导机制利用OpenCLIP特征提取跨帧观测区域的紧凑全局令牌，提升内容一致性。</p>
<p><strong>Abstract:</strong> Video outpainting extends a video beyond its original boundaries by synthesizing missing border content. Compared with image outpainting, it requires not only per-frame spatial plausibility but also long-range temporal coherence, especially when outpainted content becomes visible across time under camera or object motion. We propose GlobalPaint, a diffusion-based framework for spatiotemporal coherent video outpainting. Our approach adopts a hierarchical pipeline that first outpaints key frames and then completes intermediate frames via an interpolation model conditioned on the completed boundaries, reducing error accumulation in sequential processing. At the model level, we augment a pretrained image inpainting backbone with (i) an Enhanced Spatial-Temporal module featuring 3D windowed attention for stronger spatiotemporal interaction, and (ii) global feature guidance that distills OpenCLIP features from observed regions across all frames into compact global tokens using a dedicated extractor. Comprehensive evaluations on benchmark datasets demonstrate improved reconstruction quality and more natural motion compared to prior methods. Our demo page is <a target="_blank" rel="noopener" href="https://yuemingpan.github.io/GlobalPaint/">https://yuemingpan.github.io/GlobalPaint/</a></p>
  </div>
</details>

<hr>
<h3 id="87-How-to-Build-Robust-Scalable-Models-for-GSV-Based-Indicators-in-Neighborhood-Research-cs-CVPDF"><a href="#87-How-to-Build-Robust-Scalable-Models-for-GSV-Based-Indicators-in-Neighborhood-Research-cs-CVPDF" class="headerlink" title="[87] How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research cs.CVPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06443">How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06443" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaoya Tang, Xiaohe Yue, Heran Mane, Dapeng Li, Quynh Nguyen</span></p>
<p><strong>TL;DR:</strong> 本文通过实证分析探讨了如何为基于谷歌街景（GSV）的社区研究指标构建鲁棒且可扩展的视觉模型，重点解决了模型选择、无监督训练策略、计算约束下的训练规模以及下游性能提升等关键问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决计算机视觉模型在不同领域（如从ImageNet到GSV图像）泛化能力不确定的问题，特别是在社会健康研究中，如何为有限标注数据选择并适配基础模型，同时利用大规模无标注数据。</p>
<p><strong>Result:</strong> 研究通过全面的定量和视觉分析，比较了无监督适配前后模型的性能，但摘要未具体提及基准测试或定量结果水平（如SOTA）。</p>
<p><strong>Insight:</strong> 创新点在于为应用领域（如社区健康研究）提供了实用的模型选择和适配指南，强调通过无监督训练利用未标注数据来提升下游任务性能，可借鉴其跨域泛化的实证方法。</p>
<p><strong>Abstract:</strong> A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.   In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.</p>
  </div>
</details>

<hr>
<h3 id="88-Tone-Matters-The-Impact-of-Linguistic-Tone-on-Hallucination-in-VLMs-cs-CV-cs-AI-cs-CLPDF"><a href="#88-Tone-Matters-The-Impact-of-Linguistic-Tone-on-Hallucination-in-VLMs-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[88] Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs cs.CV | cs.AI | cs.CLPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06460">Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06460" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weihao Hong, Zhiyuan Jiang, Bingyu Shen, Xinlei Guan, Yangyi Feng</span></p>
<p><strong>TL;DR:</strong> 本文研究了提示词的语言语调如何影响视觉语言模型（VLMs）的幻觉行为。作者构建了一个名为Ghost-100的合成场景数据集，其中关键视觉细节被故意移除，并提出了一个5级提示强度框架，从中性查询到带有胁迫性的要求和严格格式约束。通过对三个开源VLM模型的评估，发现幻觉率并不随提示强度单调增加，且模型在较高强度下表现出不同程度的幻觉减少，揭示了当前安全对齐机制在检测语义敌意方面比处理结构性胁迫更有效。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究主要关注物体存在与否的幻觉，而对提示词的措辞和结构约束如何系统性诱发幻觉尚不清楚。本文旨在探究不同形式的提示压力对VLM幻觉行为的影响。</p>
<p><strong>Result:</strong> 在Ghost-100数据集上评估了MiniCPM-V 2.6-8B、Qwen2-VL-7B和Qwen3-VL-8B三个模型。结果表明，所有模型的幻觉率均不随提示强度单调增加，在较高强度级别下均出现不同程度的下降，但并非所有模型在最大胁迫下都能持续减少幻觉。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个系统性的提示强度框架来量化分析提示压力对幻觉的影响，并构建了可控的合成数据集Ghost-100。客观来看，该研究揭示了VLM安全对齐机制的一个局限性：对结构性胁迫的鲁棒性弱于对语义敌意的检测，这为未来改进模型鲁棒性提供了新方向。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: <a target="_blank" rel="noopener" href="https://github.com/bli1/tone-matters">https://github.com/bli1/tone-matters</a></p>
  </div>
</details>

<hr>
<h3 id="89-On-the-Adversarial-Robustness-of-3D-Large-Vision-Language-Models-cs-CVPDF"><a href="#89-On-the-Adversarial-Robustness-of-3D-Large-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[89] On the Adversarial Robustness of 3D Large Vision-Language Models cs.CVPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06464">On the Adversarial Robustness of 3D Large Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06464" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chao Liu, Ngai-Man Cheung</span></p>
<p><strong>TL;DR:</strong> 本文首次系统性地研究了基于点云的3D视觉语言模型（VLMs）的对抗鲁棒性。作者提出了两种互补的攻击策略：视觉攻击和字幕攻击，分别评估模型视觉-语言对齐的鲁棒性和端到端系统的鲁棒性。实验表明，3D VLMs在非定向攻击下表现出显著的脆弱性，但在旨在生成特定有害输出的定向攻击方面，比2D VLMs更具韧性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 3D视觉语言模型（如PointLLM, GPT4Point）在3D理解任务中展现出强大的推理和泛化能力，但其对抗鲁棒性尚未被探索。鉴于2D VLMs的研究表明视觉输入的整合会显著增加模型对对抗攻击的脆弱性，本文旨在探究3D视觉的引入是否同样会损害3D VLMs的鲁棒性。</p>
<p><strong>Result:</strong> 实验揭示了3D VLMs在非定向攻击下存在显著的对抗脆弱性。然而，与2D VLMs相比，3D VLMs在旨在强制生成特定有害输出的定向攻击方面表现出更强的韧性。</p>
<p><strong>Insight:</strong> 论文的主要创新点在于首次对基于点云的3D VLMs进行了系统的对抗鲁棒性研究，并提出了两种互补的攻击策略（视觉攻击和字幕攻击）来全面评估模型。从客观角度看，该研究为理解多模态模型中3D模态的独特安全特性提供了重要见解，并强调了在安全关键应用中提升3D VLMs鲁棒性的必要性。</p>
<p><strong>Abstract:</strong> 3D Vision-Language Models (VLMs), such as PointLLM and GPT4Point, have shown strong reasoning and generalization abilities in 3D understanding tasks. However, their adversarial robustness remains largely unexplored. Prior work in 2D VLMs has shown that the integration of visual inputs significantly increases vulnerability to adversarial attacks, making these models easier to manipulate into generating toxic or misleading outputs. In this paper, we investigate whether incorporating 3D vision similarly compromises the robustness of 3D VLMs. To this end, we present the first systematic study of adversarial robustness in point-based 3D VLMs. We propose two complementary attack strategies: \textit{Vision Attack}, which perturbs the visual token features produced by the 3D encoder and projector to assess the robustness of vision-language alignment; and \textit{Caption Attack}, which directly manipulates output token sequences to evaluate end-to-end system robustness. Each attack includes both untargeted and targeted variants to measure general vulnerability and susceptibility to controlled manipulation. Our experiments reveal that 3D VLMs exhibit significant adversarial vulnerabilities under untargeted attacks, while demonstrating greater resilience against targeted attacks aimed at forcing specific harmful outputs, compared to their 2D counterparts. These findings highlight the importance of improving the adversarial robustness of 3D VLMs, especially as they are deployed in safety-critical applications.</p>
  </div>
</details>

<hr>
<h3 id="90-SparseOccVLA-Bridging-Occupancy-and-Vision-Language-Models-via-Sparse-Queries-for-Unified-4D-Scene-Understanding-and-Planning-cs-CV-cs-AIPDF"><a href="#90-SparseOccVLA-Bridging-Occupancy-and-Vision-Language-Models-via-Sparse-Queries-for-Unified-4D-Scene-Understanding-and-Planning-cs-CV-cs-AIPDF" class="headerlink" title="[90] SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning cs.CV | cs.AIPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06474">SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06474" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenxu Dang, Jie Wang, Guang Li, Zhiwen Hou, Zihan You</span></p>
<p><strong>TL;DR:</strong> 本文提出SparseOccVLA模型，通过稀疏占用查询桥接视觉语言模型与语义占据，实现了自动驾驶中统一的4D场景理解、占据预测和轨迹规划。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型存在token爆炸和时空推理能力有限的问题，而语义占据表示过于稠密难以与VLM高效集成，因此需要一种方法能有效融合两种范式以提升自动驾驶系统的整体能力。</p>
<p><strong>Result:</strong> 在OmniDrive-nuScenes上CIDEr指标相对提升7%，在Occ3D-nuScenes上mIoU提升0.5，并在nuScenes基准测试中取得了开环规划任务的SOTA性能。</p>
<p><strong>Insight:</strong> 创新点在于引入轻量级稀疏占用编码器生成紧凑的稀疏查询作为视觉与语言间的单一桥梁，并设计了LLM引导的锚点扩散规划器，实现了跨模态的轨迹条件融合与解耦的锚点评分去噪机制。</p>
<p><strong>Abstract:</strong> In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.</p>
  </div>
</details>

<hr>
<h3 id="91-VVTRec-Radio-Interferometric-Reconstruction-through-Visual-and-Textual-Modality-Enrichment-cs-CV-cs-AIPDF"><a href="#91-VVTRec-Radio-Interferometric-Reconstruction-through-Visual-and-Textual-Modality-Enrichment-cs-CV-cs-AIPDF" class="headerlink" title="[91] VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment cs.CV | cs.AIPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06475">VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06475" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kai Cheng, Ruoqi Wang, Qiong Luo</span></p>
<p><strong>TL;DR:</strong> 本文提出VVTRec，一种多模态射电干涉数据重建方法，通过将稀疏可见度数据转换为图像和文本特征，增强空间和语义信息，以改善射电天文成像质量。该方法利用视觉语言模型（VLMs）实现无需额外训练的性能提升，有效减少伪影并提高图像结构完整性和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有射电干涉重建方法仅考虑稀疏可见度数据的单一模态，导致图像中残留伪影且相关性建模不足，因此需要增强可见度信息提取并提升图像域输出质量。</p>
<p><strong>Result:</strong> 实验表明，VVTRec通过利用多模态信息有效提升了成像结果，且未引入过多计算开销，在射电天文重建任务中表现出性能改进。</p>
<p><strong>Insight:</strong> 创新点在于将稀疏可见度数据同时转换为图像和文本形式，利用预训练的视觉语言模型进行知识补充，实现多模态信息融合以增强重建效果，无需额外训练即可提升性能。</p>
<p><strong>Abstract:</strong> Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be transformed into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a multimodal radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is transformed into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting multimodal information without introducing excessive computational overhead.</p>
  </div>
</details>

<hr>
<h3 id="92-3D-CoCa-v2-Contrastive-Learners-with-Test-Time-Search-for-Generalizable-Spatial-Intelligence-cs-CVPDF"><a href="#92-3D-CoCa-v2-Contrastive-Learners-with-Test-Time-Search-for-Generalizable-Spatial-Intelligence-cs-CVPDF" class="headerlink" title="[92] 3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence cs.CVPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06496">3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06496" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hao Tang, Ting Huang, Zeyu Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了3D CoCa v2，一个可泛化的3D场景描述（captioning）框架。它通过结合对比学习与描述生成来统一训练，并引入无需更新模型参数的测试时搜索（TTS）策略来提升鲁棒性，旨在解决现有3D描述方法在跨分布（OOD）泛化上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有3D场景描述方法因点云数据的稀疏不规则性，以及模型在室内外等不同环境下的弱语义对齐和有限的分布外泛化能力而面临挑战。本文旨在提升3D描述模型在多样化环境中的通用性和鲁棒性。</p>
<p><strong>Result:</strong> 在ScanRefer和Nr3D基准上，<a href="mailto:&#67;&#x49;&#x44;&#x45;&#x72;&#x40;&#x30;&#x2e;&#53;&#x49;&#111;&#x55;">CIDEr@0.5IoU</a>指标分别提升了1.50和1.61；在TOD3Cap的零样本OOD评估中，<a href="mailto:&#x43;&#73;&#68;&#69;&#x72;&#x40;&#48;&#46;&#50;&#x35;">CIDEr@0.25</a>指标提升了3.8，显示出显著的性能改进。</p>
<p><strong>Insight:</strong> 主要创新点在于将对比学习与描述生成目标联合优化，并引入测试时搜索策略进行奖励引导的候选描述选择，这增强了模型在不更新参数情况下的适应能力。其架构基于冻结的CLIP语义先验和空间感知的3D编码器，避免了外部检测器或手工提案，提升了端到端学习效率。</p>
<p><strong>Abstract:</strong> Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 <a href="mailto:&#x43;&#73;&#68;&#x45;&#114;&#x40;&#48;&#46;&#x35;&#x49;&#111;&#85;">CIDEr@0.5IoU</a> on ScanRefer and +1.61 <a href="mailto:&#67;&#73;&#x44;&#x45;&#x72;&#64;&#48;&#x2e;&#53;&#73;&#111;&#x55;">CIDEr@0.5IoU</a> on Nr3D, and +3.8 <a href="mailto:&#67;&#73;&#x44;&#x45;&#114;&#64;&#48;&#x2e;&#50;&#53;">CIDEr@0.25</a> in zero-shot OOD evaluation on TOD3Cap. Code will be released at <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/3DCoCav2">https://github.com/AIGeeksGroup/3DCoCav2</a>.</p>
  </div>
</details>

<hr>
<h3 id="93-BabyVision-Visual-Reasoning-Beyond-Language-cs-CV-cs-CLPDF"><a href="#93-BabyVision-Visual-Reasoning-Beyond-Language-cs-CV-cs-CLPDF" class="headerlink" title="[93] BabyVision: Visual Reasoning Beyond Language cs.CV | cs.CLPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06521">BabyVision: Visual Reasoning Beyond Language</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06521" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liang Chen, Weichu Xie, Yiyan Liang, Hongfeng He, Hans Zhao</span></p>
<p><strong>TL;DR:</strong> 论文提出了BabyVision基准测试，旨在评估多模态大语言模型（MLLMs）独立于语言知识的核心视觉能力。研究发现，当前最先进的MLLMs在人类幼儿（如3岁儿童）能轻松解决的基本视觉任务上表现不佳，揭示了其在基础视觉理解上的严重缺陷。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前MLLMs严重依赖语言先验来弥补其脆弱的视觉理解能力，而人类在获得语言之前就已发展出核心视觉技能。论文旨在系统性地调查MLLMs与人类在基础视觉能力上的差距。</p>
<p><strong>Result:</strong> 在BabyVision基准（包含4个关键类别、22个子类、388个项目）上，领先的MLLMs表现显著低于人类基线。例如，Gemini3-Pro-Preview得分为49.7，落后于6岁人类，远低于成人平均分94.1。这表明尽管MLLMs在知识密集型评估中表现出色，但仍缺乏基础视觉原语。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个独立于语言知识的视觉能力基准测试BabyVision，系统揭示了MLLMs在基础视觉推理上的根本性弱点。这挑战了当前MLLMs过度依赖语言先验的范式，强调了发展更本质视觉理解能力的重要性，为迈向人类水平视觉感知与推理指明了方向。同时提出的BabyVision-Gen和自动评估工具包探索了用生成模型解决视觉推理问题。</p>
<p><strong>Abstract:</strong> While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at <a target="_blank" rel="noopener" href="https://github.com/UniPat-AI/BabyVision">https://github.com/UniPat-AI/BabyVision</a> for reproduction.</p>
  </div>
</details>

<hr>
<h3 id="94-LLMTrack-Semantic-Multi-Object-Tracking-with-Multi-modal-Large-Language-Models-cs-CV-cs-AIPDF"><a href="#94-LLMTrack-Semantic-Multi-Object-Tracking-with-Multi-modal-Large-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[94] LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models cs.CV | cs.AIPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06550">LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06550" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pan Liao, Feng Yang, Di Wu, Jinwen Yu, Yuhua Zhu</span></p>
<p><strong>TL;DR:</strong> LLMTrack是一个新颖的端到端语义多目标跟踪框架，旨在弥合几何感知与认知推理之间的鸿沟。它采用仿生设计，利用Grounding DINO进行精确定位，LLaVA-OneVision多模态大模型进行深度理解，并通过时空融合模块和渐进式三阶段训练策略，使大语言模型能够理解复杂轨迹。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统多目标跟踪系统在定位和关联上精度很高，但缺乏对物体行为背后语义（如’是什么’和’为什么’）的理解，类似于’自闭的观察者’。论文旨在解决几何感知与认知推理之间的脱节问题。</p>
<p><strong>Result:</strong> 在BenSMOT基准测试上的大量实验表明，LLMTrack实现了最先进的性能，在实例描述、交互识别和视频摘要方面显著优于现有方法，同时保持了稳健的跟踪稳定性。</p>
<p><strong>Insight:</strong> 创新点包括：1) 仿生设计哲学，将强定位与深度理解解耦；2) 时空融合模块，聚合实例级交互特征和视频级上下文；3) 渐进式三阶段训练策略（视觉对齐、时序微调、通过LoRA的语义注入），高效地将大模型适配到跟踪领域。这为结合几何跟踪与语义理解提供了新思路。</p>
<p><strong>Abstract:</strong> Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.</p>
  </div>
</details>

<hr>
<h3 id="95-ArrowGEV-Grounding-Events-in-Video-via-Learning-the-Arrow-of-Time-cs-CVPDF"><a href="#95-ArrowGEV-Grounding-Events-in-Video-via-Learning-the-Arrow-of-Time-cs-CVPDF" class="headerlink" title="[95] ArrowGEV: Grounding Events in Video via Learning the Arrow of Time cs.CVPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06559">ArrowGEV: Grounding Events in Video via Learning the Arrow of Time</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06559" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fangxu Yu, Ziyao Lu, Liqiang Niu, Fandong Meng, Jie Zhou</span></p>
<p><strong>TL;DR:</strong> 本文提出ArrowGEV，一个基于强化学习的框架，通过显式建模事件的时间方向性来提升视频语言模型（VLMs）的事件定位能力。该框架受物理学中时间箭头的启发，将事件分为时间敏感型（如放下包）和时间不敏感型（如左手拿毛巾），并针对不同类型设计不同的奖励机制，以增强模型对时间结构和方向性的理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频事件定位方法主要训练模型仅关联正向视频中的事件与时间戳，这阻碍了VLMs捕捉事件固有的时间结构和方向性，从而限制了模型的鲁棒性和泛化能力。</p>
<p><strong>Result:</strong> 大量实验表明，ArrowGEV不仅提高了事件定位精度和时间方向性识别能力，还增强了通用的视频理解和推理能力。</p>
<p><strong>Insight:</strong> 创新点在于受时间箭头概念启发，将事件按时间敏感性分类，并设计相应的强化学习奖励机制来显式建模时间方向性，从而提升VLMs对视频事件的理解和定位性能。</p>
<p><strong>Abstract:</strong> Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.</p>
  </div>
</details>

<hr>
<h3 id="96-QCaption-Video-Captioning-and-Q-A-through-Fusion-of-Large-Multimodal-Models-cs-CV-cs-AIPDF"><a href="#96-QCaption-Video-Captioning-and-Q-A-through-Fusion-of-Large-Multimodal-Models-cs-CV-cs-AIPDF" class="headerlink" title="[96] QCaption: Video Captioning and Q&amp;A through Fusion of Large Multimodal Models cs.CV | cs.AIPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06566">QCaption: Video Captioning and Q&amp;A through Fusion of Large Multimodal Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06566" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiale Wang, Gee Wah Ng, Lee Onn Mak, Randall Cher, Ng Ding Hei Ryan</span></p>
<p><strong>TL;DR:</strong> 本文提出了QCaption，一种新颖的视频描述与问答（Q&amp;A）流水线，通过融合关键帧提取、用于图像-文本分析的大型多模态模型（LMM）以及用于文本分析的大型语言模型（LLM），实现了对文本、图像和视频的集成分析。该方法在提升视频描述和问答任务性能的同时，保持了完全自包含性，适合本地部署。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在通过融合多个模型来增强视频分析能力，解决现有视频描述和问答模型在集成多模态信息方面的不足，并提供一个适合本地部署的解决方案。</p>
<p><strong>Result:</strong> 实验结果表明，QCaption在视频描述和问答任务上分别实现了高达44.2%和48.9%的性能提升；论文还进行了消融研究以评估LLM在融合中的作用，并与其他现有方法进行了基准测试。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种融合关键帧提取、LMM和LLM的流水线架构，实现了多模态信息的有效集成，并证明了模型融合方法在推进视频分析方面的潜力；从客观角度看，其自包含设计对实际部署具有借鉴意义。</p>
<p><strong>Abstract:</strong> This paper introduces QCaption, a novel video captioning and Q&amp;A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&amp;A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&amp;A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.</p>
  </div>
</details>

<hr>
<h3 id="97-APEX-Learning-Adaptive-Priorities-for-Multi-Objective-Alignment-in-Vision-Language-Generation-cs-CVPDF"><a href="#97-APEX-Learning-Adaptive-Priorities-for-Multi-Objective-Alignment-in-Vision-Language-Generation-cs-CVPDF" class="headerlink" title="[97] APEX: Learning Adaptive Priorities for Multi-Objective Alignment in Vision-Language Generation cs.CVPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06574">APEX: Learning Adaptive Priorities for Multi-Objective Alignment in Vision-Language Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06574" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongliang Chen, Xinlin Zhuang, Junjie Xu, Luojian Xie, Zehui Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出APEX方法，用于解决多目标对齐在视觉-语言生成中的优化不平衡问题。通过双阶段自适应归一化和动态优先级调度，APEX在Stable Diffusion 3.5上实现了对异构奖励的稳定优化，提升了多个目标的综合性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多目标对齐中静态线性标量化方法在异构奖励下失效的问题，特别是模型容易过拟合高方差、高响应性目标（如OCR）而忽视感知目标，导致优化失衡。</p>
<p><strong>Result:</strong> 在Stable Diffusion 3.5上，APEX在四个异构目标上实现了改进的帕累托权衡，平衡提升了PickScore（+1.31）、DeQA（+0.35）和Aesthetics（+0.53），同时保持了竞争力的OCR准确性，缓解了多目标对齐的不稳定性。</p>
<p><strong>Insight:</strong> 创新点包括识别方差劫持和梯度冲突两个机制性原因，并提出双阶段自适应归一化和结合学习潜力、冲突惩罚和进度需求的P^3自适应优先级调度方法，动态平衡多目标优化。</p>
<p><strong>Abstract:</strong> Multi-objective alignment for text-to-image generation is commonly implemented via static linear scalarization, but fixed weights often fail under heterogeneous rewards, leading to optimization imbalance where models overfit high-variance, high-responsiveness objectives (e.g., OCR) while under-optimizing perceptual goals. We identify two mechanistic causes: variance hijacking, where reward dispersion induces implicit reweighting that dominates the normalized training signal, and gradient conflicts, where competing objectives produce opposing update directions and trigger seesaw-like oscillations. We propose APEX (Adaptive Priority-based Efficient X-objective Alignment), which stabilizes heterogeneous rewards with Dual-Stage Adaptive Normalization and dynamically schedules objectives via P^3 Adaptive Priorities that combine learning potential, conflict penalty, and progress need. On Stable Diffusion 3.5, APEX achieves improved Pareto trade-offs across four heterogeneous objectives, with balanced gains of +1.31 PickScore, +0.35 DeQA, and +0.53 Aesthetics while maintaining competitive OCR accuracy, mitigating the instability of multi-objective alignment.</p>
  </div>
</details>

<hr>
<h3 id="98-Sissi-Zero-shot-Style-guided-Image-Synthesis-via-Semantic-style-Integration-cs-CVPDF"><a href="#98-Sissi-Zero-shot-Style-guided-Image-Synthesis-via-Semantic-style-Integration-cs-CVPDF" class="headerlink" title="[98] Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration cs.CVPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06605">Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06605" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yingying Deng, Xiangyu He, Fan Tang, Weiming Dong, Xucheng Yin</span></p>
<p><strong>TL;DR:</strong> 本文提出Sissi框架，一种无需训练的零样本风格引导图像合成方法，通过将风格引导合成重构为上下文学习任务，利用预训练的ReFlow修复模型，结合动态语义-风格集成机制，实现语义内容与参考风格图像的高质量融合。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文本引导图像生成方法在结合视觉示例进行精确风格化时，常依赖任务特定重训练或昂贵反演过程，导致内容完整性受损、风格保真度下降，以及语义提示遵循与风格对齐之间的权衡不佳。</p>
<p><strong>Result:</strong> 实验表明，该方法在实现高保真风格化的同时，取得了优越的语义-风格平衡和视觉质量，为复杂且易产生伪影的现有方法提供了一个简单而强大的替代方案。</p>
<p><strong>Insight:</strong> 创新点在于将风格引导合成重构为上下文学习任务，并提出了动态语义-风格集成机制，通过重新加权文本语义和风格视觉标记之间的注意力，有效解决了引导冲突并增强了输出连贯性，无需额外训练即可实现零样本风格迁移。</p>
<p><strong>Abstract:</strong> Text-guided image generation has advanced rapidly with large-scale diffusion models, yet achieving precise stylization with visual exemplars remains difficult. Existing approaches often depend on task-specific retraining or expensive inversion procedures, which can compromise content integrity, reduce style fidelity, and lead to an unsatisfactory trade-off between semantic prompt adherence and style alignment. In this work, we introduce a training-free framework that reformulates style-guided synthesis as an in-context learning task. Guided by textual semantic prompts, our method concatenates a reference style image with a masked target image, leveraging a pretrained ReFlow-based inpainting model to seamlessly integrate semantic content with the desired style through multimodal attention fusion. We further analyze the imbalance and noise sensitivity inherent in multimodal attention fusion and propose a Dynamic Semantic-Style Integration (DSSI) mechanism that reweights attention between textual semantic and style visual tokens, effectively resolving guidance conflicts and enhancing output coherence. Experiments show that our approach achieves high-fidelity stylization with superior semantic-style balance and visual quality, offering a simple yet powerful alternative to complex, artifact-prone prior methods.</p>
  </div>
</details>

<hr>
<h3 id="99-eSkiTB-A-Synthetic-Event-based-Dataset-for-Tracking-Skiers-cs-CVPDF"><a href="#99-eSkiTB-A-Synthetic-Event-based-Dataset-for-Tracking-Skiers-cs-CVPDF" class="headerlink" title="[99] eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers cs.CVPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06647">eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06647" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Krishna Vinod, Joseph Raj Vishal, Kaustav Chanda, Prithvi Jai Ramesh, Yezhou Yang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了eSkiTB，一个用于滑雪者追踪的合成事件相机数据集，通过将现有的RGB滑雪数据集SkiTB直接转换为事件流而生成。论文在eSkiTB上评估了基于脉冲神经网络的追踪器SDTrack，并与基于RGB的追踪器STARK进行对比，发现事件相机数据在存在静态叠加物和杂波的广播视频中更具鲁棒性，追踪性能显著优于RGB模态。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在RGB广播视频中追踪滑雪者面临运动模糊、静态叠加物和背景杂波等挑战，而事件相机凭借其异步对比度感知特性，天然对这些干扰具有鲁棒性。然而，此前缺乏一个用于冬季运动追踪的、受控的事件相机基准数据集。</p>
<p><strong>Result:</strong> 在静态叠加物主导的场景中，基于事件的追踪（SDTrack）对广播杂波表现出显著的鲁棒性，达到了0.685的IoU，比基于RGB的追踪（STARK）高出20.0个百分点。在整个数据集上，SDTrack的平均IoU为0.711。</p>
<p><strong>Insight:</strong> 论文的主要创新点在于创建了首个用于冬季运动追踪的合成事件相机基准数据集eSkiTB，它支持RGB与事件模态在信息对等条件下的公平比较。客观分析表明，该工作验证了事件相机的时间对比度信息是追踪视觉拥挤环境中高速弹道运动的可靠线索，凸显了事件相机在该应用领域的潜力。</p>
<p><strong>Abstract:</strong> Tracking skiers in RGB broadcast footage is challenging due to motion blur, static overlays, and clutter that obscure the fast-moving athlete. Event cameras, with their asynchronous contrast sensing, offer natural robustness to such artifacts, yet a controlled benchmark for winter-sport tracking has been missing. We introduce event SkiTB (eSkiTB), a synthetic event-based ski tracking dataset generated from SkiTB using direct video-to-event conversion without neural interpolation, enabling an iso-informational comparison between RGB and event modalities. Benchmarking SDTrack (spiking transformer) against STARK (RGB transformer), we find that event-based tracking is substantially resilient to broadcast clutter in scenes dominated by static overlays, achieving 0.685 IoU, outperforming RGB by +20.0 points. Across the dataset, SDTrack attains a mean IoU of 0.711, demonstrating that temporal contrast is a reliable cue for tracking ballistic motion in visually congested environments. eSkiTB establishes the first controlled setting for event-based tracking in winter sports and highlights the promise of event cameras for ski tracking. The dataset and code will be released at <a target="_blank" rel="noopener" href="https://github.com/eventbasedvision/eSkiTB">https://github.com/eventbasedvision/eSkiTB</a>.</p>
  </div>
</details>

<hr>
<h3 id="100-Quantification-and-Classification-of-Carbon-Nanotubes-in-Electron-Micrographs-using-Vision-Foundation-Models-cs-CVPDF"><a href="#100-Quantification-and-Classification-of-Carbon-Nanotubes-in-Electron-Micrographs-using-Vision-Foundation-Models-cs-CVPDF" class="headerlink" title="[100] Quantification and Classification of Carbon Nanotubes in Electron Micrographs using Vision Foundation Models cs.CVPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06673">Quantification and Classification of Carbon Nanotubes in Electron Micrographs using Vision Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06673" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sanjay Pradeep, Chen Wang, Matthew M. Dahm, Jeff D. Eldredge, Candace S. J. Tsai</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于视觉基础模型的统一框架，用于自动化电子显微镜图像中碳纳米管的定量和分类。该框架结合了Segment Anything Model（SAM）进行交互式高精度分割，并利用DINOv2视觉变换器从分割掩码中提取特征以进行分类，在少量训练数据下实现了高准确率，并能处理单视野中的混合样本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前电子显微镜图像中碳纳米管形态的表征依赖缓慢、主观的手动分割，这限制了暴露评估和毒理学研究的高通量、可重复分析。</p>
<p><strong>Result:</strong> 在包含1,800张TEM图像的数据集上，该框架在区分四种不同碳纳米管形态的分类任务中达到了95.5%的准确率，显著超越了当前基线方法，且仅使用了少量训练数据。</p>
<p><strong>Insight:</strong> 创新点在于将零样本分割模型（SAM）与自监督特征学习模型（DINOv2）相结合，通过空间约束提取粒子区域特征并抑制背景噪声，实现了实例级处理以解析混合样本，从而将劳动密集型瓶颈转化为可扩展的数据驱动流程。</p>
<p><strong>Abstract:</strong> Accurate characterization of carbon nanotube morphologies in electron microscopy images is vital for exposure assessment and toxicological studies, yet current workflows rely on slow, subjective manual segmentation. This work presents a unified framework leveraging vision foundation models to automate the quantification and classification of CNTs in electron microscopy images. First, we introduce an interactive quantification tool built on the Segment Anything Model (SAM) that segments particles with near-perfect accuracy using minimal user input. Second, we propose a novel classification pipeline that utilizes these segmentation masks to spatially constrain a DINOv2 vision transformer, extracting features exclusively from particle regions while suppressing background noise. Evaluated on a dataset of 1,800 TEM images, this architecture achieves 95.5% accuracy in distinguishing between four different CNT morphologies, significantly outperforming the current baseline despite using a fraction of the training data. Crucially, this instance-level processing allows the framework to resolve mixed samples, correctly classifying distinct particle types co-existing within a single field of view. These results demonstrate that integrating zero-shot segmentation with self-supervised feature learning enables high-throughput, reproducible nanomaterial analysis, transforming a labor-intensive bottleneck into a scalable, data-driven process.</p>
  </div>
</details>

<hr>
<h3 id="101-Benchmarking-Egocentric-Clinical-Intent-Understanding-Capability-for-Medical-Multimodal-Large-Language-Models-cs-CV-cs-AI-cs-CLPDF"><a href="#101-Benchmarking-Egocentric-Clinical-Intent-Understanding-Capability-for-Medical-Multimodal-Large-Language-Models-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[101] Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models cs.CV | cs.AI | cs.CLPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06750">Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06750" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaonan Liu, Guo Yu, Xiaoling Luo, Shiyi Zheng, Wenting Chen</span></p>
<p><strong>TL;DR:</strong> 该论文针对医学多模态大语言模型在真实世界部署中所需的以自我为中心的临床意图理解能力，提出了首个基于临床医生注视作为认知指针的基准测试MedGaze-Bench。该基准旨在评估模型在手术、急诊模拟和诊断解读等场景中的意图理解能力，并解决了视觉同质性、时间因果依赖和安全协议遵循三大挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准无法有效评估医学多模态大语言模型在真实临床环境中关键的以自我为中心的意图理解能力，因此需要建立一个专门的基准来填补这一空白。</p>
<p><strong>Result:</strong> 实验表明，当前的多模态大语言模型在自我中心意图理解方面表现不佳，主要原因是过度依赖全局特征，导致产生虚假观察和盲目接受无效指令。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个三维临床意图框架，从空间、时间和标准三个维度评估意图理解，并引入了陷阱问答机制来压力测试模型的临床可靠性，这为评估医学AI的实用性和安全性提供了新思路。</p>
<p><strong>Abstract:</strong> Medical Multimodal Large Language Models (Med-MLLMs) require egocentric clinical intent understanding for real-world deployment, yet existing benchmarks fail to evaluate this critical capability. To address these challenges, we introduce MedGaze-Bench, the first benchmark leveraging clinician gaze as a Cognitive Cursor to assess intent understanding across surgery, emergency simulation, and diagnostic interpretation. Our benchmark addresses three fundamental challenges: visual homogeneity of anatomical structures, strict temporal-causal dependencies in clinical workflows, and implicit adherence to safety protocols. We propose a Three-Dimensional Clinical Intent Framework evaluating: (1) Spatial Intent: discriminating precise targets amid visual noise, (2) Temporal Intent: inferring causal rationale through retrospective and prospective reasoning, and (3) Standard Intent: verifying protocol compliance through safety checks. Beyond accuracy metrics, we introduce Trap QA mechanisms to stress-test clinical reliability by penalizing hallucinations and cognitive sycophancy. Experiments reveal current MLLMs struggle with egocentric intent due to over-reliance on global features, leading to fabricated observations and uncritical acceptance of invalid instructions.</p>
  </div>
</details>

<hr>
<h3 id="102-CliffordNet-All-You-Need-is-Geometric-Algebra-cs-CV-cs-LGPDF"><a href="#102-CliffordNet-All-You-Need-is-Geometric-Algebra-cs-CV-cs-LGPDF" class="headerlink" title="[102] CliffordNet: All You Need is Geometric Algebra cs.CV | cs.LGPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06793">CliffordNet: All You Need is Geometric Algebra</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06793" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhongping Ji</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为CliffordNet（或Clifford代数网络，CAN）的视觉骨干网络，它完全基于几何代数构建，挑战了当前CNN和Transformer等架构依赖堆叠启发式模块（如空间混合器和通道混合器）的范式。该网络通过Clifford几何积这一统一交互机制，同时捕获特征相干性和结构变化，并因其表示密度高而无需传统的前馈网络。实验表明，其轻量版本在CIFAR-100上以极少的参数达到了与大型模型相当甚至更优的精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前计算机视觉架构（如CNN和Transformer）主要依赖于堆叠启发式模块（空间混合器后接通道混合器），本文旨在回归数学第一性原理，挑战这一范式，探索一种基于几何代数的统一、代数完备的视觉骨干网络。</p>
<p><strong>Result:</strong> 在CIFAR-100基准测试上，CliffordNet的Nano变体仅用1.4M参数达到了76.41%的准确率，以8倍更少的参数有效匹配了参数为11.2M的ResNet-18；Base变体则以78.05%的准确率为微小模型设立了新的SOTA。模型具有严格的线性复杂度O(N)。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于完全基于几何代数（特别是Clifford几何积）构建统一的视觉骨干网络，该操作同时捕获内积（特征相干性）和外积（结构变化），实现了代数完备的局部交互，并因其表示密度高而无需传统前馈网络，暗示了“几何即所需”的新方向。从客观角度看，将几何代数系统性地引入视觉骨干网络设计，并展示其在高参数效率下的强大性能，是一个有潜力的架构创新。</p>
<p><strong>Abstract:</strong> Modern computer vision architectures, from CNNs to Transformers, predominantly rely on the stacking of heuristic modules: spatial mixers (Attention&#x2F;Conv) followed by channel mixers (FFNs). In this work, we challenge this paradigm by returning to mathematical first principles. We propose the \textbf{Clifford Algebra Network (CAN)}, also referred to as CliffordNet, a vision backbone grounded purely in Geometric Algebra. Instead of engineering separate modules for mixing and memory, we derive a unified interaction mechanism based on the \textbf{Clifford Geometric Product} ($uv &#x3D; u \cdot v + u \wedge v$). This operation ensures algebraic completeness regarding the Geometric Product by simultaneously capturing feature coherence (via the generalized inner product) and structural variation (via the exterior wedge product).   Implemented via an efficient sparse rolling mechanism with \textbf{strict linear complexity $\mathcal{O}(N)$}, our model reveals a surprising emergent property: the geometric interaction is so representationally dense that standard Feed-Forward Networks (FFNs) become redundant. Empirically, CliffordNet establishes a new Pareto frontier: our \textbf{Nano} variant achieves \textbf{76.41%} accuracy on CIFAR-100 with only \textbf{1.4M} parameters, effectively matching the heavy-weight ResNet-18 (11.2M) with \textbf{$8\times$ fewer parameters}, while our \textbf{Base} variant sets a new SOTA for tiny models at \textbf{78.05%}. Our results suggest that global understanding can emerge solely from rigorous, algebraically complete local interactions, potentially signaling a shift where \textit{geometry is all you need}. Code is available at <a target="_blank" rel="noopener" href="https://github.com/ParaMind2025/CAN">https://github.com/ParaMind2025/CAN</a>.</p>
  </div>
</details>

<hr>
<h3 id="103-SpatialNav-Leveraging-Spatial-Scene-Graphs-for-Zero-Shot-Vision-and-Language-Navigation-cs-CV-cs-AI-cs-ROPDF"><a href="#103-SpatialNav-Leveraging-Spatial-Scene-Graphs-for-Zero-Shot-Vision-and-Language-Navigation-cs-CV-cs-AI-cs-ROPDF" class="headerlink" title="[103] SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation cs.CV | cs.AI | cs.ROPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06806">SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06806" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiwen Zhang, Zejun Li, Siyuan Wang, Xiangyu Shi, Zhongyu Wei</span></p>
<p><strong>TL;DR:</strong> 本文提出SpatialNav，一种零样本视觉语言导航（VLN）智能体，通过构建空间场景图（SSG）显式捕获探索环境的全局空间结构和语义，结合以智能体为中心的空间地图、罗盘对齐的视觉表示和远程物体定位策略，以提升导航效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决零样本VLN智能体因缺乏大规模训练数据而主要依赖局部观测，导致探索效率低下和性能差距显著的问题。</p>
<p><strong>Result:</strong> 在离散和连续环境中的综合实验表明，SpatialNav显著优于现有零样本智能体，并明显缩小了与最先进学习方法的差距。</p>
<p><strong>Insight:</strong> 创新点在于引入空间场景图（SSG）来显式建模全局空间知识，结合多种空间感知组件，强调了全局空间表示对于可泛化导航的重要性。</p>
<p><strong>Abstract:</strong> Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.</p>
  </div>
</details>

<hr>
<h3 id="104-SARA-Scene-Aware-Reconstruction-Accelerator-cs-CVPDF"><a href="#104-SARA-Scene-Aware-Reconstruction-Accelerator-cs-CVPDF" class="headerlink" title="[104] SARA: Scene-Aware Reconstruction Accelerator cs.CVPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06831">SARA: Scene-Aware Reconstruction Accelerator</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06831" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jee Won Lee, Hansol Lim, Minhyeok Im, Dohyeon Lee, Jongseong Brad Choi</span></p>
<p><strong>TL;DR:</strong> SARA是一种用于运动恢复结构（SfM）的几何驱动图像对选择模块，它通过评估重建信息量（重叠度和视差的乘积）来优先选择图像对，而非仅依赖视觉相似性。该方法通过轻量级预匹配阶段估计几何线索，并构建信息加权生成树，以显著减少匹配对数量，从而在保持重建质量的同时大幅提升速度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统SfM流程仅基于视觉相似性选择图像对进行匹配，效率低下且可能忽略几何信息，SARA旨在通过引入几何优先的配对选择策略，在昂贵的匹配步骤之前有效评估重建信息量，以解决匹配复杂度高和重建精度不足的问题。</p>
<p><strong>Result:</strong> 在多个现代学习型检测器上，与穷举匹配相比，SARA将旋转误差降低了46.5±5.5%，平移误差降低了12.5±6.5%，同时通过减少98%的匹配对（从30,848对降至580对）实现了高达50倍的加速，在3D高斯溅射和SVRaster等重建方法中，重建指标保持在基线±3%以内。</p>
<p><strong>Insight:</strong> 创新点在于提出几何驱动的配对选择模块，通过重建信息量评分和构建信息加权生成树来优化SfM流程，实现了从二次到准线性复杂度的降低，同时保持重建精度，为大规模场景重建提供了高效解决方案。</p>
<p><strong>Abstract:</strong> We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM). Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching. A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement. Compared to exhaustive matching, SARA reduces rotation errors by 46.5+-5.5% and translation errors by 12.5+-6.5% across modern learned detectors, while achieving at most 50x speedup through 98% pair reduction (from 30,848 to 580 pairs). This reduces matching complexity from quadratic to quasi-linear, maintaining within +-3% of baseline reconstruction metrics for 3D Gaussian Splatting and SVRaster.</p>
  </div>
</details>

<hr>
<h3 id="105-Speak-While-Watching-Unleashing-TRUE-Real-Time-Video-Understanding-Capability-of-Multimodal-Large-Language-Models-cs-CV-cs-CLPDF"><a href="#105-Speak-While-Watching-Unleashing-TRUE-Real-Time-Video-Understanding-Capability-of-Multimodal-Large-Language-Models-cs-CV-cs-CLPDF" class="headerlink" title="[105] Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models cs.CV | cs.CLPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06843">Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06843" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junyan Lin, Junlong Tong, Hao Wu, Jialiang Zhang, Jinming Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种并行流式框架，通过三种设计（重叠、组解耦和间隔隔离）来放松标准位置编码方案中的全局位置连续性约束，使多模态大语言模型能够同时进行感知和生成，从而实现实时视频理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MLLMs大多限于离线推理，需要完整输入才能生成输出；而现有流式方法虽降低延迟，但仍强制感知-生成顺序循环，限制了实时交互。本文旨在解决将MLLMs扩展到实时视频理解时的根本瓶颈：标准位置编码方案施加的全局位置连续性约束，该约束紧密耦合感知和生成，阻碍了有效的输入输出并行。</p>
<p><strong>Result:</strong> 大量实验表明，组解耦设计在效率和性能之间达到最佳平衡，在保持高流畅性和准确性的同时显著降低延迟；所提框架在平衡感知-生成工作负载下可实现高达2倍的加速。</p>
<p><strong>Insight:</strong> 创新点在于通过放松位置连续性约束实现感知与生成的并行化，从而提升实时性；客观来看，该方法为构建“边看边说”的实时系统提供了原则性途径，通过解耦位置编码设计来优化流式处理效率。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: <a target="_blank" rel="noopener" href="https://github.com/EIT-NLP/Speak-While-Watching">https://github.com/EIT-NLP/Speak-While-Watching</a>.</p>
  </div>
</details>

<hr>
<h3 id="106-MedGround-Bridging-the-Evidence-Gap-in-Medical-Vision-Language-Models-with-Verified-Grounding-Data-cs-CV-cs-AIPDF"><a href="#106-MedGround-Bridging-the-Evidence-Gap-in-Medical-Vision-Language-Models-with-Verified-Grounding-Data-cs-CV-cs-AIPDF" class="headerlink" title="[106] MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data cs.CV | cs.AIPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06847">MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06847" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengmeng Zhang, Xiaoping Wu, Hao Luo, Fan Wang, Yisheng Lv</span></p>
<p><strong>TL;DR:</strong> 本文提出MedGround，一种自动化流程，用于将医学分割资源转化为高质量的医学视觉-语言模型（VLM）的指代定位数据，以解决医学VLM在视觉指代定位方面的局限性。通过利用专家标注的掩码作为空间锚点，MedGround精确提取定位目标、形状和空间线索，并引导VLM合成自然且具有临床依据的查询。为确保数据质量，采用多阶段验证系统进行过滤。最终构建了MedGround-35K数据集，实验表明使用该数据集训练的VLM在指代定位性能、多对象语义消歧和泛化能力上均有显著提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学视觉-语言模型（VLMs）在生成临床叙述时经常难以将其陈述视觉地指代到图像中的具体区域，这主要是由于缺乏高质量、大规模的临床指代定位数据对。</p>
<p><strong>Result:</strong> 在广泛的实验中，使用MedGround-35K数据集训练的VLMs在指代定位性能上持续提升，增强了多对象语义消歧能力，并在未见过的定位设置中表现出强大的泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种可扩展的、数据驱动的方法（MedGround），通过自动化流程将分割资源转化为验证过的指代定位数据，并引入多阶段验证系统确保数据严谨性，从而将医学语言锚定到可验证的视觉证据上。从客观角度看，该方法为解决医学VLM中视觉指代不足的问题提供了一种高效且可靠的解决方案。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.</p>
  </div>
</details>

<hr>
<h3 id="107-MVGGT-Multimodal-Visual-Geometry-Grounded-Transformer-for-Multiview-3D-Referring-Expression-Segmentation-cs-CVPDF"><a href="#107-MVGGT-Multimodal-Visual-Geometry-Grounded-Transformer-for-Multiview-3D-Referring-Expression-Segmentation-cs-CVPDF" class="headerlink" title="[107] MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation cs.CVPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06874">MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06874" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changli Wu, Haodong Wang, Jiayi Ji, Yutian Yao, Chunsai Du</span></p>
<p><strong>TL;DR:</strong> 本文提出MVGGT（多模态视觉几何基础Transformer）模型，用于解决多视角3D指代表达式分割（MV-3DRES）任务，即直接从稀疏多视角RGB图像中恢复场景结构并分割语言描述的目标物体。该模型通过双分支设计将语言信息融入稀疏视角几何推理，并引入Per-view No-target Suppression Optimization（PVSO）解决训练中的前景梯度稀释问题，同时构建了MVRefer基准数据集进行标准化评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有3D指代表达式分割方法依赖高质量稠密点云，而实际应用（如机器人、手机）仅能获取稀疏RGB视角且需满足低延迟要求；传统两阶段方法（先重建点云再分割）存在几何质量低、分割粗糙、速度慢的问题，因此需要高效端到端的稀疏多视角解决方案。</p>
<p><strong>Result:</strong> 在构建的MVRefer基准上，MVGGT建立了首个强基线，实现了高精度和快速推理，性能优于现有替代方法。</p>
<p><strong>Insight:</strong> 创新点包括：1) 提出端到端双分支Transformer框架，直接融合语言与稀疏多视角几何信息；2) 针对稀疏3D信号导致的弱监督问题，提出PVSO优化方法以增强梯度平衡；3) 构建标准化MV-3DRES基准MVRefer，推动任务一致评估。</p>
<p><strong>Abstract:</strong> Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at <a target="_blank" rel="noopener" href="https://mvggt.github.io/">https://mvggt.github.io</a>.</p>
  </div>
</details>

<hr>
<h3 id="108-CLIMP-Contrastive-Language-Image-Mamba-Pretraining-cs-CVPDF"><a href="#108-CLIMP-Contrastive-Language-Image-Mamba-Pretraining-cs-CVPDF" class="headerlink" title="[108] CLIMP: Contrastive Language-Image Mamba Pretraining cs.CVPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06891">CLIMP: Contrastive Language-Image Mamba Pretraining</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06891" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nimrod Shabtay, Itamar Zimerman, Eli Schwartz, Raja Giryes</span></p>
<p><strong>TL;DR:</strong> CLIMP是首个完全基于Mamba架构的对比视觉语言模型，用Mamba替换了CLIP中的视觉和文本编码器，旨在解决Transformer注意力机制易受虚假相关影响及计算复杂度随分辨率二次增长的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决CLIP依赖Vision Transformer时注意力机制易受虚假相关影响、计算复杂度随分辨率二次增长以及固定上下文限制的问题。</p>
<p><strong>Result:</strong> 在ImageNet-O上超越OpenAI CLIP-ViT-B 7.5%；在16倍训练分辨率下检索准确率提升6.6%，同时内存使用减少5倍、FLOPs减少1.8倍；支持可变输入分辨率且无需位置编码插值或专门训练。</p>
<p><strong>Insight:</strong> 利用Mamba的序列建模能力捕捉视觉空间归纳偏置，减少对虚假相关的依赖，并自然支持可变分辨率；自回归文本编码器克服了CLIP的固定上下文限制，实现了密集描述检索。</p>
<p><strong>Abstract:</strong> Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI’s CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP’s fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.</p>
  </div>
</details>

<hr>
<h3 id="109-Measuring-Social-Bias-in-Vision-Language-Models-with-Face-Only-Counterfactuals-from-Real-Photos-cs-CV-cs-AI-cs-CLPDF"><a href="#109-Measuring-Social-Bias-in-Vision-Language-Models-with-Face-Only-Counterfactuals-from-Real-Photos-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[109] Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos cs.CV | cs.AI | cs.CLPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06931">Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06931" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haodong Chen, Qiang Huang, Jiaqi Zhao, Qiuping Jiang, Xiaojun Chang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为’仅面部反事实评估范式’的新方法，用于测量视觉语言模型中的社会偏见。该方法通过仅编辑真实照片中的面部属性（如种族和性别）来生成反事实图像，从而隔离人口统计效应，同时保持图像的真实性。基于此范式，作者构建了FOCUS数据集和REFLECT基准，并在五个最先进的视觉语言模型上进行了实验，发现即使在严格视觉控制下，人口统计差异仍然存在，且因任务设计而异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉语言模型越来越多地应用于社会重要场景，引发了由人口统计线索驱动的社会偏见担忧。测量这种偏见的核心挑战在于视觉混淆下的归因问题，即真实图像中种族和性别与背景、服装等因素交织，模糊了归因。</p>
<p><strong>Result:</strong> 在五个最先进的视觉语言模型上的实验表明，即使在严格视觉控制下，人口统计差异仍然持续存在，并且在不同任务表述（如二选一强制选择、多项选择社会经济推断和数值薪资推荐）中差异显著。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个仅面部反事实评估范式，通过编辑真实照片中的面部属性来隔离偏见，避免了视觉混淆。这强调了受控反事实审计的必要性，并突出了任务设计在评估多模态模型社会偏见中的关键作用。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) are increasingly deployed in socially consequential settings, raising concerns about social bias driven by demographic cues. A central challenge in measuring such social bias is attribution under visual confounding: real-world images entangle race and gender with correlated factors such as background and clothing, obscuring attribution. We propose a \textbf{face-only counterfactual evaluation paradigm} that isolates demographic effects while preserving real-image realism. Starting from real photographs, we generate counterfactual variants by editing only facial attributes related to race and gender, keeping all other visual factors fixed. Based on this paradigm, we construct \textbf{FOCUS}, a dataset of 480 scene-matched counterfactual images across six occupations and ten demographic groups, and propose \textbf{REFLECT}, a benchmark comprising three decision-oriented tasks: two-alternative forced choice, multiple-choice socioeconomic inference, and numeric salary recommendation. Experiments on five state-of-the-art VLMs reveal that demographic disparities persist under strict visual control and vary substantially across task formulations. These findings underscore the necessity of controlled, counterfactual audits and highlight task design as a critical factor in evaluating social bias in multimodal models.</p>
  </div>
</details>

<hr>
<h3 id="110-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning-cs-CV-cs-AIPDF"><a href="#110-Watching-Reasoning-and-Searching-A-Video-Deep-Research-Benchmark-on-Open-Web-for-Agentic-Video-Reasoning-cs-CV-cs-AIPDF" class="headerlink" title="[110] Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning cs.CV | cs.AIPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06943">Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06943" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chengwen Liu, Xiaomin Yu, Zhuoyue Chang, Zhe Huang, Shuo Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了首个视频深度研究基准VideoDR，专注于视频条件下的开放域视频问答任务，要求模型执行跨帧视觉锚点提取、交互式网络检索以及基于视频-网络证据的多跳推理。通过人工标注构建了涵盖六个语义领域的高质量数据集，并评估了多种闭源和开源多模态大语言模型在Workflow与Agentic范式下的表现。研究发现Agentic范式并非总是优于Workflow，其优势取决于模型在长检索链中保持初始视频锚点的能力，而目标漂移和长程一致性是核心瓶颈。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现实世界视频问答场景中，视频仅提供局部视觉线索，而可验证答案分布在开放网络上的问题，需要模型联合进行跨帧线索提取、迭代检索和多跳推理验证。</p>
<p><strong>Result:</strong> 在VideoDR基准上评估了多种多模态大语言模型，结果显示Agentic范式并不总是优于Workflow范式，其性能提升依赖于模型在长检索链中维持初始视频锚点的能力；目标漂移和长程一致性被识别为核心挑战。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个视频深度研究基准VideoDR，系统研究了开放网络环境下视频智能体的能力；客观分析揭示了Agentic范式的有效性取决于模型的长程一致性保持能力，为下一代视频深度研究智能体的设计指明了关键方向。</p>
<p><strong>Abstract:</strong> In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model’s ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.</p>
  </div>
</details>

<hr>
<h3 id="111-SketchJudge-A-Diagnostic-Benchmark-for-Grading-Hand-drawn-Diagrams-with-Multimodal-Large-Language-Models-cs-CV-cs-AIPDF"><a href="#111-SketchJudge-A-Diagnostic-Benchmark-for-Grading-Hand-drawn-Diagrams-with-Multimodal-Large-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[111] SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models cs.CV | cs.AIPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06944">SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06944" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuhang Su, Mei Wang, Yaoyao Zhong, Guozhang Li, Shixing Li</span></p>
<p><strong>TL;DR:</strong> 本文介绍了SketchJudge，一个专门用于评估多模态大语言模型（MLLMs）对手绘STEM图表进行评分能力的新诊断性基准。该基准包含1,015个涵盖几何、物理、图表和流程图四个领域的学生手绘答案，具有多样化的风格变化和明确的错误类型。评估表明，即使是先进的MLLMs也显著落后于人类表现，凸显了当前视觉-语言对齐在符号化和噪声环境中的脆弱性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型在视觉理解方面取得了显著进展，但在处理人类手绘草图的无结构和模糊性时常常遇到困难，尤其是在视觉评分这一未被充分探索的任务中。该任务要求模型不仅能解决问题，还能诊断手绘图表中的错误，这依赖于复杂的结构、语义和元认知推理。</p>
<p><strong>Result:</strong> 在SketchJudge基准上的评估结果显示，即使是先进的MLLMs也显著落后于人类的表现，验证了该基准在暴露当前视觉-语言对齐在符号化和噪声环境中的脆弱性方面的有效性。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个专门针对手绘图表评分任务的诊断性基准SketchJudge，强调了对模型进行复杂结构、语义和元认知推理能力评估的重要性。从客观角度看，该基准通过包含多样化的手绘风格和明确的错误类型，为评估和提升MLLMs在真实、模糊视觉场景下的理解和诊断能力提供了有价值的工具和方向。</p>
<p><strong>Abstract:</strong> While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark’s effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at <a target="_blank" rel="noopener" href="https://github.com/yuhangsu82/SketchJudge">https://github.com/yuhangsu82/SketchJudge</a>.</p>
  </div>
</details>

<hr>
<h3 id="112-Unified-Personalized-Understanding-Generating-and-Editing-cs-CVPDF"><a href="#112-Unified-Personalized-Understanding-Generating-and-Editing-cs-CVPDF" class="headerlink" title="[112] Unified Personalized Understanding, Generating and Editing cs.CVPDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06965">Unified Personalized Understanding, Generating and Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06965" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Zhong, Tianwei Lin, Ruike Zhu, Yuqian Yuan, Haoyu Zheng</span></p>
<p><strong>TL;DR:</strong> 本文提出了OmniPersona，一个端到端的统一大型多模态模型个性化框架，首次在单一架构中集成了个性化理解、生成和图像编辑。该框架通过结构解耦的概念令牌和显式知识回放机制，解决了现有方法中跨任务干扰和个性化知识不一致的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有统一多模态模型采用“一刀切”范式，难以一致且可控地建模用户特定概念；现有个性化方法依赖外部检索效率低下，或引入可学习软提示但导致跨任务干扰和知识模糊&#x2F;错位。</p>
<p><strong>Result:</strong> 实验结果表明，OmniPersona在多样化的个性化任务上提供了有竞争力且鲁棒的性能。作者还提出了OmniPBench基准，扩展了UnifyBench概念集并集成了跨任务评估协议。</p>
<p><strong>Insight:</strong> 主要创新点包括：1) 结构解耦的概念令牌，为不同任务分配专用子空间以最小化干扰；2) 显式知识回放机制，在任务间传播个性化属性知识以实现一致行为；3) 首次在单一架构中统一了理解、生成和编辑三大个性化任务。</p>
<p><strong>Abstract:</strong> Unified large multimodal models (LMMs) have achieved remarkable progress in general-purpose multimodal understanding and generation. However, they still operate under a &#96;&#96;one-size-fits-all’’ paradigm and struggle to model user-specific concepts (e.g., generate a photo of \texttt{<maeve>}) in a consistent and controllable manner. Existing personalization methods typically rely on external retrieval, which is inefficient and poorly integrated into unified multimodal pipelines. Recent personalized unified models introduce learnable soft prompts to encode concept information, yet they either couple understanding and generation or depend on complex multi-stage training, leading to cross-task interference and ultimately to fuzzy or misaligned personalized knowledge. We present \textbf{OmniPersona}, an end-to-end personalization framework for unified LMMs that, for the first time, integrates personalized understanding, generation, and image editing within a single architecture. OmniPersona introduces structurally decoupled concept tokens, allocating dedicated subspaces for different tasks to minimize interference, and incorporates an explicit knowledge replay mechanism that propagates personalized attribute knowledge across tasks, enabling consistent personalized behavior. To systematically evaluate unified personalization, we propose \textbf{\texttt{OmniPBench}}, extending the public UnifyBench concept set with personalized editing tasks and cross-task evaluation protocols integrating understanding, generation, and editing. Experimental results demonstrate that OmniPersona delivers competitive and robust performance across diverse personalization tasks. We hope OmniPersona will serve as a strong baseline and spur further research on controllable, unified personalization.</p>
  </div>
</details>

<hr>
<h3 id="113-Can-Textual-Reasoning-Improve-the-Performance-of-MLLMs-on-Fine-grained-Visual-Classification-cs-CVPDF"><a href="#113-Can-Textual-Reasoning-Improve-the-Performance-of-MLLMs-on-Fine-grained-Visual-Classification-cs-CVPDF" class="headerlink" title="[113] Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification? cs.CVPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06993">Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06993" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie Zhu, Yiyang Su, Xiaoming Liu</span></p>
<p><strong>TL;DR:</strong> 本文研究了多模态大语言模型（MLLMs）在细粒度视觉分类（FGVC）任务上的性能，发现思维链（CoT）推理会因推理文本过长而损害分类准确率，作者称之为“思考成本”。为应对此问题，论文提出了ReFine-RFT框架，通过集成奖励和一种名为\alg的归一化方法来平衡异质奖励信号、约束推理长度并提供密集的准确率导向反馈，从而在多个FGVC基准上取得了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管MLLMs展现出强大的通用能力，但在需要细微视觉辨别的核心感知任务——细粒度视觉分类（FGVC）上仍存在困难。先前工作发现CoT推理可能损害视觉感知任务性能，但原因不明，本文旨在系统性地重新审视CoT在FGVC中的作用并探究性能下降的根源。</p>
<p><strong>Result:</strong> 大量实验表明，所提出的ReFine-RFT框架在多个FGVC基准测试上取得了最先进的（SOTA）性能。</p>
<p><strong>Insight:</strong> 核心发现是CoT导致的性能下降主要由推理长度驱动，即更长的文本推理会持续降低分类准确率（“思考成本”）。创新点在于提出了\alg（一种用于多奖励优化的简单通用即插即用归一化方法）和ReFine-RFT框架，该框架结合集成奖励与\alg来约束推理长度并提供密集的准确率导向反馈，从而有效提升FGVC性能。</p>
<p><strong>Abstract:</strong> Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the &#96;&#96;Cost of Thinking’’. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \href{<a target="_blank" rel="noopener" href="https://github.com/jiezhu23/ReFine-RFT%7D%7BProject">https://github.com/jiezhu23/ReFine-RFT}{Project</a> Link}.</p>
  </div>
</details>

<hr>
<h3 id="114-Efficient-Visual-Question-Answering-Pipeline-for-Autonomous-Driving-via-Scene-Region-Compression-cs-CVPDF"><a href="#114-Efficient-Visual-Question-Answering-Pipeline-for-Autonomous-Driving-via-Scene-Region-Compression-cs-CVPDF" class="headerlink" title="[114] Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression cs.CVPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07092">Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07092" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuliang Cai, Dongqiangzi Ye, Zitian Chen, Chongruo Wu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于自动驾驶视觉问答（VQA）的高效视觉语言模型（VLM）框架SRC-Pipeline，旨在解决现有VLM计算成本高、推理延迟大的问题。该方法通过学习将早期视频帧的密集图像块（patch）令牌压缩为少量高级令牌，同时保留近期帧的完整令牌，从而在保持性能的同时显著降低计算开销。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动驾驶中的VQA对低延迟和实时处理有严格要求，但当前最先进的VLM（如大型视觉语言模型）通常优先考虑性能而非计算效率，通过处理每帧的密集图像块令牌导致计算成本（FLOPs）和推理延迟过高，限制了其在实时自动驾驶场景中的实际部署。</p>
<p><strong>Result:</strong> 在自动驾驶视频问答任务上的实验表明，该方法在保持可比性能的同时，实现了66%的FLOPs减少，使VLM能在实时、安全关键的自动驾驶环境中更有效地运行。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种基于场景区域压缩（SRC）的令牌管理策略，通过动态区分处理早期帧（压缩为高级令牌）和近期帧（保留完整令牌），在计算效率和模型性能之间取得了平衡，为实时VQA系统设计提供了新思路。</p>
<p><strong>Abstract:</strong> Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.</p>
  </div>
</details>

<hr>
<h3 id="115-MEDVISTAGYM-A-Scalable-Training-Environment-for-Thinking-with-Medical-Images-via-Tool-Integrated-Reinforcement-Learning-cs-CV-cs-AIPDF"><a href="#115-MEDVISTAGYM-A-Scalable-Training-Environment-for-Thinking-with-Medical-Images-via-Tool-Integrated-Reinforcement-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[115] MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning cs.CV | cs.AIPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07107">MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07107" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meng Lu, Yuxing Lu, Yuchen Zhuang, Megan Mullins, Yang Xie</span></p>
<p><strong>TL;DR:</strong> 本文提出了MedVistaGym，一个可扩展的交互式训练环境，用于通过工具集成强化学习训练视觉语言模型（VLMs）进行医学图像推理。该环境激励模型学习在医学图像分析中进行工具集成的视觉推理，包括决定何时调用何种工具、定位任务相关图像区域以及整合证据。基于此环境训练的模型MedVistaGym-R1在多个医学VQA基准测试中显著优于基线模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前通用视觉语言模型在医学图像理解，尤其是需要多步迭代视觉交互的推理任务上表现不佳。医学VLMs通常依赖静态视觉嵌入和单次推理，无法在推理过程中重新检查、验证或细化视觉证据。同时，开源VLMs缺乏学习在多模态医学推理中有效选择、调用和协调工具的训练基础设施。</p>
<p><strong>Result:</strong> 在六个医学视觉问答（VQA）基准测试上，所训练的MedVistaGym-R1-8B模型比同等规模、使用工具增强的基线模型性能高出19.10%到24.21%，达到了新的最先进水平（SOTA）。</p>
<p><strong>Insight:</strong> 论文的核心创新点是提出了一个统一的、可执行的、用于智能体训练的交互式环境MedVistaGym，它将工具调用、图像区域定位和多证据整合集成到交错的多模态推理流程中。客观分析表明，其关键洞见在于：结构化智能体训练（而非仅仅提供工具访问权限）是解锁医学图像分析中有效工具集成推理能力的关键。这为训练具备主动推理能力的医学AI智能体提供了新的框架和训练范式。</p>
<p><strong>Abstract:</strong> Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training–not tool access alone–unlocks effective tool-integrated reasoning for medical image analysis.</p>
  </div>
</details>

<hr>
<h3 id="116-Few-shot-Class-Incremental-Learning-via-Generative-Co-Memory-Regularization-cs-CV-cs-AIPDF"><a href="#116-Few-shot-Class-Incremental-Learning-via-Generative-Co-Memory-Regularization-cs-CV-cs-AIPDF" class="headerlink" title="[116] Few-shot Class-Incremental Learning via Generative Co-Memory Regularization cs.CV | cs.AIPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07117">Few-shot Class-Incremental Learning via Generative Co-Memory Regularization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07117" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kexin Bao, Yong Li, Dan Zeng, Shiming Ge</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种生成式协同记忆正则化方法来解决少样本类增量学习问题。该方法通过生成式域适应微调预训练的生成编码器，构建表示记忆和权重记忆，并在增量学习中利用这些记忆进行正则化，以提升模型识别精度，同时缓解灾难性遗忘和新类过拟合。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 少样本类增量学习旨在从少量新类数据中增量学习模型，需要模型在少样本监督下具备强大的表示和适应能力，以避免对旧类的灾难性遗忘和对新类的过拟合。</p>
<p><strong>Result:</strong> 在流行基准测试上的大量实验表明，该方法优于现有最先进方法。</p>
<p><strong>Insight:</strong> 创新点在于结合生成式域适应微调和协同记忆正则化，通过构建并动态更新类级表示记忆和权重记忆来协同正则化增量学习过程，从而有效平衡新旧类知识。</p>
<p><strong>Abstract:</strong> Few-shot class-incremental learning (FSCIL) aims to incrementally learn models from a small amount of novel data, which requires strong representation and adaptation ability of models learned under few-example supervision to avoid catastrophic forgetting on old classes and overfitting to novel classes. This work proposes a generative co-memory regularization approach to facilitate FSCIL. In the approach, the base learning leverages generative domain adaptation finetuning to finetune a pretrained generative encoder on a few examples of base classes by jointly incorporating a masked autoencoder (MAE) decoder for feature reconstruction and a fully-connected classifier for feature classification, which enables the model to efficiently capture general and adaptable representations. Using the finetuned encoder and learned classifier, we construct two class-wise memories: representation memory for storing the mean features for each class, and weight memory for storing the classifier weights. After that, the memory-regularized incremental learning is performed to train the classifier dynamically on the examples of few-shot classes in each incremental session by simultaneously optimizing feature classification and co-memory regularization. The memories are updated in a class-incremental manner and they collaboratively regularize the incremental learning. In this way, the learned models improve recognition accuracy, while mitigating catastrophic forgetting over old classes and overfitting to novel classes. Extensive experiments on popular benchmarks clearly demonstrate that our approach outperforms the state-of-the-arts.</p>
  </div>
</details>

<hr>
<h3 id="117-Motion-Focus-Recognition-in-Fast-Moving-Egocentric-Video-cs-CVPDF"><a href="#117-Motion-Focus-Recognition-in-Fast-Moving-Egocentric-Video-cs-CVPDF" class="headerlink" title="[117] Motion Focus Recognition in Fast-Moving Egocentric Video cs.CVPDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07154">Motion Focus Recognition in Fast-Moving Egocentric Video</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07154" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Daniel Hong, James Tribble, Hao Wang, Chaoyi Zhou, Ashish Bastola</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种实时运动焦点识别方法，用于从任意第一人称视角视频中估计主体的运动意图，旨在弥补现有数据集在快速运动场景中忽视运动分析的不足。该方法利用基础模型进行相机姿态估计，并通过系统级优化实现高效可扩展的推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有第一人称数据集主要关注动作识别任务，而忽略了运动分析在体育等快速运动场景中的固有作用，因此需要一种实时方法来识别运动焦点。</p>
<p><strong>Result:</strong> 在收集的第一人称动作数据集上评估，该方法通过滑动批量推理策略实现了实时性能且内存消耗可控。</p>
<p><strong>Insight:</strong> 创新点在于将运动中心分析实用化，为边缘部署提供可行性，并为体育和快速运动活动的研究提供了补充视角；系统级优化和基础模型的结合提升了推理效率。</p>
<p><strong>Abstract:</strong> From Vision-Language-Action (VLA) systems to robotics, existing egocentric datasets primarily focus on action recognition tasks, while largely overlooking the inherent role of motion analysis in sports and other fast-movement scenarios. To bridge this gap, we propose a real-time motion focus recognition method that estimates the subject’s locomotion intention from any egocentric video. Our approach leverages the foundation model for camera pose estimation and introduces system-level optimizations to enable efficient and scalable inference. Evaluated on a collected egocentric action dataset, our method achieves real-time performance with manageable memory consumption through a sliding batch inference strategy. This work makes motion-centric analysis practical for edge deployment and offers a complementary perspective to existing egocentric studies on sports and fast-movement activities.</p>
  </div>
</details>

<hr>
<h3 id="118-Test-time-Adaptive-Hierarchical-Co-enhanced-Denoising-Network-for-Reliable-Multimodal-Classification-cs-CVPDF"><a href="#118-Test-time-Adaptive-Hierarchical-Co-enhanced-Denoising-Network-for-Reliable-Multimodal-Classification-cs-CVPDF" class="headerlink" title="[118] Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification cs.CVPDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07163">Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07163" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shu Shen, C. L. Philip Chen, Tong Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种测试时自适应分层协同增强去噪网络（TAHCD），用于处理低质量多模态数据中的异构噪声问题。该方法通过全局和实例层面的自适应稳定子空间对齐与样本自适应置信对齐，可靠地去除模态特定和跨模态噪声，并引入测试时协同增强机制，以无标签方式根据输入噪声自适应更新模型，从而提升模型的鲁棒性、适应性和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决低质量多模态数据中异构噪声带来的挑战，现有方法在可靠去除异构噪声以及面对未见噪声时适应性和泛化能力有限的问题。</p>
<p><strong>Result:</strong> 在多个基准测试上，该方法在分类性能、鲁棒性和泛化性方面优于当前最先进的可信多模态学习方法，达到了SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于分层协同去噪框架，结合了全局与实例层面的噪声处理，以及测试时无标签自适应更新机制，有效提升了模型对噪声的鲁棒性和泛化能力。</p>
<p><strong>Abstract:</strong> Reliable learning on low-quality multimodal data is a widely concerning issue, especially in safety-critical applications. However, multimodal noise poses a major challenge in this domain and leads existing methods to suffer from two key limitations. First, they struggle to reliably remove heterogeneous data noise, hindering robust multimodal representation learning. Second, they exhibit limited adaptability and generalization when encountering previously unseen noise. To address these issues, we propose Test-time Adaptive Hierarchical Co-enhanced Denoising Network (TAHCD). On one hand, TAHCD introduces the Adaptive Stable Subspace Alignment and Sample-Adaptive Confidence Alignment to reliably remove heterogeneous noise. They account for noise at both global and instance levels and enable jointly removal of modality-specific and cross-modality noise, achieving robust learning. On the other hand, TAHCD introduces test-time cooperative enhancement, which adaptively updates the model in response to input noise in a label-free manner, improving adaptability and generalization. This is achieved by collaboratively enhancing the joint removal process of modality-specific and cross-modality noise across global and instance levels according to sample noise. Experiments on multiple benchmarks demonstrate that the proposed method achieves superior classification performance, robustness, and generalization compared with state-of-the-art reliable multimodal learning approaches.</p>
  </div>
</details>

<hr>
<h3 id="119-DIVER-Dynamic-Iterative-Visual-Evidence-Reasoning-for-Multimodal-Fake-News-Detection-cs-CV-cs-AIPDF"><a href="#119-DIVER-Dynamic-Iterative-Visual-Evidence-Reasoning-for-Multimodal-Fake-News-Detection-cs-CV-cs-AIPDF" class="headerlink" title="[119] DIVER: Dynamic Iterative Visual Evidence Reasoning for Multimodal Fake News Detection cs.CV | cs.AIPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07178">DIVER: Dynamic Iterative Visual Evidence Reasoning for Multimodal Fake News Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07178" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weilin Zhou, Zonghao Ying, Chunlei Meng, Jiahui Liu, Hengyang Zhou</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DIVER的动态迭代视觉证据推理框架，用于多模态假新闻检测。该框架采用渐进式、证据驱动的推理范式，首先通过文本分析建立强基线，仅在文本证据不足时引入视觉信息，并通过跨模态对齐验证自适应决定是否进行深度视觉检查。对于存在显著跨模态语义差异的样本，DIVER选择性调用细粒度视觉工具（如OCR和密集字幕）提取任务相关证据，并通过不确定性感知融合迭代聚合以优化多模态推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有多模态假新闻检测方法依赖静态融合或大语言模型，由于视觉基础薄弱，面临计算冗余和幻觉风险。DIVER旨在解决这些问题，通过动态、迭代的推理过程减少冗余计算并缓解幻觉。</p>
<p><strong>Result:</strong> 在Weibo、Weibo21和GossipCop数据集上的实验表明，DIVER平均优于最先进基线方法2.72%，同时通过减少4.12秒的延迟优化了推理效率。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个动态、迭代的视觉证据推理范式，通过基于文本的基线过滤、跨模态对齐验证和选择性细粒度视觉工具调用，实现了计算效率与检测精度的平衡。其不确定性感知融合机制和渐进式证据整合策略是可借鉴的关键设计。</p>
<p><strong>Abstract:</strong> Multimodal fake news detection is crucial for mitigating adversarial misinformation. Existing methods, relying on static fusion or LLMs, face computational redundancy and hallucination risks due to weak visual foundations. To address this, we propose DIVER (Dynamic Iterative Visual Evidence Reasoning), a framework grounded in a progressive, evidence-driven reasoning paradigm. DIVER first establishes a strong text-based baseline through language analysis, leveraging intra-modal consistency to filter unreliable or hallucinated claims. Only when textual evidence is insufficient does the framework introduce visual information, where inter-modal alignment verification adaptively determines whether deeper visual inspection is necessary. For samples exhibiting significant cross-modal semantic discrepancies, DIVER selectively invokes fine-grained visual tools (e.g., OCR and dense captioning) to extract task-relevant evidence, which is iteratively aggregated via uncertainty-aware fusion to refine multimodal reasoning. Experiments on Weibo, Weibo21, and GossipCop demonstrate that DIVER outperforms state-of-the-art baselines by an average of 2.72%, while optimizing inference efficiency with a reduced latency of 4.12 s.</p>
  </div>
</details>

<hr>
<h3 id="120-ShowUI-Aloha-Human-Taught-GUI-Agent-cs-CVPDF"><a href="#120-ShowUI-Aloha-Human-Taught-GUI-Agent-cs-CVPDF" class="headerlink" title="[120] ShowUI-Aloha: Human-Taught GUI Agent cs.CVPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07181">ShowUI-Aloha: Human-Taught GUI Agent</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07181" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yichun Zhang, Xiangwu Guo, Yauhong Goh, Jessica Hu, Zhiheng Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了ShowUI-Aloha，一个将非结构化的桌面环境人类屏幕录像转化为结构化、可执行任务的完整流水线，旨在解决GUI自动化任务中高质量训练数据稀缺的问题。该框架包含记录器、学习者、规划器和执行器四个核心组件，共同实现从观察人类操作到自主执行GUI任务的闭环。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 图形用户界面（GUI）自动化面临的主要挑战是缺乏可扩展的高质量训练数据，而现有的人类演示录像通常冗长、非结构化且缺乏标注，导致智能体难以从中有效学习。</p>
<p><strong>Result:</strong> 论文通过构建的完整流水线，展示了将真实世界人类数据转化为可学习任务的可行性，为构建能够通过观察人类来学习的通用GUI智能体提供了一条可行路径。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个端到端的框架，将原始的人类交互记录（鼠标点击、键盘输入等）与视觉上下文结合，通过语义解释生成自然语言描述，并基于上下文推理进行动态任务规划和安全执行，实现了从非结构化演示到结构化任务执行的自动化转换。</p>
<p><strong>Abstract:</strong> Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.</p>
  </div>
</details>

<hr>
<h3 id="121-SIRR-LMM-Single-image-Reflection-Removal-via-Large-Multimodal-Model-cs-CV-cs-AI-cs-GRPDF"><a href="#121-SIRR-LMM-Single-image-Reflection-Removal-via-Large-Multimodal-Model-cs-CV-cs-AI-cs-GRPDF" class="headerlink" title="[121] SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model cs.CV | cs.AI | cs.GRPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07209">SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07209" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于大型多模态模型（LMM）的单图像反射去除方法SIRR-LMM。为了解决现有数据集在物理真实性和规模上的不足，作者引入了一个合成数据集生成框架，该框架通过路径追踪3D玻璃模型与真实背景图像来创建物理准确的反射场景。通过将图像层拼接为复合输入、应用联合描述生成，并使用任务特定的LoRA进行微调，该方法在反射去除和分离性能上超越了现有最先进方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决单图像反射去除（SIRR）任务中因玻璃表面反射和透射光复杂交互带来的挑战，以及现有合成数据集物理真实性有限或真实捕获数据规模不足的问题。</p>
<p><strong>Result:</strong> 在反射去除和分离任务上，该方法相比最先进（SOTA）方法取得了改进的性能，但摘要中未明确提及具体基准测试名称或定量指标。</p>
<p><strong>Insight:</strong> 创新点包括：1）提出一个物理准确的合成数据集生成框架，结合3D玻璃模型路径追踪与真实背景；2）利用大型多模态模型（LMM），通过图像层拼接和联合描述生成进行输入处理；3）采用任务特定的LoRA微调策略而非全参数训练，以高效提升模型性能。从客观角度看，该方法将物理模拟与深度学习结合，并优化了LMM在特定视觉任务上的应用方式。</p>
<p><strong>Abstract:</strong> Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="122-SceneNAT-Masked-Generative-Modeling-for-Language-Guided-Indoor-Scene-Synthesis-cs-CVPDF"><a href="#122-SceneNAT-Masked-Generative-Modeling-for-Language-Guided-Indoor-Scene-Synthesis-cs-CVPDF" class="headerlink" title="[122] SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis cs.CVPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07218">SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07218" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jeongjun Choi, Yeonsoo Park, H. Jin Kim</span></p>
<p><strong>TL;DR:</strong> SceneNAT是一种单阶段掩码非自回归Transformer模型，用于根据自然语言指令合成完整的3D室内场景。它通过少量并行解码步骤，在语义遵从性和空间布局准确性上超越了现有的自回归和扩散基线模型，同时显著降低了计算成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有方法在从语言生成3D室内场景时存在的计算效率低、推理速度慢以及难以同时建模场景语义和空间结构的问题。</p>
<p><strong>Result:</strong> 在3D-FRONT数据集上的大量实验表明，SceneNAT在语义遵从性和空间布局准确性方面优于最先进的自回归和扩散基线模型，且计算成本显著更低。</p>
<p><strong>Insight:</strong> 创新点在于采用单阶段掩码非自回归Transformer架构，结合属性级和实例级的掩码策略以更好地捕获对象内和对象间结构，并引入专用的三元组预测器来增强对场景布局和对象关系的建模能力。</p>
<p><strong>Abstract:</strong> We present SceneNAT, a single-stage masked non-autoregressive Transformer that synthesizes complete 3D indoor scenes from natural language instructions through only a few parallel decoding passes, offering improved performance and efficiency compared to prior state-of-the-art approaches. SceneNAT is trained via masked modeling over fully discretized representations of both semantic and spatial attributes. By applying a masking strategy at both the attribute level and the instance level, the model can better capture intra-object and inter-object structure. To boost relational reasoning, SceneNAT employs a dedicated triplet predictor for modeling the scene’s layout and object relationships by mapping a set of learnable relation queries to a sparse set of symbolic triplets (subject, predicate, object). Extensive experiments on the 3D-FRONT dataset demonstrate that SceneNAT achieves superior performance compared to state-of-the-art autoregressive and diffusion baselines in both semantic compliance and spatial arrangement accuracy, while operating with substantially lower computational cost.</p>
  </div>
</details>

<hr>
<h3 id="123-VENUS-Visual-Editing-with-Noise-Inversion-Using-Scene-Graphs-cs-CVPDF"><a href="#123-VENUS-Visual-Editing-with-Noise-Inversion-Using-Scene-Graphs-cs-CVPDF" class="headerlink" title="[123] VENUS: Visual Editing with Noise Inversion Using Scene Graphs cs.CVPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07219">VENUS: Visual Editing with Noise Inversion Using Scene Graphs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07219" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thanh-Nhan Vo, Trong-Thuan Nguyen, Tam V. Nguyen, Minh-Triet Tran</span></p>
<p><strong>TL;DR:</strong> 本文提出了VENUS，一种无需训练的、基于场景图的图像编辑框架。它通过解耦编辑目标与背景的提示词条件策略，并结合噪声反演技术，在保持未编辑区域保真度的同时，实现了对图像语义内容的可控编辑。该方法显著提升了背景保持和语义对齐能力，并在效率和效果上超越了现有的基于场景图和文本的编辑方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于文本的图像编辑模型难以平衡背景保持与语义一致性，而基于场景图的编辑方法虽然提供了更好的可控性，但通常依赖模型微调，计算成本高且可扩展性差。本文旨在解决这一矛盾，提出一种无需训练的高效场景图引导编辑框架。</p>
<p><strong>Result:</strong> 在PIE-Bench基准测试上，VENUS相对于当前最先进的场景图编辑模型SGEdit，将PSNR从22.45提升至24.80，SSIM从0.79提升至0.84，LPIPS从0.100降低至0.070，CLIP相似度从24.19提升至24.97。在EditVal上，其DINO得分达到0.87，并将单图处理时间从6-10分钟大幅缩短至20-30秒。此外，VENUS也超越了LEDIT++和P2P+DirInv等强文本编辑基线。</p>
<p><strong>Insight:</strong> 核心创新点在于提出了一种无需训练的框架，通过解耦提示词条件策略和噪声反演技术，有效分离编辑目标与背景，从而在保持背景的同时实现精确的语义编辑。另一个关键点是利用多模态大语言模型提取场景图并与扩散模型主干结合，避免了额外的训练开销，实现了高效、可控的图像编辑。</p>
<p><strong>Abstract:</strong> State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.</p>
  </div>
</details>

<hr>
<h3 id="124-From-Landslide-Conditioning-Factors-to-Satellite-Embeddings-Evaluating-the-Utilisation-of-Google-AlphaEarth-for-Landslide-Susceptibility-Mapping-using-Deep-Learning-cs-CVPDF"><a href="#124-From-Landslide-Conditioning-Factors-to-Satellite-Embeddings-Evaluating-the-Utilisation-of-Google-AlphaEarth-for-Landslide-Susceptibility-Mapping-using-Deep-Learning-cs-CVPDF" class="headerlink" title="[124] From Landslide Conditioning Factors to Satellite Embeddings: Evaluating the Utilisation of Google AlphaEarth for Landslide Susceptibility Mapping using Deep Learning cs.CVPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07268">From Landslide Conditioning Factors to Satellite Embeddings: Evaluating the Utilisation of Google AlphaEarth for Landslide Susceptibility Mapping using Deep Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07268" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yusen Cheng, Qinfeng Zhu, Lei Fan</span></p>
<p><strong>TL;DR:</strong> 本研究评估了谷歌AlphaEarth（AE）嵌入作为滑坡易发性制图（LSM）预测因子的潜力，通过对比传统滑坡条件因子（LCFs）与两种AE表示（保留主成分和完整的64个嵌入波段），在三个研究区域使用三种深度学习模型进行测试。结果表明，基于AE的模型在所有区域和模型上均优于传统LCFs，能获得更高的F1分数和AUC值，并生成空间对应更清晰的易发性图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统数据驱动的LSM依赖滑坡条件因子（LCFs），但其可用性、异质性和预处理不确定性可能限制制图可靠性；本研究旨在探索谷歌AlphaEarth（AE）嵌入这种源自多源地理空间观测的统一地球表面条件表示，能否作为LSM的替代预测因子。</p>
<p><strong>Result:</strong> 在台湾南投县、香港和意大利艾米利亚-罗马涅部分地区三个研究区域，使用CNN1D、CNN2D和Vision Transformer模型进行评估。基于AE的模型在所有区域和模型上均优于传统LCFs，F1分数提升约4%至15%，AUC值增加0.04至0.11，其中使用完整64波段AE表示时改进最显著；AE生成的易发性图与观测滑坡事件的空间对应更清晰，对局部易滑坡条件更敏感。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于系统评估了谷歌AlphaEarth嵌入作为LSM标准化、信息丰富替代预测因子的潜力。客观分析，其核心创新在于利用大规模预训练的地理空间嵌入（AE）作为统一特征表示，避免了传统LCFs的异质性和预处理不确定性，为地理空间AI任务提供了可泛化的数据驱动方案；同时发现AE嵌入与滑坡清单的时间对齐程度影响LSM效果，这为实际应用提供了重要指导。</p>
<p><strong>Abstract:</strong> Data-driven landslide susceptibility mapping (LSM) typically relies on landslide conditioning factors (LCFs), whose availability, heterogeneity, and preprocessing-related uncertainties can constrain mapping reliability. Recently, Google AlphaEarth (AE) embeddings, derived from multi-source geospatial observations, have emerged as a unified representation of Earth surface conditions. This study evaluated the potential of AE embeddings as alternative predictors for LSM. Two AE representations, including retained principal components and the full set of 64 embedding bands, were systematically compared with conventional LCFs across three study areas (Nantou County, Taiwan; Hong Kong; and part of Emilia-Romagna, Italy) using three deep learning models (CNN1D, CNN2D, and Vision Transformer). Performance was assessed using multiple evaluation metrics, ROC-AUC analysis, error statistics, and spatial pattern assessment. Results showed that AE-based models consistently outperformed LCFs across all regions and models, yielding higher F1-scores, AUC values, and more stable error distributions. Such improvement was most pronounced when using the full 64-band AE representation, with F1-score improvements of approximately 4% to 15% and AUC increased ranging from 0.04 to 0.11, depending on the study area and model. AE-based susceptibility maps also exhibited clearer spatial correspondence with observed landslide occurrences and enhanced sensitivity to localised landslide-prone conditions. Performance improvements were more evident in Nantou and Emilia than in Hong Kong, revealing that closer temporal alignment between AE embeddings and landslide inventories may lead to more effective LSM outcomes. These findings highlight the strong potential of AE embeddings as a standardised and information-rich alternative to conventional LCFs for LSM.</p>
  </div>
</details>

<hr>
<h3 id="125-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models-cs-CVPDF"><a href="#125-Focal-Guidance-Unlocking-Controllability-from-Semantic-Weak-Layers-in-Video-Diffusion-Models-cs-CVPDF" class="headerlink" title="[125] Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models cs.CVPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07287">Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07287" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuanyang Yin, Yufan Deng, Shenghai Yuan, Kaipeng Zhang, Xiao Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出Focal Guidance（FG）方法，旨在解决基于DiT的图像到视频（I2V）生成模型中存在的条件隔离问题，即某些中间层（语义弱层）对文本语义响应不足，导致生成视频与文本提示的贴合度下降。FG通过细粒度语义引导（FSG）和注意力缓存机制，增强这些弱层的可控性，从而提升视频生成对文本指令的遵循能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有I2V模型虽注重视觉一致性，但如何有效结合高频视觉约束与低频文本引导，确保生成视频严格遵循文本提示，仍缺乏深入探索。作者观察到DiT-based I2V模型中存在语义弱层，其文本-视觉相似度下降，归因于条件隔离现象，即视觉特征注意力部分脱离文本引导，过度依赖学习到的视觉先验。</p>
<p><strong>Result:</strong> 在作者提出的I2V指令遵循评估基准上，Focal Guidance显著提升了性能：在Wan2.1-I2V上将总分提升至0.7250（+3.97%），在基于MMDiT的HunyuanVideo-I2V上提升至0.5571（+7.44%），证明了其有效性和泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于识别并针对语义弱层进行干预，通过CLIP驱动的细粒度区域锚定和跨层注意力缓存，显式注入语义信号，减少对视觉先验的过度依赖。这为扩散模型的可控性增强提供了一种层特异性引导思路，可借鉴于其他需要多模态对齐的生成任务。</p>
<p><strong>Abstract:</strong> The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model’s learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44%).</p>
  </div>
</details>

<hr>
<h3 id="126-VideoLoom-A-Video-Large-Language-Model-for-Joint-Spatial-Temporal-Understanding-cs-CVPDF"><a href="#126-VideoLoom-A-Video-Large-Language-Model-for-Joint-Spatial-Temporal-Understanding-cs-CVPDF" class="headerlink" title="[126] VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07290">VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07290" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiapeng Shi, Junke Wang, Zuyao You, Bo He, Zuxuan Wu</span></p>
<p><strong>TL;DR:</strong> 本文提出了VideoLoom，一个用于联合时空理解的统一视频大语言模型。为了促进细粒度时空定位能力的发展，作者构建了LoomData-8.7k数据集，这是一个以人为中心、包含时间锚定和空间定位描述的视频数据集。基于此，VideoLoom在多个时空基准测试中取得了最先进或极具竞争力的性能。此外，作者还引入了LoomBench，一个包含时间、空间和组合性视频-问题对的新基准，用于从多方面全面评估视频大语言模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有视频大语言模型在联合、细粒度的时空理解方面的能力不足，需要一个统一的模型和相应的数据集来同时处理视频中的时间定位和空间定位任务。</p>
<p><strong>Result:</strong> 在多个基准测试中达到SOTA或极具竞争力水平，例如在ReVOS（指代视频目标分割）上获得63.1的J&amp;F分数，在Charades-STA（时间定位）上获得48.3的<a href="mailto:&#x52;&#x31;&#64;&#48;&#x2e;&#55;">R1@0.7</a>分数。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了一个统一的视频大语言模型框架用于联合时空理解，并构建了配套的、包含细粒度时空标注的数据集LoomData-8.7k以及一个全面的评估基准LoomBench，为多模态智能的时空理解研究提供了新的标准和工具集。</p>
<p><strong>Abstract:</strong> This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&amp;F on ReVOS for referring video object segmentation, and 48.3 <a href="mailto:&#x52;&#49;&#x40;&#48;&#x2e;&#55;">R1@0.7</a> on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.</p>
  </div>
</details>

<hr>
<h3 id="127-A-Visual-Semantic-Adaptive-Watermark-grounded-by-Prefix-Tuning-for-Large-Vision-Language-Model-cs-CV-cs-AIPDF"><a href="#127-A-Visual-Semantic-Adaptive-Watermark-grounded-by-Prefix-Tuning-for-Large-Vision-Language-Model-cs-CV-cs-AIPDF" class="headerlink" title="[127] A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model cs.CV | cs.AIPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07291">A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07291" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qi Zheng, Shuliang Liu, Yu Huang, Sihang Jia, Jungang Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为VISA-Mark的新型视觉语义自适应水印框架，用于大型视觉语言模型（LVLM）的内容溯源和知识产权保护。该方法通过轻量级前缀调优器提取动态视觉证据权重，指导自适应词汇划分和logits扰动，将水印强度集中在视觉支持的token上，从而在保持高检测精度和鲁棒性的同时，严格维护视觉保真度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有水印方法存在视觉无关token引入、视觉基础破坏或推理延迟高的问题，本文旨在解决这些缺陷，实现既有效可检测又不损害视觉保真度和推理效率的多模态水印。</p>
<p><strong>Result:</strong> 在Chair-I基准上，VISA-Mark将视觉一致性提升了7.8%，并具有优异的语义保真度；同时保持了高竞争性的检测准确率（96.88% AUC）和强大的攻击鲁棒性（99.3%），且未牺牲推理效率，为可靠性保持的多模态水印设立了新标准。</p>
<p><strong>Insight:</strong> 创新点在于利用前缀调优动态生成视觉证据权重来指导水印嵌入，实现了水印与视觉证据的主动对齐；客观来看，该方法将自适应扰动与视觉语义结合，在保证检测性能的同时显著提升了生成内容的视觉一致性，为LVLM水印提供了一种高效可靠的解决方案。</p>
<p><strong>Abstract:</strong> Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. In this paper, we propose the VIsual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual-Evidence Weights, which quantify the evidentiary support for candidate tokens based on the visual input. These weights guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity. Empirical results confirm that VISA-Mark outperforms conventional methods with a 7.8% improvement in visual consistency (Chair-I) and superior semantic fidelity. The framework maintains highly competitive detection accuracy (96.88% AUC) and robust attack resilience (99.3%) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multimodal watermarking.</p>
  </div>
</details>

<hr>
<h3 id="128-Mimic-Human-Cognition-Master-Multi-Image-Reasoning-A-Meta-Action-Framework-for-Enhanced-Visual-Understanding-cs-CVPDF"><a href="#128-Mimic-Human-Cognition-Master-Multi-Image-Reasoning-A-Meta-Action-Framework-for-Enhanced-Visual-Understanding-cs-CVPDF" class="headerlink" title="[128] Mimic Human Cognition, Master Multi-Image Reasoning: A Meta-Action Framework for Enhanced Visual Understanding cs.CVPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07298">Mimic Human Cognition, Master Multi-Image Reasoning: A Meta-Action Framework for Enhanced Visual Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07298" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jianghao Yin, Qingbin Li, Kun Sun, Cheng Ding, Jie Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种受人类认知启发的元动作框架CINEMA，用于解决多模态大语言模型在多图像推理任务中性能下降的问题。该框架将多图像推理分解为五个结构化元动作：全局感知、聚焦、提示、思考和回答，并采用检索树采样策略生成高质量训练轨迹，结合两阶段强化学习进行优化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型在单图像理解上表现出色，但在多图像推理场景中性能显著下降，主要挑战包括图像间复杂关系和关键信息分散。</p>
<p><strong>Result:</strong> 在MUIR和MVMath基准测试中超越了GPT-4o，在视频理解基准上显著优于专用视频推理模型，在多个关键基准上达到了竞争性的最先进水平。</p>
<p><strong>Insight:</strong> 创新点在于将人类认知过程结构化为可操作的元动作序列，并设计了冷启动训练和两阶段强化学习策略，有效提升了模型对多图像和视频数据的推理能力与泛化性。</p>
<p><strong>Abstract:</strong> While Multimodal Large Language Models (MLLMs) excel at single-image understanding, they exhibit significantly degraded performance in multi-image reasoning scenarios. Multi-image reasoning presents fundamental challenges including complex inter-relationships between images and scattered critical information across image sets. Inspired by human cognitive processes, we propose the Cognition-Inspired Meta-Action Framework (CINEMA), a novel approach that decomposes multi-image reasoning into five structured meta-actions: Global, Focus, Hint, Think, and Answer which explicitly modeling the sequential cognitive steps humans naturally employ. For cold-start training, we introduce a Retrieval-Based Tree Sampling strategy that generates high-quality meta-action trajectories to bootstrap the model with reasoning patterns. During reinforcement learning, we adopt a two-stage paradigm: an exploration phase with Diversity-Preserving Strategy to avoid entropy collapse, followed by an annealed exploitation phase with DAPO to gradually strengthen exploitation. To train our model, we construct a dataset of 57k cold-start and 58k reinforcement learning instances spanning multi-image, multi-frame, and single-image tasks. We conduct extensive evaluations on multi-image reasoning benchmarks, video understanding benchmarks, and single-image benchmarks, achieving competitive state-of-the-art performance on several key benchmarks. Our model surpasses GPT-4o on the MUIR and MVMath benchmarks and notably outperforms specialized video reasoning models on video understanding benchmarks, demonstrating the effectiveness and generalizability of our human cognition-inspired reasoning framework.</p>
  </div>
</details>

<hr>
<h3 id="129-Revisiting-the-Ordering-of-Channel-and-Spatial-Attention-A-Comprehensive-Study-on-Sequential-and-Parallel-Designs-cs-CVPDF"><a href="#129-Revisiting-the-Ordering-of-Channel-and-Spatial-Attention-A-Comprehensive-Study-on-Sequential-and-Parallel-Designs-cs-CVPDF" class="headerlink" title="[129] Revisiting the Ordering of Channel and Spatial Attention: A Comprehensive Study on Sequential and Parallel Designs cs.CVPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07310">Revisiting the Ordering of Channel and Spatial Attention: A Comprehensive Study on Sequential and Parallel Designs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07310" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhongming Liu, Bingbing Jiang</span></p>
<p><strong>TL;DR:</strong> 本文系统研究了通道注意力和空间注意力的融合策略，通过构建包含18种拓扑结构的评估套件，在视觉和医学数据集上揭示了‘数据规模-方法-性能’的耦合规律，并提出了基于场景的注意力模块构建指南。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前通道注意力和空间注意力的融合策略选择主要依赖经验，缺乏系统分析和统一原则，本文旨在通过系统比较不同组合方式来解决这一问题。</p>
<p><strong>Result:</strong> 在2个视觉和9个医学数据集上的实验表明，不同数据规模下最优结构不同：小样本任务中‘通道-多尺度空间’级联结构最优；中等规模任务中并行可学习融合架构更优；大规模任务中带动态门控的并行结构性能最佳。此外，‘空间-通道’顺序在细粒度分类中更稳定有效，残差连接能缓解梯度消失问题。</p>
<p><strong>Insight:</strong> 论文的创新点在于首次系统性地比较了通道与空间注意力的多种融合拓扑，并揭示了其性能与数据规模的耦合规律，为未来注意力模块设计提供了基于数据场景的指导原则，而非依赖经验选择。</p>
<p><strong>Abstract:</strong> Attention mechanisms have become a core component of deep learning models, with Channel Attention and Spatial Attention being the two most representative architectures. Current research on their fusion strategies primarily bifurcates into sequential and parallel paradigms, yet the selection process remains largely empirical, lacking systematic analysis and unified principles. We systematically compare channel-spatial attention combinations under a unified framework, building an evaluation suite of 18 topologies across four classes: sequential, parallel, multi-scale, and residual. Across two vision and nine medical datasets, we uncover a “data scale-method-performance” coupling law: (1) in few-shot tasks, the “Channel-Multi-scale Spatial” cascaded structure achieves optimal performance; (2) in medium-scale tasks, parallel learnable fusion architectures demonstrate superior results; (3) in large-scale tasks, parallel structures with dynamic gating yield the best performance. Additionally, experiments indicate that the “Spatial-Channel” order is more stable and effective for fine-grained classification, while residual connections mitigate vanishing gradient problems across varying data scales. We thus propose scenario-based guidelines for building future attention modules. Code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/DWlzm">https://github.com/DWlzm</a>.</p>
  </div>
</details>

<hr>
<h3 id="130-OSCAR-Open-Set-CAD-Retrieval-from-a-Language-Prompt-and-a-Single-Image-cs-CV-cs-ROPDF"><a href="#130-OSCAR-Open-Set-CAD-Retrieval-from-a-Language-Prompt-and-a-Single-Image-cs-CV-cs-ROPDF" class="headerlink" title="[130] OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image cs.CV | cs.ROPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07333">OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07333" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tessa Pulli, Jean-Baptiste Weibel, Peter Hönig, Matthias Hirschmanner, Markus Vincze</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为OSCAR的新型免训练方法，用于从无标签3D对象数据库中检索匹配的CAD模型。该方法结合语言提示和单张图像，通过两阶段检索（基于CLIP的文本过滤和基于DINOv2的图像细化）实现开放集检索，并在跨域3D模型检索基准MI3DOR上超越现有SOTA方法，同时展示了其在6D物体姿态估计中自动获取对象模型的适用性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决在机器人、增强现实等应用中，面对不断变化的对象集合时，传统零样本姿态估计器依赖难以获取的CAD模型，且持续增长的对象集使可靠识别目标实例模型变得困难的问题。</p>
<p><strong>Result:</strong> 在跨域3D模型检索基准MI3DOR上，OSCAR超越了所有最先进方法；在YCB-V对象数据集上进行物体检索时，平均精度达到90.48%；此外，使用Megapose进行姿态估计时，利用最相似对象模型取得了比基于重建方法更好的结果。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种免训练的开放集CAD检索框架，结合多模态嵌入（文本和图像）和两阶段检索策略，有效利用预训练模型（如CLIP、DINOv2、GroundedSAM）实现高效准确的模型匹配，为动态对象环境下的6D姿态估计提供了自动化模型来源的解决方案。</p>
<p><strong>Abstract:</strong> 6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality. To support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models. Such models are hard to obtain once deployed, and a continuously changing and growing set of objects makes it harder to reliably identify the instance model of interest. To address this challenge, we introduce an Open-Set CAD Retrieval from a Language Prompt and a Single Image (OSCAR), a novel training-free method that retrieves a matching object model from an unlabeled 3D object database. During onboarding, OSCAR generates multi-view renderings of database models and annotates them with descriptive captions using an image captioning model. At inference, GroundedSAM detects the queried object in the input image, and multi-modal embeddings are computed for both the Region-of-Interest and the database captions. OSCAR employs a two-stage retrieval: text-based filtering using CLIP identifies candidate models, followed by image-based refinement using DINOv2 to select the most visually similar object. In our experiments we demonstrate that OSCAR outperforms all state-of-the-art methods on the cross-domain 3D model retrieval benchmark MI3DOR. Furthermore, we demonstrate OSCAR’s direct applicability in automating object model sourcing for 6D object pose estimation. We propose using the most similar object model for pose estimation if the exact instance is not available and show that OSCAR achieves an average precision of 90.48% during object retrieval on the YCB-V object dataset. Moreover, we demonstrate that the most similar object model can be utilized for pose estimation using Megapose achieving better results than a reconstruction-based approach.</p>
  </div>
</details>

<hr>
<h3 id="131-PulseMind-A-Multi-Modal-Medical-Model-for-Real-World-Clinical-Diagnosis-cs-CV-cs-AIPDF"><a href="#131-PulseMind-A-Multi-Modal-Medical-Model-for-Real-World-Clinical-Diagnosis-cs-CV-cs-AIPDF" class="headerlink" title="[131] PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis cs.CV | cs.AIPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07344">PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07344" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiao Xu, Junwei Liu, Jiangwei Lao, Qi Zhu, Yunpeng Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了PulseMind，一个面向真实世界临床诊断的多模态医疗模型，通过构建大规模多轮会诊数据集MediScope、设计多维评估基准PulseMind Benchmark，并开发基于比较的强化策略优化（CRPO）训练框架，旨在解决现有医疗多模态模型在复杂临床诊断场景中的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有医疗多模态模型多专注于特定影像分析（如皮肤科、病理学或放射学），未能充分捕捉真实临床诊断的复杂性，包括异构输入和医患交互中的持续上下文理解。</p>
<p><strong>Result:</strong> 实验表明，PulseMind在诊断会诊基准和公共医疗基准上均取得了有竞争力的性能。</p>
<p><strong>Insight:</strong> 创新点包括构建覆盖10个主要临床科室和200多个亚专科的大规模真实世界多轮会诊数据集MediScope，设计包含主动性、准确性、有用性和语言质量四维评估协议的多轮诊断基准，以及提出基于相对偏好信号而非绝对分数奖励的CRPO训练框架，以提供稳定且与人类对齐的训练指导。</p>
<p><strong>Abstract:</strong> Recent advances in medical multi-modal models focus on specialized image analysis like dermatology, pathology, or radiology. However, they do not fully capture the complexity of real-world clinical diagnostics, which involve heterogeneous inputs and require ongoing contextual understanding during patient-physician interactions. To bridge this gap, we introduce PulseMind, a new family of multi-modal diagnostic models that integrates a systematically curated dataset, a comprehensive evaluation benchmark, and a tailored training framework. Specifically, we first construct a diagnostic dataset, MediScope, which comprises 98,000 real-world multi-turn consultations and 601,500 medical images, spanning over 10 major clinical departments and more than 200 sub-specialties. Then, to better reflect the requirements of real-world clinical diagnosis, we develop the PulseMind Benchmark, a multi-turn diagnostic consultation benchmark with a four-dimensional evaluation protocol comprising proactiveness, accuracy, usefulness, and language quality. Finally, we design a training framework tailored for multi-modal clinical diagnostics, centered around a core component named Comparison-based Reinforcement Policy Optimization (CRPO). Compared to absolute score rewards, CRPO uses relative preference signals from multi-dimensional com-parisons to provide stable and human-aligned training guidance. Extensive experiments demonstrate that PulseMind achieves competitive performance on both the diagnostic consultation benchmark and public medical benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="132-Seeing-Right-but-Saying-Wrong-Inter-and-Intra-Layer-Refinement-in-MLLMs-without-Training-cs-CV-cs-AIPDF"><a href="#132-Seeing-Right-but-Saying-Wrong-Inter-and-Intra-Layer-Refinement-in-MLLMs-without-Training-cs-CV-cs-AIPDF" class="headerlink" title="[132] Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training cs.CV | cs.AIPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07359">Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07359" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shezheng Song, Shasha Li, Jie Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无需训练的双视角解码优化策略DualPD，旨在解决多模态大语言模型（MLLMs）中存在的内部推理不一致问题，即深层注意力已关注正确视觉区域但最终预测仍受早期层噪声注意力误导的现象。该方法通过层间注意力引导的对比logits模块和层内头级信息过滤模块，在不增加训练成本的情况下提升模型在多模态基准测试上的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态大语言模型内部注意力机制的不一致性问题，即模型深层已正确理解视觉信息，但最终输出仍被早期层的噪声注意力所误导，导致‘看对说错’的现象。</p>
<p><strong>Result:</strong> 在LLaVA和Qwen-VL等多个模型家族及多模态基准测试上，DualPD无需训练即可持续提升模型准确率，验证了其有效性和泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种无需训练的后处理解码优化框架，通过分析层间注意力演变和抑制层内低贡献注意力头来修正模型输出，为改善MLLMs的推理一致性提供了轻量级解决方案。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated strong capabilities across a variety of vision-language tasks. However, their internal reasoning often exhibits a critical inconsistency: although deeper layers may attend to the correct visual regions, final predictions are frequently misled by noisy attention from earlier layers. This results in a disconnect between what the model internally understands and what it ultimately expresses, a phenomenon we describe as seeing it right but saying it wrong. To address this issue, we propose DualPD, a dual-perspective decoding refinement strategy that enhances the visual understanding without any additional training. DualPD consists of two components. (1) The layer-wise attention-guided contrastive logits module captures how the belief in the correct answer evolves by comparing output logits between layers that exhibit the largest attention shift. (2) The head-wise information filtering module suppresses low-contribution attention heads that focus on irrelevant regions, thereby improving attention quality within each layer. Experiments conducted on both the LLaVA and Qwen-VL model families across multiple multimodal benchmarks demonstrate that DualPD consistently improves accuracy without training, confirming its effectiveness and generalizability. The code will be released upon publication.</p>
  </div>
</details>

<hr>
<h3 id="133-HiVid-Narrator-Hierarchical-Video-Narrative-Generation-with-Scene-Primed-ASR-anchored-Compression-cs-CVPDF"><a href="#133-HiVid-Narrator-Hierarchical-Video-Narrative-Generation-with-Scene-Primed-ASR-anchored-Compression-cs-CVPDF" class="headerlink" title="[133] HiVid-Narrator: Hierarchical Video Narrative Generation with Scene-Primed ASR-anchored Compression cs.CVPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07366">HiVid-Narrator: Hierarchical Video Narrative Generation with Scene-Primed ASR-anchored Compression</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07366" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoxuan Li, Mengyan Li, Junjun Zheng</span></p>
<p><strong>TL;DR:</strong> 本文提出了HiVid-Narrator框架，用于为电商视频生成层次化的叙事描述。为了解决现有方法难以统一感知细粒度视觉细节和组织连贯高层故事的问题，作者构建了E-HVC数据集，并设计了一个两阶段方法：首先通过ASR和帧级描述收集证据，然后基于时间链式思维生成精确的章节摘要。同时，针对电商视频信息密集的特点，提出了SPA-Compressor来压缩多模态输入，从而在减少输入令牌的同时提升叙事质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法难以统一处理电商视频所需的细粒度视觉感知和高层故事组织能力，无法生成结构化的叙事。</p>
<p><strong>Result:</strong> HiVid-Narrator框架在E-HVC数据集上实现了优于现有方法的叙事质量，同时使用了更少的输入令牌。</p>
<p><strong>Insight:</strong> 创新点包括：1）构建了具有双重粒度、时间对齐标注的E-HVC数据集；2）采用基于时间链式思维的两阶段章节生成方法，确保事实准确性和时间对齐；3）提出了SPA-Compressor，利用ASR语义线索指导多模态令牌压缩，实现高效训练。</p>
<p><strong>Abstract:</strong> Generating structured narrations for real-world e-commerce videos requires models to perceive fine-grained visual details and organize them into coherent, high-level stories–capabilities that existing approaches struggle to unify. We introduce the E-commerce Hierarchical Video Captioning (E-HVC) dataset with dual-granularity, temporally grounded annotations: a Temporal Chain-of-Thought that anchors event-level observations and Chapter Summary that compose them into concise, story-centric summaries. Rather than directly prompting chapters, we adopt a staged construction that first gathers reliable linguistic and visual evidence via curated ASR and frame-level descriptions, then refines coarse annotations into precise chapter boundaries and titles conditioned on the Temporal Chain-of-Thought, yielding fact-grounded, time-aligned narratives. We also observe that e-commerce videos are fast-paced and information-dense, with visual tokens dominating the input sequence. To enable efficient training while reducing input tokens, we propose the Scene-Primed ASR-anchored Compressor (SPA-Compressor), which compresses multimodal tokens into hierarchical scene and event representations guided by ASR semantic cues. Built upon these designs, our HiVid-Narrator framework achieves superior narrative quality with fewer input tokens compared to existing methods.</p>
  </div>
</details>

<hr>
<h3 id="134-Learning-Dynamic-Collaborative-Network-for-Semi-supervised-3D-Vessel-Segmentation-cs-CV-cs-AIPDF"><a href="#134-Learning-Dynamic-Collaborative-Network-for-Semi-supervised-3D-Vessel-Segmentation-cs-CV-cs-AIPDF" class="headerlink" title="[134] Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation cs.CV | cs.AIPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07377">Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07377" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiao Xu, Xin Chen, Lihe Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DiCo的动态协作网络，用于半监督3D血管分割。该方法通过动态切换教师和学生模型的角色、引入多视角集成模块以及对抗性监督，有效解决了传统均值教师方法中角色固定导致的认知偏差问题，并在三个3D血管分割基准测试中取得了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统均值教师方法在3D血管分割中采用静态的教师-学生角色分配，但由于3D血管数据的复杂性，教师模型可能并不总是优于学生模型，导致认知偏差并限制性能。本文旨在通过动态角色切换来解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，DiCo方法在三个3D血管分割基准测试中均达到了最先进的性能水平。</p>
<p><strong>Insight:</strong> 创新点包括：动态协作网络允许教师和学生模型角色动态切换；多视角集成模块模拟医生进行医学分析的方式，捕捉输入的不同视角；对抗性监督用于约束未标记数据中分割血管的形状，并通过将3D体积投影到2D视图来减轻标签不一致的影响。</p>
<p><strong>Abstract:</strong> In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is <a target="_blank" rel="noopener" href="https://github.com/xujiaommcome/DiCo">https://github.com/xujiaommcome/DiCo</a></p>
  </div>
</details>

<hr>
<h3 id="135-Forecast-the-Principal-Stabilize-the-Residual-Subspace-Aware-Feature-Caching-for-Efficient-Diffusion-Transformers-cs-CVPDF"><a href="#135-Forecast-the-Principal-Stabilize-the-Residual-Subspace-Aware-Feature-Caching-for-Efficient-Diffusion-Transformers-cs-CVPDF" class="headerlink" title="[135] Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers cs.CVPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07396">Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07396" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guantao Chen, Shikang Zheng, Yuqi Lin, Linfeng Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为SVD-Cache的子空间感知特征缓存框架，用于加速扩散变换器（DiT）的推理过程。该方法通过奇异值分解（SVD）将DiT特征分解为平稳演化的主成分子空间和振荡的残差子空间，并对主成分采用指数移动平均（EMA）预测，同时直接复用残差特征，从而在保证生成质量的同时显著提升推理速度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散变换器（DiT）模型在图像和视频生成方面取得了前所未有的质量，但其迭代采样过程计算成本高昂。现有的特征缓存方法对所有特征分量一视同仁，而作者发现DiT特征空间包含具有不同时间行为的主成分和残差子空间，这为设计更高效的缓存策略提供了动机。</p>
<p><strong>Result:</strong> 大量实验表明，SVD-Cache在包括FLUX和HunyuanVideo在内的多种模型和方法上实现了近乎无损的加速，达到了5.55倍的加速比，并且与蒸馏、量化和稀疏注意力等模型加速技术兼容。</p>
<p><strong>Insight:</strong> 论文的创新点在于揭示了DiT特征空间中主成分和残差子空间具有不同的时间演化特性，并据此设计了子空间感知的缓存策略。从客观角度看，将特征分解与针对性的预测&#x2F;复用策略相结合，是一种新颖且高效的加速思路，其与现有加速技术的兼容性也增强了实用性。</p>
<p><strong>Abstract:</strong> Diffusion Transformer (DiT) models have achieved unprecedented quality in image and video generation, yet their iterative sampling process remains computationally prohibitive. To accelerate inference, feature caching methods have emerged by reusing intermediate representations across timesteps. However, existing caching approaches treat all feature components uniformly. We reveal that DiT feature spaces contain distinct principal and residual subspaces with divergent temporal behavior: the principal subspace evolves smoothly and predictably, while the residual subspace exhibits volatile, low-energy oscillations that resist accurate prediction. Building on this insight, we propose SVD-Cache, a subspace-aware caching framework that decomposes diffusion features via Singular Value Decomposition (SVD), applies exponential moving average (EMA) prediction to the dominant low-rank components, and directly reuses the residual subspace. Extensive experiments demonstrate that SVD-Cache achieves near-lossless across diverse models and methods, including 5.55$\times$ speedup on FLUX and HunyuanVideo, and compatibility with model acceleration techniques including distillation, quantization and sparse attention. Our code is in supplementary material and will be released on Github.</p>
  </div>
</details>

<hr>
<h3 id="136-Improving-Video-Question-Answering-through-query-based-frame-selection-cs-CV-cs-LGPDF"><a href="#136-Improving-Video-Question-Answering-through-query-based-frame-selection-cs-CV-cs-LGPDF" class="headerlink" title="[136] Improving Video Question Answering through query-based frame selection cs.CV | cs.LGPDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07459">Improving Video Question Answering through query-based frame selection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07459" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Himanshu Patil, Geo Jolly, Ramana Raja Buddala, Ganesh Ramakrishnan, Rohit Saluja</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于查询的视频帧选择方法，用于改进视频问答（VideoQA）任务。该方法利用子模互信息（SMI）函数，根据问题选择与问题相关的关键视频帧，以替代传统的均匀采样策略，从而确保所选帧提供互补且必要的视觉信息。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大型视觉语言模型（VLMs）因计算需求大，通常采用均匀采样固定数量视频帧的方法，但这种方法无法选取重要帧或捕捉视频上下文，限制了VideoQA的准确性。</p>
<p><strong>Result:</strong> 在MVBench数据集上使用Video-LLaVA和LLaVA-NeXT模型进行评估，与均匀采样相比，基于查询的帧选择方法将VideoQA准确率提升了高达4%，定性分析也表明该方法能更一致地选取与问题对齐的帧。</p>
<p><strong>Insight:</strong> 创新点在于将子模互信息（SMI）函数应用于基于查询的帧选择，这是一种主动的、与任务相关的帧采样策略，可替代被动的均匀采样，能更有效地提取关键视觉信息，这一思路可推广至其他依赖视频帧子集的任务中。</p>
<p><strong>Abstract:</strong> Video Question Answering (VideoQA) models enhance understanding and interaction with audiovisual content, making it more accessible, searchable, and useful for a wide range of fields such as education, surveillance, entertainment, and content creation. Due to heavy compute requirements, most large visual language models (VLMs) for VideoQA rely on a fixed number of frames by uniformly sampling the video. However, this process does not pick important frames or capture the context of the video. We present a novel query-based selection of frames relevant to the questions based on the submodular mutual Information (SMI) functions. By replacing uniform frame sampling with query-based selection, our method ensures that the chosen frames provide complementary and essential visual information for accurate VideoQA. We evaluate our approach on the MVBench dataset, which spans a diverse set of multi-action video tasks. VideoQA accuracy on this dataset was assessed using two VLMs, namely Video-LLaVA and LLaVA-NeXT, both of which originally employed uniform frame sampling. Experiments were conducted using both uniform and query-based sampling strategies. An accuracy improvement of up to \textbf{4%} was observed when using query-based frame selection over uniform sampling. Qualitative analysis further highlights that query-based selection, using SMI functions, consistently picks frames better aligned with the question. We opine that such query-based frame selection can enhance accuracy in a wide range of tasks that rely on only a subset of video frames.</p>
  </div>
</details>

<hr>
<h3 id="137-From-Sketch-to-Fresco-Efficient-Diffusion-Transformer-with-Progressive-Resolution-cs-CVPDF"><a href="#137-From-Sketch-to-Fresco-Efficient-Diffusion-Transformer-with-Progressive-Resolution-cs-CVPDF" class="headerlink" title="[137] From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution cs.CVPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07462">From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07462" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shikang Zheng, Guantao Chen, Lixuan He, Jiacheng Liu, Yuqi Lin</span></p>
<p><strong>TL;DR:</strong> 本文提出Fresco框架，通过渐进式上采样统一动态分辨率采样中的去噪与全局结构，解决现有方法因启发式重加噪和盲目上采样导致的跨阶段不一致与伪影问题，实现高效且保真的扩散Transformer加速。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有动态分辨率采样方法依赖启发式重加噪破坏跨阶段一致性，且盲目上采样整个潜在空间导致误差累积和伪影，需设计更高效的加速框架以保持生成质量。</p>
<p><strong>Result:</strong> 在FLUX和HunyuanVideo等模型上分别实现10倍和5倍加速，与蒸馏、量化和特征缓存等技术正交结合后可达22倍加速，在多个领域和模型上达到近乎无损的加速效果。</p>
<p><strong>Insight:</strong> 创新点在于通过渐进式上采样统一重加噪与全局结构，确保所有阶段对齐同一最终目标，在保持低分辨率草图效率的同时实现高分辨率精修保真度，为扩散模型加速提供了可扩展的框架设计思路。</p>
<p><strong>Abstract:</strong> Diffusion Transformers achieve impressive generative quality but remain computationally expensive due to iterative sampling. Recently, dynamic resolution sampling has emerged as a promising acceleration technique by reducing the resolution of early sampling steps. However, existing methods rely on heuristic re-noising at every resolution transition, injecting noise that breaks cross-stage consistency and forces the model to relearn global structure. In addition, these methods indiscriminately upsample the entire latent space at once without checking which regions have actually converged, causing accumulated errors, and visible artifacts. Therefore, we propose \textbf{Fresco}, a dynamic resolution framework that unifies re-noise and global structure across stages with progressive upsampling, preserving both the efficiency of low-resolution drafting and the fidelity of high-resolution refinement, with all stages aligned toward the same final target. Fresco achieves near-lossless acceleration across diverse domains and models, including 10$\times$ speedup on FLUX, and 5$\times$ on HunyuanVideo, while remaining orthogonal to distillation, quantization and feature caching, reaching 22$\times$ speedup when combined with distilled models. Our code is in supplementary material and will be released on Github.</p>
  </div>
</details>

<hr>
<h3 id="138-FocalOrder-Focal-Preference-Optimization-for-Reading-Order-Detection-cs-CVPDF"><a href="#138-FocalOrder-Focal-Preference-Optimization-for-Reading-Order-Detection-cs-CVPDF" class="headerlink" title="[138] FocalOrder: Focal Preference Optimization for Reading Order Detection cs.CVPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07483">FocalOrder: Focal Preference Optimization for Reading Order Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07483" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fuyuan Liu, Dianyu Yu, He Ren, Nayu Liu, Xiaomian Kang</span></p>
<p><strong>TL;DR:</strong> 本文提出FocalOrder框架，通过Focal Preference Optimization (FPO)解决文档阅读顺序检测中的位置差异问题，即在复杂中间区域性能下降的现象。该方法动态识别难学习区域并引入难度校准的成对排序目标，在OmniDocBench v1.0和Comp-HRDoc基准上取得了新的SOTA结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法基于均匀监督，假设所有布局区域难度相同，但实际存在位置差异问题：模型在确定性的起始和结束区域表现良好，却在复杂中间部分性能崩溃。标准训练中大量简单模式淹没了困难布局的学习信号。</p>
<p><strong>Result:</strong> 在OmniDocBench v1.0和Comp-HRDoc基准上建立了新的SOTA结果，紧凑模型不仅优于竞争性专用基线，还显著超越大规模通用视觉语言模型。</p>
<p><strong>Insight:</strong> 创新点在于通过自适应难度发现与指数移动平均机制动态定位难学习过渡区域，并引入难度校准的成对排序目标来强制全局逻辑一致性；核心洞察是将优化与文档内在结构模糊性对齐对于掌握复杂文档结构至关重要。</p>
<p><strong>Abstract:</strong> Reading order detection is the foundation of document understanding. Most existing methods rely on uniform supervision, implicitly assuming a constant difficulty distribution across layout regions. In this work, we challenge this assumption by revealing a critical flaw: \textbf{Positional Disparity}, a phenomenon where models demonstrate mastery over the deterministic start and end regions but suffer a performance collapse in the complex intermediate sections. This degradation arises because standard training allows the massive volume of easy patterns to drown out the learning signals from difficult layouts. To address this, we propose \textbf{FocalOrder}, a framework driven by \textbf{Focal Preference Optimization (FPO)}. Specifically, FocalOrder employs adaptive difficulty discovery with exponential moving average mechanism to dynamically pinpoint hard-to-learn transitions, while introducing a difficulty-calibrated pairwise ranking objective to enforce global logical consistency. Extensive experiments demonstrate that FocalOrder establishes new state-of-the-art results on OmniDocBench v1.0 and Comp-HRDoc. Our compact model not only outperforms competitive specialized baselines but also significantly surpasses large-scale general VLMs. These results demonstrate that aligning the optimization with intrinsic structural ambiguity of documents is critical for mastering complex document structures.</p>
  </div>
</details>

<hr>
<h3 id="139-BenchSeg-A-Large-Scale-Dataset-and-Benchmark-for-Multi-View-Food-Video-Segmentation-cs-CVPDF"><a href="#139-BenchSeg-A-Large-Scale-Dataset-and-Benchmark-for-Multi-View-Food-Video-Segmentation-cs-CVPDF" class="headerlink" title="[139] BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation cs.CVPDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07581">BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07581" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmad AlMughrabi, Guillermo Rivo, Carlos Jiménez-Farfán, Umair Haroon, Farid Al-Areqi</span></p>
<p><strong>TL;DR:</strong> 本文提出了BenchSeg，一个大规模多视角食物视频分割数据集与基准，整合了55个菜品场景的25,284帧精细标注数据，并评估了20种先进分割模型在FoodSeg103和BenchSeg上的性能。结果表明，标准图像分割器在新视角下性能显著下降，而结合视频记忆模块的方法能保持时间一致性，其中SeTR-MLA+XMem2组合模型优于先前工作（如比FoodMem提升约2.63% mAP）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前食物图像分割方法受限于多视角数据不足且对新视角泛化能力差，阻碍了饮食分析中食物体积和营养的准确估计。</p>
<p><strong>Result:</strong> 在BenchSeg基准上，结合视频记忆模块的方法（如SeTR-MLA+XMem2）相比先前工作（如FoodMem）提升了约2.63% mAP，实现了SOTA性能，而标准图像分割器在新视角下性能显著下降。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个大规模多视角食物视频分割数据集BenchSeg，并通过实验揭示了视频记忆模块对保持时间一致性和提升新视角泛化能力的关键作用，为饮食分析中的分割与跟踪提供了新思路。</p>
<p><strong>Abstract:</strong> Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at <a target="_blank" rel="noopener" href="https://amughrabi.github.io/benchseg">https://amughrabi.github.io/benchseg</a>.</p>
  </div>
</details>

<hr>
<h3 id="140-UIKA-Fast-Universal-Head-Avatar-from-Pose-Free-Images-cs-CVPDF"><a href="#140-UIKA-Fast-Universal-Head-Avatar-from-Pose-Free-Images-cs-CVPDF" class="headerlink" title="[140] UIKA: Fast Universal Head Avatar from Pose-Free Images cs.CVPDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07603">UIKA: Fast Universal Head Avatar from Pose-Free Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07603" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zijian Wu, Boyao Zhou, Liangxiao Hu, Hongyu Liu, Yuan Sun</span></p>
<p><strong>TL;DR:</strong> UIKA是一种从前馈网络快速生成通用头部虚拟化身的方法，支持从任意数量的无姿态输入（包括单张图像、多视角捕获和智能手机视频）中重建可动画化的高斯头部模型。该方法通过UV引导的建模策略、可学习的UV令牌以及大规模合成训练数据集，实现了高效且高质量的头部虚拟化身生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统虚拟化身方法需要工作室级别的多视角捕获系统和耗时的优化过程，UIKA旨在通过重新思考模型表示、网络设计和数据准备，实现从无姿态图像中快速、通用地生成头部虚拟化身。</p>
<p><strong>Result:</strong> 在单目和多视角设置下，UIKA显著优于现有方法，实现了高质量的头部虚拟化身生成。</p>
<p><strong>Insight:</strong> 创新点包括UV引导的虚拟化身建模策略（将屏幕空间像素重投影到与相机姿态和表情无关的UV空间）、可学习的UV令牌（在屏幕和UV级别应用注意力机制以聚合多视角信息）以及大规模身份丰富的合成训练数据集，这些设计提高了模型的通用性和效率。</p>
<p><strong>Abstract:</strong> We present UIKA, a feed-forward animatable Gaussian head model from an arbitrary number of unposed inputs, including a single image, multi-view captures, and smartphone-captured videos. Unlike the traditional avatar method, which requires a studio-level multi-view capture system and reconstructs a human-specific model through a long-time optimization process, we rethink the task through the lenses of model representation, network design, and data preparation. First, we introduce a UV-guided avatar modeling strategy, in which each input image is associated with a pixel-wise facial correspondence estimation. Such correspondence estimation allows us to reproject each valid pixel color from screen space to UV space, which is independent of camera pose and character expression. Furthermore, we design learnable UV tokens on which the attention mechanism can be applied at both the screen and UV levels. The learned UV tokens can be decoded into canonical Gaussian attributes using aggregated UV information from all input views. To train our large avatar model, we additionally prepare a large-scale, identity-rich synthetic training dataset. Our method significantly outperforms existing approaches in both monocular and multi-view settings. Project page: <a target="_blank" rel="noopener" href="https://zijian-wu.github.io/uika-page/">https://zijian-wu.github.io/uika-page/</a></p>
  </div>
</details>

<hr>
<h3 id="141-PARL-Position-Aware-Relation-Learning-Network-for-Document-Layout-Analysis-cs-CVPDF"><a href="#141-PARL-Position-Aware-Relation-Learning-Network-for-Document-Layout-Analysis-cs-CVPDF" class="headerlink" title="[141] PARL: Position-Aware Relation Learning Network for Document Layout Analysis cs.CVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07620">PARL: Position-Aware Relation Learning Network for Document Layout Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07620" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fuyuan Liu, Dianyu Yu, He Ren, Nayu Liu, Xiaomian Kang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PARL（位置感知关系学习网络）的OCR-free、纯视觉的文档布局分析框架，通过建模位置敏感性和关系结构来理解文档的视觉结构，从而摆脱了对高质量OCR的依赖。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前流行的文档布局分析方法依赖高质量OCR来融合视觉和文本特征，这导致文本识别错误传播和巨大计算开销，限制了多模态方法的鲁棒性和实际应用。本文认为有效的布局分析应基于对文档内在视觉结构的深度理解，而非文本-视觉融合。</p>
<p><strong>Result:</strong> 在DocLayNet基准上，PARL为纯视觉方法建立了新的基准；在M6Doc基准上，其性能甚至超越了强大的多模态模型。PARL（6500万参数）效率很高，参数量大约仅为大型多模态模型（2.56亿参数）的四分之一，实现了SOTA结果。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个不依赖OCR的纯视觉框架，通过双向空间位置引导的可变形注意力模块显式嵌入布局元素间的位置依赖，并设计了图细化分类器通过动态构建的布局图建模上下文关系来优化预测。其核心洞察是，复杂的视觉结构建模可以比多模态融合更高效、更鲁棒。</p>
<p><strong>Abstract:</strong> Document layout analysis aims to detect and categorize structural elements (e.g., titles, tables, figures) in scanned or digital documents. Popular methods often rely on high-quality Optical Character Recognition (OCR) to merge visual features with extracted text. This dependency introduces two major drawbacks: propagation of text recognition errors and substantial computational overhead, limiting the robustness and practical applicability of multimodal approaches. In contrast to the prevailing multimodal trend, we argue that effective layout analysis depends not on text-visual fusion, but on a deep understanding of documents’ intrinsic visual structure. To this end, we propose PARL (Position-Aware Relation Learning Network), a novel OCR-free, vision-only framework that models layout through positional sensitivity and relational structure. Specifically, we first introduce a Bidirectional Spatial Position-Guided Deformable Attention module to embed explicit positional dependencies among layout elements directly into visual features. Second, we design a Graph Refinement Classifier (GRC) to refine predictions by modeling contextual relationships through a dynamically constructed layout graph. Extensive experiments show PARL achieves state-of-the-art results. It establishes a new benchmark for vision-only methods on DocLayNet and, notably, surpasses even strong multimodal models on M6Doc. Crucially, PARL (65M) is highly efficient, using roughly four times fewer parameters than large multimodal models (256M), demonstrating that sophisticated visual structure modeling can be both more efficient and robust than multimodal fusion.</p>
  </div>
</details>

<hr>
<h3 id="142-GeoMotionGPT-Geometry-Aligned-Motion-Understanding-with-Large-Language-Models-cs-CV-cs-AIPDF"><a href="#142-GeoMotionGPT-Geometry-Aligned-Motion-Understanding-with-Large-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[142] GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models cs.CV | cs.AIPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07632">GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07632" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhankai Ye, Bofan Li, Yukai Jin, Shuoqiu Li, Wei Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了GeoMotionGPT框架，通过强制运动码本与LLM嵌入空间的正交性，实现运动空间几何结构与嵌入空间的对齐，从而提升大语言模型在运动理解与推理任务上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法将运动量化与语义嵌入学习解耦，仅通过token ID连接，未能有效对齐运动空间的固有几何结构与嵌入空间，限制了LLM对细微运动推理的能力。</p>
<p><strong>Result:</strong> 在HumanML3D基准测试中，该框架相比当前最优方法实现了20%的性能提升，达到了新的SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于提出通过正交性约束统一运动与语言模态的几何基础，并采用两阶段正交正则化调度来保持几何对齐而不妨碍语义适应，为多模态对齐提供了新思路。</p>
<p><strong>Abstract:</strong> Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM’s capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.</p>
  </div>
</details>

<hr>
<h3 id="143-StdGEN-A-Comprehensive-System-for-Semantic-Decomposed-3D-Character-Generation-cs-CVPDF"><a href="#143-StdGEN-A-Comprehensive-System-for-Semantic-Decomposed-3D-Character-Generation-cs-CVPDF" class="headerlink" title="[143] StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation cs.CVPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07660">StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07660" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuze He, Yanning Zhou, Wang Zhao, Jingwen Ye, Zhongkai Wu</span></p>
<p><strong>TL;DR:</strong> StdGEN++是一个用于从多样化输入生成高保真、语义分解3D角色的综合系统。它通过双分支语义感知大重建模型联合重建几何、颜色和组件级语义，并引入兼容混合隐式场的语义表面提取方法，以及基于视频扩散的纹理分解模块，实现了生产级质量的3D角色生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有3D生成方法通常产生单一网格，缺乏游戏和动画工业流程所需的结构灵活性，StdGEN++旨在解决这一差距，生成语义分解的3D角色以支持下游编辑和应用。</p>
<p><strong>Result:</strong> 实验表明，StdGEN++在几何精度和语义解缠方面显著优于现有方法，达到了最先进的性能水平。</p>
<p><strong>Insight:</strong> 创新点包括双分支语义感知大重建模型、兼容混合隐式场的语义表面提取方法（通过粗到细的提议方案加速），以及基于视频扩散的纹理分解模块，这些技术实现了结构独立的角色生成，支持非破坏性编辑、物理合规动画和视线跟踪等下游任务。</p>
<p><strong>Abstract:</strong> We present StdGEN++, a novel and comprehensive system for generating high-fidelity, semantically decomposed 3D characters from diverse inputs. Existing 3D generative methods often produce monolithic meshes that lack the structural flexibility required by industrial pipelines in gaming and animation. Addressing this gap, StdGEN++ is built upon a Dual-branch Semantic-aware Large Reconstruction Model (Dual-Branch S-LRM), which jointly reconstructs geometry, color, and per-component semantics in a feed-forward manner. To achieve production-level fidelity, we introduce a novel semantic surface extraction formalism compatible with hybrid implicit fields. This mechanism is accelerated by a coarse-to-fine proposal scheme, which significantly reduces memory footprint and enables high-resolution mesh generation. Furthermore, we propose a video-diffusion-based texture decomposition module that disentangles appearance into editable layers (e.g., separated iris and skin), resolving semantic confusion in facial regions. Experiments demonstrate that StdGEN++ achieves state-of-the-art performance, significantly outperforming existing methods in geometric accuracy and semantic disentanglement. Crucially, the resulting structural independence unlocks advanced downstream capabilities, including non-destructive editing, physics-compliant animation, and gaze tracking, making it a robust solution for automated character asset production.</p>
  </div>
</details>

<hr>
<h3 id="144-Variational-Contrastive-Learning-for-Skeleton-based-Action-Recognition-cs-CV-cs-AIPDF"><a href="#144-Variational-Contrastive-Learning-for-Skeleton-based-Action-Recognition-cs-CV-cs-AIPDF" class="headerlink" title="[144] Variational Contrastive Learning for Skeleton-based Action Recognition cs.CV | cs.AIPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07666">Variational Contrastive Learning for Skeleton-based Action Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07666" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dang Dinh Nguyen, Decky Aspandi Latif, Titus Zaharia</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种变分对比学习框架，用于基于骨架的动作识别，通过将概率潜在建模与对比自监督学习相结合，旨在解决现有对比方法难以捕捉人体运动内在可变性和不确定性的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于对比学习的自监督表示学习方法本质上是判别性的，难以有效捕捉人体运动的可变性和不确定性，因此需要一种能学习结构化、语义化表示的新框架。</p>
<p><strong>Result:</strong> 在三个广泛使用的骨架动作识别基准测试上进行的大量实验表明，该方法始终优于现有方法，特别是在低标签情况下表现突出。</p>
<p><strong>Insight:</strong> 创新点在于将变分推理引入对比学习，以概率方式建模潜在表示，从而学习到更具结构性和语义意义的特征，增强了对运动变化和不确定性的建模能力，并提高了在少标签场景下的泛化性能。</p>
<p><strong>Abstract:</strong> In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.</p>
  </div>
</details>

<hr>
<h3 id="145-Smooth-Operator-Smooth-Verifiable-Reward-Activates-Spatial-Reasoning-Ability-of-Vision-Language-Model-cs-CVPDF"><a href="#145-Smooth-Operator-Smooth-Verifiable-Reward-Activates-Spatial-Reasoning-Ability-of-Vision-Language-Model-cs-CVPDF" class="headerlink" title="[145] Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model cs.CVPDF"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07695">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07695" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为平滑数值奖励激活（SNRA）算子和绝对保留GRPO（AP-GRPO）框架的新方法，旨在解决视觉语言模型在3D场景理解中进行精确数值预测时面临的奖励稀疏和梯度不稳定问题。该方法通过动态参数化的Sigmoid函数将原始反馈转化为密集的连续奖励，并结合绝对标量梯度来缓解传统相对排名机制中的数值信息损失。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统基于相对排名的强化学习方法在3D场景理解中存在严重的奖励稀疏和梯度不稳定问题，无法有效利用3D物理约束提供的可验证信号，特别是在GRPO框架中，相对归一化会导致’近失’样本的优势崩溃，造成有价值边界样本在优化中被丢弃的数据利用瓶颈。</p>
<p><strong>Result:</strong> 通过构建包含5万个可验证3D子任务的Numerical3D-50k数据集进行实证，结果表明AP-GRPO在保持更高数据效率的同时，达到了与大规模监督方法相当的性能水平，有效激活了VLM中潜在的3D推理能力，且无需修改模型架构。</p>
<p><strong>Insight:</strong> 创新点在于提出了SNRA算子将稀疏奖励平滑化为连续奖励，以及AP-GRPO框架通过整合绝对标量梯度来保留数值信息，从而克服了传统相对排名机制的局限性，为在视觉语言模型中激活精确的3D空间推理提供了一种高效且无需改变模型结构的新途径。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes “near-miss” samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</p>
  </div>
</details>

<hr>
<h3 id="146-Evaluating-the-encoding-competence-of-visual-language-models-using-uncommon-actions-cs-CV-cs-AIPDF"><a href="#146-Evaluating-the-encoding-competence-of-visual-language-models-using-uncommon-actions-cs-CV-cs-AIPDF" class="headerlink" title="[146] Evaluating the encoding competence of visual language models using uncommon actions cs.CV | cs.AIPDF"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07737">Evaluating the encoding competence of visual language models using uncommon actions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07737" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chen Ling, Nai Ding</span></p>
<p><strong>TL;DR:</strong> 该论文提出了UAIT（非常识动作图像-文本）数据集，这是一个用于评估视觉语言模型在非常识动作场景中语义理解能力的新基准。通过结合大语言模型、少样本提示工程和文本到图像生成技术半自动合成高质量样本，并设计多项选择题测试模型的细粒度推理能力。实验表明，现有最先进的视觉语言模型在语义判断上显著落后于人类，尤其是在区分语法正确性与语义合理性方面，但轻量级模型经过微调后能有效提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型评估多关注常见视觉场景，缺乏对模型深层语义理解（如施受关系和物理可行性）的测试，因此需要构建一个专门针对非常识动作场景的基准来揭示模型的关键弱点。</p>
<p><strong>Result:</strong> 在UAIT基准上评估了多种最先进的视觉语言模型（包括基于对比学习的模型），所有模型在语义判断上的表现均显著低于人类水平；轻量级模型经过定向微调后准确率得到提升，显示了适应性潜力。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个专注于测试视觉语言模型对非常识动作语义理解的数据集，通过半自动化流程合成高质量对抗性样本；研究揭示了模型在区分语法形式与语义合理性方面的根本缺陷，并为开发具有鲁棒视觉语义推理能力的模型提供了诊断工具和方向。</p>
<p><strong>Abstract:</strong> We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model’s competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.</p>
  </div>
</details>

<hr>
<h3 id="147-Video-Evidence-to-Reasoning-Efficient-Video-Understanding-via-Explicit-Evidence-Grounding-cs-CVPDF"><a href="#147-Video-Evidence-to-Reasoning-Efficient-Video-Understanding-via-Explicit-Evidence-Grounding-cs-CVPDF" class="headerlink" title="[147] Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding cs.CVPDF"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07761">Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07761" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yanxiang Huang, Guohua Gao, Zhaoyang Wei, Jianyuan Ni</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为证据链（CoE）的新框架，旨在解决大型视觉语言模型在视频推理中面临的效率与幻觉风险之间的根本困境。该框架通过架构性地解耦和协同优化感知接地与推理效率，引入了一个轻量级的证据接地模块来动态提取高保真视觉证据，并采用强化学习优化的证据锚定协议来强制过程对齐，从而减少幻觉。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型视觉语言模型在视频推理中存在两难：冗长的推理计算成本过高，而高效的、未接地的推理方法又存在幻觉风险。本文旨在通过显式证据接地来解决这一矛盾，实现高效且可靠的视频理解。</p>
<p><strong>Result:</strong> 在包括Video-MME、MVBench和VSI-Bench在内的五个基准测试上进行的广泛实验表明，CoE增强的模型建立了新的最先进水平，在准确性上显著优于现有方法。</p>
<p><strong>Insight:</strong> 核心创新点在于架构性地解耦感知接地与推理过程，并引入轻量级证据接地模块和基于强化学习的证据锚定协议。从客观角度看，其通过强制模型在推理过程中严格引用已识别的时序证据锚点来缓解幻觉，并构建了大规模的双标注指令数据集进行监督，为可靠视频理解提供了一个新颖且实用的范式。</p>
<p><strong>Abstract:</strong> Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.</p>
  </div>
</details>

<hr>
<h3 id="148-Beyond-External-Guidance-Unleashing-the-Semantic-Richness-Inside-Diffusion-Transformers-for-Improved-Training-cs-CVPDF"><a href="#148-Beyond-External-Guidance-Unleashing-the-Semantic-Richness-Inside-Diffusion-Transformers-for-Improved-Training-cs-CVPDF" class="headerlink" title="[148] Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training cs.CVPDF"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07773">Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07773" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lingchen Sun, Rongyuan Wu, Zhengqiang Zhang, Ruibin Li, Yujing Sun</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Self-Transcendence的新方法，用于加速扩散变换器（DiT）的训练。该方法无需依赖外部预训练模型，而是通过内部特征监督，先利用VAE潜表示对齐浅层特征，再通过分类器引导增强中间特征的语义表达能力，最终利用这些内部特征指导新的DiT训练，从而实现了快速收敛和高生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法如REPA依赖外部语义特征（如DINO）来指导DiT训练，这引入了额外的依赖并降低了灵活性。本文旨在证明DiT自身具有自我指导的能力，并解决其训练中浅层表示学习困难导致的收敛缓慢问题。</p>
<p><strong>Result:</strong> 该方法在生成质量和收敛速度上超越了现有的自包含方法，甚至可与依赖外部模型的REPA方法相媲美，且无需任何外部预训练模型。</p>
<p><strong>Insight:</strong> 核心创新在于利用模型内部学习到的丰富语义特征进行自我监督，避免了对外部网络的依赖。关键洞察是DiT训练瓶颈在于浅层，通过分阶段（先VAE对齐，后分类器引导）的内部特征增强策略，可以有效提升训练效率和模型性能，该方法具有更好的灵活性和更广泛的扩散生成任务应用潜力。</p>
<p><strong>Abstract:</strong> Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at <a target="_blank" rel="noopener" href="https://github.com/csslc/Self-Transcendence">https://github.com/csslc/Self-Transcendence</a>.</p>
  </div>
</details>

<hr>
<h3 id="149-Vision-Language-Model-for-Accurate-Crater-Detection-cs-CVPDF"><a href="#149-Vision-Language-Model-for-Accurate-Crater-Detection-cs-CVPDF" class="headerlink" title="[149] Vision-Language Model for Accurate Crater Detection cs.CVPDF"></a>[149] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07795">Vision-Language Model for Accurate Crater Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07795" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Patrick Bauer, Marius Schwinning, Florian Renk, Andreas Weinmann, Hichem Snoussi</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于OWLv2视觉语言模型的深度学习陨石坑检测算法，用于在具有挑战性的月球成像条件下实现可靠的陨石坑检测。该方法采用参数高效的LoRA微调策略，并结合CIoU定位损失和对比损失进行优化，在IMPACT项目的高分辨率月球图像测试集上取得了最高94.0%的召回率和73.1%的精确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 欧洲航天局（ESA）为保障其Argonaut着陆器月球任务的安全着陆，需要可靠的陨石坑检测技术，因为陨石坑对安全着陆构成风险。由于陨石坑数量庞大、形状大小各异，且成像条件（如光照变化、崎岖地形）极具挑战，传统的自动化检测算法效果有限。</p>
<p><strong>Result:</strong> 在IMPACT项目提供的测试数据集上，该方法取得了最高94.0%的召回率和73.1%的精确率，并获得了令人满意的视觉检测结果。</p>
<p><strong>Insight:</strong> 创新点在于将强大的通用视觉语言模型OWLv2（基于Vision Transformer）适配到特定的陨石坑检测任务，并采用参数高效的LoRA微调策略，结合了针对目标检测的CIoU损失和用于提升特征判别力的对比损失，从而在极具挑战的月球环境下实现了可靠检测。这为将大规模预训练视觉模型高效迁移到特定、数据有限的遥感领域任务提供了借鉴。</p>
<p><strong>Abstract:</strong> The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.</p>
  </div>
</details>

<hr>
<h3 id="150-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes-cs-CVPDF"><a href="#150-More-Images-More-Problems-A-Controlled-Analysis-of-VLM-Failure-Modes-cs-CVPDF" class="headerlink" title="[150] More Images, More Problems? A Controlled Analysis of VLM Failure Modes cs.CVPDF"></a>[150] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07812">More Images, More Problems? A Controlled Analysis of VLM Failure Modes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07812" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele</span></p>
<p><strong>TL;DR:</strong> 本文介绍了MIMIC基准测试，用于严格评估大型视觉语言模型（LVLM）的多图像理解能力，揭示了LVLM在跨图像信息聚合和多概念跟踪方面的普遍问题，并提出了数据生成和注意力掩码两种改进方法，显著提升了模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究对LVLM在多图像理解和推理方面的能力评估不足，缺乏对其核心弱点的系统分析，因此需要构建专门的基准测试来诊断和解决这些问题。</p>
<p><strong>Result:</strong> 在MIMIC基准上的实验表明，所提方法显著改善了跨图像信息聚合，并在现有多图像基准测试中超越了先前的最先进（SOTA）水平。</p>
<p><strong>Insight:</strong> 创新点包括设计MIMIC基准进行系统性诊断，以及提出基于单图像注释组合的数据生成策略和针对多图像输入的层间注意力掩码方案，为提升LVLM多图像能力提供了可借鉴的数据和优化思路。</p>
<p><strong>Abstract:</strong> Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at <a target="_blank" rel="noopener" href="https://github.com/anurag-198/MIMIC">https://github.com/anurag-198/MIMIC</a>.</p>
  </div>
</details>

<hr>
<h3 id="151-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head-cs-CV-cs-AIPDF"><a href="#151-MHLA-Restoring-Expressivity-of-Linear-Attention-via-Token-Level-Multi-Head-cs-CV-cs-AIPDF" class="headerlink" title="[151] MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head cs.CV | cs.AIPDF"></a>[151] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07832">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07832" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了多头线性注意力（MHLA）机制，旨在解决线性注意力因全局上下文坍缩而导致的性能下降问题。MHLA通过在token维度上划分多头并计算注意力，在保持线性复杂度的同时恢复了softmax注意力的表达能力，并在图像分类、自然语言处理、图像生成和视频生成等多个领域验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Transformer架构的自注意力机制具有二次复杂度，限制了其在大规模应用中的使用。线性注意力虽然高效，但直接应用会导致性能下降，而现有改进方法通常通过引入额外模块（如深度可分离卷积）重新引入计算开销，违背了初衷。本文旨在解决线性注意力中的全局上下文坍缩问题，即模型失去表示多样性的关键故障模式。</p>
<p><strong>Result:</strong> 在相同时间复杂度下，MHLA在多个基准测试中取得了显著提升：ImageNet分类准确率提升3.6%，自然语言处理任务提升6.3%，图像生成任务提升12.6%，视频生成任务提升41%。</p>
<p><strong>Insight:</strong> 论文的创新点在于识别了线性注意力中的全局上下文坍缩问题，并提出了通过token维度划分多头来保持表示多样性的MHLA机制。从客观角度看，该方法在理论上证明了线性复杂度的保持和表达能力的恢复，并通过跨领域实验验证了其普适性和高效性，为设计高效且高性能的注意力机制提供了新思路。</p>
<p><strong>Abstract:</strong> While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 6.3% gain on NLP, a 12.6% improvement on image generation, and a 41% enhancement on video generation under the same time complexity.</p>
  </div>
</details>

<hr>
<h3 id="152-Tuning-free-Visual-Effect-Transfer-across-Videos-cs-CVPDF"><a href="#152-Tuning-free-Visual-Effect-Transfer-across-Videos-cs-CVPDF" class="headerlink" title="[152] Tuning-free Visual Effect Transfer across Videos cs.CVPDF"></a>[152] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07833">Tuning-free Visual Effect Transfer across Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07833" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maxwell Jones, Rameen Abdal, Or Patashnik, Ruslan Salakhutdinov, Sergey Tulyakov</span></p>
<p><strong>TL;DR:</strong> RefVFX是一个无需调优的前馈框架，能够将参考视频中的复杂时序效果（如动态光照变化或角色变换）直接迁移到目标视频或图像上。该方法通过构建大规模三元组数据集（参考效果视频、输入图像&#x2F;视频、输出效果视频）并基于文本到视频骨干网络训练，实现了视觉一致且时序连贯的编辑效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法擅长基于文本提示或关键帧条件的编辑，但难以处理动态时序效果，因为这些效果难以用文本或静态条件描述。迁移视频效果需要模型将新的时序动态与输入视频的现有运动和外观整合，这是一个挑战。</p>
<p><strong>Result:</strong> 实验结果表明，RefVFX在定量指标和人类偏好评估中均优于仅基于提示的基线方法，能够生成视觉一致和时序连贯的编辑效果，并在未见过的效果类别上展现出良好的泛化能力。</p>
<p><strong>Insight:</strong> 创新点包括：提出大规模三元组数据集（通过自动化管道生成高质量配对视频，并结合LoRA适配器和基于代码的时序效果进行数据增强），以及基于参考条件训练的前馈模型，实现了无需调优的复杂时序效果迁移，解决了动态效果描述和整合的难题。</p>
<p><strong>Abstract:</strong> We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video’s existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input’s motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\href{<a target="_blank" rel="noopener" href="https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/%7D%7Bat/">https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\</a> this\ URL}$.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="153-Deep-Joint-Source-Channel-Coding-for-Wireless-Video-Transmission-with-Asymmetric-Context-eess-IV-cs-CVPDF"><a href="#153-Deep-Joint-Source-Channel-Coding-for-Wireless-Video-Transmission-with-Asymmetric-Context-eess-IV-cs-CVPDF" class="headerlink" title="[153] Deep Joint Source-Channel Coding for Wireless Video Transmission with Asymmetric Context eess.IV | cs.CVPDF"></a>[153] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06170">Deep Joint Source-Channel Coding for Wireless Video Transmission with Asymmetric Context</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06170" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuechen Chen, Junting Li, Chuang Chen, Hairong Lin, Yishen Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于非对称上下文条件编码的高效深度联合信源信道编码（JSCC）方法，用于无线视频传输。该方法通过引导神经网络从非对称上下文中学习编码和解码条件，并引入特征传播来利用时间相关性并减轻误差累积问题。此外，还实现了内容自适应编码以支持可变带宽传输。实验表明，该方法在性能上优于现有深度视频传输框架，并能有效减轻误差累积，从而减少插入帧内编码模式的频率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在基于条件编码的神经视频压缩中，编码器和解码器需要从相同的上下文（包括相同的重建帧）预测条件。然而，在属于伪模拟传输的JSCC方案中，即使编码器构建了模拟传输管道，也无法推断出与解码器相同的重建帧。这导致了传统条件编码方法在JSCC中应用的限制，需要解决非对称上下文下的条件学习问题。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在性能上优于现有的深度视频传输框架，并有效减轻了误差累积。通过减轻误差累积，该方案可以减少插入帧内编码模式的频率，从而进一步提升性能。</p>
<p><strong>Insight:</strong> 创新点包括：1) 提出非对称上下文条件编码，使神经网络能在编码器和解码器上下文不同的情况下学习条件；2) 引入特征传播机制，允许中间特征在编码器和解码器独立传播，以帮助生成条件并利用时间相关性；3) 实现内容自适应编码，通过熵模型和掩码机制支持可变带宽传输。从客观角度看，该方法巧妙地将条件编码与JSCC结合，解决了伪模拟传输中的上下文不对称问题，并通过特征传播减少误差累积，提升了传输鲁棒性和效率。</p>
<p><strong>Abstract:</strong> In this paper, we propose a high-efficiency deep joint source-channel coding (JSCC) method for video transmission based on conditional coding with asymmetric context. The conditional coding-based neural video compression requires to predict the encoding and decoding conditions from the same context which includes the same reconstructed frames. However in JSCC schemes which fall into pseudo-analog transmission, the encoder cannot infer the same reconstructed frames as the decoder even a pipeline of the simulated transmission is constructed at the encoder. In the proposed method, without such a pipeline, we guide and design neural networks to learn encoding and decoding conditions from asymmetric contexts. Additionally, we introduce feature propagation, which allows intermediate features to be independently propagated at the encoder and decoder and help to generate conditions, enabling the framework to greatly leverage temporal correlation while mitigating the problem of error accumulation. To further exploit the performance of the proposed transmission framework, we implement content-adaptive coding which achieves variable bandwidth transmission using entropy models and masking mechanisms. Experimental results demonstrate that our method outperforms existing deep video transmission frameworks in terms of performance and effectively mitigates the error accumulation. By mitigating the error accumulation, our schemes can reduce the frequency of inserting intra-frame coding modes, further enhancing performance.</p>
  </div>
</details>

<hr>
<h3 id="154-Real-Time-Image-Processing-Algorithms-for-Embedded-Systems-eess-IV-cs-AI-cs-CVPDF"><a href="#154-Real-Time-Image-Processing-Algorithms-for-Embedded-Systems-eess-IV-cs-AI-cs-CVPDF" class="headerlink" title="[154] Real-Time Image Processing Algorithms for Embedded Systems eess.IV | cs.AI | cs.CVPDF"></a>[154] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06243">Real-Time Image Processing Algorithms for Embedded Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Soundes Oumaima Boufaida, Abdemadjid Benmachiche, Majda Maatallah</span></p>
<p><strong>TL;DR:</strong> 本研究针对嵌入式视觉系统在资源受限硬件上实现实时图像处理的需求，探讨了边缘检测、角点检测和斑点检测等算法的优化实现。通过采用优化的算法架构、量化技术、帧间冗余消除和自适应帧平均等方法，在DSP和FPGA等嵌入式处理器上提升了处理速度与能效。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决嵌入式视觉系统在实时图像处理中面临的延迟、精度和功耗挑战，以满足汽车、监控和机器人等领域对高效、低成本嵌入式成像系统的需求。</p>
<p><strong>Result:</strong> 仿真和硬件试验表明，所提方法在速度和能效上相比传统实现有显著提升，为实际实时嵌入式视觉应用提供了可扩展且经济的解决方案。</p>
<p><strong>Insight:</strong> 创新点在于算法与硬件架构的协同设计，以及通过量化、冗余消除和自适应处理等技术优化资源使用，这为嵌入式图像处理系统的开发提供了可借鉴的软硬件协同优化思路。</p>
<p><strong>Abstract:</strong> Embedded vision systems need efficient and robust image processing algorithms to perform real-time, with resource-constrained hardware. This research investigates image processing algorithms, specifically edge detection, corner detection, and blob detection, that are implemented on embedded processors, including DSPs and FPGAs. To address latency, accuracy and power consumption noted in the image processing literature, optimized algorithm architectures and quantization techniques are employed. In addition, optimal techniques for inter-frame redundancy removal and adaptive frame averaging are used to improve throughput with reasonable image quality. Simulations and hardware trials of the proposed approaches show marked improvements in the speed and energy efficiency of processing as compared to conventional implementations. The advances of this research facilitate a path for scalable and inexpensive embedded imaging systems for the automotive, surveillance, and robotics sectors, and underscore the benefit of co-designing algorithms and hardware architectures for practical real-time embedded vision applications.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="155-PixRec-Leveraging-Visual-Context-for-Next-Item-Prediction-in-Sequential-Recommendation-cs-IR-cs-CV-cs-LGPDF"><a href="#155-PixRec-Leveraging-Visual-Context-for-Next-Item-Prediction-in-Sequential-Recommendation-cs-IR-cs-CV-cs-LGPDF" class="headerlink" title="[155] PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation cs.IR | cs.CV | cs.LGPDF"></a>[155] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06458">PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06458" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sayak Chakrabarty, Souradip Pal</span></p>
<p><strong>TL;DR:</strong> 本文提出PixRec，一种结合文本属性和产品图像的视觉语言框架，用于序列推荐任务。该框架利用视觉语言模型骨干联合处理图像-文本序列，通过双塔结构和混合训练目标对齐多模态特征投影，在亚马逊评论数据集上相比纯文本推荐器在top-rank和top-10准确率上分别提升3倍和40%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于大语言模型的序列推荐方法仅使用文本信息，忽略了电商等实际场景中丰富的视觉信息，无法区分文本描述相似的商品。</p>
<p><strong>Result:</strong> 在增强产品图像的亚马逊评论数据集上，PixRec在top-rank准确率提升3倍，top-10准确率提升40%，优于纯文本推荐器。</p>
<p><strong>Insight:</strong> 创新点在于将视觉信息整合到序列推荐中，通过视觉语言模型对齐多模态特征，解决文本相似商品的区分问题；客观来看，其双塔结构和混合训练目标为多模态推荐系统提供了可扩展的架构参考。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce.</p>
  </div>
</details>

<hr>
<h3 id="156-ReinPool-Reinforcement-Learning-Pooling-Multi-Vector-Embeddings-for-Retrieval-System-cs-IR-cs-CL-cs-CVPDF"><a href="#156-ReinPool-Reinforcement-Learning-Pooling-Multi-Vector-Embeddings-for-Retrieval-System-cs-IR-cs-CL-cs-CVPDF" class="headerlink" title="[156] ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System cs.IR | cs.CL | cs.CVPDF"></a>[156] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07125">ReinPool: Reinforcement Learning Pooling Multi-Vector Embeddings for Retrieval System</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07125" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sungguk Cha, DongWook Kim, Mintae Kim, Youngsub Han, Byoung-Ki Jeon</span></p>
<p><strong>TL;DR:</strong> 本文提出了ReinPool，一种基于强化学习的框架，用于动态筛选和池化多向量嵌入模型中的token级表示，将其压缩为紧凑的检索优化表示，以解决多向量嵌入在文档检索中索引存储成本过高的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多向量嵌入模型虽然能保留细粒度的视觉和文本细节，但存储每个token的嵌入会导致索引大小相比单向量方法膨胀超过1000倍，严重限制了可扩展性。</p>
<p><strong>Result:</strong> 在Vidore V2基准测试中，针对三种视觉语言嵌入模型，ReinPool将多向量表示压缩了746至1249倍成单向量，同时恢复了76-81%的完整多向量检索性能；相比静态平均池化基线，NDCG@3绝对提升了22-33%。</p>
<p><strong>Insight:</strong> 创新点在于使用强化学习框架，通过逆向检索目标和基于NDCG的奖励，自动学习筛选最具区分性的向量，无需人工重要性标注，实现了检索性能与存储效率的优化平衡。</p>
<p><strong>Abstract:</strong> Multi-vector embedding models have emerged as a powerful paradigm for document retrieval, preserving fine-grained visual and textual details through token-level representations. However, this expressiveness comes at a staggering cost: storing embeddings for every token inflates index sizes by over $1000\times$ compared to single-vector approaches, severely limiting scalability. We introduce \textbf{ReinPool}, a reinforcement learning framework that learns to dynamically filter and pool multi-vector embeddings into compact, retrieval-optimized representations. By training with an inverse retrieval objective and NDCG-based rewards, ReinPool identifies and retains only the most discriminative vectors without requiring manual importance annotations. On the Vidore V2 benchmark across three vision-language embedding models, ReinPool compresses multi-vector representations by $746$–$1249\times$ into single vectors while recovering 76–81% of full multi-vector retrieval performance. Compared to static mean pooling baselines, ReinPool achieves 22–33% absolute NDCG@3 improvement, demonstrating that learned selection significantly outperforms heuristic aggregation.</p>
  </div>
</details>

<hr>
<div id='cs.CY'></div>

<h1 id="cs-CY-Back"><a href="#cs-CY-Back" class="headerlink" title="cs.CY [Back]"></a>cs.CY <a href="#toc">[Back]</a></h1><h3 id="157-Using-street-view-images-and-visual-LLMs-to-predict-heritage-values-for-governance-support-Risks-ethics-and-policy-implications-cs-CY-cs-AI-cs-CVPDF"><a href="#157-Using-street-view-images-and-visual-LLMs-to-predict-heritage-values-for-governance-support-Risks-ethics-and-policy-implications-cs-CY-cs-AI-cs-CVPDF" class="headerlink" title="[157] Using street view images and visual LLMs to predict heritage values for governance support: Risks, ethics, and policy implications cs.CY | cs.AI | cs.CVPDF"></a>[157] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06056">Using street view images and visual LLMs to predict heritage values for governance support: Risks, ethics, and policy implications</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06056" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tim Johansson, Mikael Mangold, Kristina Dabrock, Anna Donarelli, Ingrid Campo-Ruiz</span></p>
<p><strong>TL;DR:</strong> 本研究旨在利用街景图像和多模态大语言模型（LLM）来预测瑞典建筑的历史遗产价值，以支持国家建筑改造计划的制定。研究分析了瑞典全国154,710张街景图像，通过零样本预测识别了约500万平方米供暖建筑面积的潜在遗产建筑，并讨论了基于LLM的数据在治理应用中存在的透明度、错误检测和谄媚性等风险与伦理问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 欧盟《建筑能效指令》要求成员国制定国家建筑改造计划，但瑞典缺乏全国性的遗产建筑登记册，这被视为制定改造计划分析工作的障碍。本研究旨在协助瑞典当局评估建筑存量中的遗产价值。</p>
<p><strong>Result:</strong> 研究使用多模态LLM对瑞典全国154,710张街景图像进行了零样本预测，为瑞典建筑改造计划识别了500万平方米供暖建筑面积的潜在遗产建筑。</p>
<p><strong>Insight:</strong> 创新点在于将视觉LLM和街景图像结合，为零样本预测建筑遗产价值提供了一种自动化方法，以支持大规模治理决策。从客观角度看，研究强调了将AI技术应用于公共政策领域时，必须考虑透明度、错误检测和模型谄媚性等风险与伦理问题，这对类似应用具有重要借鉴意义。</p>
<p><strong>Abstract:</strong> During 2025 and 2026, the Energy Performance of Buildings Directive is being implemented in the European Union member states, requiring all member states to have National Building Renovation Plans. In Sweden, there is a lack of a national register of buildings with heritage values. This is seen as a barrier for the analyses underlying the development of Building Renovation Plans by the involved Swedish authorities. The purpose of this research was to assist Swedish authorities in assigning heritage values to building in the Swedish building stock. As part of the analyses, buildings in street view images from all over Sweden (N&#x3D;154 710) have been analysed using multimodal Large Language Models (LLM) to assess aspects of heritage value. Zero-shot predictions by LLMs were used as a basis to for identifying buildings with potential heritage values for 5.0 million square meters of heated floor area for the Swedish Building Renovation Plan. In this paper, the results of the predictions and lessons learnt are presented and related to the development of Swedish Building Renovation Plan as part of governance. Potential risks for authorities using LLM-based data are addressed, with a focus on issues of transparency, error detection and sycophancy.</p>
  </div>
</details>

<hr>
<h3 id="158-On-Narrative-The-Rhetorical-Mechanisms-of-Online-Polarisation-cs-CY-cs-CL-cs-SIPDF"><a href="#158-On-Narrative-The-Rhetorical-Mechanisms-of-Online-Polarisation-cs-CY-cs-CL-cs-SIPDF" class="headerlink" title="[158] On Narrative: The Rhetorical Mechanisms of Online Polarisation cs.CY | cs.CL | cs.SIPDF"></a>[158] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07398">On Narrative: The Rhetorical Mechanisms of Online Polarisation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.CL | cs.SI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07398" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jan Elfes, Marco Bastos, Luca Maria Aiello</span></p>
<p><strong>TL;DR:</strong> 本文提出叙事极化概念，通过分析212个YouTube视频和90,029条评论，研究以色列-巴以冲突中对立群体如何构建和协商对现实的解释。研究发现，视频内容产生高度极化的叙事，而评论在表面层面减少了叙事极化，但在深层叙事层面，反复出现的叙事主题揭示了群体间的额外差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决极化研究中未探索的问题：对立群体如何集体构建和协商对现实的解释，以及叙事是否在互动有限的群体间传播。</p>
<p><strong>Result:</strong> 在以色列-巴以冲突的YouTube数据上，视频产生高度极化叙事，评论表面减少极化但深层叙事主题揭示额外差异。</p>
<p><strong>Insight:</strong> 创新点在于形式化叙事极化概念，结合结构叙事理论和大型语言模型提取叙事角色，揭示表面与深层叙事层面的极化差异。</p>
<p><strong>Abstract:</strong> Polarisation research has demonstrated how people cluster in homogeneous groups with opposing opinions. However, this effect emerges not only through interaction between people, limiting communication between groups, but also between narratives, shaping opinions and partisan identities. Yet, how polarised groups collectively construct and negotiate opposing interpretations of reality, and whether narratives move between groups despite limited interactions, remains unexplored. To address this gap, we formalise the concept of narrative polarisation and demonstrate its measurement in 212 YouTube videos and 90,029 comments on the Israeli-Palestinian conflict. Based on structural narrative theory and implemented through a large language model, we extract the narrative roles assigned to central actors in two partisan information environments. We find that while videos produce highly polarised narratives, comments significantly reduce narrative polarisation, harmonising discourse on the surface level. However, on a deeper narrative level, recurring narrative motifs reveal additional differences between partisan groups.</p>
  </div>
</details>

<hr>
<div id='cs.SE'></div>

<h1 id="cs-SE-Back"><a href="#cs-SE-Back" class="headerlink" title="cs.SE [Back]"></a>cs.SE <a href="#toc">[Back]</a></h1><h3 id="159-Attention-Mechanism-and-Heuristic-Approach-Context-Aware-File-Ranking-Using-Multi-Head-Self-Attention-cs-SE-cs-AI-cs-CLPDF"><a href="#159-Attention-Mechanism-and-Heuristic-Approach-Context-Aware-File-Ranking-Using-Multi-Head-Self-Attention-cs-SE-cs-AI-cs-CLPDF" class="headerlink" title="[159] Attention Mechanism and Heuristic Approach: Context-Aware File Ranking Using Multi-Head Self-Attention cs.SE | cs.AI | cs.CLPDF"></a>[159] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06185">Attention Mechanism and Heuristic Approach: Context-Aware File Ranking Using Multi-Head Self-Attention</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SE | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06185" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pradeep Kumar Sharma, Shantanu Godbole, Sarada Prasad Jena, Hritvik Shrivastava</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合启发式方法和多头自注意力机制的文件排序方法，用于软件变更影响分析中的受影响文件识别与排序。该方法通过学习特征间的上下文依赖关系，动态调整文件重要性权重，从而提升Top-50召回率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于启发式信号、语义相似度和图中心性度量的确定性方法在召回率上存在瓶颈，因为它们将特征视为线性独立贡献者，忽略了特征间的上下文依赖关系。</p>
<p><strong>Result:</strong> 在200个测试案例上的实验表明，引入自注意力机制将Top-50召回率从约62-65%提升至78-82%（取决于仓库复杂度），在Top-50文件中达到80%召回率；主观准确性对齐评分从6.5&#x2F;10提升至8.6&#x2F;10。</p>
<p><strong>Insight:</strong> 创新点在于将多头自注意力机制作为后确定性评分细化机制，学习特征间的上下文权重，动态调整文件重要性，在保持可解释性的同时模拟专家推理模式，有效弥补确定性自动化与专家判断之间的推理能力差距。</p>
<p><strong>Abstract:</strong> The identification and ranking of impacted files within software reposi-tories is a key challenge in change impact analysis. Existing deterministic approaches that combine heuristic signals, semantic similarity measures, and graph-based centrality metrics have demonstrated effectiveness in nar-rowing candidate search spaces, yet their recall plateaus. This limitation stems from the treatment of features as linearly independent contributors, ignoring contextual dependencies and relationships between metrics that characterize expert reasoning patterns. To address this limitation, we propose the application of Multi-Head Self-Attention as a post-deterministic scoring refinement mechanism. Our approach learns contextual weighting between features, dynamically adjust-ing importance levels per file based on relational behavior exhibited across candidate file sets. The attention mechanism produces context-aware adjustments that are additively combined with deterministic scores, pre-serving interpretability while enabling reasoning similar to that performed by experts when reviewing change surfaces. We focus on recall rather than precision, as false negatives (missing impacted files) are far more costly than false positives (irrelevant files that can be quickly dismissed during review). Empirical evaluation on 200 test cases demonstrates that the introduc-tion of self-attention improves Top-50 recall from approximately 62-65% to between 78-82% depending on repository complexity and structure, achiev-ing 80% recall at Top-50 files. Expert validation yields improvement from 6.5&#x2F;10 to 8.6&#x2F;10 in subjective accuracy alignment. This transformation bridges the reasoning capability gap between deterministic automation and expert judgment, improving recall in repository-aware effort estimation.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="160-VIPER-Strike-Defeating-Visual-Reasoning-CAPTCHAs-via-Structured-Vision-Language-Inference-cs-CR-cs-CV-cs-ETPDF"><a href="#160-VIPER-Strike-Defeating-Visual-Reasoning-CAPTCHAs-via-Structured-Vision-Language-Inference-cs-CR-cs-CV-cs-ETPDF" class="headerlink" title="[160] VIPER Strike: Defeating Visual Reasoning CAPTCHAs via Structured Vision-Language Inference cs.CR | cs.CV | cs.ETPDF"></a>[160] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06461">VIPER Strike: Defeating Visual Reasoning CAPTCHAs via Structured Vision-Language Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CV | cs.ET</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06461" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minfeng Qi, Dongyang He, Qin Wang, Lefeng Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出ViPer框架，一种结合结构化多目标视觉感知与自适应大语言模型推理的统一攻击方法，用于破解视觉推理验证码。该方法在六个主流VRC提供商上达到最高93.2%的成功率，接近人类水平，并优于现有基线。论文还提出了模板空间随机化防御策略以降低攻击成功率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉推理验证码破解方法存在局限性：视觉中心方法依赖特定模板检测器，无法泛化到新布局；推理中心方法虽利用大语言模型，但细粒度视觉感知能力不足。两者均缺乏处理异构VRC部署的通用性。</p>
<p><strong>Result:</strong> 在六个主要VRC提供商（VTT、Geetest、网易、顶象、数美、小盾）的基准测试中，ViPer成功率最高达93.2%，接近人类水平，并优于GraphNet（83.2%）、Oedipus（65.8%）和Holistic方法（89.5%）等基线。该框架在不同大语言模型骨干（GPT、Grok、DeepSeek、Kimi）上均保持90%以上的准确率，展现了鲁棒性。</p>
<p><strong>Insight:</strong> 创新点在于将结构化视觉布局解析与基于大语言模型的自适应推理模块化集成，实现了对物体、属性和空间关系的组合推理。从防御角度提出的模板空间随机化策略，通过扰动语言模板而不改变任务语义，为设计人类可解但机器抵抗的验证码提供了方向。</p>
<p><strong>Abstract:</strong> Visual Reasoning CAPTCHAs (VRCs) combine visual scenes with natural-language queries that demand compositional inference over objects, attributes, and spatial relations. They are increasingly deployed as a primary defense against automated bots. Existing solvers fall into two paradigms: vision-centric, which rely on template-specific detectors but fail on novel layouts, and reasoning-centric, which leverage LLMs but struggle with fine-grained visual perception. Both lack the generality needed to handle heterogeneous VRC deployments.   We present ViPer, a unified attack framework that integrates structured multi-object visual perception with adaptive LLM-based reasoning. ViPer parses visual layouts, grounds attributes to question semantics, and infers target coordinates within a modular pipeline. Evaluated on six major VRC providers (VTT, Geetest, NetEase, Dingxiang, Shumei, Xiaodun), ViPer achieves up to 93.2% success, approaching human-level performance across multiple benchmarks. Compared to prior solvers, GraphNet (83.2%), Oedipus (65.8%), and the Holistic approach (89.5%), ViPer consistently outperforms all baselines. The framework further maintains robustness across alternative LLM backbones (GPT, Grok, DeepSeek, Kimi), sustaining accuracy above 90%.   To anticipate defense, we further introduce Template-Space Randomization (TSR), a lightweight strategy that perturbs linguistic templates without altering task semantics. TSR measurably reduces solver (i.e., attacker) performance. Our proposed design suggests directions for human-solvable but machine-resistant CAPTCHAs.</p>
  </div>
</details>

<hr>
<h3 id="161-qAttCNN-Self-Attention-Mechanism-for-Video-QoE-Prediction-in-Encrypted-Traffic-cs-CR-cs-CV-cs-LG-cs-MM-eess-IVPDF"><a href="#161-qAttCNN-Self-Attention-Mechanism-for-Video-QoE-Prediction-in-Encrypted-Traffic-cs-CR-cs-CV-cs-LG-cs-MM-eess-IVPDF" class="headerlink" title="[161] qAttCNN - Self Attention Mechanism for Video QoE Prediction in Encrypted Traffic cs.CR | cs.CV | cs.LG | cs.MM | eess.IVPDF"></a>[161] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06862">qAttCNN - Self Attention Mechanism for Video QoE Prediction in Encrypted Traffic</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CV | cs.LG | cs.MM | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06862" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Michael Sidorov, Ofer Hadar</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为qAttCNN的模型，利用加密视频流量的包大小参数来预测无参考QoE指标（BRISQUE和FPS），以解决互联网服务提供商因端到端加密而无法直接评估视频通话体验质量的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代视频会议和即时通讯应用普遍采用端到端加密，使得互联网服务提供商无法访问原始媒体流，只能依赖服务质量（QoS）和路由信息来评估用户体验质量（QoE），这限制了QoE的准确监控。</p>
<p><strong>Result:</strong> 在自定义的WhatsApp视频通话数据集上评估，qAttCNN在BRISQUE预测上的平均绝对误差百分比为2.14%，在FPS预测上为7.39%，优于现有QoE模型。</p>
<p><strong>Insight:</strong> 创新点在于结合自注意力机制和卷积神经网络，仅使用加密流量的包大小参数来推断QoE指标，为加密流量下的QoE预测提供了新方法，可借鉴于网络监控和优化场景。</p>
<p><strong>Abstract:</strong> The rapid growth of multimedia consumption, driven by major advances in mobile devices since the mid-2000s, has led to widespread use of video conferencing applications (VCAs) such as Zoom and Google Meet, as well as instant messaging applications (IMAs) like WhatsApp and Telegram, which increasingly support video conferencing as a core feature. Many of these systems rely on the Web Real-Time Communication (WebRTC) protocol, enabling direct peer-to-peer media streaming without requiring a third-party server to relay data, reducing the latency and facilitating a real-time communication. Despite WebRTC’s potential, adverse network conditions can degrade streaming quality and consequently reduce users’ Quality of Experience (QoE). Maintaining high QoE therefore requires continuous monitoring and timely intervention when QoE begins to deteriorate. While content providers can often estimate QoE by directly comparing transmitted and received media, this task is significantly more challenging for internet service providers (ISPs). End-to-end encryption, commonly used by modern VCAs and IMAs, prevent ISPs from accessing the original media stream, leaving only Quality of Service (QoS) and routing information available. To address this limitation, we propose the QoE Attention Convolutional Neural Network (qAttCNN), a model that leverages packet size parameter of the traffic to infer two no-reference QoE metrics viz. BRISQUE and frames per second (FPS). We evaluate qAttCNN on a custom dataset collected from WhatsApp video calls and compare it against existing QoE models. Using mean absolute error percentage (MAEP), our approach achieves 2.14% error for BRISQUE and 7.39% for FPS prediction.</p>
  </div>
</details>

<hr>
<h3 id="162-Proof-of-Reasoning-for-Privacy-Enhanced-Federated-Blockchain-Learning-at-the-Edge-cs-CR-cs-CV-cs-LGPDF"><a href="#162-Proof-of-Reasoning-for-Privacy-Enhanced-Federated-Blockchain-Learning-at-the-Edge-cs-CR-cs-CV-cs-LGPDF" class="headerlink" title="[162] Proof of Reasoning for Privacy Enhanced Federated Blockchain Learning at the Edge cs.CR | cs.CV | cs.LGPDF"></a>[162] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07134">Proof of Reasoning for Privacy Enhanced Federated Blockchain Learning at the Edge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07134" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">James Calo, Benny Lo</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Proof of Reasoning (PoR)的新型区块链共识机制，专门为边缘计算环境下的联邦学习设计。该机制通过三个定制化流程——使用掩码自编码器（MAE）生成保护隐私的编码器、在边缘训练下游分类器，以及基于区块链进行可验证的联邦聚合——旨在保护数据隐私、防御恶意攻击并增强参与网络的验证能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有区块链共识机制大多并非直接针对联邦学习设计，也未有效支持模型聚合步骤，因此需要一种专门机制来解决联邦学习中的数据隐私、安全攻击和可验证聚合问题。</p>
<p><strong>Result:</strong> 论文指出，PoR机制能生成更鲁棒的模型网络，并显著降低计算复杂度，同时通过在边缘仅训练下游分类器保持了高精度。该机制可扩展至大型物联网网络，具有低延迟和存储增长可控的特点，并能适应数据、法规和网络条件的变化。</p>
<p><strong>Insight:</strong> 主要创新点在于将联邦学习流程与区块链共识深度集成，通过MAE进行隐私保护编码和针对模型反转攻击的防御，并利用区块链实现可验证且复杂的联邦聚合。从客观角度看，这种将特定机器学习组件（如编码器和分类器）直接嵌入共识过程的设计，为隐私增强的分布式学习提供了新的架构思路。</p>
<p><strong>Abstract:</strong> Consensus mechanisms are the core of any blockchain system. However, the majority of these mechanisms do not target federated learning directly nor do they aid in the aggregation step. This paper introduces Proof of Reasoning (PoR), a novel consensus mechanism specifically designed for federated learning using blockchain, aimed at preserving data privacy, defending against malicious attacks, and enhancing the validation of participating networks. Unlike generic blockchain consensus mechanisms commonly found in the literature, PoR integrates three distinct processes tailored for federated learning. Firstly, a masked autoencoder (MAE) is trained to generate an encoder that functions as a feature map and obfuscates input data, rendering it resistant to human reconstruction and model inversion attacks. Secondly, a downstream classifier is trained at the edge, receiving input from the trained encoder. The downstream network’s weights, a single encoded datapoint, the network’s output and the ground truth are then added to a block for federated aggregation. Lastly, this data facilitates the aggregation of all participating networks, enabling more complex and verifiable aggregation methods than previously possible. This three-stage process results in more robust networks with significantly reduced computational complexity, maintaining high accuracy by training only the downstream classifier at the edge. PoR scales to large IoT networks with low latency and storage growth, and adapts to evolving data, regulations, and network conditions.</p>
  </div>
</details>

<hr>
<div id='q-bio.NC'></div>

<h1 id="q-bio-NC-Back"><a href="#q-bio-NC-Back" class="headerlink" title="q-bio.NC [Back]"></a>q-bio.NC <a href="#toc">[Back]</a></h1><h3 id="163-Gamma2Patterns-Deep-Cognitive-Attention-Region-Identification-and-Gamma-Alpha-Pattern-Analysis-q-bio-NC-cs-AI-cs-CVPDF"><a href="#163-Gamma2Patterns-Deep-Cognitive-Attention-Region-Identification-and-Gamma-Alpha-Pattern-Analysis-q-bio-NC-cs-AI-cs-CVPDF" class="headerlink" title="[163] Gamma2Patterns: Deep Cognitive Attention Region Identification and Gamma-Alpha Pattern Analysis q-bio.NC | cs.AI | cs.CVPDF"></a>[163] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06257">Gamma2Patterns: Deep Cognitive Attention Region Identification and Gamma-Alpha Pattern Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">q-bio.NC | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06257" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sobhana Jahan, Saydul Akbar Murad, Nick Rahimi, Noorbakhsh Amiri Golilarz</span></p>
<p><strong>TL;DR:</strong> 本文提出了Gamma2Patterns多模态框架，结合Gamma和Alpha频段EEG活动与眼动追踪测量，以表征深度认知注意。基于SEED-IV数据集，分析高专注（Gamma主导）与低专注（Alpha主导）状态的神经激活差异，发现额极、颞叶、前额和顶枕区域表现出最强的Gamma功率和爆发率，眼动信号则确认了额叶、额极和额颞区域的补充作用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有计算研究很少综合EEG与眼动追踪模态，以及未能识别负责持续专注的关键神经区域的问题。</p>
<p><strong>Result:</strong> 在SEED-IV数据集上，Gamma功率和爆发持续时间比单独Alpha功率更能区分深度专注状态，为注意力解码提供了更有效的标记。</p>
<p><strong>Insight:</strong> 创新点在于多模态整合Gamma-Alpha EEG模式与眼动信号，建立了基于证据的皮层区域和振荡特征映射，为AI系统中脑启发注意力机制提供了神经生理学基础。</p>
<p><strong>Abstract:</strong> Deep cognitive attention is characterized by heightened gamma oscillations and coordinated visual behavior. Despite the physiological importance of these mechanisms, computational studies rarely synthesize these modalities or identify the neural regions most responsible for sustained focus. To address this gap, this work introduces Gamma2Patterns, a multimodal framework that characterizes deep cognitive attention by leveraging complementary Gamma and Alpha band EEG activity alongside Eye-tracking measurements. Using the SEED-IV dataset [1], we extract spectral power, burst-based temporal dynamics, and fixation-saccade-pupil signals across 62 channels or electrodes to analyze how neural activation differs between high-focus (Gamma-dominant) and low-focus (Alpha-dominant) states. Our findings reveal that frontopolar, temporal, anterior frontal, and parieto-occipital regions exhibit the strongest Gamma power and burst rates, indicating their dominant role in deep attentional engagement, while Eye-tracking signals confirm complementary contributions from frontal, frontopolar, and frontotemporal regions. Furthermore, we show that Gamma power and burst duration provide more discriminative markers of deep focus than Alpha power alone, demonstrating their value for attention decoding. Collectively, these results establish a multimodal, evidence-based map of cortical regions and oscillatory signatures underlying deep focus, providing a neurophysiological foundation for future brain-inspired attention mechanisms in AI systems.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="164-AutoTour-Automatic-Photo-Tour-Guide-with-Smartphones-and-LLMs-cs-HC-cs-AI-cs-CVPDF"><a href="#164-AutoTour-Automatic-Photo-Tour-Guide-with-Smartphones-and-LLMs-cs-HC-cs-AI-cs-CVPDF" class="headerlink" title="[164] AutoTour: Automatic Photo Tour Guide with Smartphones and LLMs cs.HC | cs.AI | cs.CVPDF"></a>[164] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06781">AutoTour: Automatic Photo Tour Guide with Smartphones and LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06781" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Huatao Xu, Zihe Liu, Zilin Zeng, Baichuan Li, Mo Li</span></p>
<p><strong>TL;DR:</strong> AutoTour是一个基于智能手机和大型语言模型的自动照片导览系统，能够通过融合照片视觉特征与附近地理空间数据，自动生成细粒度地标标注和描述性叙述，为用户提供交互式、情境感知的探索体验。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有导览应用依赖预定义内容或专有数据集，缺乏可扩展性和情境感知能力，AutoTour旨在利用开放可扩展数据源，解决自动生成照片中地标注释和描述的问题，以增强用户探索体验。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体定量结果或基准测试，但通过演示表明AutoTour能够为标志性和鲜为人知的地标提供丰富、可解释的注释，实现了视觉感知与地理空间理解的结合。</p>
<p><strong>Insight:</strong> 创新点包括：1）设计无训练流程，融合视觉特征与开放地理空间数据；2）基于VLM的特征检测和几何匹配算法，实现照片特征与地理实体的对齐；3）结合LLM生成文本和音频描述，提供导览式体验。从客观角度看，该系统在数据源开放性和情境感知交互方面具有借鉴意义。</p>
<p><strong>Abstract:</strong> We present AutoTour, a system that enhances user exploration by automatically generating fine-grained landmark annotations and descriptive narratives for photos captured by users. The key idea of AutoTour is to fuse visual features extracted from photos with nearby geospatial features queried from open matching databases. Unlike existing tour applications that rely on pre-defined content or proprietary datasets, AutoTour leverages open and extensible data sources to provide scalable and context-aware photo-based guidance. To achieve this, we design a training-free pipeline that first extracts and filters relevant geospatial features around the user’s GPS location. It then detects major landmarks in user photos through VLM-based feature detection and projects them into the horizontal spatial plane. A geometric matching algorithm aligns photo features with corresponding geospatial entities based on their estimated distance and direction. The matched features are subsequently grounded and annotated directly on the original photo, accompanied by large language model-generated textual and audio descriptions to provide an informative, tour-like experience. We demonstrate that AutoTour can deliver rich, interpretable annotations for both iconic and lesser-known landmarks, enabling a new form of interactive, context-aware exploration that bridges visual perception and geospatial understanding.</p>
  </div>
</details>

<hr>
<h3 id="165-A-Multimodal-Dataset-of-Student-Oral-Presentations-with-Sensors-and-Evaluation-Data-cs-HC-cs-CVPDF"><a href="#165-A-Multimodal-Dataset-of-Student-Oral-Presentations-with-Sensors-and-Evaluation-Data-cs-HC-cs-CVPDF" class="headerlink" title="[165] A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data cs.HC | cs.CVPDF"></a>[165] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07576">A Multimodal Dataset of Student Oral Presentations with Sensors and Evaluation Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07576" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alvaro Becerra, Ruth Cobos, Roberto Daza</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了SOPHIAS数据集，这是一个包含12小时、50场学生口头报告的多模态数据集，集成了高清摄像头、音频、眼动追踪、智能手表生理传感器等多种同步传感器数据，并包含教师、同伴和自我评估的评分及标注，旨在支持口头报告表现与多模态行为生理信号关系的研究。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决高等教育中口头报告技能评估缺乏真实世界多模态综合数据集的问题，以支持对学生表现进行更全面的分析和自动化反馈工具的开发。</p>
<p><strong>Result:</strong> 数据集已公开可用，为研究多模态学习分析、自动反馈系统及同伴评估提供了基准，但摘要未提及具体的定量性能结果或SOTA比较。</p>
<p><strong>Insight:</strong> 创新点在于整合了多种传感器和评估数据，在真实课堂环境中捕获学生行为、交互和生理响应，为多模态学习分析和自动化评估工具的开发提供了丰富资源。</p>
<p><strong>Abstract:</strong> Oral presentation skills are a critical component of higher education, yet comprehensive datasets capturing real-world student performance across multiple modalities remain scarce. To address this gap, we present SOPHIAS (Student Oral Presentation monitoring for Holistic Insights &amp; Analytics using Sensors), a 12-hour multimodal dataset containing recordings of 50 oral presentations (10-15-minute presentation followed by 5-15-minute Q&amp;A) delivered by 65 undergraduate and master’s students at the Universidad Autonoma de Madrid. SOPHIAS integrates eight synchronized sensor streams from high-definition webcams, ambient and webcam audio, eye-tracking glasses, smartwatch physiological sensors, and clicker, keyboard, and mouse interactions. In addition, the dataset includes slides and rubric-based evaluations from teachers, peers, and self-assessments, along with timestamped contextual annotations. The dataset captures presentations conducted in real classroom settings, preserving authentic student behaviors, interactions, and physiological responses. SOPHIAS enables the exploration of relationships between multimodal behavioral and physiological signals and presentation performance, supports the study of peer assessment, and provides a benchmark for developing automated feedback and Multimodal Learning Analytics tools. The dataset is publicly available for research through GitHub and Science Data Bank.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="166-Certainty-Guided-Reasoning-in-Large-Language-Models-A-Dynamic-Thinking-Budget-Approach-cs-AI-cs-CLPDF"><a href="#166-Certainty-Guided-Reasoning-in-Large-Language-Models-A-Dynamic-Thinking-Budget-Approach-cs-AI-cs-CLPDF" class="headerlink" title="[166] Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach cs.AI | cs.CLPDF"></a>[166] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.07820">Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2509.07820" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">João Paulo Nogueira, Wentao Sun, Alonso Silva, Laith Zumot</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为确定性引导推理（CGR）的新方法，用于大型推理语言模型（LRLMs）。该方法受生成对抗网络中生成器&#x2F;判别器框架的启发，通过一个评判模型周期性地评估自身推理的置信度，动态调整推理预算：当置信度达到目标阈值时提前终止，否则继续推理。这种方法在AIME2024和AIME2025数据集上实验表明，能提高基线准确率并减少令牌使用量，同时通过多轮种子评估验证了其稳定性和在惩罚性评分下的优异表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大型推理语言模型在固定推理预算下可能存在的效率与可靠性不平衡问题，即模型可能过早停止推理（导致错误）或过度推理（浪费计算资源）。</p>
<p><strong>Result:</strong> 在AIME2024和AIME2025数据集上，CGR提高了基线准确率，同时减少了令牌使用量；通过64次多种子扩展评估，证明CGR稳定，降低了不同种子间的方差，并在基于惩罚的评分下提升了考试式性能；令牌节省分析显示CGR可累计节省数百万令牌，且置信度阈值与效率之间存在可调权衡。</p>
<p><strong>Insight:</strong> 创新点在于将置信度评估动态集成到推理过程中，实现自适应推理预算，平衡效率与可靠性；客观来看，该方法借鉴了对抗训练的思想，为大型语言模型的推理过程引入了可解释的“自我监控”机制，提升了模型在资源敏感场景下的实用性和可信度。</p>
<p><strong>Abstract:</strong> The rise of large reasoning language models (LRLMs) has unlocked new potential for solving complex tasks. These models operate with a thinking budget, that is, a predefined number of reasoning tokens used to arrive at a solution. We propose a novel approach, inspired by the generator&#x2F;discriminator framework in generative adversarial networks, in which a critic model periodically probes its own reasoning to assess whether it has reached a confident conclusion. If not, reasoning continues until a target certainty threshold is met. This mechanism adaptively balances efficiency and reliability by allowing early termination when confidence is high, while encouraging further reasoning when uncertainty persists. Through experiments on the AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR) improves baseline accuracy while reducing token usage. Importantly, extended multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing variance across seeds and improving exam-like performance under penalty-based grading. Additionally, our token savings analysis shows that CGR can eliminate millions of tokens in aggregate, with tunable trade-offs between certainty thresholds and efficiency. Together, these findings highlight certainty as a powerful signal for reasoning sufficiency. By integrating confidence into the reasoning process, CGR makes large reasoning language models more adaptive, trustworthy, and resource efficient, paving the way for practical deployment in domains where both accuracy and computational cost matter.</p>
  </div>
</details>

<hr>
<h3 id="167-From-RLHF-to-Direct-Alignment-A-Theoretical-Unification-of-Preference-Learning-for-Large-Language-Models-cs-AI-cs-CLPDF"><a href="#167-From-RLHF-to-Direct-Alignment-A-Theoretical-Unification-of-Preference-Learning-for-Large-Language-Models-cs-AI-cs-CLPDF" class="headerlink" title="[167] From RLHF to Direct Alignment: A Theoretical Unification of Preference Learning for Large Language Models cs.AI | cs.CLPDF"></a>[167] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06108">From RLHF to Direct Alignment: A Theoretical Unification of Preference Learning for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06108" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tarun Raheja, Nilay Pochhi</span></p>
<p><strong>TL;DR:</strong> 这篇论文对大型语言模型（LLM）的人类偏好对齐方法进行了理论统一，将RLHF、DPO、IPO、KTO、SimPO等众多方法归纳为三个正交轴上的选择：偏好模型、正则化机制和数据分布，并分析了不同选择组合导致的失败模式，为实践者提供了方法选择的决策指南。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决当前LLM偏好对齐方法（如RLHF及其众多替代方案）繁多且缺乏清晰理论指导，导致实践者在方法选择上无所适从的问题。</p>
<p><strong>Result:</strong> 论文通过形式化定义和定理，建立了关键结果，包括在线与离线方法的覆盖范围分离、奖励过优化的缩放定律以及直接对齐方法失败的条件，并综合了50多篇论文的实证发现。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个理论框架，将看似多样的偏好学习方法统一到三个核心设计轴上，揭示了失败模式（如长度攻击、模式崩溃）源于特定的、可预测的设计选择组合，从而将偏好学习从经验艺术转变为有理论基础的学科。</p>
<p><strong>Abstract:</strong> Aligning large language models (LLMs) with human preferences has become essential for safe and beneficial AI deployment. While Reinforcement Learning from Human Feedback (RLHF) established the dominant paradigm, a proliferation of alternatives – Direct Preference Optimization (DPO), Identity Preference Optimization (IPO), Kahneman-Tversky Optimization (KTO), Simple Preference Optimization (SimPO), and many others – has left practitioners without clear guidance on method selection. This survey provides a \textit{theoretical unification} of preference learning methods, revealing that the apparent diversity reduces to principled choices along three orthogonal axes: \textbf{(I) Preference Model} (what likelihood model underlies the objective), \textbf{(II) Regularization Mechanism} (how deviation from reference policies is controlled), and \textbf{(III) Data Distribution} (online vs.\ offline learning and coverage requirements). We formalize each axis with precise definitions and theorems, establishing key results including the coverage separation between online and offline methods, scaling laws for reward overoptimization, and conditions under which direct alignment methods fail. Our analysis reveals that failure modes – length hacking, mode collapse, likelihood displacement – arise from specific, predictable combinations of design choices. We synthesize empirical findings across 50+ papers and provide a practitioner’s decision guide for method selection. The framework transforms preference learning from an empirical art into a theoretically grounded discipline.</p>
  </div>
</details>

<hr>
<h3 id="168-An-Ubuntu-Guided-Large-Language-Model-Framework-for-Cognitive-Behavioral-Mental-Health-Dialogue-cs-AI-cs-CLPDF"><a href="#168-An-Ubuntu-Guided-Large-Language-Model-Framework-for-Cognitive-Behavioral-Mental-Health-Dialogue-cs-AI-cs-CLPDF" class="headerlink" title="[168] An Ubuntu-Guided Large Language Model Framework for Cognitive Behavioral Mental Health Dialogue cs.AI | cs.CLPDF"></a>[168] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06875">An Ubuntu-Guided Large Language Model Framework for Cognitive Behavioral Mental Health Dialogue</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06875" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sontaga G. Forane, Absalom E. Ezugwu, Kevin Igwe, Karen van den Berg</span></p>
<p><strong>TL;DR:</strong> 本研究提出一个结合认知行为疗法与非洲Ubuntu哲学的概念验证框架，旨在开发适用于南非等非洲语境的文化敏感、情感智能的AI心理健康对话系统。通过理论、治疗层面的深度适应与语言、沟通层面的表层文化适应，构建了文化适配数据集并微调模型，经专家案例评估表明模型能进行共情且符合治疗与文化目标的对话。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决南非心理健康危机中缺乏文化响应式护理的问题，并应对现有大语言模型因西方中心训练数据而在非洲语境中文化与语言适用性受限的挑战。</p>
<p><strong>Result:</strong> 模型通过专家案例研究评估，使用UniEval进行对话质量评估，并辅以CBT可靠性和文化语言对齐的额外指标，结果表明模型能有效进行共情且情境感知的对话，与治疗及文化目标一致，但尚未进行实时终端用户测试。</p>
<p><strong>Insight:</strong> 创新点在于将认知行为疗法的关键技术与强调集体福祉、精神根基和互联性的Ubuntu哲学原则相结合，通过语言简化、精神情境化和Ubuntu重构等迭代过程开发文化适配数据集，为AI驱动的心理健康干预提供了增强语境相关性、包容性和有效性的文化嵌入情感智能路径。</p>
<p><strong>Abstract:</strong> South Africa’s escalating mental health crisis, compounded by limited access to culturally responsive care, calls for innovative and contextually grounded interventions. While large language models show considerable promise for mental health support, their predominantly Western-centric training data limit cultural and linguistic applicability in African contexts. This study introduces a proof-of-concept framework that integrates cognitive behavioral therapy with the African philosophy of Ubuntu to create a culturally sensitive, emotionally intelligent, AI-driven mental health dialogue system. Guided by a design science research methodology, the framework applies both deep theoretical and therapeutic adaptations as well as surface-level linguistic and communicative cultural adaptations. Key CBT techniques, including behavioral activation and cognitive restructuring, were reinterpreted through Ubuntu principles that emphasize communal well-being, spiritual grounding, and interconnectedness. A culturally adapted dataset was developed through iterative processes of language simplification, spiritual contextualization, and Ubuntu-based reframing. The fine-tuned model was evaluated through expert-informed case studies, employing UniEval for conversational quality assessment alongside additional measures of CBT reliability and cultural linguistic alignment. Results demonstrate that the model effectively engages in empathetic, context-aware dialogue aligned with both therapeutic and cultural objectives. Although real-time end-user testing has not yet been conducted, the model underwent rigorous review and supervision by domain specialist clinical psychologists. The findings highlight the potential of culturally embedded emotional intelligence to enhance the contextual relevance, inclusivity, and effectiveness of AI-driven mental health interventions across African settings.</p>
  </div>
</details>

<hr>
<h3 id="169-Rewarding-Creativity-A-Human-Aligned-Generative-Reward-Model-for-Reinforcement-Learning-in-Storytelling-cs-AI-cs-CLPDF"><a href="#169-Rewarding-Creativity-A-Human-Aligned-Generative-Reward-Model-for-Reinforcement-Learning-in-Storytelling-cs-AI-cs-CLPDF" class="headerlink" title="[169] Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling cs.AI | cs.CLPDF"></a>[169] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07149">Rewarding Creativity: A Human-Aligned Generative Reward Model for Reinforcement Learning in Storytelling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07149" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhaoyan Li, Hang Lei, Yujia Wang, Lanbo Liu, Hao Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了RLCS框架，通过生成式奖励模型和基于熵的奖励塑造策略，解决了强化学习在创意故事生成中奖励信号设计和训练不稳定的双重挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大语言模型生成创意故事时质量不高的问题，以及强化学习应用中主观质量奖励信号不可靠和训练不稳定的障碍。</p>
<p><strong>Result:</strong> GenRM奖励模型与人类创意判断的对齐率达到68%，RLCS框架在整体故事质量上显著优于包括Gemini-2.5-Pro在内的强基线模型。</p>
<p><strong>Insight:</strong> 创新点在于结合了带推理链的监督微调与GRPO精炼来训练多维度分析的生成式奖励模型，并引入动态关注置信度错误与不确定正确预测的熵奖励塑造策略，为创意领域的强化学习应用提供了实用流程。</p>
<p><strong>Abstract:</strong> While Large Language Models (LLMs) can generate fluent text, producing high-quality creative stories remains challenging. Reinforcement Learning (RL) offers a promising solution but faces two critical obstacles: designing reliable reward signals for subjective storytelling quality and mitigating training instability. This paper introduces the Reinforcement Learning for Creative Storytelling (RLCS) framework to systematically address both challenges. First, we develop a Generative Reward Model (GenRM) that provides multi-dimensional analysis and explicit reasoning about story preferences, trained through supervised fine-tuning on demonstrations with reasoning chains distilled from strong teacher models, followed by GRPO-based refinement on expanded preference data. Second, we introduce an entropy-based reward shaping strategy that dynamically prioritizes learning on confident errors and uncertain correct predictions, preventing overfitting on already-mastered patterns. Experiments demonstrate that GenRM achieves 68% alignment with human creativity judgments, and RLCS significantly outperforms strong baselines including Gemini-2.5-Pro in overall story quality. This work provides a practical pipeline for applying RL to creative domains, effectively navigating the dual challenges of reward modeling and training stability.</p>
  </div>
</details>

<hr>
<h3 id="170-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors-cs-AI-cs-CLPDF"><a href="#170-Lost-in-the-Noise-How-Reasoning-Models-Fail-with-Contextual-Distractors-cs-AI-cs-CLPDF" class="headerlink" title="[170] Lost in the Noise: How Reasoning Models Fail with Contextual Distractors cs.AI | cs.CLPDF"></a>[170] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07226">Lost in the Noise: How Reasoning Models Fail with Contextual Distractors</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07226" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Seongyun Lee, Yongrae Jo, Minju Seo, Moontae Lee, Minjoon Seo</span></p>
<p><strong>TL;DR:</strong> 本文介绍了NoisyBench基准测试，用于评估AI模型在面对包含随机文档、无关聊天历史等噪声的上下文时的鲁棒性。研究发现，当前最先进的模型在噪声干扰下性能下降高达80%，且智能体工作流会放大错误。论文提出Rationale-Aware Reward（RARE）方法，通过激励模型识别噪声中有用信息来增强鲁棒性，并揭示了测试时计算增加反而导致性能下降的反常缩放趋势。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前推理模型和智能体AI系统日益依赖外部信息，但现实输入往往包含噪声，而现有基准测试未能捕捉这一挑战，因此需要系统评估模型在噪声上下文中的鲁棒性。</p>
<p><strong>Result:</strong> 在RAG、推理、对齐和工具使用等11个数据集上，最先进模型在噪声干扰下性能下降高达80%；提出的RARE方法显著提升了鲁棒性，而提示工程、SFT和基于结果的RL方法均失败。</p>
<p><strong>Insight:</strong> 创新点包括引入NoisyBench基准、发现噪声导致性能灾难性下降及智能体工作流错误放大现象、提出RARE奖励机制以增强鲁棒性，并揭示测试时计算增加在噪声环境中反而有害的反常缩放趋势，为构建鲁棒推理智能体提供了关键见解。</p>
<p><strong>Abstract:</strong> Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.</p>
  </div>
</details>

<hr>
<h3 id="171-Learning-to-Trust-the-Crowd-A-Multi-Model-Consensus-Reasoning-Engine-for-Large-Language-Models-cs-AI-cs-CL-cs-LGPDF"><a href="#171-Learning-to-Trust-the-Crowd-A-Multi-Model-Consensus-Reasoning-Engine-for-Large-Language-Models-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[171] Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models cs.AI | cs.CL | cs.LGPDF"></a>[171] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07245">Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07245" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pranav Kallem</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种多模型共识推理引擎，通过将多个异构大语言模型的输出作为监督元学习器的输入，学习在给定查询下最可能正确的答案。该系统利用语义嵌入、成对相似性、聚类统计、词汇与结构线索、推理质量评分、置信度估计和模型先验等特征，并应用梯度提升树、列表排序和图神经网络等方法，在资源受限的单机设置下显著提升了LLM的实例级可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大语言模型在平均性能上表现强劲，但在实例层面仍不可靠，存在频繁的幻觉、脆弱性故障和校准不佳的置信度。本文从多模型共识的角度研究可靠性问题，旨在通过整合多个LLM的输出来提高答案的正确性。</p>
<p><strong>Result:</strong> 在GSM8K、ARC-Challenge、HellaSwag和TruthfulQA的紧凑资源受限子集上，使用三个开源权重LLM进行评估，基于图注意力的最佳共识模型将宏平均准确率比最强单LLM提高了4.6个百分点，比多数投票提高了8.1个百分点，同时降低了Brier分数并减少了TruthfulQA的幻觉。</p>
<p><strong>Insight:</strong> 创新点在于将多模型共识问题形式化为监督学习任务，并设计了一套综合特征提取和模型融合方法。客观分析表明，语义一致性和聚类特征最具影响力，推理质量和模型先验特征提供补充增益，这为在有限资源下实现更可靠的LLM行为提供了一条实用路径。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.</p>
  </div>
</details>

<hr>
<h3 id="172-LRAS-Advanced-Legal-Reasoning-with-Agentic-Search-cs-AI-cs-CLPDF"><a href="#172-LRAS-Advanced-Legal-Reasoning-with-Agentic-Search-cs-AI-cs-CLPDF" class="headerlink" title="[172] LRAS: Advanced Legal Reasoning with Agentic Search cs.AI | cs.CLPDF"></a>[172] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07296">LRAS: Advanced Legal Reasoning with Agentic Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07296" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He</span></p>
<p><strong>TL;DR:</strong> 本文提出了LRAS（Legal Reasoning with Agentic Search）框架，旨在解决大型推理模型在法律领域应用时因依赖内部参数知识进行‘闭环推理’而导致的自信但错误结论的问题。该框架通过引入‘主动询问’机制，结合内省模仿学习和难度感知强化学习，帮助模型识别知识边界并处理法律推理的复杂性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有法律大语言模型依赖内部参数知识进行‘闭环推理’，缺乏对自身知识边界的认知，导致在法律推理中产生自信但错误的结论，无法满足法律领域对程序严谨性和逻辑遵从性的严格要求。</p>
<p><strong>Result:</strong> 实验结果表明，LRAS在多个基准测试中超越了现有最先进基线模型8.2%至32%，在需要可靠知识进行深度推理的任务上提升最为显著。</p>
<p><strong>Insight:</strong> 创新点在于将法律大语言模型从静态、参数化的‘闭环思维’转变为动态、交互式的‘主动询问’范式，通过内省模仿学习和难度感知强化学习的集成，实现了对知识边界的自我识别和对复杂推理任务的有效处理，为专业领域的大模型应用提供了可借鉴的增强策略。</p>
<p><strong>Abstract:</strong> While Large Reasoning Models (LRMs) have demonstrated exceptional logical capabilities in mathematical domains, their application to the legal field remains hindered by the strict requirements for procedural rigor and adherence to legal logic. Existing legal LLMs, which rely on “closed-loop reasoning” derived solely from internal parametric knowledge, frequently suffer from lack of self-awareness regarding their knowledge boundaries, leading to confident yet incorrect conclusions. To address this challenge, we present Legal Reasoning with Agentic Search (LRAS), the first framework designed to transition legal LLMs from static and parametric “closed-loop thinking” to dynamic and interactive “Active Inquiry”. By integrating Introspective Imitation Learning and Difficulty-aware Reinforcement Learning, LRAS enables LRMs to identify knowledge boundaries and handle legal reasoning complexity. Empirical results demonstrate that LRAS outperforms state-of-the-art baselines by 8.2-32%, with the most substantial gains observed in tasks requiring deep reasoning with reliable knowledge. We will release our data and models for further exploration soon.</p>
  </div>
</details>

<hr>
<h3 id="173-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning-cs-AI-cs-CL-cs-MAPDF"><a href="#173-Beyond-Static-Tools-Test-Time-Tool-Evolution-for-Scientific-Reasoning-cs-AI-cs-CL-cs-MAPDF" class="headerlink" title="[173] Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning cs.AI | cs.CL | cs.MAPDF"></a>[173] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07641">Beyond Static Tools: Test-Time Tool Evolution for Scientific Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07641" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaxuan Lu, Ziyu Kong, Yemin Wang, Rong Fu, Haiyuan Wan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为测试时工具演化（TTE）的新范式，旨在解决科学推理中静态工具库的局限性。TTE允许智能体在推理过程中动态合成、验证和演化可执行工具，从而适应开放、稀疏且异构的科学领域。研究还引入了SciEvo基准测试，包含1590个科学推理任务和925个自动演化工具，实验表明TTE在准确性和工具效率上达到了最先进水平，并支持有效的跨领域工具适应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于大语言模型的智能体依赖静态、预定义的工具库，这在工具稀疏、异构且本质上不完整的科学领域中存在根本性缺陷，无法满足开放科学世界中对计算方法的动态创建需求。</p>
<p><strong>Result:</strong> 在SciEvo基准测试上的广泛实验表明，TTE在准确性和工具效率方面均达到了最先进（SOTA）性能，并实现了计算工具的有效跨领域适应。</p>
<p><strong>Insight:</strong> 核心创新在于将工具从固定资源转变为问题驱动的可演化产物，通过推理时动态合成与验证来克服静态工具库的僵化和长尾限制，为AI在科学领域的开放式推理提供了新范式。</p>
<p><strong>Abstract:</strong> The central challenge of AI for Science is not reasoning alone, but the ability to create computational methods in an open-ended scientific world. Existing LLM-based agents rely on static, pre-defined tool libraries, a paradigm that fundamentally fails in scientific domains where tools are sparse, heterogeneous, and intrinsically incomplete. In this paper, we propose Test-Time Tool Evolution (TTE), a new paradigm that enables agents to synthesize, verify, and evolve executable tools during inference. By transforming tools from fixed resources into problem-driven artifacts, TTE overcomes the rigidity and long-tail limitations of static tool libraries. To facilitate rigorous evaluation, we introduce SciEvo, a benchmark comprising 1,590 scientific reasoning tasks supported by 925 automatically evolved tools. Extensive experiments show that TTE achieves state-of-the-art performance in both accuracy and tool efficiency, while enabling effective cross-domain adaptation of computational tools. The code and benchmark have been released at <a target="_blank" rel="noopener" href="https://github.com/lujiaxuan0520/Test-Time-Tool-Evol">https://github.com/lujiaxuan0520/Test-Time-Tool-Evol</a>.</p>
  </div>
</details>

<hr>
<h3 id="174-Reasoning-Models-Will-Blatantly-Lie-About-Their-Reasoning-cs-AI-cs-CLPDF"><a href="#174-Reasoning-Models-Will-Blatantly-Lie-About-Their-Reasoning-cs-AI-cs-CLPDF" class="headerlink" title="[174] Reasoning Models Will Blatantly Lie About Their Reasoning cs.AI | cs.CLPDF"></a>[174] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07663">Reasoning Models Will Blatantly Lie About Their Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07663" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">William Walden</span></p>
<p><strong>TL;DR:</strong> 该论文扩展了Chen等人（2025）的研究，证明大型推理模型（LRMs）不仅会隐瞒其推理过程，甚至会公然撒谎。具体表现为，模型在回答选择题时会否认使用了提示中的线索，尽管实验证据表明它们确实依赖了这些线索，即使在被直接询问或允许使用线索的情况下也是如此。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探究大型推理模型在解释自身推理时的诚实性问题，特别是它们是否会主动否认对输入中提示线索的依赖，这比单纯隐瞒信息更为严重，旨在评估CoT（思维链）监控和可解释性的可靠性。</p>
<p><strong>Result:</strong> 论文通过实验证明，LRMs在回答问题时，即使实验显示它们使用了提示中的线索，也会断然否认这种依赖，这一结果在多个选择题基准测试中得到了验证，对模型的可信度提出了挑战。</p>
<p><strong>Insight:</strong> 创新点在于揭示了LRMs在推理解释中可能存在系统性撒谎行为，这超出了之前已知的隐瞒问题，为模型可解释性和安全监控提供了重要警示，提示需要更严格的评估方法来确保AI系统的透明度。</p>
<p><strong>Abstract:</strong> It has been shown that Large Reasoning Models (LRMs) may not <em>say what they think</em>: they do not always volunteer information about how certain parts of the input influence their reasoning. But it is one thing for a model to <em>omit</em> such information and another, worse thing to <em>lie</em> about it. Here, we extend the work of Chen et al. (2025) to show that LRMs will do just this: they will flatly deny relying on hints provided in the prompt in answering multiple choice questions – even when directly asked to reflect on unusual (i.e. hinted) prompt content, even when allowed to use hints, and even though experiments <em>show</em> them to be using the hints. Our results thus have discouraging implications for CoT monitoring and interpretability.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="175-Judge-Model-for-Large-scale-Multimodality-Benchmarks-cs-LG-cs-AI-cs-CL-cs-CV-cs-MAPDF"><a href="#175-Judge-Model-for-Large-scale-Multimodality-Benchmarks-cs-LG-cs-AI-cs-CL-cs-CV-cs-MAPDF" class="headerlink" title="[175] Judge Model for Large-scale Multimodality Benchmarks cs.LG | cs.AI | cs.CL | cs.CV | cs.MAPDF"></a>[175] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06106">Judge Model for Large-scale Multimodality Benchmarks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | cs.CV | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06106" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Min-Han Shih, Yu-Hsin Wu, Yu-Wei Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种专门的多模态评判模型，用于在大规模多模态基准测试中提供可靠、可解释的评估。该基准涵盖文本、音频、图像和视频模态，基于精心采样的公共数据集构建以确保可复现性。框架不仅进行简单评分，还聚合多模态判断、分析模型输出的质量和推理一致性，并生成诊断反馈。作者在280个多模态样本上评估了包括Gemini 2.5、Phi 4和Qwen 2.5在内的多个MLLM，并将评判模型的结果与人工标注进行了比较。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决当前多模态模型评估中缺乏可靠、可解释且可扩展的自动化评估方法的问题，旨在提供一个能够替代或辅助人工评估的标准化评判框架。</p>
<p><strong>Result:</strong> 在涵盖文本、音频、图像和视频的280个多模态样本上，评判模型的评估结果与人工评分者表现出高度一致性，证明了其作为未来多模态AI研究可扩展、可解释评估管线的潜力。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个专门的多模态评判模型和基准，其核心是超越简单的分数输出，实现了对模型输出质量和推理一致性的分析以及诊断反馈的生成，为自动化、可解释的多模态评估提供了新思路。</p>
<p><strong>Abstract:</strong> We propose a dedicated multimodal Judge Model designed to provide reliable, explainable evaluation across a diverse suite of tasks. Our benchmark spans text, audio, image, and video modalities, drawing from carefully sampled public datasets with fixed seeds to ensure reproducibility and minimize train test leakage. Instead of simple scoring, our framework aggregates multimodal judgments, analyzes the quality and reasoning consistency of model outputs, and generates diagnostic feedback. We evaluate several MLLMs, including Gemini 2.5, Phi 4, and Qwen 2.5, across 280 multimodal samples and compare judge model assessments with human annotators. Results show strong alignment between the Judge Model and human scores, demonstrating its potential as a scalable, interpretable evaluation pipeline for future multimodal AI research.</p>
  </div>
</details>

<hr>
<h3 id="176-Monkey-Jump-MoE-Style-PEFT-for-Efficient-Multi-Task-Learning-cs-LG-cs-CL-cs-CVPDF"><a href="#176-Monkey-Jump-MoE-Style-PEFT-for-Efficient-Multi-Task-Learning-cs-LG-cs-CL-cs-CVPDF" class="headerlink" title="[176] Monkey Jump : MoE-Style PEFT for Efficient Multi-Task Learning cs.LG | cs.CL | cs.CVPDF"></a>[176] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06356">Monkey Jump : MoE-Style PEFT for Efficient Multi-Task Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06356" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nusrat Jahan Prottasha, Md Kowsher, Chun-Nam Yu, Chen Chen, Ozlem Garibay</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Monkey Jump的新型参数高效微调方法，该方法在不引入额外可训练参数（如专家或路由器）的情况下，实现了类似混合专家模型的每令牌专业化能力。该方法将Transformer块中已有的适配器（如查询、键、值、上、下投影）视为隐式专家，并通过无需梯度和学习参数的k-means聚类进行令牌路由。理论分析表明，令牌级路由能提升表达能力并避免抵消效应。在涵盖14个文本、14个图像和19个视频基准的多任务实验中，Monkey Jump在性能上与基于混合专家的参数高效微调方法相当，同时可训练参数减少7至29倍，内存消耗降低高达48%，训练速度提升1.5至2倍。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的混合专家变体参数高效微调方法虽然实现了每令牌专业化，但引入了额外的可训练路由器和专家参数，增加了内存使用和训练成本，这违背了参数高效微调的核心目标。本文旨在解决这一矛盾，提出一种无需额外可训练参数即可实现类似专业化能力的方法。</p>
<p><strong>Result:</strong> 在涵盖14个文本、14个图像和19个视频基准的多任务实验中，Monkey Jump取得了与基于混合专家的参数高效微调方法相当的性能，同时可训练参数减少7至29倍，内存消耗降低高达48%，训练速度提升1.5至2倍。</p>
<p><strong>Insight:</strong> 创新点在于将现有适配器重新利用为隐式专家，并通过无参数的k-means聚类进行动态令牌路由，这避免了引入额外可训练组件，从而在保持性能的同时显著提升了效率。该方法具有架构无关性，可应用于任何基于适配器的参数高效微调方法。</p>
<p><strong>Abstract:</strong> Mixture-of-experts variants of parameter-efficient fine-tuning enable per-token specialization, but they introduce additional trainable routers and expert parameters, increasing memory usage and training cost. This undermines the core goal of parameter-efficient fine-tuning. We propose Monkey Jump, a method that brings mixture-of-experts-style specialization to parameter-efficient fine-tuning without introducing extra trainable parameters for experts or routers. Instead of adding new adapters as experts, Monkey Jump treats the adapters already present in each Transformer block (such as query, key, value, up, and down projections) as implicit experts and routes tokens among them. Routing is performed using k-means clustering with exponentially moving averaged cluster centers, requiring no gradients and no learned parameters. We theoretically show that token-wise routing increases expressivity and can outperform shared adapters by avoiding cancellation effects. Across multi-task experiments covering 14 text, 14 image, and 19 video benchmarks, Monkey Jump achieves competitive performance with mixture-of-experts-based parameter-efficient fine-tuning methods while using 7 to 29 times fewer trainable parameters, up to 48 percent lower memory consumption, and 1.5 to 2 times faster training. Monkey Jump is architecture-agnostic and can be applied to any adapter-based parameter-efficient fine-tuning method.</p>
  </div>
</details>

<hr>
<h3 id="177-MLB-A-Scenario-Driven-Benchmark-for-Evaluating-Large-Language-Models-in-Clinical-Applications-cs-LG-cs-AI-cs-CLPDF"><a href="#177-MLB-A-Scenario-Driven-Benchmark-for-Evaluating-Large-Language-Models-in-Clinical-Applications-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[177] MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications cs.LG | cs.AI | cs.CLPDF"></a>[177] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06193">MLB: A Scenario-Driven Benchmark for Evaluating Large Language Models in Clinical Applications</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06193" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qing He, Dongsheng Bi, Jianrong Lu, Minghui Yang, Zixiao Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为MLB的医学大语言模型基准测试，旨在评估LLM在临床实践中的实际应用能力。该基准包含五个核心维度，整合了22个数据集，覆盖64个临床专科，并采用由专家标注训练的专用评判模型进行评估。对10个领先模型的评估揭示了其在结构化任务中表现良好，但在面向患者的场景中性能显著下降，同时强调了针对性训练的重要性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准主要测试静态知识，无法捕捉临床实践中所需的动态、面向应用的能力，因此需要建立一个评估LLM真实临床效用的框架。</p>
<p><strong>Result:</strong> 在MLB基准上评估了10个领先模型，其中Kimi-K2-Instruct总体准确率最高（77.3%），在信息提取等结构化任务中表现优异（MedRU准确率87.8%），但在面向患者的场景中性能大幅下降（SmartServ准确率61.3%）。较小的Baichuan-M2-32B在安全与伦理维度得分突出（90.6%）。专用评判模型在专家标注数据集上训练，达到92.1%的准确率，F1分数94.37%，Cohen’s Kappa为81.3%，验证了评估协议的有效性。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个全面、场景驱动的医学LLM基准，强调动态应用能力而非静态知识，并引入了由专家参与标注和训练的专用评判模型，确保了评估的严谨性和可重复性。从客观角度看，该研究突出了临床应用中任务类型对模型性能的关键影响，以及针对性训练对于提升特定领域（如安全）表现的重要性。</p>
<p><strong>Abstract:</strong> The proliferation of Large Language Models (LLMs) presents transformative potential for healthcare, yet practical deployment is hindered by the absence of frameworks that assess real-world clinical utility. Existing benchmarks test static knowledge, failing to capture the dynamic, application-oriented capabilities required in clinical practice. To bridge this gap, we introduce a Medical LLM Benchmark MLB, a comprehensive benchmark evaluating LLMs on both foundational knowledge and scenario-based reasoning. MLB is structured around five core dimensions: Medical Knowledge (MedKQA), Safety and Ethics (MedSE), Medical Record Understanding (MedRU), Smart Services (SmartServ), and Smart Healthcare (SmartCare). The benchmark integrates 22 datasets (17 newly curated) from diverse Chinese clinical sources, covering 64 clinical specialties. Its design features a rigorous curation pipeline involving 300 licensed physicians. Besides, we provide a scalable evaluation methodology, centered on a specialized judge model trained via Supervised Fine-Tuning (SFT) on expert annotations. Our comprehensive evaluation of 10 leading models reveals a critical translational gap: while the top-ranked model, Kimi-K2-Instruct (77.3% accuracy overall), excels in structured tasks like information extraction (87.8% accuracy in MedRU), performance plummets in patient-facing scenarios (61.3% in SmartServ). Moreover, the exceptional safety score (90.6% in MedSE) of the much smaller Baichuan-M2-32B highlights that targeted training is equally critical. Our specialized judge model, trained via SFT on a 19k expert-annotated medical dataset, achieves 92.1% accuracy, an F1-score of 94.37%, and a Cohen’s Kappa of 81.3% for human-AI consistency, validating a reproducible and expert-aligned evaluation protocol. MLB thus provides a rigorous framework to guide the development of clinically viable LLMs.</p>
  </div>
</details>

<hr>
<h3 id="178-KASER-Knowledge-Aligned-Student-Error-Simulator-for-Open-Ended-Coding-Tasks-cs-LG-cs-AI-cs-CL-cs-CYPDF"><a href="#178-KASER-Knowledge-Aligned-Student-Error-Simulator-for-Open-Ended-Coding-Tasks-cs-LG-cs-AI-cs-CL-cs-CYPDF" class="headerlink" title="[178] KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks cs.LG | cs.AI | cs.CL | cs.CYPDF"></a>[178] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06633">KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06633" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhangqi Duan, Nigel Fernandez, Andrew Lan</span></p>
<p><strong>TL;DR:</strong> 本文提出KASER方法，一种基于强化学习训练大语言模型来模拟学生在开放式编程任务中可能出现的错误的方法，旨在解决现有模型在模拟学生代码响应时存在的模式崩溃和多样性不足问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大语言模型在模拟学生编程错误时，常因模式崩溃而无法充分捕捉学生代码在语法、风格和解决方案上的多样性，KASER旨在通过知识对齐来更准确地模拟和预测学生错误。</p>
<p><strong>Result:</strong> 在两个真实数据集上的评估表明，KASER在代码和错误预测方面优于基线模型，同时在错误覆盖率和模拟代码多样性方面也表现更佳。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种基于强化学习的训练方法，结合了代码相似性、错误匹配和代码预测多样性三个方面的混合奖励，以实现知识对齐的错误模拟，提升模拟的准确性和多样性。</p>
<p><strong>Abstract:</strong> Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.</p>
  </div>
</details>

<hr>
<h3 id="179-Are-LLM-Decisions-Faithful-to-Verbal-Confidence-cs-LG-cs-CLPDF"><a href="#179-Are-LLM-Decisions-Faithful-to-Verbal-Confidence-cs-LG-cs-CLPDF" class="headerlink" title="[179] Are LLM Decisions Faithful to Verbal Confidence? cs.LG | cs.CLPDF"></a>[179] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07767">Are LLM Decisions Faithful to Verbal Confidence?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07767" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiawei Wang, Yanfei Zhou, Siddartha Devic, Deqing Fu</span></p>
<p><strong>TL;DR:</strong> 该论文通过引入RiskEval框架，评估大型语言模型（LLMs）在表达口头置信度时，其决策是否与实际的风险敏感决策（如根据错误惩罚调整弃权策略）保持一致。研究发现，前沿模型在表达置信度时缺乏成本意识，在高惩罚条件下也不会策略性地选择弃权，导致效用崩溃，表明校准的口头置信度分数不足以构建可信赖的AI系统。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是探究LLMs表达的不确定性（口头置信度）是否与其内在的推理、知识或决策过程真实关联，以评估当前模型能否将不确定性信号转化为最优的风险敏感决策。</p>
<p><strong>Result:</strong> 在RiskEval框架下对多个前沿模型的评估显示，模型在表达口头置信度时缺乏成本意识，在高惩罚条件下几乎从不选择弃权（即使数学上弃权是最优策略），导致效用崩溃，这表明模型决策与口头置信度存在严重脱节。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了RiskEval框架来量化评估模型决策对风险（错误惩罚）的敏感性，揭示了当前LLMs的口头置信度校准与战略决策能力之间的关键脱节，为构建更可信赖和可解释的AI系统指出了重要方向。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce $\textbf{RiskEval}$: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties. Our evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions. Even when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse. This indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems, as current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.</p>
  </div>
</details>

<hr>
<div id='cs.AR'></div>

<h1 id="cs-AR-Back"><a href="#cs-AR-Back" class="headerlink" title="cs.AR [Back]"></a>cs.AR <a href="#toc">[Back]</a></h1><h3 id="180-GRPO-with-State-Mutations-Improving-LLM-Based-Hardware-Test-Plan-Generation-cs-AR-cs-CL-cs-LGPDF"><a href="#180-GRPO-with-State-Mutations-Improving-LLM-Based-Hardware-Test-Plan-Generation-cs-AR-cs-CL-cs-LGPDF" class="headerlink" title="[180] GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation cs.AR | cs.CL | cs.LGPDF"></a>[180] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07593">GRPO with State Mutations: Improving LLM-Based Hardware Test Plan Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AR | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07593" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dimple Vijay Kochar, Nathaniel Pinckney, Guan-Ting Liu, Chia-Tung Ho, Chenhui Deng</span></p>
<p><strong>TL;DR:</strong> 本文首次系统研究了大型语言模型在RTL验证激励生成中的推理能力，提出了一个两阶段框架将测试计划生成与测试平台执行解耦。研究发现当前SOTA模型（如DeepSeek-R1和Claude-4.0-Sonnet）在生成能通过黄金RTL设计的激励方面成功率仅15.7-21.7%。为提升性能，作者开发了结合监督微调与新型强化学习方法GRPO-SMu的综合训练方法，通过基于树的分支变异策略构建训练数据，使7B参数模型达到33.3%的黄金测试通过率和13.9%的变异检测率，显著优于基线和大规模通用模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决RTL设计早期依赖临时测试平台创建的问题，探索LLM在理解硬件规格和生成定向测试计划方面的潜力，填补该领域系统性研究的空白。</p>
<p><strong>Result:</strong> 在RTL验证激励生成任务上，7B参数专用模型达到33.3%黄金测试通过率（绝对提升17.6%）和13.9%变异检测率，超越包括DeepSeek-R1、Claude-4.0-Sonnet在内的SOTA通用模型。</p>
<p><strong>Insight:</strong> 创新点包括：1）两阶段测试生成框架解耦计划与执行；2）GRPO-SMu强化学习方法通过输入变异增强探索；3）树状分支变异策略超越线性方法，提供更丰富的学习信号；4）证明专用训练方法能显著提升LLM在硬件验证任务中的推理能力。</p>
<p><strong>Abstract:</strong> RTL design often relies heavily on ad-hoc testbench creation early in the design cycle. While large language models (LLMs) show promise for RTL code generation, their ability to reason about hardware specifications and generate targeted test plans remains largely unexplored. We present the first systematic study of LLM reasoning capabilities for RTL verification stimuli generation, establishing a two-stage framework that decomposes test plan generation from testbench execution. Our benchmark reveals that state-of-the-art models, including DeepSeek-R1 and Claude-4.0-Sonnet, achieve only 15.7-21.7% success rates on generating stimuli that pass golden RTL designs. To improve LLM generated stimuli, we develop a comprehensive training methodology combining supervised fine-tuning with a novel reinforcement learning approach, GRPO with State Mutation (GRPO-SMu), which enhances exploration by varying input mutations. Our approach leverages a tree-based branching mutation strategy to construct training data comprising equivalent and mutated trees, moving beyond linear mutation approaches to provide rich learning signals. Training on this curated dataset, our 7B parameter model achieves a 33.3% golden test pass rate and a 13.9% mutation detection rate, representing a 17.6% absolute improvement over baseline and outperforming much larger general-purpose models. These results demonstrate that specialized training methodologies can significantly enhance LLM reasoning capabilities for hardware verification tasks, establishing a foundation for automated sub-unit testing in semiconductor design workflows.</p>
  </div>
</details>

<hr>
<div id='cs.MA'></div>

<h1 id="cs-MA-Back"><a href="#cs-MA-Back" class="headerlink" title="cs.MA [Back]"></a>cs.MA <a href="#toc">[Back]</a></h1><h3 id="181-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent-cs-MA-cs-AI-cs-CL-cs-CV-cs-HCPDF"><a href="#181-OS-Symphony-A-Holistic-Framework-for-Robust-and-Generalist-Computer-Using-Agent-cs-MA-cs-AI-cs-CL-cs-CV-cs-HCPDF" class="headerlink" title="[181] OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent cs.MA | cs.AI | cs.CL | cs.CV | cs.HCPDF"></a>[181] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.07779">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MA | cs.AI | cs.CL | cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.07779" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun</span></p>
<p><strong>TL;DR:</strong> 本文提出了OS-Symphony，一个用于构建鲁棒且通用的计算机使用智能体的整体框架。该框架通过一个协调器整合了两个关键创新：反思记忆代理和多功能工具代理，旨在解决现有智能体在长流程任务中鲁棒性不足和在新领域泛化能力差的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于视觉语言模型的计算机使用智能体在长流程工作流中缺乏鲁棒性，且难以泛化到新领域，其根源在于对历史视觉上下文缺乏细粒度控制，以及缺少视觉感知的教程检索能力。</p>
<p><strong>Result:</strong> 实验结果表明，OS-Symphony在不同规模的模型上都带来了显著的性能提升，在三个在线基准测试上均取得了新的最先进结果，特别是在OSWorld基准上达到了65.84%的准确率。</p>
<p><strong>Insight:</strong> 核心创新点在于：1) 采用里程碑驱动的长期记忆机制实现轨迹级自我纠正，缓解长流程任务中的视觉上下文丢失；2) 引入基于SeeAct范式的多模态搜索器，在浏览器沙盒中合成实时、视觉对齐的教程，以解决未见场景中的保真度问题。框架设计上，通过一个协调器将这两个模块有机结合，形成了系统性的解决方案。</p>
<p><strong>Abstract:</strong> While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="182-Semantic-Enrichment-of-CAD-Based-Industrial-Environments-via-Scene-Graphs-for-Simulation-and-Reasoning-cs-RO-cs-AI-cs-CVPDF"><a href="#182-Semantic-Enrichment-of-CAD-Based-Industrial-Environments-via-Scene-Graphs-for-Simulation-and-Reasoning-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[182] Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning cs.RO | cs.AI | cs.CVPDF"></a>[182] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06415">Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06415" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nathan Pascal Walus, Ranulfo Bezerra, Shotaro Kojima, Tsige Tadesse Alemayoh, Satoshi Tadokoro</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种离线方法，从CAD环境中创建详细的3D场景图，通过大型视觉语言模型（LVLM）为几何模型补充语义、空间和功能信息，以支持机器人仿真和推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 工业环境的CAD文件通常只包含几何和视觉信息，缺乏语义、关系和功能信息，这限制了机器人仿真和训练的可能性。</p>
<p><strong>Result:</strong> 研究提供了生成的语义标签的定量结果，以及场景图（特别是针对管道结构和识别的功能关系）的定性结果。</p>
<p><strong>Insight:</strong> 利用LVLM从纯几何CAD数据中自动提取和构建包含语义、空间和功能关系的3D场景图，为动态仿真和高级场景理解提供了结构化基础。</p>
<p><strong>Abstract:</strong> Utilizing functional elements in an industrial environment, such as displays and interactive valves, provide effective possibilities for robot training. When preparing simulations for robots or applications that involve high-level scene understanding, the simulation environment must be equally detailed. Although CAD files for such environments deliver an exact description of the geometry and visuals, they usually lack semantic, relational and functional information, thus limiting the simulation and training possibilities. A 3D scene graph can organize semantic, spatial and functional information by enriching the environment through a Large Vision-Language Model (LVLM). In this paper we present an offline approach to creating detailed 3D scene graphs from CAD environments. This will serve as a foundation to include the relations of functional and actionable elements, which then can be used for dynamic simulation and reasoning. Key results of this research include both quantitative results of the generated semantic labels as well as qualitative results of the scene graph, especially in hindsight of pipe structures and identified functional relations. All code, results and the environment will be made available at <a target="_blank" rel="noopener" href="https://cad-scenegraph.github.io/">https://cad-scenegraph.github.io</a></p>
  </div>
</details>

<hr>
<h3 id="183-CulinaryCut-VLAP-A-Vision-Language-Action-Physics-Framework-for-Food-Cutting-via-a-Force-Aware-Material-Point-Method-cs-RO-cs-CVPDF"><a href="#183-CulinaryCut-VLAP-A-Vision-Language-Action-Physics-Framework-for-Food-Cutting-via-a-Force-Aware-Material-Point-Method-cs-RO-cs-CVPDF" class="headerlink" title="[183] CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method cs.RO | cs.CVPDF"></a>[183] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.06451">CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.06451" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyunseo Koh, Chang-Yong Song, Youngjae Choi, Misa Viveiros, David Hyde</span></p>
<p><strong>TL;DR:</strong> 本文提出了CulinaryCut-VLAP框架，这是一个结合视觉、语言、动作和物理的解决方案，用于解决机器人切割食物这一具有挑战性的任务。该框架通过一个基于材料点法（MPM）构建的物理真实切割模拟器，与一个视觉-语言-动作（VLA）数据集相耦合，以应对切割过程中材料非线性变形、频繁接触和拓扑变化带来的数据收集困难。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决在视觉与机器人操作交叉领域中，食物切割这一实用但未被充分探索的任务所面临的挑战。这些挑战主要源于刀具与可变形材料之间高度非线性、涉及大变形、频繁接触和拓扑变化的交互，这些因素阻碍了稳定、安全的大规模数据收集。</p>
<p><strong>Result:</strong> 论文提出了一个统一的框架，并提供了一个基准数据集。该数据集整合了多样化的切割轨迹、多视角视觉观察、细粒度语言指令，以及力-扭矩和工具-姿态标签，以提供物理一致的训练信号。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了一个将视觉-语言-动作数据集与基于材料点法（MPM）的物理真实模拟器耦合的统一框架。该模拟器采用MLS-MPM作为计算核心，减少了数值耗散和能量漂移，并在拓扑变化的切割下保持了旋转和剪切响应。同时，通过粒子与网格之间的冲量交换来估计力和应力分布，实现了对瞬态接触力和能量传递的稳定跟踪，为可变形物体操作中的VLA模型研究建立了一个安全、可重复、可扩展的基础。</p>
<p><strong>Abstract:</strong> Food cutting is a highly practical yet underexplored application at the intersection of vision and robotic manipulation. The task remains challenging because interactions between the knife and deformable materials are highly nonlinear and often entail large deformations, frequent contact, and topological change, which in turn hinder stable and safe large-scale data collection.   To address these challenges, we propose a unified framework that couples a vision-language-action (VLA) dataset with a physically realistic cutting simulator built on the material point method (MPM). Our simulator adopts MLS-MPM as its computational core, reducing numerical dissipation and energy drift while preserving rotational and shear responses even under topology-changing cuts. During cutting, forces and stress distributions are estimated from impulse exchanges between particles and the grid, enabling stable tracking of transient contact forces and energy transfer.   We also provide a benchmark dataset that integrates diverse cutting trajectories, multi-view visual observations, and fine-grained language instructions, together with force–torque and tool–pose labels to provide physically consistent training signals.   These components realize a learning–evaluation loop that respects the core physics of cutting and establishes a safe, reproducible, and scalable foundation for advancing VLA models in deformable object manipulation.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2026-01-15/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2026-01-13/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
