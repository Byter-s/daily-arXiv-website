<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-10-25 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-10-25
            
            
        </div>
        <span class="post-date">
            Oct 25, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 39]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 59]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 3]</li>
<li><a href="#stat.AP">stat.AP</a> [Total: 1]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 5]</li>
<li><a href="#cs.PL">cs.PL</a> [Total: 1]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 1]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 2]</li>
<li><a href="#quant-ph">quant-ph</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-An-Evaluation-of-the-Pedagogical-Soundness-and-Usability-of-AI-Generated-Lesson-Plans-Across-Different-Models-and-Prompt-Frameworks-in-High-School-Physics"><a href="#1-An-Evaluation-of-the-Pedagogical-Soundness-and-Usability-of-AI-Generated-Lesson-Plans-Across-Different-Models-and-Prompt-Frameworks-in-High-School-Physics" class="headerlink" title="[1] An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19866">An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</a></h3><p><em>Xincheng Liu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文评估了五种主流大语言模型（如GPT-5、Claude等）在生成高中物理教案时的教学合理性与可用性，测试了三种提示框架（TAG、RACE、COSTAR）。结果表明，模型选择主要影响语言可读性，而提示框架对内容准确性影响更大。最优配置是结合可读性优化模型与RACE框架。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机是探究不同AI模型和提示框架在生成教案时的表现，尤其是教学品质和实用性，为教育者提供可靠的AI辅助工具选择依据。

<p>Contribution: 主要贡献在于系统地评估了模型和提示框架对教案生成的影响，并提出了优化组合（如RACE框架与特定模型），为AI在教育中的应用提供了实证支持。</p>
<p>Method: 方法包括：1）使用五种模型和三种提示框架生成15份教案；2）通过四项计算指标（可读性、事实准确性、标准对齐和认知需求）进行分析。</p>
<p>Result: 结果显示：1）DeepSeek模型可读性最佳；2）RACE框架内容准确性最高；3）教案目标多集中在低阶认知水平（记忆与理解）。</p>
<p>Insight: 核心洞见在于：模型设计决定可读性，而提示框架影响教学可靠性。最优方案需结合可读性强的模型与结构化提示框架（如RACE）。</p>
<p>Abstract: This study evaluates the pedagogical soundness and usability of AI-generated lesson plans across five leading large language models: ChatGPT (GPT-5), Claude Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice, three structured prompt frameworks were tested: TAG (Task, Audience, Goal), RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective, Style, Tone, Audience, Response Format).   Fifteen lesson plans were generated for a single high-school physics topic, The Electromagnetic Spectrum. The lesson plans were analyzed through four automated computational metrics: (1) readability and linguistic complexity, (2) factual accuracy and hallucination detection, (3) standards and curriculum alignment, and (4) cognitive demand of learning objectives.   Results indicate that model selection exerted the strongest influence on linguistic accessibility, with DeepSeek producing the most readable teaching plan (FKGL &#x3D; 8.64) and Claude generating the densest language (FKGL &#x3D; 19.89).   The prompt framework structure most strongly affected the factual accuracy and pedagogical completeness, with the RACE framework yielding the lowest hallucination index and the highest incidental alignment with NGSS curriculum standards. Across all models, the learning objectives in the fifteen lesson plans clustered at the Remember and Understand tiers of Bloom’s taxonomy. There were limited higher-order verbs in the learning objectives extracted.   Overall, the findings suggest that readability is significantly governed by model design, while instructional reliability and curricular alignment depend more on the prompt framework. The most effective configuration for lesson plans identified in the results was to combine a readability-optimized model with the RACE framework and an explicit checklist of physics concepts, curriculum standards, and higher-order objectives.</p>
</details>


<h3 id="2-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model"><a href="#2-From-Denoising-to-Refining-A-Corrective-Framework-for-Vision-Language-Diffusion-Model" class="headerlink" title="[2] From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19871">From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model</a></h3><p><em>Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ReDiff通过主动纠错框架解决视觉语言扩散模型中的错误级联问题，显著提升了生成内容的连贯性和事实准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在视觉语言任务中虽有潜力，但训练与推理的不一致性导致错误级联，影响生成质量。

<p>Contribution: 提出ReDiff框架，将生成过程从被动去噪转为主动修正，赋予模型自我纠错能力。</p>
<p>Method: 采用两阶段训练：首阶段训练模型修正合成错误；第二阶段引入在线自修正循环，模型学习专家的修正。</p>
<p>Result: ReDiff明显改善了生成内容的连贯性和事实准确性，并行生成效率优于传统去噪方法。</p>
<p>Insight: 主动修正机制能有效打破错误级联，提升模型生成的稳定性和质量。</p>
<p>Abstract: Discrete diffusion models have emerged as a promising direction for vision-language tasks, offering bidirectional context modeling and theoretical parallelization. However, their practical application is severely hindered by a train-inference discrepancy, which leads to catastrophic error cascades: initial token errors during parallel decoding pollute the generation context, triggering a chain reaction of compounding errors and leading to syntactic errors and semantic hallucinations. To address this fundamental challenge, we reframe the generation process from passive denoising to active refining. We introduce ReDiff, a refining-enhanced diffusion framework that teaches the model to identify and correct its own errors. Our approach features a two-stage training process: first, we instill a foundational revision capability by training the model to revise synthetic errors; second, we implement a novel online self-correction loop where the model is explicitly trained to revise its own flawed drafts by learning from an expert’s corrections. This mistake-driven learning endows the model with the crucial ability to revisit and refine its already generated output, effectively breaking the error cascade. Extensive experiments demonstrate that ReDiff significantly improves the coherence and factual accuracy of generated content, enabling stable and efficient parallel generation far superior to traditional denoising methods. Our codes and models are available at <a target="_blank" rel="noopener" href="https://rediff-hku.github.io/">https://rediff-hku.github.io/</a>.</p>
</details>


<h3 id="3-Stream-Scaling-up-Mechanistic-Interpretability-to-Long-Context-in-LLMs-via-Sparse-Attention"><a href="#3-Stream-Scaling-up-Mechanistic-Interpretability-to-Long-Context-in-LLMs-via-Sparse-Attention" class="headerlink" title="[3] Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19875">Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention</a></h3><p><em>J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: Stream通过稀疏注意力技术实现了长上下文LLM的高效机制可解释性分析，解决了传统方法内存需求大的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 随着LLM上下文长度增长，传统机制可解释性方法因内存需求呈二次方增长而难以扩展。

<p>Contribution: 提出了Sparse Tracing技术和Stream算法，实现了线性时间和空间复杂度的稀疏注意力分析。</p>
<p>Method: Stream采用分层剪枝和二分搜索保留关键注意力区块，保持模型性能的同时大幅减少交互数量。</p>
<p>Result: 在RULER基准测试中，Stream保留了关键检索路径并剪枝90-96%的交互，显着降低内存需求。</p>
<p>Insight: Stream使得长上下文机制可解释性分析在消费级GPU上变得可行，推动了思维链监控的普及。</p>
<p>Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional Mechanistic Interpretability techniques for analyzing attention scale quadratically with context length, demanding terabytes of memory beyond 100,000 tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic sparse attention to efficiently analyze long context attention patterns. We present Stream, a compilable hierarchical pruning algorithm that estimates per-head sparse attention masks in near-linear time $O(T \log T)$ and linear space $O(T)$, enabling one-pass interpretability at scale. Stream performs a binary-search-style refinement to retain only the top-$k$ key blocks per query while preserving the model’s next-token behavior. We apply Stream to long chain-of-thought reasoning traces and identify thought anchors while pruning 97-99% of token interactions. On the RULER benchmark, Stream preserves critical retrieval paths while discarding 90-96% of interactions and exposes layer-wise routes from the needle to output. Our method offers a practical drop-in tool for analyzing attention patterns and tracing information flow without terabytes of caches. By making long context interpretability feasible on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring. Code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/stream-03B8/">https://anonymous.4open.science/r/stream-03B8/</a>.</p>
</details>


<h3 id="4-Automated-HIV-Screening-on-Dutch-EHR-with-Large-Language-Models"><a href="#4-Automated-HIV-Screening-on-Dutch-EHR-with-Large-Language-Models" class="headerlink" title="[4] Automated HIV Screening on Dutch EHR with Large Language Models"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19879">Automated HIV Screening on Dutch EHR with Large Language Models</a></h3><p><em>Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart–Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了一种利用大型语言模型（LLM）分析电子健康记录（EHR）中非结构化文本的新方法，用于自动化HIV筛查，并展示了其在荷兰伊拉斯谟大学医学中心临床数据上的高准确性和低假阴性率。</p>
<details>
  <summary>Details</summary>
Motivation: HIV的高效筛查和早期诊断对减少传播至关重要。尽管大规模实验室检测不可行，但电子健康记录（EHR）的普及为利用非结构化文本数据（如临床笔记）提供了新机会。

<p>Contribution: 论文的主要贡献是提出了一种基于LLM的自动化HIV筛查新方法，能够有效挖掘EHR中的非结构化文本数据，填补了现有研究主要依赖结构化数据的空白。</p>
<p>Method: 该方法结合了LLM和EHR非结构化文本分析技术，设计了一个自动化筛查流程，用于判断患者是否需要进一步HIV检测。</p>
<p>Result: 实验结果表明，该方法在临床数据上表现出高准确性和低假阴性率。</p>
<p>Insight: 研究展示了LLM在医疗领域非结构化数据处理中的潜力，为HIV筛查提供了新的技术路径。</p>
<p>Abstract: Efficient screening and early diagnosis of HIV are critical for reducing onward transmission. Although large scale laboratory testing is not feasible, the widespread adoption of Electronic Health Records (EHRs) offers new opportunities to address this challenge. Existing research primarily focuses on applying machine learning methods to structured data, such as patient demographics, for improving HIV diagnosis. However, these approaches often overlook unstructured text data such as clinical notes, which potentially contain valuable information relevant to HIV risk. In this study, we propose a novel pipeline that leverages a Large Language Model (LLM) to analyze unstructured EHR text and determine a patient’s eligibility for further HIV testing. Experimental results on clinical data from Erasmus University Medical Center Rotterdam demonstrate that our pipeline achieved high accuracy while maintaining a low false negative rate.</p>
</details>


<h3 id="5-Can-They-Dixit-Yes-they-Can-Dixit-as-a-Playground-for-Multimodal-Language-Model-Capabilities"><a href="#5-Can-They-Dixit-Yes-they-Can-Dixit-as-a-Playground-for-Multimodal-Language-Model-Capabilities" class="headerlink" title="[5] Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19892">Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</a></h3><p><em>Nishant Balepur,Dang Nguyen,Dayeon Ki</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了一种基于游戏的评估方法（Dixit游戏）来全面评估多模态大语言模型（MLM）的能力，克服了传统静态或主观评估方法的局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法通常是静态的或依赖主观比较，存在无法全面评估、主观性强、成本高以及模型可能利用表面捷径等问题。因此，作者希望通过游戏（Dixit）提供一个更全面、客观且有趣的评估框架。

<p>Contribution: 论文的主要贡献是提出了一种基于游戏的评估方法（Dixit），通过游戏的竞争性和多任务性，更全面地评估MLM的能力，并与传统评估方法的相关性进行了验证。</p>
<p>Method: 作者使用Dixit游戏作为评估平台，要求MLM为幻想卡片生成标题，目标是迷惑部分但非全部玩家选择正确的卡片。通过定量实验比较了五种MLM的表现。</p>
<p>Result: 实验表明，Dixit游戏的胜率排名与传统MLM基准测试的排名完全一致。同时，人类与MLM玩家的对局揭示了MLM推理能力的差异和改进空间。</p>
<p>Insight: 游戏可以作为评估多模态语言模型能力的有效工具，不仅提供客观的评估指标，还能揭示模型的策略和推理能力的局限性。</p>
<p>Abstract: Multi-modal large language models (MLMs) are often assessed on static, individual benchmarks – which cannot jointly assess MLM capabilities in a single task – or rely on human or model pairwise comparisons – which is highly subjective, expensive, and allows models to exploit superficial shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these issues, we propose game-based evaluations to holistically assess MLM capabilities. Games require multiple abilities for players to win, are inherently competitive, and are governed by fix, objective rules, and makes evaluation more engaging, providing a robust framework to address the aforementioned challenges. We manifest this evaluation specifically through Dixit, a fantasy card game where players must generate captions for a card that trick some, but not all players, into selecting the played card. Our quantitative experiments with five MLMs show Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks, while games between human and MLM players in Dixit reveal several differences between agent strategies and areas of improvement for MLM reasoning.</p>
</details>


<h3 id="6-Large-Language-Model-enabled-Mathematical-Modeling"><a href="#6-Large-Language-Model-enabled-Mathematical-Modeling" class="headerlink" title="[6] Large Language Model enabled Mathematical Modeling"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19895">Large Language Model enabled Mathematical Modeling</a></h3><p><em>Guoyun Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 这篇论文探讨了大型语言模型（LLMs）与优化建模的结合如何改进运筹学（OR）中的决策制定。研究表明，DeepSeek-R1模型能够通过自然语言理解和代码生成弥补传统建模中的专业性差距。</p>
<details>
  <summary>Details</summary>
Motivation: 传统优化方法（如线性规划和混合整数规划）高度依赖领域专业知识，限制了非专家用户的建模能力。论文提出通过LLMs降低建模门槛，提升运筹学问题的实际应用能力。

<p>Contribution: 论文的主要贡献包括：1）系统评估DeepSeek-R1在四个OR基准（NL4OPT、IndustryOR、EasyLP、ComplexOR）上的表现；2）提出幻觉分类和缓解策略（如LLM-as-a-Judge、Few-shot学习等）以提高模型准确性。</p>
<p>Method: 研究方法包括基准测试、幻觉分类开发和应用多种缓解技术（Few-shot学习、工具调用、多智能体框架），以提升模型在OR任务中的准确性和用户对齐度。</p>
<p>Result: DeepSeek-R1在LiveCodeBench和Math-500等基准测试中表现优异，但在实际OR场景中的应用仍有待验证，论文提供了系统性评估和改进方法。</p>
<p>Insight: 论文表明，尽管LLMs（如GPT-4）在自然语言处理和推理任务中表现出色，但其高昂的成本和幻觉问题限制了实际应用。DeepSeek-R1作为一种高效替代方案，有望通过针对性优化解决这些问题。</p>
<p>Abstract: The integration of Large Language Models (LLMs) with optimization modeling offers a promising avenue for advancing decision-making in operations research (OR). Traditional optimization methods,such as linear programming, mixed integer programming, and simulation depend heavily on domain expertise to translate real-world problems into solvable mathematical models. While solvers like Gurobi and COPT are powerful, expert input remains essential for defining objectives, constraints, and variables. This research investigates the potential of LLMs, specifically the DeepSeek-R1 model, to bridge this formulation gap using natural language understanding and code generation. Although prior models like GPT-4, Claude, and Bard have shown strong performance in NLP and reasoning tasks, their high token costs and tendency toward hallucinations limit real-world applicability in supply chain contexts. In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained with reinforcement learning, presents a viable alternative. Despite its success in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied OR scenarios remains under explored. This study systematically evaluates DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and ComplexOR. Our methodology includes baseline assessments, the development of a hallucination taxonomy, and the application of mitigation strategies like LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent Framework. These techniques aim to reduce hallucinations, enhance formulation accuracy, and better align model outputs with user intent.</p>
</details>


<h3 id="7-Learning-from-Supervision-with-Semantic-and-Episodic-Memory-A-Reflective-Approach-to-Agent-Adaptation"><a href="#7-Learning-from-Supervision-with-Semantic-and-Episodic-Memory-A-Reflective-Approach-to-Agent-Adaptation" class="headerlink" title="[7] Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19897">Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</a></h3><p><em>Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文提出了一种基于记忆增强的框架，利用语义记忆和情景记忆，通过LLM生成的评论来优化代理的学习能力，无需参数更新，实现了显著性能提升。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的微调方法成本高、灵活性差且不透明，而仅依赖标签的检索增强生成方法性能有限。本文旨在探索一种更灵活、高效的替代方案。

<p>Contribution: 1. 提出了一个结合语义记忆和情景记忆的框架；2. 引入了一种新指标“可提示性”来解释模型行为；3. 展示了LLM生成的评论能显著提升性能。</p>
<p>Method: 利用情景记忆存储实例级评论，语义记忆提炼为任务级指导，结合LLM生成的评论进行学习，避免了参数更新。</p>
<p>Result: 在多样化任务上，结合评论的方法比仅依赖标签的检索增强基线提升了24.8%的准确率。</p>
<p>Insight: 开源模型和OpenAI模型在处理事实性和偏好性数据时表现不同，记忆策略与模型特性共同影响学习动态。</p>
<p>Abstract: We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.</p>
</details>


<h3 id="8-LyriCAR-A-Difficulty-Aware-Curriculum-Reinforcement-Learning-Framework-For-Controllable-Lyric-Translation"><a href="#8-LyriCAR-A-Difficulty-Aware-Curriculum-Reinforcement-Learning-Framework-For-Controllable-Lyric-Translation" class="headerlink" title="[8] LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19967">LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation</a></h3><p><em>Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: LyriCAR是一个基于课程强化学习的歌词翻译框架，通过难度感知设计和自适应策略提升翻译质量，减少训练步骤。</p>
<details>
  <summary>Details</summary>
Motivation: 现有歌词翻译方法依赖手工规则和句子级建模，难以处理段落级的音乐-语言模式，缺乏泛化能力。

<p>Contribution: 提出了LyriCAR框架，包含难度感知课程设计和自适应策略，实现高效无监督训练，提升翻译质量和效率。</p>
<p>Method: 采用课程强化学习，动态调整训练难度，逐步引导模型学习复杂任务。</p>
<p>Result: 在EN-ZH歌词翻译任务中达到SOTA，训练步骤减少40%，保持高性能。</p>
<p>Insight: 课程设计能有效分配训练资源，加速收敛，提升模型对复杂约束的处理能力。</p>
<p>Abstract: Lyric translation is a challenging task that requires balancing multiple musical constraints. Existing methods often rely on hand-crafted rules and sentence-level modeling, which restrict their ability to internalize musical-linguistic patterns and to generalize effectively at the paragraph level, where cross-line coherence and global rhyme are crucial. In this work, we propose LyriCAR, a novel framework for controllable lyric translation that operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware curriculum designer and an adaptive curriculum strategy, ensuring efficient allocation of training resources, accelerating convergence, and improving overall translation quality by guiding the model with increasingly complex challenges. Extensive experiments on the EN-ZH lyric translation task show that LyriCAR achieves state-of-the-art results across both standard translation metrics and multi-dimensional reward scores, surpassing strong baselines. Notably, the adaptive curriculum strategy reduces training steps by nearly 40% while maintaining superior performance. Code, data and model can be accessed at <a target="_blank" rel="noopener" href="https://github.com/rle27/LyriCAR">https://github.com/rle27/LyriCAR</a>.</p>
</details>


<h3 id="9-LLM-Augmented-Symbolic-NLU-System-for-More-Reliable-Continuous-Causal-Statement-Interpretation"><a href="#9-LLM-Augmented-Symbolic-NLU-System-for-More-Reliable-Continuous-Causal-Statement-Interpretation" class="headerlink" title="[9] LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19988">LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation</a></h3><p><em>Xin Lian,Kenneth D. Forbus</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种结合大型语言模型（LLM）和符号化自然语言理解（NLU）系统的混合方法，以提高因果陈述的解释可靠性。</p>
<details>
  <summary>Details</summary>
Motivation: LLM依赖概率推理，容易产生事实幻觉和不一致的结构输出；符号化NLU系统虽可解释但覆盖范围有限且维护困难。

<p>Contribution: 提出一种混合方法，整合LLM的广泛语言处理能力和符号化NLU的结构化表示能力，提升任务的准确性。</p>
<p>Method: 使用LLM进行文本重述和简化，填补知识空白；符号化NLU生成结构化表示以支持推理和增量学习。</p>
<p>Result: 实验结果表明，混合方法在提取和解释常识科学文本中的数量和因果规律任务上显著优于纯符号化方法。</p>
<p>Insight: 结合LLM的广泛性和符号化NLU的可靠性，有望在自然语言理解任务中取得更好的效果。</p>
<p>Abstract: Despite the broad applicability of large language models (LLMs), their reliance on probabilistic inference makes them vulnerable to errors such as hallucination in generated facts and inconsistent output structure in natural language understanding (NLU) tasks. By contrast, symbolic NLU systems provide interpretable understanding grounded in curated lexicons, semantic resources, and syntactic &amp; semantic interpretation rules. They produce relational representations that can be used for accurate reasoning and planning, as well as incremental debuggable learning. However, symbolic NLU systems tend to be more limited in coverage than LLMs and require scarce knowledge representation and linguistics skills to extend and maintain. This paper explores a hybrid approach that integrates the broad-coverage language processing of LLMs with the symbolic NLU capabilities of producing structured relational representations to hopefully get the best of both approaches. We use LLMs for rephrasing and text simplification, to provide broad coverage, and as a source of information to fill in knowledge gaps more automatically. We use symbolic NLU to produce representations that can be used for reasoning and for incremental learning. We evaluate this approach on the task of extracting and interpreting quantities and causal laws from commonsense science texts, along with symbolic- and LLM-only pipelines. Our results suggest that our hybrid method works significantly better than the symbolic-only pipeline.</p>
</details>


<h3 id="10-A-Fundamental-Algorithm-for-Dependency-Parsing-With-Corrections"><a href="#10-A-Fundamental-Algorithm-for-Dependency-Parsing-With-Corrections" class="headerlink" title="[10] A Fundamental Algorithm for Dependency Parsing (With Corrections)"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19996">A Fundamental Algorithm for Dependency Parsing (With Corrections)</a></h3><p><em>Michael A. Covington</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 这篇论文提出了一种基础算法，用于将自然语言句子解析为依存树，其特点是逐词处理，并在可以附加时立即附加每个词。</p>
<details>
  <summary>Details</summary>
Motivation: 动机在于模拟人脑解析语言的特性，提出一种更自然且高效的依存解析方法。

<p>Contribution: 主要贡献是提出了一种新的依存解析算法，其复杂度为O(n^3)，但在自然语言中，最坏情况仅在小规模时发生。</p>
<p>Method: 方法采用逐词处理的方式，每次处理一个词并在可能时立即附加到依存树中。</p>
<p>Result: 结果表明，该方法在理论上具有较高的效率，且在自然语言中表现良好。</p>
<p>Insight: 文章的洞见在于通过模拟人脑的解析过程，证明了逐词处理的依存解析方法的可行性和高效性。</p>
<p>Abstract: This paper presents a fundamental algorithm for parsing natural language sentences into dependency trees. Unlike phrase-structure (constituency) parsers, this algorithm operates one word at a time, attaching each word as soon as it can be attached, corresponding to properties claimed for the parser in the human brain. Like phrase-structure parsing, its worst-case complexity is $O(n^3)$, but in human language, the worst case occurs only for small $n$.</p>
</details>


<h3 id="11-Beyond-MedQA-Towards-Real-world-Clinical-Decision-Making-in-the-Era-of-LLMs"><a href="#11-Beyond-MedQA-Towards-Real-world-Clinical-Decision-Making-in-the-Era-of-LLMs" class="headerlink" title="[11] Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20001">Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs</a></h3><p><em>Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 这篇论文提出了一种超越MedQA的统一范式，用于评估和改进大型语言模型（LLMs）在真实临床决策中的应用，强调临床背景和问题的复杂性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的医疗问答数据集（如MedQA）过于简化，无法反映真实临床决策的复杂性，因此需要一种更全面的评估范式。

<p>Contribution: 论文提出了一个基于临床背景和临床问题的统一范式，用于标准化LLMs在临床决策中的评估和改进。</p>
<p>Method: 总结了现有数据集和方法在两个维度（临床背景和问题）上的设置，提出了包括训练时和测试时技术的解决方案。</p>
<p>Result: 通过扩展评估指标（如效率、可解释性），论文揭示了LLMs在临床决策中的潜在优势和限制。</p>
<p>Insight: 真实临床环境的复杂性需要更全面的评估和改进方法，而不仅限于传统的问答准确性。</p>
<p>Abstract: Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.</p>
</details>


<h3 id="12-Forging-GEMs-Advancing-Greek-NLP-through-Quality-Based-Corpus-Curation-and-Specialized-Pre-training"><a href="#12-Forging-GEMs-Advancing-Greek-NLP-through-Quality-Based-Corpus-Curation-and-Specialized-Pre-training" class="headerlink" title="[12] Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20002">Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training</a></h3><p><em>Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文针对希腊语自然语言处理的瓶颈问题，通过高质量语料库构建和多样化预训练模型提出了Greek Embedding Models (GEMs)，显著提升了希腊语尤其是法律领域任务的表现。</p>
<details>
  <summary>Details</summary>
Motivation: 希腊语作为形态丰富但资源中等的语言，现有NLP研究分散且依赖早期Transformer架构，尤其在高价值法律领域缺乏长文本处理能力。

<p>Contribution: 1. 提出GEMs模型家族；2. 构建高质量通用和法律领域希腊语料库；3. 预训练多样化架构（如ELECTRA、ConvBERT）；4. 推出首个希腊-英语双语法律领域嵌入模型。</p>
<p>Method: 基于质量驱动的语料筛选与预处理，训练ELECTRA、ConvBERT等多种现代架构，并评估其在希腊语任务中的表现。</p>
<p>Result: GEM-RoBERTa和GEM-ConvBERT在下游任务中显著超越基线模型。</p>
<p>Insight: 高质量的语料库和多样化模型架构是提升中等资源语言NLP性能的关键。</p>
<p>Abstract: The advancement of natural language processing for morphologically rich, moderately-resourced languages like Modern Greek is often hindered by a fragmented research landscape, a lack of architectural diversity and reliance on limited context-length models. This is particularly true in specialized, high-value domains such as law, where existing models are frequently confined to early transformer architectures with a restrictive 512-token window, insufficient for analyzing long legal documents. To address these challenges, this paper presents Greek Embedding Models, a new family of transformer models for Greek language built upon a foundation of extensive, quality-driven data curation. We detail the construction of several large-scale Greek corpora, emphasizing a rigorous, quality-based filtering and preprocessing methodology to create high-value training datasets from both general-domain and specialized legal sources. On this carefully curated foundation, we pre-train and systematically evaluate a diverse suite of modern architectures, which has not previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT. Furthermore, we propose the first bilingual Greek-English Embedding Models tailored for the legal domain. The extensive experiments on downstream tasks demonstrate that the new class of models establish the effectiveness of the proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models significantly outperform existing baselines.</p>
</details>


<h3 id="13-Enhancing-Reasoning-Skills-in-Small-Persian-Medical-Language-Models-Can-Outperform-Large-Scale-Data-Training"><a href="#13-Enhancing-Reasoning-Skills-in-Small-Persian-Medical-Language-Models-Can-Outperform-Large-Scale-Data-Training" class="headerlink" title="[13] Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20059">Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training</a></h3><p><em>Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 利用RLAIF和DPO方法提升波斯语小语言模型的推理能力，在医学问答任务中表现优于大模型。</p>
<details>
  <summary>Details</summary>
Motivation: 在波斯语等资源稀缺语言中，小语言模型的推理能力对医学问答等专业应用至关重要。研究旨在探索高效训练方法，减少对大规模数据的依赖。

<p>Contribution: 1. 提出结合RLAIF和DPO的训练框架，提升小模型的推理能力；2. 构建波斯语医学推理数据集；3. 小模型在数据量显著减少的情况下超越基线（gaokerena-V）。</p>
<p>Method: 1. 翻译医学多选题数据集到波斯语；2. 使用RLAIF生成偏好-拒绝答案对；3. 结合CoT提示生成推理轨迹数据集；4. 通过DPO训练模型。</p>
<p>Result: 训练后的模型（仅用2M&#x2F;2.5M token数据集）在波斯语医学推理任务中优于gaokerena-V（57M token训练）。</p>
<p>Insight: 推理能力训练（如CoT和DPO）对小语言模型至关重要，能在数据稀缺情况下实现高效性能提升。</p>
<p>Abstract: Enhancing reasoning capabilities in small language models is critical for specialized applications such as medical question answering, particularly in underrepresented languages like Persian. In this study, we employ Reinforcement Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to improve the reasoning skills of a general-purpose Persian language model. To achieve this, we translated a multiple-choice medical question-answering dataset into Persian and used RLAIF to generate rejected-preferred answer pairs, which are essential for DPO training. By prompting both teacher and student models to produce Chain-of-Thought (CoT) reasoning responses, we compiled a dataset containing correct and incorrect reasoning trajectories. This dataset, comprising 2 million tokens in preferred answers and 2.5 million tokens in rejected ones, was used to train a baseline model, significantly enhancing its medical reasoning capabilities in Persian. Remarkably, the resulting model outperformed its predecessor, gaokerena-V, which was trained on approximately 57 million tokens, despite leveraging a much smaller dataset. These results highlight the efficiency and effectiveness of reasoning-focused training approaches in developing domain-specific language models with limited data availability.</p>
</details>


<h3 id="14-CreativityPrism-A-Holistic-Benchmark-for-Large-Language-Model-Creativity"><a href="#14-CreativityPrism-A-Holistic-Benchmark-for-Large-Language-Model-Creativity" class="headerlink" title="[14] CreativityPrism: A Holistic Benchmark for Large Language Model Creativity"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20091">CreativityPrism: A Holistic Benchmark for Large Language Model Creativity</a></h3><p><em>Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了CreativityPrism，一个全面评估大型语言模型（LLM）创造力的框架，将创造力分解为质量、新颖性和多样性三个维度，并设计了多任务、多领域和多指标的评估方法。通过评估17个先进LLM，发现专有模型与开源模型之间存在显著差距，且模型在同一领域内的任务表现高度相关，但在不同领域间相关较弱。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的创造力评估方法分散且不一致，缺乏统一的框架来衡量LLM的创造力表现。因此，作者提出CreativityPrism，旨在提供一个全面的评估工具，以更科学地分析LLM的创造力。

<p>Contribution: 论文的主要贡献是提出了CreativityPrism框架，将创造力分解为质量、新颖性和多样性三个维度，并设计了涵盖多个任务、领域和指标的综合性评估方法。此外，论文还通过实验揭示了LLM在不同领域和维度上的表现差异。</p>
<p>Method: CreativityPrism基于创造力三维度（质量、新颖性、多样性），设计了九个任务、三个领域（发散思维、创意写作、逻辑推理）和二十个评估指标。作者评估了17个先进LLM，分析了模型在任务、领域和维度上的表现相关性。</p>
<p>Result: 实验结果显示，专有模型与开源模型之间存在显著差距；模型在同一领域内的任务表现高度相关，而在跨领域任务中相关较弱。质量和多样性表现出强相关性，而新颖性与前两者相关性较弱。</p>
<p>Insight: 论文表明，LLM的创造力表现具有领域和维度特异性，单一任务或维度的评估无法全面反映模型的创造力水平。这强调了需要采用多维度、多领域的综合性评估方法。</p>
<p>Abstract: Creativity is often seen as a hallmark of human intelligence. While large language models (LLMs) are increasingly perceived as producing creative text, there is still no holistic framework to evaluate their creativity across diverse scenarios. Existing evaluation methods remain fragmented, with dramatic variation across domains and tasks, largely due to differing definitions and measurements of creativity. Inspired by the hypothesis that creativity is not one fixed idea, we propose CreativityPrism, an evaluation analysis framework that decomposes creativity into three dimensions: quality, novelty, and diversity. CreativityPrism incorporates nine tasks, three domains, i.e., divergent thinking, creative writing, and logical reasoning, and twenty evaluation metrics, which measure each dimension in task-specific, unique ways. We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on CreativityPrism and analyze the performance correlations among different metrics and task domains. Our results reveal a notable gap between proprietary and open-source models. Overall, model performance tends to be highly correlated across tasks within the same domain and less so across different domains. Among evaluation dimensions, diversity and quality metrics show strong correlations - models that perform well on one often excel on the other - whereas novelty exhibits much weaker correlation with either. These findings support our hypothesis that strong performance in one creativity task or dimension does not necessarily generalize to others, underscoring the need for a holistic evaluation of LLM creativity.</p>
</details>


<h3 id="15-Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning"><a href="#15-Leveraging-the-Power-of-Large-Language-Models-in-Entity-Linking-via-Adaptive-Routing-and-Targeted-Reasoning" class="headerlink" title="[15] Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20098">Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning</a></h3><p><em>Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ARTER提出了一种结合自适应路由和选择性推理的结构化流水线，通过候选生成、上下文评分和高效推理，显著提升了实体链接的性能和效率。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的实体链接方法依赖大量标注数据和精细调优，而现有的few-shot方法虽然减少了训练需求，但推理成本高且效率低下。

<p>Contribution: ARTER通过自适应路由和选择性LLM推理，在不依赖深度调优的情况下，实现了高性能和高效率的实体链接。</p>
<p>Method: ARTER分为候选生成、上下文评分、自适应路由和选择性LLM推理四步，动态区分易处理与难处理mention，分别用低计算链接器和LLM处理。</p>
<p>Result: 在标准基准测试中，ARTER比ReFinED性能提升最高4.47%，平均提升2.53%，且效率是纯LLM流水线的两倍。</p>
<p>Insight: 结合嵌入和LLM信号的自适应策略能显著提升任务性能，同时通过动态路由优化计算开销。</p>
<p>Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and extensive model fine-tuning. While recent few-shot methods leverage large language models (LLMs) through prompting to reduce training requirements, they often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER (Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline that achieves high performance without deep fine-tuning by strategically combining candidate generation, context-based scoring, adaptive routing, and selective reasoning. ARTER computes a small set of complementary signals(both embedding and LLM-based) over the retrieved candidates to categorize contextual mentions into easy and hard cases. The cases are then handled by a low-computational entity linker (e.g. ReFinED) and more expensive targeted LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets, and performs comparably to pipelines using LLM-based reasoning for all mentions, while being as twice as efficient in terms of the number of LLM tokens.</p>
</details>


<h3 id="16-BoundRL-Efficient-Structured-Text-Segmentation-through-Reinforced-Boundary-Generation"><a href="#16-BoundRL-Efficient-Structured-Text-Segmentation-through-Reinforced-Boundary-Generation" class="headerlink" title="[16] BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20151">BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation</a></h3><p><em>Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: BoundRL提出了一种高效的结构化文本分割方法，通过强化学习的边界生成实现语义对齐和文档重构，显著降低了推理成本并减少了幻觉。</p>
<details>
  <summary>Details</summary>
Motivation: 随着结构化文本（如技术报告和生成式AI提示）的复杂性增加，传统句子或段落级别的分割方法无法有效处理包含表格、代码片段和占位符等内容。这激发了对高效分割方法的需求。

<p>Contribution: 1. 提出了BoundRL方法，通过仅生成起始标记序列实现高效分割和标签预测；2. 设计了结合重建保真度和语义对齐的强化学习奖励（RLVR）；3. 引入中间候选集缓解熵崩塌问题。</p>
<p>Method: BoundRL使用强化学习生成起始标记序列，结合RLVR奖励函数优化分割质量。为缓解熵崩塌，系统扰动生成序列生成中间候选集。</p>
<p>Result: 实验表明，BoundRL让小模型（1.7B参数）优于大模型的少样本提示；RLVR和监督微调相比带来显著改进，中间候选集进一步提升性能和泛化能力。</p>
<p>Insight: 仅生成边界而非完整内容可以高效分割并减少幻觉；强化学习的奖励设计对性能至关重要；扰动生成序列能提高模型泛化能力。</p>
<p>Abstract: As structured texts become increasingly complex across diverse domains – from technical reports to generative AI prompts – the need for text segmentation into semantically meaningful components becomes critical. Such texts often contain elements beyond plain language, including tables, code snippets, and placeholders, which conventional sentence- or paragraph-level segmentation methods cannot handle effectively. To address this challenge, we propose BoundRL, a novel and efficient approach that jointly performs token-level text segmentation and label prediction for long structured texts. Instead of generating complete contents for each segment, it generates only a sequence of starting tokens and reconstructs the complete contents by locating these tokens within the original texts, thereby reducing inference costs by orders of magnitude and minimizing hallucination. To adapt the model for the output format, BoundRL~performs reinforcement learning with verifiable rewards (RLVR) with a specifically designed reward that jointly optimizes document reconstruction fidelity and semantic alignment. To mitigate entropy collapse, it further constructs intermediate candidates by systematically perturbing a fraction of generated sequences of segments to create stepping stones toward higher-quality solutions. To demonstrate BoundRL’s effectiveness on particularly challenging structured texts, we focus evaluation on complex prompts used for LLM applications. Experiments show that BoundRL enables small language models (1.7B parameters) to outperform few-shot prompting of much larger models. Moreover, RLVR with our designed reward yields significant improvements over supervised fine-tuning, and incorporating intermediate candidates further improves both performance and generalization.</p>
</details>


<h3 id="17-DeepWideSearch-Benchmarking-Depth-and-Width-in-Agentic-Information-Seeking"><a href="#17-DeepWideSearch-Benchmarking-Depth-and-Width-in-Agentic-Information-Seeking" class="headerlink" title="[17] DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20168">DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking</a></h3><p><em>Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: DeepWideSearch 是一个新的基准测试，旨在评估信息搜索代理在同时实现深度推理和广域信息收集的能力。实验表明，当前最先进的代理在此任务上表现不佳，揭示了其在反思、内部知识依赖、检索不足和上下文溢出等方面的局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的信息搜索代理无法同时实现深度推理和广域信息收集，这在现实应用（如市场分析）中是一个关键缺陷。

<p>Contribution: 1. 提出了首个专注于深度与广度结合的信息搜索基准测试 DeepWideSearch。2. 通过实验揭示了当前代理的四大失败模式。3. 公开了包含 220 个问题和 15 个领域的评测数据集。</p>
<p>Method: 1. 通过转换现有数据集构建了 DeepWideSearch。2. 设计了评测指标和方法来分析代理的表现和失败模式。</p>
<p>Result: 实验结果显示，即使是当前最先进的代理平均成功率仅为 2.39%，验证了任务的高难度。</p>
<p>Insight: 1. 同时实现深度推理和广域信息收集是信息搜索代理的重要挑战。2. 当前的代理架构在多跳检索和大规模上下文处理方面存在显著缺陷。</p>
<p>Abstract: Current search agents fundamentally lack the ability to simultaneously perform \textit{deep} reasoning over multi-hop retrieval and \textit{wide}-scale information collection-a critical deficiency for real-world applications like comprehensive market analysis and business development. To bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly designed to evaluate agents to integrate depth and width in information seeking. In DeepWideSearch, agents must process a large volume of data, each requiring deep reasoning over multi-hop retrieval paths. Specifically, we propose two methods to converse established datasets, resulting in a curated collection of 220 questions spanning 15 diverse domains. Extensive experiments demonstrate that even state-of-the-art agents achieve only 2.39% average success rate on DeepWideSearch, highlighting the substantial challenge of integrating depth and width search in information-seeking tasks. Furthermore, our error analysis reveals four failure modes: lack of reflection, overreliance on internal knowledge, insufficient retrieval, and context overflow-exposing key limitations in current agent architectures. We publicly release DeepWideSearch to catalyze future research on more capable and robust information-seeking agents.</p>
</details>


<h3 id="18-Mixture-of-Minds-Multi-Agent-Reinforcement-Learning-for-Table-Understanding"><a href="#18-Mixture-of-Minds-Multi-Agent-Reinforcement-Learning-for-Table-Understanding" class="headerlink" title="[18] Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20176">Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</a></h3><p><em>Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了Mixture-of-Minds框架，通过多智能体强化学习（RL）分解表格理解任务为规划、编码和回答三个角色，结合代码执行和自改进训练框架，显著提升了表格理解的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前用于表格理解的方法存在局限性：基于微调的LLMs易产生算术错误和幻觉，而基于工具的方法缺乏语义理解且依赖固定模式。需要一种结合鲁棒推理和可靠表格处理的方法。

<p>Contribution: 1. 提出了多智能体框架Mixture-of-Minds，将表格理解任务分解为规划、编码和回答三个角色；2. 引入了基于MCTS的自改进训练框架，通过RL优化智能体；3. 在TableBench上取得了62.13%的性能，超越现有方法。</p>
<p>Method: 1. 设计三智能体框架（规划、编码、回答），各司其职；2. 利用代码执行实现精确表格操作；3. 使用MCTS生成伪黄金轨迹，并通过RL优化智能体。</p>
<p>Result: 在TableBench上达到62.13%的性能，超越OpenAI-o4-mini-high等现有方法。</p>
<p>Insight: 多智能体分工协作结合RL可以有效提升表格理解的鲁棒性和精确性，同时避免了单一方法的局限性。</p>
<p>Abstract: Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.</p>
</details>


<h3 id="19-Stuck-in-the-Matrix-Probing-Spatial-Reasoning-in-Large-Language-Models"><a href="#19-Stuck-in-the-Matrix-Probing-Spatial-Reasoning-in-Large-Language-Models" class="headerlink" title="[19] Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20198">Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</a></h3><p><em>Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文通过五项任务探索了大型语言模型（LLMs）在文本输入上的空间推理能力，发现模型在小规模任务中表现尚可，但随着任务复杂性增加，性能急剧下降，揭示其缺乏稳健的空间表征能力。</p>
<details>
  <summary>Details</summary>
Motivation: 研究LLMs在空间推理任务中的表现，揭示其在语言理解和几何推理之间的能力差距。

<p>Contribution: 1）设计了五项任务全面评估LLMs的空间推理能力；2）揭示了LLMs在复杂空间任务中的性能衰减现象；3）指出了模型缺乏稳健空间表征的问题。</p>
<p>Method: 通过五项任务（如象限识别、几何变换、距离评估等）测试LLMs，逐步增加任务的网格维度和复杂性。</p>
<p>Result: LLMs在小规模任务中表现尚可（50%以上准确率），但随着复杂性增加，性能平均下降42.7%，最高达84%。所有初始准确率超过50%的任务均下降至少48%。</p>
<p>Insight: LLMs的空间推理能力较弱，性能随任务复杂性显著下降，表明其架构中缺乏有效的空间表征机制。这为未来结合语言与几何的基准测试提供了方向。</p>
<p>Abstract: This paper explores the spatial reasoning capability of large language models (LLMs) over textual input through a suite of five tasks aimed at probing their spatial understanding and computational abilities. The models were tested on both fundamental spatial reasoning and multi-step problem-solving within structured grid-based environments using tasks such as quadrant identification, geometric transformations, distance evaluation, word searches, and tile sliding. Each task was scaled in complexity through increasing grid dimensions, requiring models to extend beyond simple pattern recognition into abstract spatial reasoning. Our results reveal that while LLMs demonstrate moderate success in all tasks with small complexity and size, performance drops off rapidly as scale increases, with an average loss in accuracy of 42.7%, and reaching as high as 84%. Every test that began with over 50% accuracy showed a loss of at least 48%, illustrating the consistent nature of the deterioration. Furthermore, their struggles with scaling complexity hint at a lack of robust spatial representations in their underlying architectures. This paper underscores the gap between linguistic and spatial reasoning in LLMs, offering insights into their current limitations, and laying the groundwork for future integrative benchmarks at the intersection of language and geometry.</p>
</details>


<h3 id="20-Context-level-Language-Modeling-by-Learning-Predictive-Context-Embeddings"><a href="#20-Context-level-Language-Modeling-by-Learning-Predictive-Context-Embeddings" class="headerlink" title="[20] Context-level Language Modeling by Learning Predictive Context Embeddings"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20280">Context-level Language Modeling by Learning Predictive Context Embeddings</a></h3><p><em>Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了ContextLM框架，通过引入next-context prediction目标来增强传统的NTP方法，从而学习多token上下文的表示，提升了语言模型的语义捕捉和长距离上下文关系能力。</p>
<details>
  <summary>Details</summary>
Motivation: 传统NTP方法在捕捉高级语义结构和长距离上下文关系方面存在局限性，ContextLM的目标是通过预测多token上下文来解决这一问题。

<p>Contribution: 提出了ContextLM框架，引入了next-context prediction目标，增强了模型的上下文建模能力，同时保持了与传统自回归评估范式的兼容性。</p>
<p>Method: 通过在标准预训练中加入next-context prediction目标，学习预测性多token上下文表示，利用未来token块的误差信号进行训练。</p>
<p>Result: 在GPT2和Pythia模型上的实验表明，ContextLM在困惑度和下游任务性能上均有显著提升，同时计算开销极小。</p>
<p>Insight: next-context prediction为语言模型提供了一种可扩展且高效的增强路径，有助于提升长距离连贯性和注意力分配效果。</p>
<p>Abstract: Next-token prediction (NTP) is the cornerstone of modern large language models (LLMs) pretraining, driving their unprecedented capabilities in text generation, reasoning, and instruction following. However, the token-level prediction limits the model’s capacity to capture higher-level semantic structures and long-range contextual relationships. To overcome this limitation, we introduce \textbf{ContextLM}, a framework that augments standard pretraining with an inherent \textbf{next-context prediction} objective. This mechanism trains the model to learn predictive representations of multi-token contexts, leveraging error signals derived from future token chunks. Crucially, ContextLM achieves this enhancement while remaining fully compatible with the standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity). Extensive experiments on the GPT2 and Pythia model families, scaled up to $1.5$B parameters, show that ContextLM delivers consistent improvements in both perplexity and downstream task performance. Our analysis indicates that next-context prediction provides a scalable and efficient pathway to stronger language modeling, yielding better long-range coherence and more effective attention allocation with minimal computational overhead.</p>
</details>


<h3 id="21-Citation-Failure-Definition-Analysis-and-Efficient-Mitigation"><a href="#21-Citation-Failure-Definition-Analysis-and-Efficient-Mitigation" class="headerlink" title="[21] Citation Failure: Definition, Analysis and Efficient Mitigation"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20303">Citation Failure: Definition, Analysis and Efficient Mitigation</a></h3><p><em>Jan Buchmann,Iryna Gurevych</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了「引用失败」的新概念，并将其与「响应失败」区分开。通过CITECONTROL基准分析了引用失败的发生条件，并提出CITENTION框架来高效提升LLM的引用质量，实验证明了其有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的LLM基于RAG系统的引用功能旨在简化响应验证，但存在「引用失败」现象，即模型提供了有用响应却未能完整引用证据。论文旨在分析这一现象并找到高效解决方案，而不是将其与「响应失败」混淆。

<p>Contribution: 1. 明确定义并区分了「引用失败」和「响应失败」；2. 提出了CITECONTROL基准，用于系统性分析引用失败的发生条件；3. 提出了CITENTION框架，结合生成式、注意力机制和检索方法，显著提升了引用质量。</p>
<p>Method: 1. 通过CITECONTROL基准研究了响应与证据之间的关系对引用质量的影响；2. 提出了CITENTION框架，整合生成式、注意力机制和检索方法，优化LLM的引用表现。</p>
<p>Result: 实验表明，随着响应与证据关系的复杂度增加，引用失败率上升。CITENTION在CITECONTROL基准和迁移任务中均显著提升了引用表现。</p>
<p>Insight: 1. 引用失败与响应失败需分开处理；2. 响应与证据的关系复杂度是引用失败的关键因素；3. 多方法结合的框架能有效提升引用质量。</p>
<p>Abstract: Citations from LLM-based RAG systems are supposed to simplify response verification. However, this does not hold for citation failure, when a model generates a helpful response, but fails to cite complete evidence. In contrast to previous work, we propose to disentangle this from response failure, where the response itself is flawed, and citing complete evidence is impossible. To address citation failure, this work follows a two-step approach: (1) We study when citation failure occurs and (2) how it can be mitigated. For step 1, we extend prior work by investigating how the relation between response and evidence affects citation quality. We introduce CITECONTROL, a benchmark that systematically varies this relation to analyze failure modes. Experiments show that failures increase with relational complexity and suggest that combining citation methods could improve performance, motivating step 2. To improve LLM citation efficiently, we propose CITENTION, a framework integrating generative, attention-based, and retrieval-based methods. Results demonstrate substantial citation improvements on CITECONTROL and in transfer settings. We make our data and code publicly available.</p>
</details>


<h3 id="22-Exploring-Generative-Process-Reward-Modeling-for-Semi-Structured-Data-A-Case-Study-of-Table-Question-Answering"><a href="#22-Exploring-Generative-Process-Reward-Modeling-for-Semi-Structured-Data-A-Case-Study-of-Table-Question-Answering" class="headerlink" title="[22] Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20304">Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering</a></h3><p><em>Lei Tang,Wei Zhou,Mohsen Mesgar</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文首次系统地研究了过程奖励模型（PRMs）在表格问答（TQA）任务中的应用，发现结合文本和代码验证的PRMs有助于答案选择，但在跨域泛化上表现不佳，揭示了当前PRMs的局限性。</p>
<details>
  <summary>Details</summary>
Motivation: PRMs在复杂推理任务（如数学）中表现优异，但在半结构化数据（如表格问答）中的应用尚未探索。TQA的独特挑战（如冗余信息、步骤松散关联）需要深入研究。

<p>Contribution: 首次系统研究了PRMs在TQA任务中的表现，评估了结合文本和代码验证的PRMs的优缺点，并提出改进方向。</p>
<p>Method: 评估了最新的生成式PRMs在TQA任务中的应用，从答案和步骤两个角度分析其性能，特别关注文本与代码验证的结合效果。</p>
<p>Result: PRMs在TQA中能辅助答案选择，但跨域泛化能力弱；步骤验证性能与答案准确性相关性低，可能是由于步骤依赖性和因果联系较弱。</p>
<p>Insight: 当前PRMs在TQA中存在局限性，需设计更具鲁棒性、过程感知的验证模型以应对半结构化数据的挑战。</p>
<p>Abstract: Process reward models (PRMs) improve complex reasoning in large language models (LLMs) by grading candidate solutions step-by-step and selecting answers via aggregated step scores. While effective in domains such as mathematics, their applicability to tasks involving semi-structured data, like table question answering (TQA) remains unexplored. TQA poses unique challenges for PRMs, including abundant irrelevant information, loosely connected reasoning steps, and domain-specific reasoning. This work presents the first systematic study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from both answer and step perspectives. Results show that PRMs that combine textual and code verification can aid solution selection but struggle to generalize to out-of-domain data. Analysis reveals a weak correlation between performance in step-level verification and answer accuracy, possibly stemming from weak step dependencies and loose causal links. Our findings highlight limitations of current PRMs on TQA and offer valuable insights for building more robust, process-aware verifiers.</p>
</details>


<h3 id="23-Teaching-Language-Models-to-Reason-with-Tools"><a href="#23-Teaching-Language-Models-to-Reason-with-Tools" class="headerlink" title="[23] Teaching Language Models to Reason with Tools"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20342">Teaching Language Models to Reason with Tools</a></h3><p><em>Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了CoRT框架，通过Hint-Engineering和数据合成策略，教导大型推理模型有效利用代码解释器（CIs），优化其与外部工具的交互，显著提升了数学推理能力和效率。</p>
<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂数学运算中表现不佳，而直接集成外部工具（如代码解释器）会导致模型内部推理与外部确定性知识之间的冲突。为解决这一问题，CoRT框架被设计用于优化模型与工具的交互。

<p>Contribution: 1. 提出CoRT框架，通过Hint-Engineering合成高质量代码集成推理数据；2. 结合拒绝采样和强化学习优化多轮外部工具使用与内部思考的交替；3. 显著提升了模型性能和效率。</p>
<p>Method: 1. Hint-Engineering：在推理路径中注入多样化提示；2. 合成高质量数据用于监督微调；3. 结合拒绝采样和强化学习优化交互过程。</p>
<p>Result: 在五个数学推理数据集上，CoRT对32B和1.5B模型的绝对性能分别提升4%和8%，并显著减少token使用（32B和1.5B分别降低30%和50%）。</p>
<p>Insight: 1. 外部工具与模型内部推理的协作是关键；2. Hint-Engineering是高效优化交互的有效策略；3. CoRT框架为融合概率与确定性推理提供了新思路。</p>
<p>Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model’s internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT’s effectiveness, yielding absolute improvements of 4% and 8% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30% for the 32B model and 50% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: <a target="_blank" rel="noopener" href="https://github.com/ChengpengLi1003/CoRT">https://github.com/ChengpengLi1003/CoRT</a>.</p>
</details>


<h3 id="24-Evaluating-Latent-Knowledge-of-Public-Tabular-Datasets-in-Large-Language-Models"><a href="#24-Evaluating-Latent-Knowledge-of-Public-Tabular-Datasets-in-Large-Language-Models" class="headerlink" title="[24] Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20351">Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models</a></h3><p><em>Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该研究探讨了大型语言模型（LLM）是否对广泛使用的表格数据集（如Adult Income、Titanic等）存在先验知识污染。实验表明，当数据集包含强语义线索（如可理解的列名或值类别）时，LLMs表现出明显的污染效应；而在移除或随机化这些线索后，性能急剧下降至接近随机水平。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的LLM评估常忽略数据集污染问题，尤其是在表格推理任务中。研究旨在揭示LLMs是否依赖对公开数据集的记忆而非真实泛化能力。

<p>Contribution: 1. 揭示了LLMs对表格数据集的污染效应；2. 提出了区分语义污染与真实推理能力的策略，为未来评估提供参考。</p>
<p>Method: 通过控制实验，对比LLMs在原始数据集与语义线索移除&#x2F;随机化版本上的表现。</p>
<p>Result: LLMs在强语义线索数据集上表现优异，但在线索移除后性能骤降至随机水平。</p>
<p>Insight: LLMs的表格推理能力可能部分依赖数据集的语义记忆而非真正的泛化能力，需更严谨的评估设计。</p>
<p>Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to reason over structured data, yet such assessments often overlook a crucial confound: dataset contamination. In this work, we investigate whether LLMs exhibit prior knowledge of widely used tabular benchmarks such as Adult Income, Titanic, and others. Through a series of controlled probing experiments, we reveal that contamination effects emerge exclusively for datasets containing strong semantic cues-for instance, meaningful column names or interpretable value categories. In contrast, when such cues are removed or randomized, performance sharply declines to near-random levels. These findings suggest that LLMs’ apparent competence on tabular reasoning tasks may, in part, reflect memorization of publicly available datasets rather than genuine generalization. We discuss implications for evaluation protocols and propose strategies to disentangle semantic leakage from authentic reasoning ability in future LLM assessments.</p>
</details>


<h3 id="25-Dialogue-Is-Not-Enough-to-Make-a-Communicative-BabyLM-But-Neither-Is-Developmentally-Inspired-Reinforcement-Learning"><a href="#25-Dialogue-Is-Not-Enough-to-Make-a-Communicative-BabyLM-But-Neither-Is-Developmentally-Inspired-Reinforcement-Learning" class="headerlink" title="[25] Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20358">Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)</a></h3><p><em>Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文研究了仅使用对话数据预训练是否能生成功能合适的小语言模型。尽管在标准BabyLM基准测试中表现不佳，但模型在对话延续预测任务中表现优异，DPO微调进一步提升了性能。</p>
<details>
  <summary>Details</summary>
Motivation: 动机是探究仅依赖对话数据的预训练是否能生成功能完善的小型语言模型，并研究不同微调策略对生成结果的影响。

<p>Contribution: 主要贡献是验证了仅基于对话数据的预训练模型在对话任务中的潜力，并通过DPO微调策略进一步优化其性能。</p>
<p>Method: 方法包括使用对话数据进行预训练，随后采用多种微调策略（如PPO和DPO）对模型进行优化，并在BabyLM基准和自定义对话任务上测试性能。</p>
<p>Result: 结果表明，预训练模型在标准BabyLM基准中表现欠佳，但在对话延续任务中表现突出；DPO微调进一步提升了模型在对话任务中的表现。</p>
<p>Insight: 研究发现，对话数据足以支持模型在特定任务中表现优异，但通用的微调方法（如PPO）可能不适合所有任务，而DPO微调更适用于提升对话能力。</p>
<p>Abstract: We investigate whether pre-training exclusively on dialogue data results in formally and functionally apt small language models. Based on this pre-trained llamalogue model, we employ a variety of fine-tuning strategies to enforce “more communicative” text generations by our models. Although our models underperform on most standard BabyLM benchmarks, they excel at dialogue continuation prediction in a minimal pair setting. While PPO fine-tuning has mixed to adversarial effects on our models, DPO fine-tuning further improves their performance on our custom dialogue benchmark.</p>
</details>


<h3 id="26-The-Impact-of-Negated-Text-on-Hallucination-with-Large-Language-Models"><a href="#26-The-Impact-of-Negated-Text-on-Hallucination-with-Large-Language-Models" class="headerlink" title="[26] The Impact of Negated Text on Hallucination with Large Language Models"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20375">The Impact of Negated Text on Hallucination with Large Language Models</a></h3><p><em>Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文探讨了大型语言模型（LLMs）在处理否定文本时的幻觉问题，发现LLMs难以有效检测否定文本中的幻觉，并设计了一个名为NegHalu的数据集来验证这一点。</p>
<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言处理中表现出色，但否定文本对其幻觉行为的影响尚不明确。研究旨在填补这一空白。

<p>Contribution: 1. 提出了三个重要研究问题并尝试解答；2. 设计了NegHalu数据集；3. 揭示了LLMs在否定文本中检测幻觉的困难。</p>
<p>Method: 通过重构现有的幻觉检测数据集（加入否定表达）创建NegHalu，并对LLMs的内部状态进行令牌级分析。</p>
<p>Result: 实验表明，LLMs在否定文本中检测幻觉的能力较差，容易产生逻辑不一致或不忠实的判断。</p>
<p>Insight: 否定文本会增加LLMs的幻觉风险，且这种现象可能与模型内部处理机制有关，未来需进一步优化模型设计。</p>
<p>Abstract: Recent studies on hallucination in large language models (LLMs) have been actively progressing in natural language processing. However, the impact of negated text on hallucination with LLMs remains largely unexplored. In this paper, we set three important yet unanswered research questions and aim to address them. To derive the answers, we investigate whether LLMs can recognize contextual shifts caused by negation and still reliably distinguish hallucinations comparable to affirmative cases. We also design the NegHalu dataset by reconstructing existing hallucination detection datasets with negated expressions. Our experiments demonstrate that LLMs struggle to detect hallucinations in negated text effectively, often producing logically inconsistent or unfaithful judgments. Moreover, we trace the internal state of LLMs as they process negated inputs at the token level and reveal the challenges of mitigating their unintended effects.</p>
</details>


<h3 id="27-VLSP-2025-MLQA-TSR-Challenge-Vietnamese-Multimodal-Legal-Question-Answering-on-Traffic-Sign-Regulation"><a href="#27-VLSP-2025-MLQA-TSR-Challenge-Vietnamese-Multimodal-Legal-Question-Answering-on-Traffic-Sign-Regulation" class="headerlink" title="[27] VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20381">VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation</a></h3><p><em>Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文介绍了VLSP 2025 MLQA-TSR任务，这是一个多模态法律问答任务，专注于越南交通标志法规，包含多模态法律检索和多模态问答两个子任务。</p>
<details>
  <summary>Details</summary>
Motivation: 推动越南多模态法律文本处理的研究，并为多模态法律领域提供基准数据集，特别针对交通标志法规。

<p>Contribution: 提供了VLSP 2025 MLQA-TSR基准数据集和共享任务，促进了多模态法律问答系统的开发和评估。</p>
<p>Method: 任务分为多模态法律检索和多模态问答两个子任务，通过基准测试评估系统性能。</p>
<p>Result: 最佳结果在多模态法律检索上达到F2分数64.55%，在多模态问答上达到准确率86.30%。</p>
<p>Insight: 该研究为多模态法律领域的智能系统开发和评估提供了重要基准，尤其适用于越南交通标志法规场景。</p>
<p>Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025 MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal question answering. The goal is to advance research on Vietnamese multimodal legal text processing and to provide a benchmark dataset for building and evaluating intelligent systems in multimodal legal domains, with a focus on traffic sign regulation in Vietnam. The best-reported results on VLSP 2025 MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an accuracy of 86.30% for multimodal question answering.</p>
</details>


<h3 id="28-NeoDictaBERT-Pushing-the-Frontier-of-BERT-models-for-Hebrew"><a href="#28-NeoDictaBERT-Pushing-the-Frontier-of-BERT-models-for-Hebrew" class="headerlink" title="[28] NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20386">NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew</a></h3><p><em>Shaltiel Shmidman,Avi Shmidman,Moshe Koppel</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文介绍了NeoDictaBERT和NeoDictaBERT-bilingual，这是基于NeoBERT架构的BERT风格模型，专注于希伯来语文本，并在希伯来语基准测试中表现出色。</p>
<details>
  <summary>Details</summary>
Motivation: 为了解决BERT模型在希伯来语任务上的性能限制，并利用新型Transformer架构的优势，作者提出了专门针对希伯来语的改进型BERT模型。

<p>Contribution: 提出了NeoDictaBERT和NeoDictaBERT-bilingual，这两种模型在希伯来语任务中表现优异，特别是在检索任务上超越了类似规模的多语言模型。</p>
<p>Method: 采用NeoBERT的架构进行训练，专注于希伯来语文本，并发布了详细的训练过程和实验结果。</p>
<p>Result: 模型在几乎所有希伯来语基准测试中表现优异，NeoDictaBERT-bilingual在检索任务上尤其突出。</p>
<p>Insight: 新型Transformer架构可以显著提升BERT模型在特定语言任务上的性能，尤其是在资源相对较少的语言（如希伯来语）中。</p>
<p>Abstract: Since their initial release, BERT models have demonstrated exceptional performance on a variety of tasks, despite their relatively small size (BERT-base has ~100M parameters). Nevertheless, the architectural choices used in these models are outdated compared to newer transformer-based models such as Llama3 and Qwen3. In recent months, several architectures have been proposed to close this gap. ModernBERT and NeoBERT both show strong improvements on English benchmarks and significantly extend the supported context window. Following their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual: BERT-style models trained using the same architecture as NeoBERT, with a dedicated focus on Hebrew texts. These models outperform existing ones on almost all Hebrew benchmarks and provide a strong foundation for downstream tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on retrieval tasks, outperforming other multilingual models of similar size. In this paper, we describe the training process and report results across various benchmarks. We release the models to the community as part of our goal to advance research and development in Hebrew NLP.</p>
</details>


<h3 id="29-LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup"><a href="#29-LM-mixup-Text-Data-Augmentation-via-Language-Model-based-Mixup" class="headerlink" title="[29] LM-mixup: Text Data Augmentation via Language Model based Mixup"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20449">LM-mixup: Text Data Augmentation via Language Model based Mixup</a></h3><p><em>Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: LM-mixup是一种基于语言模型的数据增强方法，通过蒸馏低质量指令数据生成高质量数据，显著提升指令微调LLM的效率与性能。</p>
<details>
  <summary>Details</summary>
Motivation: 高质量指令数据稀缺，而低质量数据常被丢弃造成信息损失。现有数据增强方法对低质量数据效果不佳，且评估标准不明确。

<p>Contribution: 1) 正式定义Instruction Distillation任务；2) 构建MIXTURE数据集（144K样本）；3) 提出LM-Mixup方法，结合监督微调与强化学习优化。</p>
<p>Method: 1) 监督微调MIXTURE数据集；2) 使用GRPO强化学习优化，结合质量、语义对齐和格式合规三种奖励信号。</p>
<p>Result: 仅使用3%蒸馏数据微调的LLM，性能超越全数据集训练，并媲美SOTA高质量数据选择方法。</p>
<p>Insight: 低质量数据经合理蒸馏和增强后是宝贵资源，可显著提升LLM的效率和性能。</p>
<p>Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet the quality of instruction-following data varies significantly. While high-quality data is paramount, it is often scarce; conversely, abundant low-quality data is frequently discarded, leading to substantial information loss. Existing data augmentation methods struggle to augment this low-quality data effectively, and the evaluation of such techniques remains poorly defined. To address this, we formally define the task of Instruction Distillation: distilling multiple low-quality and redundant inputs into high-quality and coherent instruction-output pairs. Specifically, we introduce a comprehensive data construction pipeline to create MIXTURE, a 144K-sample dataset pairing low-quality or semantically redundant imperfect instruction clusters with their high-quality distillations. We then introduce LM-Mixup, by first performing supervised fine-tuning on MIXTURE and then optimizing it with reinforcement learning. This process uses three complementary reward signals: quality, semantic alignment, and format compliance, via Group Relative Policy Optimization (GRPO). We demonstrate that LM-Mixup effectively augments imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for only about 3% of the entire dataset, not only surpasses full-dataset training but also competes with state-of-the-art high-quality data selection methods across multiple benchmarks. Our work establishes that low-quality data is a valuable resource when properly distilled and augmented with LM-Mixup, significantly enhancing the efficiency and performance of instruction-tuned LLMs.</p>
</details>


<h3 id="30-ARC-Encoder-learning-compressed-text-representations-for-large-language-models"><a href="#30-ARC-Encoder-learning-compressed-text-representations-for-large-language-models" class="headerlink" title="[30] ARC-Encoder: learning compressed text representations for large language models"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20535">ARC-Encoder: learning compressed text representations for large language models</a></h3><p><em>Hippolyte Pilchen,Edouard Grave,Patrick Pérez</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ARC-Encoder提出了一种新的文本压缩编码器，通过将上下文压缩为连续表征以替换解码LLM中的词嵌入，从而在不微调目标模型或修改其架构的情况下提高计算效率。</p>
<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成或思维链推理等方法导致上下文更长、推理成本更高，而最有效的压缩方法需要微调或修改模型架构，可能损害模型的通用能力。本文探索了一种替代方案。

<p>Contribution: 1. 设计了ARC-Encoder，可输出比文本tokens更少的连续表征。2. 研究了训练策略和架构选择。3. 展示了ARC-Encoder在多任务和多解码LLM中的高效性和泛化能力。</p>
<p>Method: 提出了一种可适配的文本压缩编码器（ARC-Encoder），通过系统研究训练策略和架构选择，将上下文压缩为连续表征，并替换解码LLM中的词嵌入。</p>
<p>Result: 在多个基准测试中达到SOTA性能，同时提升推理的计算效率，并展示了其可适配多个解码LLM的能力。</p>
<p>Insight: ARC-Encoder是一种灵活高效的便携式编码器，无需修改目标模型即可实现上下文压缩，且能泛化到不同LLM。</p>
<p>Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought reasoning have led to longer contexts and increased inference costs. Context compression techniques can reduce these costs, but the most effective approaches require fine-tuning the target model or even modifying its architecture. This can degrade its general abilities when not used for this specific purpose. Here we explore an alternative approach: an encoder that compresses the context into continuous representations which replace token embeddings in decoder LLMs. First, we perform a systematic study of training strategies and architecture choices for the encoder. Our findings led to the design of an Adaptable text Representations Compressor, named ARC-Encoder, which outputs $x$-times fewer continuous representations (typically $x!\in!{4,8}$) than text tokens. We evaluate ARC-Encoder across a variety of LLM usage scenarios, ranging from in-context learning to context window extension, on both instruct and base decoders. Results show that ARC-Encoder achieves state-of-the-art performance on several benchmarks while improving computational efficiency at inference. Finally, we demonstrate that our models can be adapted to multiple decoders simultaneously, allowing a single encoder to generalize across different decoder LLMs. This makes ARC-Encoder a flexible and efficient solution for portable encoders that work seamlessly with multiple LLMs. We release a training code at <a target="_blank" rel="noopener" href="https://github.com/kyutai-labs/ARC-Encoder">https://github.com/kyutai-labs/ARC-Encoder</a> , fine-tuning dataset and pretrained models are available at <a target="_blank" rel="noopener" href="https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047">https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047</a> .</p>
</details>


<h3 id="31-The-Dog-the-Cat-Chased-Stumped-the-Model-Measuring-When-Language-Models-Abandon-Structure-for-Shortcuts"><a href="#31-The-Dog-the-Cat-Chased-Stumped-the-Model-Measuring-When-Language-Models-Abandon-Structure-for-Shortcuts" class="headerlink" title="[31] The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20543">The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts</a></h3><p><em>Sangmitra Madhusudan,Kaige Chen,Ali Emami</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了CenterBench数据集，用于测试语言模型在理解复杂嵌套句子时是否依赖语法结构还是语义模式匹配。研究发现语言模型在处理高复杂度句子时会放弃结构分析，转而依赖语义关联。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法无法区分语言模型是在进行语法分析还是简单的语义模式匹配。这限制了对其真实语言理解能力的评估。

<p>Contribution: 1) 提出CenterBench数据集，包含9,720个中心嵌套句子及其语义不可行的变体；2) 量化模型在高复杂度句子中放弃结构分析的倾向；3) 首次揭示语义合理性在某些情况下反而会降低模型性能。</p>
<p>Method: 使用CenterBench数据集，测试六种语言模型在不同复杂度的中心嵌套句子上的表现，比较语义合理与不合理句子的性能差异。</p>
<p>Result: 模型在高复杂度句子中表现差异显著（中位差距达26.8%），表明其倾向于依赖语义关联而非结构分析。人类的表现则更具多样性。</p>
<p>Insight: 语义合理性并非总是有益，尤其在需要因果推理的任务中可能适得其反。模型的推理能力仍受限于语义捷径和过度思考等问题。</p>
<p>Abstract: When language models correctly parse “The cat that the dog chased meowed,” are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like “The cat [that the dog chased] meowed”) where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.</p>
</details>


<h3 id="32-GlobalRAG-Enhancing-Global-Reasoning-in-Multi-hop-Question-Answering-via-Reinforcement-Learning"><a href="#32-GlobalRAG-Enhancing-Global-Reasoning-in-Multi-hop-Question-Answering-via-Reinforcement-Learning" class="headerlink" title="[32] GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20548">GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning</a></h3><p><em>Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: GlobalRAG是一种基于强化学习的框架，旨在解决多跳问答中的全局规划和证据执行问题。通过分解问题和奖励机制，该方法显著提升了性能。</p>
<details>
  <summary>Details</summary>
Motivation: 多跳问答中存在全局规划不足和证据执行不一致的问题，限制了强化学习在检索增强生成（RAG）中的有效性。

<p>Contribution: 提出了GlobalRAG框架，通过分解问题、协调检索和推理，并引入奖励机制，显著提升了多跳问答的性能。</p>
<p>Method: 框架结合了Planning Quality Reward和SubGoal Completion Reward，采用渐进式权重退火策略平衡目标，优化全局推理和证据迭代。</p>
<p>Result: 在仅使用8k训练数据的情况下，GlobalRAG在多跳问答任务中实现了14.2%的平均性能提升（EM和F1）。</p>
<p>Insight: 强化学习在多跳问答中的应用潜力巨大，尤其是在优化全局规划和证据执行方面。</p>
<p>Abstract: Reinforcement learning has recently shown promise in improving retrieval-augmented generation (RAG). Despite these advances, its effectiveness in multi-hop question answering (QA) remains limited by two fundamental limitations: (i) global planning absence to structure multi-step reasoning, and (ii) unfaithful execution, which hinders effective query formulation and consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement learning framework designed to enhance global reasoning in multi-hop QA. GlobalRAG decomposes questions into subgoals, coordinates retrieval with reasoning, and refines evidence iteratively. To guide this process, we introduce Planning Quality Reward and SubGoal Completion Reward, which encourage coherent planning and reliable subgoal execution. In addition, a progressive weight annealing strategy balances process-oriented and outcome-based objectives. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms strong baselines while using only 8k training data (42% of the training data used by strong baselines), achieving average improvements of 14.2% in both EM and F1.</p>
</details>


<h3 id="33-Beyond-Retrieval-Ranking-A-Multi-Agent-Cognitive-Decision-Framework-for-E-Commerce-Search"><a href="#33-Beyond-Retrieval-Ranking-A-Multi-Agent-Cognitive-Decision-Framework-for-E-Commerce-Search" class="headerlink" title="[33] Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20567">Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search</a></h3><p><em>Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一个多智能体认知决策框架（MACDF），用于改进电商搜索的局限性，将传统检索排序范式转变为主动决策支持，显著提升了复杂查询下的推荐准确性和用户满意度。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的检索排序范式存在语义鸿沟、高决策成本和缺乏专业购物指导的问题，无法满足用户在电商平台上的多阶段认知决策需求。

<p>Contribution: 提出了MACDF框架，通过多智能体系统主动支持用户决策，解决了传统检索排序的局限性，并在复杂查询场景中表现出色。</p>
<p>Method: MACDF采用多智能体协作方式，模拟用户认知决策过程，提供主动支持，取代被动的检索排序。</p>
<p>Result: 离线和在线测试表明，MACDF显著提升了推荐准确性和用户满意度，尤其是在复杂查询（如否定、多约束或推理需求）下表现优异。</p>
<p>Insight: 多智能体认知系统具有重新定义电商搜索的潜力，未来可以进一步探索其在其他场景中的应用。</p>
<p>Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its reliance on query-item matching fundamentally misaligns with multi-stage cognitive decision processes of platform users. This misalignment introduces critical limitations: semantic gaps in complex queries, high decision costs due to cross-platform information foraging, and the absence of professional shopping guidance. To address these issues, we propose a Multi-Agent Cognitive Decision Framework (MACDF), which shifts the paradigm from passive retrieval to proactive decision support. Extensive offline evaluations demonstrate MACDF’s significant improvements in recommendation accuracy and user satisfaction, particularly for complex queries involving negation, multi-constraint, or reasoning demands. Online A&#x2F;B testing on JD search platform confirms its practical efficacy. This work highlights the transformative potential of multi-agent cognitive systems in redefining e-commerce search.</p>
</details>


<h3 id="34-Can-ChatGPT-Code-Communication-Data-Fairly-Empirical-Evidence-from-Multiple-Collaborative-Tasks"><a href="#34-Can-ChatGPT-Code-Communication-Data-Fairly-Empirical-Evidence-from-Multiple-Collaborative-Tasks" class="headerlink" title="[34] Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20584">Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks</a></h3><p><em>Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文研究了ChatGPT在协作任务中编码通信数据的公平性，发现其在性别和种族群体间无显著偏见。</p>
<details>
  <summary>Details</summary>
Motivation: 大规模评估协作和通信依赖于人工标注通信数据，ChatGPT虽能胜任但可能对某些人口统计学群体存在偏见，需验证其公平性。

<p>Contribution: 实证表明ChatGPT在协作问题解决的通信数据编码中无性别和种族偏见，支持其在大规模评估中的应用。</p>
<p>Method: 使用典型协作问题解决编码框架，分析ChatGPT在谈判、问题解决和决策三类任务中的表现，对比性别和种族差异。</p>
<p>Result: ChatGPT编码在不同人口统计学群体间无显著差异，验证了其公平性。</p>
<p>Insight: ChatGPT的公平性验证为其在教育和协作评估中的广泛使用提供了依据。</p>
<p>Abstract: Assessing communication and collaboration at scale depends on a labor intensive task of coding communication data into categories according to different frameworks. Prior research has established that ChatGPT can be directly instructed with coding rubrics to code the communication data and achieves accuracy comparable to human raters. However, whether the coding from ChatGPT or similar AI technology exhibits bias against different demographic groups, such as gender and race, remains unclear. To fill this gap, this paper investigates ChatGPT-based automated coding of communication data using a typical coding framework for collaborative problem solving, examining differences across gender and racial groups. The analysis draws on data from three types of collaborative tasks: negotiation, problem solving, and decision making. Our results show that ChatGPT-based coding exhibits no significant bias across gender and racial groups, paving the road for its adoption in large-scale assessment of collaboration and communication.</p>
</details>


<h3 id="35-Why-Did-Apple-Fall-To-The-Ground-Evaluating-Curiosity-In-Large-Language-Model"><a href="#35-Why-Did-Apple-Fall-To-The-Ground-Evaluating-Curiosity-In-Large-Language-Model" class="headerlink" title="[35] Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20635">Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model</a></h3><p><em>Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文探讨了大语言模型（LLMs）是否具备类似人类的好奇心驱动学习能力，并通过基于人类好奇心评估问卷5DCR的框架评估了LLMs在不同维度（如信息寻求、刺激寻求和社会好奇心）上的表现。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨LLMs是否具备好奇心驱动的学习能力，以及这种能力如何影响其推理和主动学习能力。

<p>Contribution: 主要贡献是设计了一个全面的评估框架，首次系统地量化了LLMs的好奇心，并发现其在知识渴求方面优于人类，但在不确定性环境中表现保守。</p>
<p>Method: 研究基于Five-Dimensional Curiosity scale Revised（5DCR）设计评估框架，涵盖了多个好奇心维度，并通过实验验证LLMs的表现。</p>
<p>Result: 结果显示LLMs在知识渴求方面强于人类，但在不确定性环境中更保守；同时证实好奇心行为能增强模型的推理和主动学习能力。</p>
<p>Insight: 研究发现LLMs具备类似人类好奇心的潜力，为未来LLMs的学习能力和创新研究提供了实验支持。</p>
<p>Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn new knowledge. Recent advancements of large language models (LLMs) in natural language processing have sparked discussions regarding whether these models possess capability of curiosity-driven learning akin to humans. In this paper, starting from the human curiosity assessment questionnaire Five-Dimensional Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework that covers dimensions such as Information Seeking, Thrill Seeking, and Social Curiosity to assess the extent of curiosity exhibited by LLMs. The results demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but still tend to make conservative choices when faced with uncertain environments. We further investigated the relationship between curiosity and thinking of LLMs, confirming that curious behaviors can enhance the model’s reasoning and active learning abilities. These findings suggest that LLMs have the potential to exhibit curiosity similar to that of humans, providing experimental support for the future development of learning capabilities and innovative research in LLMs.</p>
</details>


<h3 id="36-The-Reasoning-Lingua-Franca-A-Double-Edged-Sword-for-Multilingual-AI"><a href="#36-The-Reasoning-Lingua-Franca-A-Double-Edged-Sword-for-Multilingual-AI" class="headerlink" title="[36] The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20647">The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI</a></h3><p><em>Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 大型推理模型（LRMs）在多语言推理中以英语为通用语言，虽提高复杂任务的准确性，但可能因翻译错误导致失败。</p>
<details>
  <summary>Details</summary>
Motivation: 探索LRMs在多语言环境下的推理能力，关注其在非英语问题中以英语为默认语言的现象及其潜在的翻译错误问题。

<p>Contribution: 系统性比较LRMs在英语与问题语言中的推理表现，揭示英语推理的优势及其在翻译中的潜在风险。</p>
<p>Method: 在MGSM和GPQA Diamond任务中评估LRMs，分析推理痕迹的认知行为及其与准确性的关系。</p>
<p>Result: 英语推理痕迹展现更多认知行为，且准确性更高，但随着任务复杂度增加，翻译错误风险加剧。</p>
<p>Insight: LRMs在多语言推理中依赖英语可能导致翻译相关的失败模式，需平衡语言通用性与准确性。</p>
<p>Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM’s reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting “Lost in Translation,” where translation steps lead to errors that would have been avoided by question’s language reasoning.</p>
</details>


<h3 id="37-textsc-CantoNLU-A-benchmark-for-Cantonese-natural-language-understanding"><a href="#37-textsc-CantoNLU-A-benchmark-for-Cantonese-natural-language-understanding" class="headerlink" title="[37] \textsc{CantoNLU}: A benchmark for Cantonese natural language understanding"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20670">\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding</a></h3><p><em>Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出了一个名为CantoNLU的粤语自然语言理解基准，涵盖七项任务，并比较了不同模型的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 粤语虽然使用者众多，但由于政策和双语现象，相关资源匮乏。缺少评估框架限制了粤语自然语言处理的发展。

<p>Contribution: 引入了CantoNLU基准，填补了粤语自然语言理解的空白，并提供了数据集、代码和模型权重以促进研究。</p>
<p>Method: 构建了涵盖七项任务的基准，比较了四种模型的性能：未经粤语训练的普通话模型、通过持续预训练调整的粤语模型，以及从头训练的粤语单语模型。</p>
<p>Result: 调整的粤语模型整体表现最佳，单语模型在句法任务上更优，而普通话模型在某些场景下仍具竞争力。</p>
<p>Insight: 当粤语领域数据稀缺时，直接迁移可能足够；持续预训练对提升多任务性能有效。</p>
<p>Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy and diglossia. To address this scarcity of evaluation frameworks for Cantonese, we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural language understanding (NLU). This novel benchmark spans seven tasks covering syntax and semantics, including word sense disambiguation, linguistic acceptability judgment, language detection, natural language inference, sentiment analysis, part-of-speech tagging, and dependency parsing. In addition to the benchmark, we provide model baseline performance across a set of models: a Mandarin model without Cantonese training, two Cantonese-adapted models obtained by continual pre-training a Mandarin model on Cantonese text, and a monolingual Cantonese model trained from scratch. Results show that Cantonese-adapted models perform best overall, while monolingual models perform better on syntactic tasks. Mandarin models remain competitive in certain settings, indicating that direct transfer may be sufficient when Cantonese domain data is scarce. We release all datasets, code, and model weights to facilitate future research in Cantonese NLP.</p>
</details>


<h3 id="38-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost"><a href="#38-Are-Large-Reasoning-Models-Good-Translation-Evaluators-Analysis-and-Performance-Boost" class="headerlink" title="[38] Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20780">Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost</a></h3><p><em>Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该文首次系统分析了大型推理模型（LRM）作为机器翻译质量评估器的潜力，提出了校准LRM思考的方法，显著提升了评估性能。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型在复杂任务中表现优异，但它们在机器翻译评估中的潜力尚未被充分探索。

<p>Contribution: 提供了LRM作为翻译评估器的首次系统分析，并提出了一种通过训练合成的人类思维轨迹来校准LRM的方法。</p>
<p>Method: 通过训练LRM在合成的人类思维轨迹上进行校准，减少其思考预算并提升评估性能。</p>
<p>Result: 在WMT24基准测试中，该方法将思考预算降低约35倍，并在不同规模的LRM（7B到32B）上提升了评估性能（如R1-Distill-Qwen-7B相关性提升了8.7个百分点）。</p>
<p>Insight: 校准LRM的思考过程可以显著提升其在机器翻译评估中的效率和性能，展示了其在细粒度自动评估中的潜力。</p>
<p>Abstract: Recent advancements in large reasoning models (LRMs) have introduced an intermediate “thinking” process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to “overthink” simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation.</p>
</details>


<h3 id="39-A-Use-Case-Specific-Dataset-for-Measuring-Dimensions-of-Responsible-Performance-in-LLM-generated-Text"><a href="#39-A-Use-Case-Specific-Dataset-for-Measuring-Dimensions-of-Responsible-Performance-in-LLM-generated-Text" class="headerlink" title="[39] A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20782">A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text</a></h3><p><em>Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文构建了一个基于真实应用场景的数据集，用于评估大型语言模型（LLM）在负责任AI维度（如公平性、质量和安全性）上的表现，填补了现有评估方法的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常关注高层次任务（如文本生成），而忽略了特定应用中负责任AI的维度（如公平性）。例如，某些受保护属性在某些应用中可能更重要。

<p>Contribution: 1）提出了一个基于真实应用场景（产品描述生成）的标注数据集；2）展示了如何使用该数据集评估LLM的质量、真实性、安全性和公平性。</p>
<p>Method: 通过将公平性属性与性别化形容词和产品类别交叉，构建了一个丰富的标注提示数据集，并提出了评估LLM在这些维度上的方法。</p>
<p>Result: 数据集被证明可用于识别LLM在质量、真实性、安全性和公平性方面的差距，为研究社区提供了具体资源。</p>
<p>Insight: 评估LLM需要结合具体应用场景，关注负责任AI的多维度表现，而不仅仅是通用任务的表现。</p>
<p>Abstract: Current methods for evaluating large language models (LLMs) typically focus on high-level tasks such as text generation, without targeting a particular AI application. This approach is not sufficient for evaluating LLMs for Responsible AI dimensions like fairness, since protected attributes that are highly relevant in one application may be less relevant in another. In this work, we construct a dataset that is driven by a real-world application (generate a plain-text product description, given a list of product features), parameterized by fairness attributes intersected with gendered adjectives and product categories, yielding a rich set of labeled prompts. We show how to use the data to identify quality, veracity, safety, and fairness gaps in LLMs, contributing a proposal for LLM evaluation paired with a concrete resource for the research community.</p>
</details>


<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="40-Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning"><a href="#40-Transformed-Multi-view-3D-Shape-Features-with-Contrastive-Learning" class="headerlink" title="[40] Transformed Multi-view 3D Shape Features with Contrastive Learning"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19955">Transformed Multi-view 3D Shape Features with Contrastive Learning</a></h3><p><em>Márcus Vinícius Lobo Costa,Sherlon Almeida da Silva,Bárbara Caroline Benato,Leo Sampaio Ferraz Ribeiro,Moacir Antonelli Ponti</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种结合Vision Transformers（ViTs）和对比学习目标的方法，用于多视角3D形状特征表示学习，提升了3D形状分析的性能，并在ModelNet10上取得了90.6%的准确率。</p>
<details>
  <summary>Details</summary>
Motivation: 计算机视觉方法在处理从2D图像识别3D物体时面临挑战，通常需要大量标注数据且依赖CNNs，这些方法可能忽略了关键的形状关系。本文旨在通过结合ViTs和对比学习，克服这些限制。

<p>Contribution: 1. 提出了一种结合ViTs和对比学习目标的新方法；2. 展示了这种方法在3D形状表示学习中的有效性；3. 通过实验验证了方法的优越性。</p>
<p>Method: 1. 使用Vision Transformers（ViTs）作为主干网络；2. 结合监督和自监督对比学习目标；3. 通过全局形状语义和局部判别特征的联合优化提升性能。</p>
<p>Result: 在ModelNet10上实现了约90.6%的准确率，证明了ViTs和对比学习结合的优越性。</p>
<p>Insight: ViTs能够捕捉全局形状语义，而对比学习则优化了局部判别特征，两者的结合有效减少了标注数据的需求并克服了CNNs的局限性。</p>
<p>Abstract: This paper addresses the challenges in representation learning of 3D shape features by investigating state-of-the-art backbones paired with both contrastive supervised and self-supervised learning objectives. Computer vision methods struggle with recognizing 3D objects from 2D images, often requiring extensive labeled data and relying on Convolutional Neural Networks (CNNs) that may overlook crucial shape relationships. Our work demonstrates that Vision Transformers (ViTs) based architectures, when paired with modern contrastive objectives, achieve promising results in multi-view 3D analysis on our downstream tasks, unifying contrastive and 3D shape understanding pipelines. For example, supervised contrastive losses reached about 90.6% accuracy on ModelNet10. The use of ViTs and contrastive learning, leveraging ViTs’ ability to understand overall shapes and contrastive learning’s effectiveness, overcomes the need for extensive labeled data and the limitations of CNNs in capturing crucial shape relationships. The success stems from capturing global shape semantics via ViTs and refining local discriminative features through contrastive optimization. Importantly, our approach is empirical, as it is grounded on extensive experimental evaluation to validate the effectiveness of combining ViTs with contrastive objectives for 3D representation learning.</p>
</details>


<h3 id="41-FutrTrack-A-Camera-LiDAR-Fusion-Transformer-for-3D-Multiple-Object-Tracking"><a href="#41-FutrTrack-A-Camera-LiDAR-Fusion-Transformer-for-3D-Multiple-Object-Tracking" class="headerlink" title="[41] FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19981">FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</a></h3><p><em>Martha Teiko Teye,Ori Maoz,Matthias Rottmann</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: FutrTrack提出了一种基于相机-LiDAR融合的3D多目标跟踪框架，通过引入基于transformer的平滑器和融合驱动的跟踪器，显著提升了多传感器特征下的跟踪性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的单传感器跟踪方法在多目标跟踪中（尤其是在遮挡和视角变化情况下）表现不足。FutrTrack通过融合相机和LiDAR的多模态特征，提升跟踪的鲁棒性和准确性。

<p>Contribution: 1. 提出了基于transformer的多模态两阶段细化与跟踪流程；2. 集成了相机和LiDAR的BEV特征，无需显式运动模型；3. 引入了时间平滑器以减少轨迹抖动并提升空间一致性。</p>
<p>Method: 使用transformer架构对相机和LiDAR的多模态特征进行融合，通过两阶段流程（细化与跟踪）实现鲁棒的目标跟踪。时间平滑器用于优化轨迹。</p>
<p>Result: 在nuScenes和KITTI数据集上表现优异，nuScenes测试集的aMOTA达到74.7，减少了ID切换并保持了高精度。</p>
<p>Insight: 多模态特征融合（尤其是相机-LiDAR）显著提升了transformer-based跟踪器的性能，即使数据有限或无预训练也能与基于神经网络的方法竞争。</p>
<p>Abstract: We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird’s-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.</p>
</details>


<h3 id="42-Endoshare-A-Source-Available-Solution-to-De-Identify-and-Manage-Surgical-Videos"><a href="#42-Endoshare-A-Source-Available-Solution-to-De-Identify-and-Manage-Surgical-Videos" class="headerlink" title="[42] Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20087">Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos</a></h3><p><em>Lorenzo Arboit,Dennis N. Schneider,Britty Baby,Vinkle Srivastav,Pietro Mascagni,Nicolas Padoy</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Endoshare是一个开源工具，用于标准化和去识别化内窥镜手术视频，解决了视频管理和隐私问题，通过用户反馈不断优化，展示了高可用性和实用性。</p>
<details>
  <summary>Details</summary>
Motivation: 手术视频在培训和研究中具有重要价值，但视频格式多样性和隐私问题限制了其广泛使用。Endoshare旨在提供一个标准化且隐私保护的解决方案。

<p>Contribution: 提出了Endoshare，一个开源、跨平台的工具，用于合并、标准化和去识别化内窥镜手术视频，并通过用户反馈优化设计。</p>
<p>Method: 采用软件开发生命周期方法，结合用户中心的反馈和隐私优先的架构设计，并通过内部和外部用户测试评估可用性和实用性。</p>
<p>Result: 用户测试显示高可用性和实用性（最高评分为5.07&#x2F;7），处理时间受视频时长和硬件性能影响。</p>
<p>Insight: 通过开源和隐私设计的结合，Endoshare提供了一种透明、实用的手术视频管理方案，但需要进一步验证合规性和互操作性。</p>
<p>Abstract: Video-based assessment and surgical data science can advance surgical training, research, and quality improvement. However, widespread use remains limited by heterogeneous recording formats and privacy concerns associated with video sharing. We present Endoshare, a source-available, cross-platform application for merging, standardizing, and de-identifying endoscopic videos in minimally invasive surgery. Development followed the software development life cycle with iterative, user-centered feedback. During the analysis phase, an internal survey of clinicians and computer scientists based on ten usability heuristics identified key requirements that guided a privacy-by-design architecture. In the testing phase, an external clinician survey combined the same heuristics with Technology Acceptance Model constructs to assess usability and adoption, complemented by benchmarking across different hardware configurations. Four clinicians and four computer scientists initially tested the prototype, reporting high usability (4.68 +&#x2F;- 0.40&#x2F;5 and 4.03 +&#x2F;- 0.51&#x2F;5), with the lowest score (4.00 +&#x2F;- 0.93&#x2F;5) relating to label clarity. After refinement, the testing phase surveyed ten surgeons who reported high perceived usefulness (5.07 +&#x2F;- 1.75&#x2F;7), ease of use (5.15 +&#x2F;- 1.71&#x2F;7), heuristic usability (4.38 +&#x2F;- 0.48&#x2F;5), and strong recommendation (9.20 +&#x2F;- 0.79&#x2F;10). Processing time varied with processing mode, video duration (both p &lt;&#x3D; 0.001), and machine computational power (p &#x3D; 0.041). Endoshare provides a transparent, user-friendly pipeline for standardized, privacy-preserving surgical video management. Compliance certification and broader interoperability validation are needed to establish it as a deployable alternative to proprietary systems. The software is available at <a target="_blank" rel="noopener" href="https://camma-public.github.io/Endoshare/">https://camma-public.github.io/Endoshare/</a></p>
</details>


<h3 id="43-Attentive-Convolution-Unifying-the-Expressivity-of-Self-Attention-with-Convolutional-Efficiency"><a href="#43-Attentive-Convolution-Unifying-the-Expressivity-of-Self-Attention-with-Convolutional-Efficiency" class="headerlink" title="[43] Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20092">Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency</a></h3><p><em>Hao Yu,Haoyu Chen,Yan Jiang,Wei Peng,Zhaodong Sun,Samuel Kaski,Guoying Zhao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为Attentive Convolution（ATConv）的新型卷积操作，结合了自注意力（SA）的灵活性和传统卷积（Conv）的高效性，揭示了SA优于Conv的两个关键原则：自适应路由和侧向抑制，并基于此设计了一个高效的CNN家族AttNet，在多项任务中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 自注意力机制在视觉任务中表现出强大的表达能力，但其二次复杂度限制了实际应用。传统卷积虽然高效，但在表达能力上存在不足。论文旨在揭示SA的核心优势，并将其融入卷积设计中，以兼顾高效性和表达能力。

<p>Contribution: 1. 揭示了SA优于Conv的两个关键原则：自适应路由和侧向抑制。2. 提出了ATConv，一种新型卷积操作，结合了SA的动态性和Conv的高效性。3. 设计了AttNet，一个高效的CNN家族，在ImageNet等任务中表现出色。</p>
<p>Method: 1. 分析SA的动态特性，提出自适应路由和侧向抑制原则。2. 设计ATConv，通过动态调节位置信息和引入竞争机制，增强卷积的表达能力。3. 基于ATConv构建AttNet，并进行实验验证。</p>
<p>Result: 1. ATConv仅用3×3核即优于多种SA机制。2. AttNet在ImageNet-1K上达到84.4%的Top-1准确率（27M参数）。3. 在扩散模型SiT-XL&#x2F;2中替换SA，ImageNet FID降低0.15，采样速度更快。</p>
<p>Insight: 1. SA的优势在于动态性和竞争机制，而非单纯的感受野设计。2. 高效性和表达能力可以通过原则性设计相结合，ATConv是一个成功案例。</p>
<p>Abstract: Self-attention (SA) has become the cornerstone of modern vision backbones for its powerful expressivity over traditional Convolutions (Conv). However, its quadratic complexity remains a critical bottleneck for practical applications. Given that Conv offers linear complexity and strong visual priors, continuing efforts have been made to promote the renaissance of Conv. However, a persistent performance chasm remains, highlighting that these modernizations have not yet captured the intrinsic expressivity that defines SA. In this paper, we re-examine the design of the CNNs, directed by a key question: what principles give SA its edge over Conv? As a result, we reveal two fundamental insights that challenge the long-standing design intuitions in prior research (e.g., Receptive field). The two findings are: (1) \textit{Adaptive routing}: SA dynamically regulates positional information flow according to semantic content, whereas Conv employs static kernels uniformly across all positions. (2) \textit{Lateral inhibition}: SA induces score competition among token weighting, effectively suppressing redundancy and sharpening representations, whereas Conv filters lack such inhibitory dynamics and exhibit considerable redundancy. Based on this, we propose \textit{Attentive Convolution} (ATConv), a principled reformulation of the convolutional operator that intrinsically injects these principles. Interestingly, with only $3\times3$ kernels, ATConv consistently outperforms various SA mechanisms in fundamental vision tasks. Building on ATConv, we introduce AttNet, a CNN family that can attain \textbf{84.4%} ImageNet-1K Top-1 accuracy with only 27M parameters. In diffusion-based image generation, replacing all SA with the proposed $3\times 3$ ATConv in SiT-XL&#x2F;2 reduces ImageNet FID by 0.15 in 400k steps with faster sampling. Code is available at: github.com&#x2F;price112&#x2F;Attentive-Convolution.</p>
</details>


<h3 id="44-StableSketcher-Enhancing-Diffusion-Model-for-Pixel-based-Sketch-Generation-via-Visual-Question-Answering-Feedback"><a href="#44-StableSketcher-Enhancing-Diffusion-Model-for-Pixel-based-Sketch-Generation-via-Visual-Question-Answering-Feedback" class="headerlink" title="[44] StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20093">StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback</a></h3><p><em>Jiho Park,Sieun Choi,Jaeyoon Seo,Jihie Kim</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: StableSketcher是一个新框架，通过视觉问答反馈增强扩散模型，生成更忠实的像素级手绘草图。</p>
<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像方面取得了进展，但在生成抽象表达（如手绘草图）时仍面临挑战。

<p>Contribution: 提出了StableSketcher框架，通过优化变分自编码器和强化学习奖励函数，提升草图的风格忠实度和文本对齐性；还发布了首个实例级草图数据集SketchDUO。</p>
<p>Method: 1. 微调变分自编码器以优化隐空间解码；2. 引入基于视觉问答的强化学习奖励函数，提升文本-图像对齐。</p>
<p>Result: 实验表明，StableSketcher优于Stable Diffusion基线，生成了风格更忠实、语义更一致的草图。</p>
<p>Insight: 视觉问答反馈可作为提升生成任务中语义对齐的有效工具；高质量标注数据集对抽象表达生成至关重要。</p>
<p>Abstract: Although recent advancements in diffusion models have significantly enriched the quality of generated images, challenges remain in synthesizing pixel-based human-drawn sketches, a representative example of abstract expression. To combat these challenges, we propose StableSketcher, a novel framework that empowers diffusion models to generate hand-drawn sketches with high prompt fidelity. Within this framework, we fine-tune the variational autoencoder to optimize latent decoding, enabling it to better capture the characteristics of sketches. In parallel, we integrate a new reward function for reinforcement learning based on visual question answering, which improves text-image alignment and semantic consistency. Extensive experiments demonstrate that StableSketcher generates sketches with improved stylistic fidelity, achieving better alignment with prompts compared to the Stable Diffusion baseline. Additionally, we introduce SketchDUO, to the best of our knowledge, the first dataset comprising instance-level sketches paired with captions and question-answer pairs, thereby addressing the limitations of existing datasets that rely on image-label pairs. Our code and dataset will be made publicly available upon acceptance.</p>
</details>


<h3 id="45-BIOCAP-Exploiting-Synthetic-Captions-Beyond-Labels-in-Biological-Foundation-Models"><a href="#45-BIOCAP-Exploiting-Synthetic-Captions-Beyond-Labels-in-Biological-Foundation-Models" class="headerlink" title="[45] BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20095">BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</a></h3><p><em>Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该研究探讨了利用合成描述性标注作为生物多模态基础模型的额外监督来源，通过生成合成标注并训练BIOCAP模型，提升了物种分类和图文检索性能。</p>
<details>
  <summary>Details</summary>
Motivation: 生物多模态基础模型通常仅依赖标签作为监督信号，而忽视了描述性标注的潜力。研究旨在填补这一空白，利用合成标注增强模型对生物特征的捕捉能力。

<p>Contribution: 1) 提出利用多模态大语言模型生成合成描述性标注的方法；2) 训练BIOCAP模型，证明描述性标注对生物基础模型的性能提升效果。</p>
<p>Method: 利用Wikipedia的视觉信息和分类学特化的格式示例，通过多模态大语言模型生成合成标注，并训练BIOCAP模型。</p>
<p>Result: BIOCAP在物种分类和图文检索任务中表现优异，验证了描述性标注的价值。</p>
<p>Insight: 描述性标注能帮助模型捕捉潜在的生物特征结构，抑制虚假相关性，为生物多模态模型提供了新的监督视角。</p>
<p>Abstract: This work investigates descriptive captions as an additional source of supervision for biological multimodal foundation models. Images and captions can be viewed as complementary samples from the latent morphospace of a species, each capturing certain biological traits. Incorporating captions during training encourages alignment with this shared latent structure, emphasizing potentially diagnostic characters while suppressing spurious correlations. The main challenge, however, lies in obtaining faithful, instance-specific captions at scale. This requirement has limited the utilization of natural language supervision in organismal biology compared with many other scientific domains. We complement this gap by generating synthetic captions with multimodal large language models (MLLMs), guided by Wikipedia-derived visual information and taxon-tailored format examples. These domain-specific contexts help reduce hallucination and yield accurate, instance-based descriptive captions. Using these captions, we train BIOCAP (i.e., BIOCLIP with Captions), a biological foundation model that captures rich semantics and achieves strong performance in species classification and text-image retrieval. These results demonstrate the value of descriptive captions beyond labels in bridging biological images with multimodal foundation models.</p>
</details>


<h3 id="46-Physics-Guided-Fusion-for-Robust-3D-Tracking-of-Fast-Moving-Small-Objects"><a href="#46-Physics-Guided-Fusion-for-Robust-3D-Tracking-of-Fast-Moving-Small-Objects" class="headerlink" title="[46] Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20126">Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects</a></h3><p><em>Prithvi Raj Singh,Raju Gottumukkala,Anthony S. Maida,Alan B. Barhorst,Vijaya Gopu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种结合深度学习和物理模型的系统，用于实时检测和跟踪快速移动的小物体，显著提升了3D跟踪性能。</p>
<details>
  <summary>Details</summary>
Motivation: 快速移动的小物体在计算机视觉中的检测和跟踪问题缺乏深入研究，现有方法在复杂场景中表现不佳，如遮挡和快速方向变化。

<p>Contribution: 1. 设计了一个完整的3D检测与跟踪系统；2. 提出了一种基于物理学的跟踪算法，结合运动学方程处理异常和漏检；3. 引入了异常检测与修正模块，显著提升性能。</p>
<p>Method: 结合深度学习检测模块与基于物理的运动模型（如运动学方程），并加入异常检测与修正机制。</p>
<p>Result: 在自定义数据集上，系统比基于卡尔曼滤波的方法平均位移误差减少70%。</p>
<p>Insight: 结合物理模型与深度学习可以提高复杂场景下的实时3D跟踪性能，尤其在快速移动小物体的应用中效果显著。</p>
<p>Abstract: While computer vision has advanced considerably for general object detection and tracking, the specific problem of fast-moving tiny objects remains underexplored. This paper addresses the significant challenge of detecting and tracking rapidly moving small objects using an RGB-D camera. Our novel system combines deep learning-based detection with physics-based tracking to overcome the limitations of existing approaches. Our contributions include: (1) a comprehensive system design for object detection and tracking of fast-moving small objects in 3D space, (2) an innovative physics-based tracking algorithm that integrates kinematics motion equations to handle outliers and missed detections, and (3) an outlier detection and correction module that significantly improves tracking performance in challenging scenarios such as occlusions and rapid direction changes. We evaluated our proposed system on a custom racquetball dataset. Our evaluation shows our system surpassing kalman filter based trackers with up to 70% less Average Displacement Error. Our system has significant applications for improving robot perception on autonomous platforms and demonstrates the effectiveness of combining physics-based models with deep learning approaches for real-time 3D detection and tracking of challenging small objects.</p>
</details>


<h3 id="47-Revisiting-Logit-Distributions-for-Reliable-Out-of-Distribution-Detection"><a href="#47-Revisiting-Logit-Distributions-for-Reliable-Out-of-Distribution-Detection" class="headerlink" title="[47] Revisiting Logit Distributions for Reliable Out-of-Distribution Detection"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20134">Revisiting Logit Distributions for Reliable Out-of-Distribution Detection</a></h3><p><em>Jiachen Liang,Ruibing Hou,Minyang Hu,Hong Chang,Shiguang Shan,Xilin Chen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出LogitGap，一种新颖的后处理OOD检测方法，通过利用最大logit与其余logit之间的关系，增强ID与OOD样本的可分离性，并通过聚焦更紧凑的logit子集进一步提升效果。</p>
<details>
  <summary>Details</summary>
Motivation: 现有后处理方法未充分利用模型logit空间的丰富信息，影响OOD检测的可靠性。

<p>Contribution: 提出LogitGap方法，结合理论分析和实证，显著提升OOD检测性能。</p>
<p>Method: 1. 利用最大logit与其余logit的关系；2. 引入无训练策略，自动选择信息量最大的logit子集。</p>
<p>Result: 在多种OOD检测场景和基准测试中，LogitGap表现卓越。</p>
<p>Insight: logit空间的探索对提升OOD检测性能至关重要，尤其是对最大logit与其余logit的关系的利用。</p>
<p>Abstract: Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning models in open-world applications. While post-hoc methods are favored for their efficiency and ease of deployment, existing approaches often underexploit the rich information embedded in the model’s logits space. In this paper, we propose LogitGap, a novel post-hoc OOD detection method that explicitly exploits the relationship between the maximum logit and the remaining logits to enhance the separability between in-distribution (ID) and OOD samples. To further improve its effectiveness, we refine LogitGap by focusing on a more compact and informative subset of the logit space. Specifically, we introduce a training-free strategy that automatically identifies the most informative logits for scoring. We provide both theoretical analysis and empirical evidence to validate the effectiveness of our approach. Extensive experiments on both vision-language and vision-only models demonstrate that LogitGap consistently achieves state-of-the-art performance across diverse OOD detection scenarios and benchmarks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/GIT-LJc/LogitGap">https://github.com/GIT-LJc/LogitGap</a>.</p>
</details>


<h3 id="48-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding"><a href="#48-PartNeXt-A-Next-Generation-Dataset-for-Fine-Grained-and-Hierarchical-3D-Part-Understanding" class="headerlink" title="[48] PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20155">PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding</a></h3><p><em>Penghao Wang,Yiyang He,Xin Lv,Yukai Zhou,Lan Xu,Jingyi Yu,Jiayuan Gu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PartNeXt是一个新一代的高质量纹理3D数据集，包含超过23,000个模型和50个类别的细粒度分层部件标注，旨在推动3D部件理解的研究，并在部件分割和3D部件问答任务中展现出优越性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的3D部件理解数据集（如PartNet）依赖于无纹理几何和专家标注，限制了其扩展性和可用性。PartNeXt旨在填补这一空白，提供高质量的纹理模型和多任务评估。

<p>Contribution: 1. 引入了PartNeXt数据集，包含高质量纹理模型和细粒度分层标注。2. 提出了一个新的3D部件问答任务基准。3. 展示了PartNeXt在部件分割任务中的优越性能，特别是在细粒度和叶级部件上。</p>
<p>Method: PartNeXt通过纹理感知标注和分层部件标签扩展了数据集的多样性，并使用Point-SAM等先进方法进行基准测试。</p>
<p>Result: 在部件分割任务中，PartNeXt推动了方法的性能提升；在3D部件问答任务中，暴露了开放词汇部件定位的显著差距。</p>
<p>Insight: 高质量纹理数据和细粒度标注是提升3D部件理解性能的关键，同时多任务评估揭示了未来研究的潜在方向。</p>
<p>Abstract: Understanding objects at the level of their constituent parts is fundamental to advancing computer vision, graphics, and robotics. While datasets like PartNet have driven progress in 3D part understanding, their reliance on untextured geometries and expert-dependent annotation limits scalability and usability. We introduce PartNeXt, a next-generation dataset addressing these gaps with over 23,000 high-quality, textured 3D models annotated with fine-grained, hierarchical part labels across 50 categories. We benchmark PartNeXt on two tasks: (1) class-agnostic part segmentation, where state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with fine-grained and leaf-level parts, and (2) 3D part-centric question answering, a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary part grounding. Additionally, training Point-SAM on PartNeXt yields substantial gains over PartNet, underscoring the dataset’s superior quality and diversity. By combining scalable annotation, texture-aware labels, and multi-task evaluation, PartNeXt opens new avenues for research in structured 3D understanding.</p>
</details>


<h3 id="49-Monocular-Visual-8D-Pose-Estimation-for-Articulated-Bicycles-and-Cyclists"><a href="#49-Monocular-Visual-8D-Pose-Estimation-for-Articulated-Bicycles-and-Cyclists" class="headerlink" title="[49] Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20158">Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists</a></h3><p><em>Eduardo R. Corral-Soto,Yang Liu,Yuan Ren,Bai Dongfeng,Liu Bingbing</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种用于估计铰接式自行车和骑行者的8D位姿的单目视觉方法，通过估计自行车车把和踏板的旋转，改进了传统的6D位姿估计。</p>
<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，骑行者的位姿估计对安全至关重要。传统的6D位姿估计方法无法捕捉铰接式自行车的动态变化（如车把和踏板的旋转），而这些变化直接影响骑行方向和意图预测。

<p>Contribution: 提出了首个从单幅RGB图像中估计铰接式自行车和骑行者的8D位姿的方法，新增了对车把和踏板旋转的估计，从而更精确地描述自行车的位姿和行驶方向。</p>
<p>Method: 模型联合估计8D位姿和3D关键点，并通过合成与真实图像的混合数据进行训练。</p>
<p>Result: 实验表明，该方法在8D位姿参数估计上表现优异，性能与当前最先进的6D位姿估计方法相当。</p>
<p>Insight: 对铰接式物体的位姿估计需要考虑其动态部分（如车把和踏板），这能显著提升位姿描述的准确性和实用性。</p>
<p>Abstract: In Autonomous Driving, cyclists belong to the safety-critical class of Vulnerable Road Users (VRU), and accurate estimation of their pose is critical for cyclist crossing intention classification, behavior prediction, and collision avoidance. Unlike rigid objects, articulated bicycles are composed of movable rigid parts linked by joints and constrained by a kinematic structure. 6D pose methods can estimate the 3D rotation and translation of rigid bicycles, but 6D becomes insufficient when the steering&#x2F;pedals angles of the bicycle vary. That is because: 1) varying the articulated pose of the bicycle causes its 3D bounding box to vary as well, and 2) the 3D box orientation is not necessarily aligned to the orientation of the steering which determines the actual intended travel direction. In this work, we introduce a method for category-level 8D pose estimation for articulated bicycles and cyclists from a single RGB image. Besides being able to estimate the 3D translation and rotation of a bicycle from a single image, our method also estimates the rotations of its steering handles and pedals with respect to the bicycle body frame. These two new parameters enable the estimation of a more fine-grained bicycle pose state and travel direction. Our proposed model jointly estimates the 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix of synthetic and real image data to generalize on real images. We include an evaluation section where we evaluate the accuracy of our estimated 8D pose parameters, and our method shows promising results by achieving competitive scores when compared against state-of-the-art category-level 6D pose estimators that use rigid canonical object templates for matching.</p>
</details>


<h3 id="50-TOMCAT-Test-time-Comprehensive-Knowledge-Accumulation-for-Compositional-Zero-Shot-Learning"><a href="#50-TOMCAT-Test-time-Comprehensive-Knowledge-Accumulation-for-Compositional-Zero-Shot-Learning" class="headerlink" title="[50] TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20162">TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</a></h3><p><em>Xudong Yan,Songhe Feng</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为TOMCAT的方法，通过在测试时积累文本和视觉模态的综合知识来更新多模态原型，以解决组合零样本学习中分布偏移的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的组合零样本学习方法在测试时会因标签空间的分布偏移（由未见过的属性-对象组合引起）而性能下降，因此需要一种能够在测试时动态适应分布变化的方法。

<p>Contribution: 1. 提出了一种测试时综合知识积累（TOMCAT）方法；2. 设计了自适应更新权重和动态优先级队列；3. 通过多模态协同表示学习对齐文本和视觉原型。</p>
<p>Method: 1. 在测试时利用无监督数据积累文本和视觉模态知识；2. 采用自适应权重控制原型调整程度；3. 引入动态队列存储高置信度图像；4. 通过多模态协同表示学习对齐原型。</p>
<p>Result: 在四个基准数据集上（封闭和开放世界设置下）取得了最先进的性能。</p>
<p>Insight: 在测试时动态积累知识可以有效缓解分布偏移问题，同时多模态对齐增强了模型的鲁棒性。</p>
<p>Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual knowledge from historical images for inference. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/xud-yan/TOMCAT">https://github.com/xud-yan/TOMCAT</a> .</p>
</details>


<h3 id="51-PPMStereo-Pick-and-Play-Memory-Construction-for-Consistent-Dynamic-Stereo-Matching"><a href="#51-PPMStereo-Pick-and-Play-Memory-Construction-for-Consistent-Dynamic-Stereo-Matching" class="headerlink" title="[51] PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20178">PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</a></h3><p><em>Yun Wang,Junjie Hu,Qiaole Dong,Yongjian Zhang,Yanwei Fu,Tin Lun Lam,Dapeng Wu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: PPMStereo提出了一种动态立体匹配方法，通过Pick-and-Play Memory（PPM）模块高效建模长时空一致性，解决了传统方法在计算效率和长程依赖性之间的权衡问题。</p>
<details>
  <summary>Details</summary>
Motivation: 动态立体匹配中，时间一致性对于增强现实等应用至关重要。传统方法在建模长时间一致性时面临计算效率与性能的权衡问题。

<p>Contribution: 提出了PPMStereo框架，包括PPM模块，通过‘pick’和‘play’两阶段协作，实现了高效的长时空信息聚合。</p>
<p>Method: PPM模块通过‘pick’选择最相关帧，‘play’自适应加权聚合信息，保持紧凑且高信息量的内存缓冲区。</p>
<p>Result: 在Sintel数据集上取得了0.62&#x2F;1.11 TEPE的SOTA性能，计算成本更低。</p>
<p>Insight: 两阶段协作的内存构造机制有效平衡了长时空建模的效率与性能。</p>
<p>Abstract: Temporally consistent depth estimation from stereo video is critical for real-world applications such as augmented reality, where inconsistent depth estimation disrupts the immersion of users. Despite its importance, this task remains challenging due to the difficulty in modeling long-term temporal consistency in a computationally efficient manner. Previous methods attempt to address this by aggregating spatio-temporal information but face a fundamental trade-off: limited temporal modeling provides only modest gains, whereas capturing long-range dependencies significantly increases computational cost. To address this limitation, we introduce a memory buffer for modeling long-range spatio-temporal consistency while achieving efficient dynamic stereo matching. Inspired by the two-stage decision-making process in humans, we propose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) construction module for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPM consists of a <code>pick&#39; process that identifies the most relevant frames and a </code>play’ process that weights the selected frames adaptively for spatio-temporal aggregation. This two-stage collaborative process maintains a compact yet highly informative memory buffer while achieving temporally consistent information aggregation. Extensive experiments validate the effectiveness of PPMStereo, demonstrating state-of-the-art performance in both accuracy and temporal consistency. % Notably, PPMStereo achieves 0.62&#x2F;1.11 TEPE on the Sintel clean&#x2F;final (17.3% &amp; 9.02% improvements over BiDAStereo) with fewer computational costs. Codes are available at \textcolor{blue}{<a target="_blank" rel="noopener" href="https://github.com/cocowy1/PPMStereo%7D">https://github.com/cocowy1/PPMStereo}</a>.</p>
</details>


<h3 id="52-Evaluating-Video-Models-as-Simulators-of-Multi-Person-Pedestrian-Trajectories"><a href="#52-Evaluating-Video-Models-as-Simulators-of-Multi-Person-Pedestrian-Trajectories" class="headerlink" title="[52] Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20182">Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories</a></h3><p><em>Aaron Appelle,Jerome P. Lynch</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种评估视频生成模型（T2V和I2V）作为多行人轨迹模拟器的严格方法，揭示模型在多智能体行为上的有效性及其局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管大规模视频生成模型在多样化场景中表现出高视觉真实感，但它们在多行人交互场景中的动态合理性尚未得到验证，因此需要一种评估方法来填补这一空白。

<p>Contribution: 1) 提出了一个严格的评估协议，用于衡量T2V和I2V模型作为行人动态隐式模拟器的能力；2) 开发了一种在未知相机参数的情况下从像素空间重建2D鸟瞰轨迹的方法；3) 分析了主流模型在多智能体行为上的表现及其失败模式。</p>
<p>Method: 1) 对于I2V模型，使用已知数据集的起始帧以与真实视频数据集进行比较；2) 对于T2V模型，设计了一套提示词来探索不同行人密度和交互场景；3) 提出了一种从像素空间重建2D鸟瞰轨迹的方法。</p>
<p>Result: 分析表明，主流模型在多智能体行为的先验学习上表现优异，但存在行人合并或消失等失败模式，需要进一步改进。</p>
<p>Insight: 视频生成模型在多行人交互场景中展现出潜力，但仍需解决动态合理性不足的问题，尤其是在复杂交互和多密度场景下。</p>
<p>Abstract: Large-scale video generation models have demonstrated high visual realism in diverse contexts, spurring interest in their potential as general-purpose world simulators. Existing benchmarks focus on individual subjects rather than scenes with multiple interacting people. However, the plausibility of multi-agent dynamics in generated videos remains unverified. We propose a rigorous evaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V) models as implicit simulators of pedestrian dynamics. For I2V, we leverage start frames from established datasets to enable comparison with a ground truth video dataset. For T2V, we develop a prompt suite to explore diverse pedestrian densities and interactions. A key component is a method to reconstruct 2D bird’s-eye view trajectories from pixel-space without known camera parameters. Our analysis reveals that leading models have learned surprisingly effective priors for plausible multi-agent behavior. However, failure modes like merging and disappearing people highlight areas for future improvement.</p>
</details>


<h3 id="53-SPAN-Continuous-Modeling-of-Suspicion-Progression-for-Temporal-Intention-Localization"><a href="#53-SPAN-Continuous-Modeling-of-Suspicion-Progression-for-Temporal-Intention-Localization" class="headerlink" title="[53] SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20189">SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization</a></h3><p><em>Xinyi Hu,Yuran Wang,Yue Li,Wenxuan Liu,Zheng Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出SPAN网络，将可疑意图定位从离散分类转为连续回归，捕捉可疑意图的动态变化。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的离散分类方法无法捕捉可疑意图的连续性和动态变化，限制了早期干预和可解释性。

<p>Contribution: 1. 提出SPAN网络，引入连续回归模型；2. 揭示了可疑意图的长时依赖性和累积效应；3. 设计了‘可疑系数调制’和多模态信息结合的动态调整方法；4. 提出概念锚定映射方法增强可解释性。</p>
<p>Method: 1. 使用连续回归而非离散分类；2. 结合TPP理论建模长时依赖；3. 动态调整可疑系数；4. 概念锚定映射链接行为和意图。</p>
<p>Result: 在HAI数据集上，MSE降低19.8%，mAP提升1.78%，低频场景下mAP提升2.74%。</p>
<p>Insight: 连续建模可疑意图可以更早检测微小行为变化，增强系统的可解释性和实用性。</p>
<p>Abstract: Temporal Intention Localization (TIL) is crucial for video surveillance, focusing on identifying varying levels of suspicious intentions to improve security monitoring. However, existing discrete classification methods fail to capture the continuous nature of suspicious intentions, limiting early intervention and explainability. In this paper, we propose the Suspicion Progression Analysis Network (SPAN), which shifts from discrete classification to continuous regression, enabling the capture of fluctuating and evolving suspicious intentions. We reveal that suspicion exhibits long-term dependencies and cumulative effects, similar to Temporal Point Process (TPP) theory. Based on these insights, we define a suspicion score formula that models continuous changes while accounting for temporal characteristics. We also introduce Suspicion Coefficient Modulation, which adjusts suspicion coefficients using multimodal information to reflect the varying impacts of suspicious actions. Additionally, the Concept-Anchored Mapping method is proposed to link suspicious actions to predefined intention concepts, offering insights into both the actions and their potential underlying intentions. Extensive experiments on the HAI dataset show that SPAN significantly outperforms existing methods, reducing MSE by 19.8% and improving average mAP by 1.78%. Notably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating its superior ability to capture subtle behavioral changes. Compared to discrete classification systems, our continuous suspicion modeling approach enables earlier detection and proactive intervention, greatly enhancing system explainability and practical utility in security applications.</p>
</details>


<h3 id="54-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling"><a href="#54-RAPO-Cross-Stage-Prompt-Optimization-for-Text-to-Video-Generation-via-Data-Alignment-and-Test-Time-Scaling" class="headerlink" title="[54] RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20206">RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling</a></h3><p><em>Bingjie Gao,Qianli Ma,Xiaoxue Wu,Shuai Yang,Guanzhou Lan,Haonan Zhao,Jiaxuan Chen,Qingyang Liu,Yu Qiao,Xinyuan Chen,Yaohui Wang,Li Niu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: RAPO++提出了一种跨阶段的提示优化框架，通过数据对齐和测试时尺度调整来提升文本到视频生成的质量，无需修改生成模型本身。该框架包括检索增强的提示优化、基于反馈的迭代优化和LLM微调三个阶段，实验表明其在多个基准上显著优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 用户提供的提示词通常是简短、非结构化且与训练数据不对齐的，这限制了扩散模型在文本到视频生成中的能力。RAPO++通过优化提示词来解决这一问题。

<p>Contribution: 提出了一种跨阶段的提示优化框架RAPO++，包括数据对齐的提示增强、测试时迭代优化和LLM微调，显著提升了生成视频的质量。</p>
<p>Method: 1. 第一阶段（RAPO）：通过检索增强的提示优化对齐训练数据；2. 第二阶段（SSPO）：基于多源反馈迭代优化提示词；3. 第三阶段：利用优化后的提示对微调LLM，内化任务特定的优化模式。</p>
<p>Result: 在五个先进的T2V模型和五个基准测试中，RAPO++在语义对齐、组合推理、时间稳定性和物理合理性方面显著优于现有方法。</p>
<p>Insight: RAPO++是一种模型无关、高效且可扩展的方案，通过优化提示词而非模型本身来提升生成质量，为T2V领域的提示优化设立了新标准。</p>
<p>Abstract: Prompt design plays a crucial role in text-to-video (T2V) generation, yet user-provided prompts are often short, unstructured, and misaligned with training data, limiting the generative potential of diffusion-based T2V models. We present \textbf{RAPO++}, a cross-stage prompt optimization framework that unifies training-data–aligned refinement, test-time iterative scaling, and large language model (LLM) fine-tuning to substantially improve T2V generation without modifying the underlying generative backbone. In \textbf{Stage 1}, Retrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with semantically relevant modifiers retrieved from a relation graph and refactors them to match training distributions, enhancing compositionality and multi-object fidelity. \textbf{Stage 2} introduces Sample-Specific Prompt Optimization (SSPO), a closed-loop mechanism that iteratively refines prompts using multi-source feedback – including semantic alignment, spatial fidelity, temporal coherence, and task-specific signals such as optical flow – yielding progressively improved video generation quality. \textbf{Stage 3} leverages optimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing task-specific optimization patterns and enabling efficient, high-quality prompt generation even before inference. Extensive experiments across five state-of-the-art T2V models and five benchmarks demonstrate that RAPO++ achieves significant gains in semantic alignment, compositional reasoning, temporal stability, and physical plausibility, outperforming existing methods by large margins. Our results highlight RAPO++ as a model-agnostic, cost-efficient, and scalable solution that sets a new standard for prompt optimization in T2V generation. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Vchitect/RAPO">https://github.com/Vchitect/RAPO</a>.</p>
</details>


<h3 id="55-Towards-Objective-Obstetric-Ultrasound-Assessment-Contrastive-Representation-Learning-for-Fetal-Movement-Detection"><a href="#55-Towards-Objective-Obstetric-Ultrasound-Assessment-Contrastive-Representation-Learning-for-Fetal-Movement-Detection" class="headerlink" title="[55] Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20214">Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</a></h3><p><em>Talha Ilyas,Duong Nhu,Allison Thomas,Arie Levin,Lim Wei Yap,Shu Gong,David Vera Anaya,Yiwen Jiang,Deval Mehta,Ritesh Warty,Vinayak Smith,Maya Reddy,Euan Wallace,Wenlong Cheng,Zongyuan Ge,Faezeh Marzbanrad</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 为了解决胎儿运动检测中的主观性和准确性问题，论文提出了一种名为CURL的自监督对比学习框架，通过双对比损失和任务特定采样策略，提升了超声视频中胎儿运动的检测性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有胎儿运动检测方法（如母体感知和胎心监护）存在主观性强和准确性低的问题，亟需一种客观可靠的自动化方法。

<p>Contribution: 提出了CURL框架，结合空间和时间对比学习，设计双对比损失；引入任务特定采样策略，优化自监督训练；通过概率微调方法支持长时超声视频的灵活推理。</p>
<p>Method: 采用自监督对比学习框架（CURL），结合空间和时间对比学习（双对比损失），配合任务特定采样策略和概率微调方法。</p>
<p>Result: 在92名受试者的30分钟超声视频数据集上，CURL达到78.01%的灵敏度和81.60%的AUROC。</p>
<p>Insight: 自监督对比学习能有效提取胎儿运动特征，为产前监测和临床决策提供客观依据。</p>
<p>Abstract: Accurate fetal movement (FM) detection is essential for assessing prenatal health, as abnormal movement patterns can indicate underlying complications such as placental dysfunction or fetal distress. Traditional methods, including maternal perception and cardiotocography (CTG), suffer from subjectivity and limited accuracy. To address these challenges, we propose Contrastive Ultrasound Video Representation Learning (CURL), a novel self-supervised learning framework for FM detection from extended fetal ultrasound video recordings. Our approach leverages a dual-contrastive loss, incorporating both spatial and temporal contrastive learning, to learn robust motion representations. Additionally, we introduce a task-specific sampling strategy, ensuring the effective separation of movement and non-movement segments during self-supervised training, while enabling flexible inference on arbitrarily long ultrasound recordings through a probabilistic fine-tuning approach. Evaluated on an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions, CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its potential for reliable and objective FM analysis. These results highlight the potential of self-supervised contrastive learning for fetal movement analysis, paving the way for improved prenatal monitoring and clinical decision-making.</p>
</details>


<h3 id="56-EditInfinity-Image-Editing-with-Binary-Quantized-Generative-Models"><a href="#56-EditInfinity-Image-Editing-with-Binary-Quantized-Generative-Models" class="headerlink" title="[56] EditInfinity: Image Editing with Binary-Quantized Generative Models"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20217">EditInfinity: Image Editing with Binary-Quantized Generative Models</a></h3><p><em>Jiahuan Wang,Yuxin Chen,Jun Yu,Guangming Lu,Wenjie Pei</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: EditInfinity 是一种基于二进制量化生成模型的图像编辑方法，通过精确的反转机制和全面的平滑策略，显著提升了文本驱动图像编辑的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的图像编辑方法在反转过程中存在近似误差，限制了编辑性能。因此，作者探索了基于 VQ 的生成模型的参数高效适配方法，利用其可获取精确中间量化表示的特性，以实现更精确的图像反转。

<p>Contribution: 1) 提出了 EditInfinity，通过适应二进制量化生成模型 Infinity 实现高效图像编辑；2) 设计了高效且精确的图像反转机制；3) 提出了全面的平滑策略，确保编辑结果与源图像的高保真性和文本提示的语义对齐。</p>
<p>Method: 利用 Infinity 模型的二进制量化特性，设计了结合文本提示修正和图像风格保留的图像反转机制，并提出了全流程平滑策略以实现高保真编辑。</p>
<p>Result: 在 PIE-Bench 基准测试中，EditInfinity 在“添加”、“修改”和“删除”操作上优于现有的基于扩散模型的基线方法。</p>
<p>Insight: 二进制量化生成模型的精确中间表示特性为图像编辑提供了新的监督来源，可以有效避免扩散模型反转中的近似误差问题。</p>
<p>Abstract: Adapting pretrained diffusion-based generative models for text-driven image editing with negligible tuning overhead has demonstrated remarkable potential. A classical adaptation paradigm, as followed by these methods, first infers the generative trajectory inversely for a given source image by image inversion, then performs image editing along the inferred trajectory guided by the target text prompts. However, the performance of image editing is heavily limited by the approximation errors introduced during image inversion by diffusion models, which arise from the absence of exact supervision in the intermediate generative steps. To circumvent this issue, we investigate the parameter-efficient adaptation of VQ-based generative models for image editing, and leverage their inherent characteristic that the exact intermediate quantized representations of a source image are attainable, enabling more effective supervision for precise image inversion. Specifically, we propose \emph{EditInfinity}, which adapts \emph{Infinity}, a binary-quantized generative model, for image editing. We propose an efficient yet effective image inversion mechanism that integrates text prompting rectification and image style preservation, enabling precise image inversion. Furthermore, we devise a holistic smoothing strategy which allows our \emph{EditInfinity} to perform image editing with high fidelity to source images and precise semantic alignment to the text prompts. Extensive experiments on the PIE-Bench benchmark across “add”, “change”, and “delete” editing operations, demonstrate the superior performance of our model compared to state-of-the-art diffusion-based baselines. Code available at: <a target="_blank" rel="noopener" href="https://github.com/yx-chen-ust/EditInfinity">https://github.com/yx-chen-ust/EditInfinity</a>.</p>
</details>


<h3 id="57-Why-LVLMs-Are-More-Prone-to-Hallucinations-in-Longer-Responses-The-Role-of-Context"><a href="#57-Why-LVLMs-Are-More-Prone-to-Hallucinations-in-Longer-Responses-The-Role-of-Context" class="headerlink" title="[57] Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20229">Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context</a></h3><p><em>Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文研究了大型视觉语言模型（LVLM）在长响应中更容易产生幻觉的原因，提出幻觉的增加并非单纯由长度引起，而是因为长响应更依赖上下文。作者提出了一个’诱导-检测-抑制’框架，显著改善了幻觉问题。</p>
<details>
  <summary>Details</summary>
Motivation: LVLM在长响应中表现出更多的幻觉，传统认为是由长度导致的累积不确定性引起，但作者质疑是否有更深层次的机制。

<p>Contribution: 1. 提出幻觉的增加是由于长响应更依赖上下文，而非长度本身；2. 提出了一个新颖的’诱导-检测-抑制’框架，有效减少了幻觉。</p>
<p>Method: 设计了一种’诱导-检测-抑制’框架：通过故意设计的上下文诱导幻觉，利用诱导实例早期检测高风险案例，并在解码阶段抑制潜在的幻觉。</p>
<p>Result: 该方法在所有基准测试中均取得了显著的改进，验证了框架的有效性。</p>
<p>Insight: 幻觉的根源在于上下文依赖而非长度，这一发现为未来研究LVLM的幻觉问题提供了新的方向。</p>
<p>Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent years but are also prone to hallucination issues. They exhibit more hallucinations in longer, free-form responses, often attributed to accumulated uncertainties. In this paper, we ask: Does increased hallucination result solely from length-induced errors, or is there a deeper underlying mechanism? After a series of preliminary experiments and findings, we suggest that the risk of hallucinations is not caused by length itself but by the increased reliance on context for coherence and completeness in longer responses. Building on these insights, we propose a novel “induce-detect-suppress” framework that actively induces hallucinations through deliberately designed contexts, leverages induced instances for early detection of high-risk cases, and ultimately suppresses potential object-level hallucinations during actual decoding. Our approach achieves consistent, significant improvements across all benchmarks, demonstrating its efficacy. The strong detection and improved hallucination mitigation not only validate our framework but, more importantly, re-validate our hypothesis on context. Rather than solely pursuing performance gains, this study aims to provide new insights and serves as a first step toward a deeper exploration of hallucinations in LVLMs’ longer responses.</p>
</details>


<h3 id="58-Empower-Words-DualGround-for-Structured-Phrase-and-Sentence-Level-Temporal-Grounding"><a href="#58-Empower-Words-DualGround-for-Structured-Phrase-and-Sentence-Level-Temporal-Grounding" class="headerlink" title="[58] Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20244">Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding</a></h3><p><em>Minseok Kang,Minhyeok Lee,Minjung Kim,Donghyeong Kim,Sangyoun Lee</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DualGround是一种双分支架构，通过分离全局和局部语义来提升视频时序定位任务的表现。该方法通过结构化短语和句子级语义的分离建模，实现了更精细的时序对齐。</p>
<details>
  <summary>Details</summary>
Motivation: 现有VTG模型在处理文本标记时忽视了其语义角色的差异，过度依赖句子级全局语义，导致无法有效利用词级信号进行细粒度的时序对齐。

<p>Contribution: 1）提出了DualGround双分支架构，分离句子级和短语级语义；2）设计了角色感知的跨模态交互策略和联合建模框架，提升了全局和局部语义的对齐能力。</p>
<p>Method: 使用双分支结构：句子级路径处理[EOS]标记，短语级路径聚类词标记进行局部定位；引入角色感知的跨模态交互策略和联合优化框架。</p>
<p>Result: 在QVHighlights和Charades-STA基准上，DualGround在Moment Retrieval和Highlight Detection任务中均取得了SOTA表现。</p>
<p>Insight: 通过分离建模不同语义角色的文本标记，模型能够同时捕获全局和局部语义，提升视频时序定位的精度和鲁棒性。</p>
<p>Abstract: Video Temporal Grounding (VTG) aims to localize temporal segments in long, untrimmed videos that align with a given natural language query. This task typically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection (HD). While recent advances have been progressed by powerful pretrained vision-language models such as CLIP and InternVideo2, existing approaches commonly treat all text tokens uniformly during crossmodal attention, disregarding their distinct semantic roles. To validate the limitations of this approach, we conduct controlled experiments demonstrating that VTG models overly rely on [EOS]-driven global semantics while failing to effectively utilize word-level signals, which limits their ability to achieve fine-grained temporal alignment. Motivated by this limitation, we propose DualGround, a dual-branch architecture that explicitly separates global and local semantics by routing the [EOS] token through a sentence-level path and clustering word tokens into phrase-level units for localized grounding. Our method introduces (1) tokenrole- aware cross modal interaction strategies that align video features with sentence-level and phrase-level semantics in a structurally disentangled manner, and (2) a joint modeling framework that not only improves global sentence-level alignment but also enhances finegrained temporal grounding by leveraging structured phrase-aware context. This design allows the model to capture both coarse and localized semantics, enabling more expressive and context-aware video grounding. DualGround achieves state-of-the-art performance on both Moment Retrieval and Highlight Detection tasks across QVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of disentangled semantic modeling in video-language alignment.</p>
</details>


<h3 id="59-Calibrating-Multimodal-Consensus-for-Emotion-Recognition"><a href="#59-Calibrating-Multimodal-Consensus-for-Emotion-Recognition" class="headerlink" title="[59] Calibrating Multimodal Consensus for Emotion Recognition"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20256">Calibrating Multimodal Consensus for Emotion Recognition</a></h3><p><em>Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种称为校准多模态共识（CMC）的模型，用于解决多模态情感识别中语义不一致和文本模态主导的问题。通过伪标签生成模块和参数无关的融合模块，CMC在多模态微调中实现了更好的共识，并在多个数据集上取得了优异的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 多模态情感识别（MER）中存在跨模态语义不一致和文本模态主导的问题，这会影响识别准确性。论文旨在解决这些问题。

<p>Contribution: 提出了CMC模型，引入了伪标签生成模块（PLGM）和参数无关的融合模块（PFM），以缓解文本模态的主导地位并实现多模态共识。</p>
<p>Method: 利用PLGM生成伪单模态标签进行自监督预训练，再通过PFM和多模态共识路由器（MCR）进行多模态微调。</p>
<p>Result: 在CH-SIMS、CH-SIMS v2、CMU-MOSI和CMU-MOSEI数据集上，CMC的性能与或优于现有最佳方法，尤其在语义不一致场景中表现突出。</p>
<p>Insight: 通过伪标签和多模态共识机制，可以有效减少模态间的不一致性，提升情感识别的鲁棒性和准确性。</p>
<p>Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial progress. Nevertheless, most existing approaches neglect the semantic inconsistencies that may arise across modalities, such as conflicting emotional cues between text and visual inputs. Besides, current methods are often dominated by the text modality due to its strong representational capacity, which can compromise recognition accuracy. To address these challenges, we propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels, enabling unimodal pretraining in a self-supervised fashion. It then employs a Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for multimodal finetuning, thereby mitigating text dominance and guiding the fusion process toward a more reliable consensus. Experimental results demonstrate that CMC achieves performance on par with or superior to state-of-the-art methods across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and exhibits notable advantages in scenarios with semantic inconsistencies on CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible at <a target="_blank" rel="noopener" href="https://github.com/gw-zhong/CMC">https://github.com/gw-zhong/CMC</a>.</p>
</details>


<h3 id="60-Real-Time-Currency-Detection-and-Voice-Feedback-for-Visually-Impaired-Individuals"><a href="#60-Real-Time-Currency-Detection-and-Voice-Feedback-for-Visually-Impaired-Individuals" class="headerlink" title="[60] Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20267">Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals</a></h3><p><em>Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出一种实时货币检测系统，结合YOLOv8 nano模型和SE块，帮助视障人士通过语音反馈识别货币，准确率高达97.73%。</p>
<details>
  <summary>Details</summary>
Motivation: 视障人士在日常生活中处理货币依赖他人，智能手机和机器学习技术可以为他们提供独立操作的解决方案。

<p>Contribution: 提出了一种高精度的实时货币检测系统，支持多种货币类别识别，并通过语音反馈提升实用性。</p>
<p>Method: 采用YOLOv8 nano模型，结合深度卷积层和Squeeze-and-Excitation（SE）块，优化特征提取与检测性能。</p>
<p>Result: 模型准确率达97.73%、召回率95.23%、F1分数95.85%，mAP50(B)为97.21%。</p>
<p>Insight: 结合轻量级模型和SE块可实现高效检测，语音反馈为视障人士提供了实用且易用的解决方案。</p>
<p>Abstract: Technologies like smartphones have become an essential in our daily lives. It has made accessible to everyone including visually impaired individuals. With the use of smartphone cameras, image capturing and processing have become more convenient. With the use of smartphones and machine learning, the life of visually impaired can be made a little easier. Daily tasks such as handling money without relying on someone can be troublesome for them. For that purpose this paper presents a real-time currency detection system designed to assist visually impaired individuals. The proposed model is trained on a dataset containing 30 classes of notes and coins, representing 3 types of currency: US dollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a YOLOv8 nano model with a custom detection head featuring deep convolutional layers and Squeeze-and-Excitation blocks to enhance feature extraction and detection accuracy. Our model has achieved a higher accuracy of 97.73%, recall of 95.23%, f1-score of 95.85% and a mean Average Precision at IoU&#x3D;0.5 (mAP50(B)) of 97.21%. Using the voice feedback after the detection would help the visually impaired to identify the currency. This paper aims to create a practical and efficient currency detection system to empower visually impaired individuals independent in handling money.</p>
</details>


<h3 id="61-GMFVAD-Using-Grained-Multi-modal-Feature-to-Improve-Video-Anomaly-Detection"><a href="#61-GMFVAD-Using-Grained-Multi-modal-Feature-to-Improve-Video-Anomaly-Detection" class="headerlink" title="[61] GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20268">GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection</a></h3><p><em>Guangyu Dai,Dong Chen,Siliang Tang,Yueting Zhuang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文GMFVAD提出了一种粒度更细的多模态特征方法，用于改进视频异常检测（VAD），通过结合视频片段的主要内容和文本特征，减少冗余信息，提升检测性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的视频异常检测方法多依赖于粗粒度的多模态信息融合，导致冗余信息未被有效处理，影响了检测性能。论文旨在通过更细粒度的特征提取和多模态信息融合，解决这一问题。

<p>Contribution: 1. 提出GMFVAD方法，通过细粒度的多模态特征提取和融合减少冗余信息；2. 结合视频片段的主要内容和文本特征，增强视觉特征的表达能力；3. 在四个主要数据集上达到SOTA性能。</p>
<p>Method: 1. 基于视频片段生成细粒度的多模态特征；2. 引入视频描述的文本特征，增强视觉特征的突出部分；3. 通过特征融合和冗余信息减少策略优化检测性能。</p>
<p>Result: 在四个主要数据集上实现了SOTA性能，并通过消融实验验证了冗余信息减少的有效性。</p>
<p>Insight: 细粒度的多模态特征融合能够有效减少冗余信息，从而提升视频异常检测的性能；文本特征的引入可以进一步增强视觉特征的表达能力。</p>
<p>Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous frames in continuous surveillance videos. Most previous work utilizes the spatio-temporal correlation of visual features to distinguish whether there are abnormalities in video snippets. Recently, some works attempt to introduce multi-modal information, like text feature, to enhance the results of video anomaly detection. However, these works merely incorporate text features into video snippets in a coarse manner, overlooking the significant amount of redundant information that may exist within the video snippets. Therefore, we propose to leverage the diversity among multi-modal information to further refine the extracted features, reducing the redundancy in visual features, and we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD). Specifically, we generate more grained multi-modal feature based on the video snippet, which summarizes the main content, and text features based on the captions of original video will be introduced to further enhance the visual features of highlighted portions. Experiments show that the proposed GMFVAD achieves state-of-the-art performance on four mainly datasets. Ablation experiments also validate that the improvement of GMFVAD is due to the reduction of redundant information.</p>
</details>


<h3 id="62-Causal-Debiasing-for-Visual-Commonsense-Reasoning"><a href="#62-Causal-Debiasing-for-Visual-Commonsense-Reasoning" class="headerlink" title="[62] Causal Debiasing for Visual Commonsense Reasoning"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20281">Causal Debiasing for Visual Commonsense Reasoning</a></h3><p><em>Jiayi Zou,Gengyun Jia,Bing-Kun Bao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种针对视觉常识推理（VCR）的去偏方法，通过分析数据集中的共现和统计偏差，并引入VCR-OOD数据集评估模型的泛化能力，同时采用后门调整方法消除预测捷径。</p>
<details>
  <summary>Details</summary>
Motivation: 现有VCR方法忽略了数据集中的偏差问题，缺乏去偏策略，可能导致模型依赖于数据中的统计相关性而非真正的因果关系。

<p>Contribution: 1. 揭示了VCR中文本和视觉数据的共现与统计偏差；2. 提出了VCR-OOD数据集以评估跨模态泛化能力；3. 采用因果图和后门调整方法消除预测捷径。</p>
<p>Method: 通过分析因果图和预测捷径，使用后门调整方法，基于正确答案集合构建字典以消除偏差。</p>
<p>Result: 实验证明了该去偏方法在不同数据集上的有效性。</p>
<p>Insight: 去偏方法可以提升模型的泛化能力，避免依赖数据中的统计相关性，从而更接近真实因果推理。</p>
<p>Abstract: Visual Commonsense Reasoning (VCR) refers to answering questions and providing explanations based on images. While existing methods achieve high prediction accuracy, they often overlook bias in datasets and lack debiasing strategies. In this paper, our analysis reveals co-occurrence and statistical biases in both textual and visual data. We introduce the VCR-OOD datasets, comprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate the generalization capabilities of models across two modalities. Furthermore, we analyze the causal graphs and prediction shortcuts in VCR and adopt a backdoor adjustment method to remove bias. Specifically, we create a dictionary based on the set of correct answers to eliminate prediction shortcuts. Experiments demonstrate the effectiveness of our debiasing method across different datasets.</p>
</details>


<h3 id="63-Knowledge-Informed-Neural-Network-for-Complex-Valued-SAR-Image-Recognition"><a href="#63-Knowledge-Informed-Neural-Network-for-Complex-Valued-SAR-Image-Recognition" class="headerlink" title="[63] Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20284">Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition</a></h3><p><em>Haodong Yang,Zhongling Huang,Shaojie Guo,Zhe Zhang,Gong Cheng,Junwei Han</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种知识引导的轻量级神经网络（KINN），用于解决复杂值SAR图像识别中数据稀缺和领域迁移下的泛化、可解释性和效率三者之间的矛盾。KINN通过物理压缩、特征聚合和语义压缩三阶段架构，结合电磁散射先验知识，显著提升了识别性能。</p>
<details>
  <summary>Details</summary>
Motivation: 传统数据驱动模型在CV-SAR图像识别中难以平衡泛化性、可解释性和效率，而CV-SAR数据中丰富的电磁散射特征未被充分利用。论文希望通过引入物理先验知识，解决这一三重困境。

<p>Contribution: 提出了KINN框架，通过三阶段压缩-聚合-压缩架构，结合电磁散射物理先验，实现了参数高效的SAR图像识别，并在泛化性、可解释性和效率上取得平衡。</p>
<p>Method: KINN采用物理引导压缩、特征聚合和语义压缩三阶段架构。第一阶段通过字典处理器嵌入物理先验，第二阶段聚合特征，第三阶段通过自蒸馏压缩语义信息。框架支持CNN和ViT两种变体。</p>
<p>Result: 在五个SAR基准测试中，KINN（0.7M&#x2F;0.95M参数）在数据稀缺和领域迁移场景下表现优异，泛化能力强且具可解释性，成为参数高效识别的SOTA。</p>
<p>Insight: 结合物理先验的轻量级设计可有效解决多目标优化问题；KINN为SAR图像分析提供了一条可信AI的新路径。</p>
<p>Abstract: Deep learning models for complex-valued Synthetic Aperture Radar (CV-SAR) image recognition are fundamentally constrained by a representation trilemma under data-limited and domain-shift scenarios: the concurrent, yet conflicting, optimization of generalization, interpretability, and efficiency. Our work is motivated by the premise that the rich electromagnetic scattering features inherent in CV-SAR data hold the key to resolving this trilemma, yet they are insufficiently harnessed by conventional data-driven models. To this end, we introduce the Knowledge-Informed Neural Network (KINN), a lightweight framework built upon a novel “compression-aggregation-compression” architecture. The first stage performs a physics-guided compression, wherein a novel dictionary processor adaptively embeds physical priors, enabling a compact unfolding network to efficiently extract sparse, physically-grounded signatures. A subsequent aggregation module enriches these representations, followed by a final semantic compression stage that utilizes a compact classification head with self-distillation to learn maximally task-relevant and discriminative embeddings. We instantiate KINN in both CNN (0.7M) and Vision Transformer (0.95M) variants. Extensive evaluations on five SAR benchmarks confirm that KINN establishes a state-of-the-art in parameter-efficient recognition, offering exceptional generalization in data-scarce and out-of-distribution scenarios and tangible interpretability, thereby providing an effective solution to the representation trilemma and offering a new path for trustworthy AI in SAR image analysis.</p>
</details>


<h3 id="64-DMC-3-Dual-Modal-Counterfactual-Contrastive-Construction-for-Egocentric-Video-Question-Answering"><a href="#64-DMC-3-Dual-Modal-Counterfactual-Contrastive-Construction-for-Egocentric-Video-Question-Answering" class="headerlink" title="[64] DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20285">DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</a></h3><p><em>Jiayi Zou,Chaofan Chen,Bing-Kun Bao,Changsheng Xu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DMC³提出了一个双模态反事实对比构建框架，解决了第一人称视角视频问答中的多事件理解和手-物交互识别挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在Egocentric VideoQA中忽视了第一人称视角的独特挑战，如多事件理解和手-物交互识别，DMC³旨在解决这些问题。

<p>Contribution: 提出了DMC³框架，包括一个Egocentric VideoQA基线、反事实样本构建模块和反事实样本对比优化模块，显著提升了性能。</p>
<p>Method: 通过事件描述转述和核心交互挖掘生成正负样本，并利用对比损失优化特征距离。</p>
<p>Result: 在EgoTaskQA和QAEGO4D上分别达到了52.51%、46.04%和13.2%的SOTA性能。</p>
<p>Insight: 反事实样本对比优化能有效提升模型对多事件和交互的理解能力。</p>
<p>Abstract: Egocentric Video Question Answering (Egocentric VideoQA) plays an important role in egocentric video understanding, which refers to answering questions based on first-person videos. Although existing methods have made progress through the paradigm of pre-training and fine-tuning, they ignore the unique challenges posed by the first-person perspective, such as understanding multiple events and recognizing hand-object interactions. To deal with these challenges, we propose a Dual-Modal Counterfactual Contrastive Construction (DMC$^3$) framework, which contains an egocentric videoqa baseline, a counterfactual sample construction module and a counterfactual sample-involved contrastive optimization. Specifically, We first develop a counterfactual sample construction module to generate positive and negative samples for textual and visual modalities through event description paraphrasing and core interaction mining, respectively. Then, We feed these samples together with the original samples into the baseline. Finally, in the counterfactual sample-involved contrastive optimization module, we apply contrastive loss to minimize the distance between the original sample features and the positive sample features, while maximizing the distance from the negative samples. Experiments show that our method achieve 52.51% and 46.04% on the \textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2% on QAEGO4D, both reaching the state-of-the-art performance.</p>
</details>


<h3 id="65-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning"><a href="#65-UI-Ins-Enhancing-GUI-Grounding-with-Multi-Perspective-Instruction-as-Reasoning" class="headerlink" title="[65] UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20286">UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction-as-Reasoning</a></h3><p><em>Liangyu Chen,Hanzhang Zhou,Chenglin Cai,Jianan Zhang,Panrong Tong,Quyu Kong,Xu Zhang,Chen Liu,Yuqi Liu,Wenxuan Wang,Yue Wang,Qin Jin,Steven Hoi</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为Instruction-as-Reasoning的新范式，通过动态分析多样化指令，提升GUI grounding的性能。该方法结合了监督微调和强化学习，显著提高了多个基准测试的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的GUI grounding方法将指令视为静态代理，忽略了指令多样性和质量对性能的影响。研究发现现有数据集中指令存在23.3%的错误率，利用指令多样性可在推理时提升76%的性能。

<p>Contribution: 1. 提出Instruction-as-Reasoning范式，将指令作为动态分析路径。2. 设计了两阶段训练框架（SFT+RL），优化指令路径选择和组合。3. 在多个基准测试上实现了SOTA性能。</p>
<p>Method: 1. 监督微调（SFT）：使用合成的多样化指令训练模型具备多视角推理能力。2. 强化学习（RL）：优化指令路径的选择和组合。模型UI-Ins-7B和UI-Ins-32B分别应用于不同场景。</p>
<p>Result: UI-Ins-32B在UI-I2E-Bench（87.3%）、ScreenSpot-Pro（57.0%）和MMBench-GUI L2（84.9%）上取得最佳性能。UI-Ins-7B在AndroidWorld任务中实现了74.1%的成功率。</p>
<p>Insight: 1. 多样化指令的动态利用能够显著提升grounding性能。2. SFT+RL框架能够避免策略崩溃问题。3. 该方法具备较强的智能代理潜力。</p>
<p>Abstract: GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior works largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity and quality on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a substantial 76% relative performance improvement. In this paper, we introduce the Instruction-as-Reasoning paradigm, treating instructions as dynamic analytical pathways that offer distinct perspectives and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging grounding benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy, scoring 87.3% on UI-I2E-Bench, 57.0% on ScreenSpot-Pro, and 84.9% on MMBench-GUI L2. Furthermore, our model demonstrates strong agentic potential, achieving a 74.1% success rate on AndroidWorld using UI-Ins-7B as the executor. Our in-depth analysis reveals additional insights such as how reasoning can be formulated to enhance rather than hinder grounding performance, and how our method mitigates policy collapse in the SFT+RL framework. All code and model checkpoints will be publicly released in <a target="_blank" rel="noopener" href="https://github.com/alibaba/UI-Ins">https://github.com/alibaba/UI-Ins</a>.</p>
</details>


<h3 id="66-Breakdance-Video-classification-in-the-age-of-Generative-AI"><a href="#66-Breakdance-Video-classification-in-the-age-of-Generative-AI" class="headerlink" title="[66] Breakdance Video classification in the age of Generative AI"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20287">Breakdance Video classification in the age of Generative AI</a></h3><p><em>Sauptik Dhar,Naveen Ramakrishnan,Michelle Munson</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文探讨了现代视频基础模型（编码器和解码器）在断舞视频分类中的适用性，发现编码器模型仍然优于最先进的视频语言模型，并提供了选择编码器模型的见解和解码器模型的微调分析。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型主要应用于热门运动如足球、篮球等，而在小众但流行的断舞运动中研究较少。本文旨在填补这一空白。

<p>Contribution: 1. 分析了现代视频基础模型在断舞视频分类中的表现；2. 发现编码器模型在预测任务中优于视频语言模型；3. 提供了选择编码器模型的见解和解码器模型微调的详细分析。</p>
<p>Method: 使用现代视频基础模型（编码器和解码器），对断舞视频进行分类任务，并对比编码器和解码器的性能表现。</p>
<p>Result: 编码器模型在断舞视频分类任务中表现优于视频语言模型，展示了其在特定领域中的优势。</p>
<p>Insight: 1. 编码器模型在特定领域（如断舞）的分类任务中仍具有优势；2. 解码器模型的微调需要更细致的分析；3. 小众运动领域的研究对推动基模型的应用具有重要意义。</p>
<p>Abstract: Large Vision Language models have seen huge application in several sports use-cases recently. Most of these works have been targeted towards a limited subset of popular sports like soccer, cricket, basketball etc; focusing on generative tasks like visual question answering, highlight generation. This work analyzes the applicability of the modern video foundation models (both encoder and decoder) for a very niche but hugely popular dance sports - breakdance. Our results show that Video Encoder models continue to outperform state-of-the-art Video Language Models for prediction tasks. We provide insights on how to choose the encoder model and provide a thorough analysis into the workings of a finetuned decoder model for breakdance video classification.</p>
</details>


<h3 id="67-A-Parameter-Efficient-Mixture-of-Experts-Framework-for-Cross-Modal-Geo-Localization"><a href="#67-A-Parameter-Efficient-Mixture-of-Experts-Framework-for-Cross-Modal-Geo-Localization" class="headerlink" title="[67] A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20291">A Parameter-Efficient Mixture-of-Experts Framework for Cross-Modal Geo-Localization</a></h3><p><em>LinFeng Li,Jian Zhao,Zepeng Yang,Yuhang Song,Bojun Lin,Tianle Zhang,Yuchen Yuan,Chi Zhang,Xuelong Li</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一个参数高效的混合专家（MoE）框架，用于解决跨模态地理定位中的异构视角问题，通过领域对齐预处理和两阶段训练策略取得了比赛第一名。</p>
<details>
  <summary>Details</summary>
Motivation: 跨模态地理定位任务中存在严重的平台间异构性和训练与测试领域的差距问题，需要一种高效的框架来解决这些挑战。

<p>Contribution: 1. 提出了领域对齐的预处理流程；2. 设计了基于混合专家（MoE）的框架；3. 采用两阶段训练策略增强判别力。</p>
<p>Method: 1. 平台划分、卫星增强和方向词移除；2. 基于LLM的文本对齐流程；3. 使用BGE-M3和EVA-CLIP训练三个平台专家，并通过硬负样本挖掘增强性能。</p>
<p>Result: 系统在官方排行榜上排名第一，展示了在异构视角下强大的跨模态地理定位能力。</p>
<p>Insight: 通过领域对齐和专家混合模型，可以有效缓解跨模态任务中的异构性问题，提升定位精度。</p>
<p>Abstract: We present a winning solution to RoboSense 2025 Track 4: Cross-Modal Drone Navigation. The task retrieves the most relevant geo-referenced image from a large multi-platform corpus (satellite&#x2F;drone&#x2F;ground) given a natural-language query. Two obstacles are severe inter-platform heterogeneity and a domain gap between generic training descriptions and platform-specific test queries. We mitigate these with a domain-aligned preprocessing pipeline and a Mixture-of-Experts (MoE) framework: (i) platform-wise partitioning, satellite augmentation, and removal of orientation words; (ii) an LLM-based caption refinement pipeline to align textual semantics with the distinct visual characteristics of each platform. Using BGE-M3 (text) and EVA-CLIP (image), we train three platform experts using a progressive two-stage, hard-negative mining strategy to enhance discriminative power, and fuse their scores at inference. The system tops the official leaderboard, demonstrating robust cross-modal geo-localization under heterogeneous viewpoints.</p>
</details>


<h3 id="68-HyperET-Efficient-Training-in-Hyperbolic-Space-for-Multi-modal-Large-Language-Models"><a href="#68-HyperET-Efficient-Training-in-Hyperbolic-Space-for-Multi-modal-Large-Language-Models" class="headerlink" title="[68] HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20322">HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</a></h3><p><em>Zelin Peng,Zhengqin Xu,Qingyang Liu,Xiaokang Yang,Wei Shen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: HyperET提出了一种在双曲空间中高效训练多模态大语言模型（MLLMs）的方法，通过动态调整双曲半径实现视觉与文本模态的多粒度对齐，显著减少了计算资源需求。</p>
<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）训练需要大量计算资源，主要原因在于视觉编码器（如CLIP和SAM）缺乏与语言的多粒度对齐能力。HyperET旨在通过双曲空间解决这一问题。

<p>Contribution: 提出HyperET，一种基于双曲空间的高效训练范式，动态调整双曲半径以实现视觉与文本的多粒度对齐，并引入可学习矩阵和M&quot;{o}bius乘法操作优化参数化策略。</p>
<p>Method: 利用双曲空间的层级建模能力，通过动态调整双曲半径和可学习矩阵（对角缩放、块对角、带状矩阵）实现高效参数化，提升MLLMs的训练效率。</p>
<p>Result: 实验表明，HyperET在多个MLLM基准测试中显著提升了现有预训练和微调模型的性能，仅增加不到1%的参数。</p>
<p>Insight: 双曲空间为多粒度跨模态对齐提供了自然框架，动态参数化策略兼顾灵活性与效率，为MLLMs的高效训练提供了新思路。</p>
<p>Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with M&quot;{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1% additional parameters.</p>
</details>


<h3 id="69-Positional-Encoding-Field"><a href="#69-Positional-Encoding-Field" class="headerlink" title="[69] Positional Encoding Field"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20385">Positional Encoding Field</a></h3><p><em>Yunpeng Bai,Haoxiang Li,Qixing Huang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文研究了扩散Transformer（DiTs）中patch token的独立性，并提出了一种新的位置编码场（PE-Field），将其从2D平面扩展到结构化3D场，以实现更好的几何建模和控制。</p>
<details>
  <summary>Details</summary>
Motivation: 研究发现，DiTs中的patch token表现出惊人的独立性，即使位置编码（PEs）被扰动，DiTs仍能生成全局一致的输出。这表明空间一致性主要由PEs控制，因此作者提出了PE-Field以更好地建模3D空间。

<p>Contribution: 主要贡献是提出了一种新的位置编码场（PE-Field），将位置编码扩展到3D结构化场，增强了DiTs在3D空间中的几何建模能力，并支持细粒度的子patch控制。</p>
<p>Method: PE-Field通过引入深度感知编码和层次编码，将DiTs中的2D位置编码扩展到3D场，从而实现对体积推理和细粒度控制的支持。</p>
<p>Result: 实验结果展示了PE-Field增强的DiT在单图像新视角合成任务上达到了最优性能，并能推广到可控的空间图像编辑任务。</p>
<p>Insight: 研究表明，DiTs的空间一致性主要依赖于位置编码，而非patch token之间的依赖性，这表明改进位置编码可能是提升视觉生成任务的关键。</p>
<p>Abstract: Diffusion Transformers (DiTs) have emerged as the dominant architecture for visual generation, powering state-of-the-art image and video models. By representing images as patch tokens with positional encodings (PEs), DiTs combine Transformer scalability with spatial and temporal inductive biases. In this work, we revisit how DiTs organize visual content and discover that patch tokens exhibit a surprising degree of independence: even when PEs are perturbed, DiTs still produce globally coherent outputs, indicating that spatial coherence is primarily governed by PEs. Motivated by this finding, we introduce the Positional Encoding Field (PE-Field), which extends positional encodings from the 2D plane to a structured 3D field. PE-Field incorporates depth-aware encodings for volumetric reasoning and hierarchical encodings for fine-grained sub-patch control, enabling DiTs to model geometry directly in 3D space. Our PE-Field-augmented DiT achieves state-of-the-art performance on single-image novel view synthesis and generalizes to controllable spatial image editing.</p>
</details>


<h3 id="70-Dynamic-Weight-Adjustment-for-Knowledge-Distillation-Leveraging-Vision-Transformer-for-High-Accuracy-Lung-Cancer-Detection-and-Real-Time-Deployment"><a href="#70-Dynamic-Weight-Adjustment-for-Knowledge-Distillation-Leveraging-Vision-Transformer-for-High-Accuracy-Lung-Cancer-Detection-and-Real-Time-Deployment" class="headerlink" title="[70] Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20438">Dynamic Weight Adjustment for Knowledge Distillation: Leveraging Vision Transformer for High-Accuracy Lung Cancer Detection and Real-Time Deployment</a></h3><p><em>Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种名为FuzzyDistillViT-MobileNet的新方法，用于肺癌分类，通过动态模糊逻辑驱动的知识蒸馏（KD）处理诊断中的不确定性和复杂性。</p>
<details>
  <summary>Details</summary>
Motivation: 传统静态知识蒸馏方法的权重固定，无法灵活处理肺癌图像中不同区域的不确定性。为解决这一问题，研究者提出了一种动态调整蒸馏权重的方法。

<p>Contribution: 1. 提出了动态模糊逻辑驱动的KD方法；2. 采用ViT-B32作为教师模型，MobileNet作为学生模型；3. 引入像素级图像融合改进技术；4. 使用遗传算法优化学生模型选择。</p>
<p>Method: 1. 使用模糊逻辑动态调整KD权重；2. 采用小波融合方法提升图像质量；3. 通过遗传算法选择最优学生模型；4. 优化训练过程以提高收敛性和性能。</p>
<p>Result: 模型在两个数据集上表现出色：LC25000组织病理图像（99.16%准确率）和IQOTH&#x2F;NCCD CT扫描图像（99.54%准确率）。</p>
<p>Insight: 动态权重调整和模糊逻辑的结合能显著提升模型对不确定区域的适应能力，同时图像融合技术有助于提升特征提取的效率。</p>
<p>Abstract: This paper presents the FuzzyDistillViT-MobileNet model, a novel approach for lung cancer (LC) classification, leveraging dynamic fuzzy logic-driven knowledge distillation (KD) to address uncertainty and complexity in disease diagnosis. Unlike traditional models that rely on static KD with fixed weights, our method dynamically adjusts the distillation weight using fuzzy logic, enabling the student model to focus on high-confidence regions while reducing attention to ambiguous areas. This dynamic adjustment improves the model ability to handle varying uncertainty levels across different regions of LC images. We employ the Vision Transformer (ViT-B32) as the instructor model, which effectively transfers knowledge to the student model, MobileNet, enhancing the student generalization capabilities. The training process is further optimized using a dynamic wait adjustment mechanism that adapts the training procedure for improved convergence and performance. To enhance image quality, we introduce pixel-level image fusion improvement techniques such as Gamma correction and Histogram Equalization. The processed images (Pix1 and Pix2) are fused using a wavelet-based fusion method to improve image resolution and feature preservation. This fusion method uses the wavedec2 function to standardize images to a 224x224 resolution, decompose them into multi-scale frequency components, and recursively average coefficients at each level for better feature representation. To address computational efficiency, Genetic Algorithm (GA) is used to select the most suitable pre-trained student model from a pool of 12 candidates, balancing model performance with computational cost. The model is evaluated on two datasets, including LC25000 histopathological images (99.16% accuracy) and IQOTH&#x2F;NCCD CT-scan images (99.54% accuracy), demonstrating robustness across different imaging domains.</p>
</details>


<h3 id="71-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence"><a href="#71-Conan-Progressive-Learning-to-Reason-Like-a-Detective-over-Multi-Scale-Visual-Evidence" class="headerlink" title="[71] Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20470">Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</a></h3><p><em>Kun Ouyang,Yuanxin Liu,Linli Yao,Yishuo Cai,Hao Zhou,Jie Zhou,Fandong Meng,Xu Sun</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Conan是一个用于视频推理的框架，通过多阶段渐进式学习和强化学习，结合视觉证据生成多步推理。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的视频推理方法要么依赖文本链导致虚假结论，要么基于帧检索但定位不准确，因此需要一种新的方法来结合视觉证据和多步推理。

<p>Contribution: 1) 构建了Conan-91K数据集，包含自动生成的多步推理轨迹；2) 设计了多阶段渐进式冷启动策略和AIR RLVR训练框架，提升了多步视觉推理能力。</p>
<p>Method: Conan通过识别上下文和证据帧，进行跨帧推理，并自适应决定何时结束或继续探索。采用了多阶段渐进式学习和AIR RLVR训练框架。</p>
<p>Result: 在六个多步推理基准测试中，Conan平均准确率超过基线Qwen2.5-VL-7B-Instruct 10%以上，实现了最优性能。</p>
<p>Insight: Conan展示了在长视频理解任务中的强扩展性和鲁棒性，表明其方法可推广到更复杂的场景。</p>
<p>Abstract: Video reasoning, which requires multi-step deduction across frames, remains a major challenge for multimodal large language models (MLLMs). While reinforcement learning (RL)-based methods enhance reasoning capabilities, they often rely on text-only chains that yield ungrounded or hallucinated conclusions. Conversely, frame-retrieval approaches introduce visual grounding but still struggle with inaccurate evidence localization. To address these challenges, we present Conan, a framework for evidence-grounded multi-step video reasoning. Conan identifies contextual and evidence frames, reasons over cross-frame clues, and adaptively decides when to conclude or explore further. To achieve this, we (1) construct Conan-91K, a large-scale dataset of automatically generated reasoning traces that includes frame identification, evidence reasoning, and action decision, and (2) design a multi-stage progressive cold-start strategy combined with an Identification-Reasoning-Action (AIR) RLVR training framework to jointly enhance multi-step visual reasoning. Extensive experiments on six multi-step reasoning benchmarks demonstrate that Conan surpasses the baseline Qwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achieving state-of-the-art performance. Furthermore, Conan generalizes effectively to long-video understanding tasks, validating its strong scalability and robustness.</p>
</details>


<h3 id="72-Metis-HOME-Hybrid-Optimized-Mixture-of-Experts-for-Multimodal-Reasoning"><a href="#72-Metis-HOME-Hybrid-Optimized-Mixture-of-Experts-for-Multimodal-Reasoning" class="headerlink" title="[72] Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20519">Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</a></h3><p><em>Xiaohan Lan,Fanfan Liu,Haibo Qiu,Siqi Yang,Delian Ruan,Peng Shi,Lin Ma</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了Metis-HOME框架，通过混合优化的专家混合（MoE）架构解决多模态推理模型在复杂推理和泛化能力之间的权衡问题。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型推理模型在简单查询上也采用计算密集型推理，效率低下；同时，专注于专门推理削弱了其泛化能力。

<p>Contribution: 提出了Metis-HOME框架，引入’混合思维’范式，通过动态路由将任务分配给专门优化的分支（复杂推理分支和快速推理分支）。</p>
<p>Method: 将原始密集模型重构为两个专家分支：复杂推理分支和快速推理分支，并通过轻量级可训练路由器动态分配任务。采用了Qwen2.5-VL-7B作为基础模型。</p>
<p>Result: Metis-HOME显著提升了复杂推理能力，同时改善了模型的泛化性能，解决了现有推理模型的性能退化问题。</p>
<p>Insight: 通过动态路由和分支优化，可以实现高效的多模态推理和泛化能力的平衡，为构建强大且通用的MLLM提供了新范式。</p>
<p>Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ‘’Hybrid Thinking’’ paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model’s general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma.</p>
</details>


<h3 id="73-Fake-in-Facext-Towards-Fine-Grained-Explainable-DeepFake-Analysis"><a href="#73-Fake-in-Facext-Towards-Fine-Grained-Explainable-DeepFake-Analysis" class="headerlink" title="[73] Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20531">Fake-in-Facext: Towards Fine-Grained Explainable DeepFake Analysis</a></h3><p><em>Lixiong Qin,Yang Zhang,Mei Wang,Jiani Hu,Weihong Deng,Weiran Xu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了Fake-in-Facext（FiFa）框架，通过细粒度的面部区域划分和新型任务Artifact-Grounding Explanation（AGE），提升了DeepFake分析的可靠性和可解释性，并设计了统一的多任务学习架构FiFa-MLLM。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）在细粒度DeepFake分析中存在不足，例如数据标注不可靠、解释与视觉证据缺乏关联，以及不支持任意面部区域的查询。FiFa旨在解决这些问题。

<p>Contribution: 主要贡献包括：1. 定义了Facial Image Concept Tree（FICT）分割面部图像，提供可靠的数据标注流程FiFa-Annotator；2. 提出了AGE任务，生成与篡改区域分割掩码交织的文本解释；3. 设计了统一的多任务架构FiFa-MLLM。</p>
<p>Method: FiFa-MLLM是一个多任务学习架构，支持丰富的多模态输入输出，包括文本解释和分割掩码，并通过辅助监督任务提升性能。</p>
<p>Result: FiFa-MLLM在AGE任务上优于基线方法，并在现有XDFA数据集上实现了SOTA性能。</p>
<p>Insight: 通过细粒度面部区域划分和任务设计，FiFa展示了如何结合视觉和语言任务提升DeepFake分析的可靠性与可解释性。</p>
<p>Abstract: The advancement of Multimodal Large Language Models (MLLMs) has bridged the gap between vision and language tasks, enabling the implementation of Explainable DeepFake Analysis (XDFA). However, current methods suffer from a lack of fine-grained awareness: the description of artifacts in data annotation is unreliable and coarse-grained, and the models fail to support the output of connections between textual forgery explanations and the visual evidence of artifacts, as well as the input of queries for arbitrary facial regions. As a result, their responses are not sufficiently grounded in Face Visual Context (Facext). To address this limitation, we propose the Fake-in-Facext (FiFa) framework, with contributions focusing on data annotation and model construction. We first define a Facial Image Concept Tree (FICT) to divide facial images into fine-grained regional concepts, thereby obtaining a more reliable data annotation pipeline, FiFa-Annotator, for forgery explanation. Based on this dedicated data annotation, we introduce a novel Artifact-Grounding Explanation (AGE) task, which generates textual forgery explanations interleaved with segmentation masks of manipulated artifacts. We propose a unified multi-task learning architecture, FiFa-MLLM, to simultaneously support abundant multimodal inputs and outputs for fine-grained Explainable DeepFake Analysis. With multiple auxiliary supervision tasks, FiFa-MLLM can outperform strong baselines on the AGE task and achieve SOTA performance on existing XDFA datasets. The code and data will be made open-source at <a target="_blank" rel="noopener" href="https://github.com/lxq1000/Fake-in-Facext">https://github.com/lxq1000/Fake-in-Facext</a>.</p>
</details>


<h3 id="74-Blur2seq-Blind-Deblurring-and-Camera-Trajectory-Estimation-from-a-Single-Camera-Motion-blurred-Image"><a href="#74-Blur2seq-Blind-Deblurring-and-Camera-Trajectory-Estimation-from-a-Single-Camera-Motion-blurred-Image" class="headerlink" title="[74] Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20539">Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image</a></h3><p><em>Guillermo Carbajal,Andrés Almansa,Pablo Musé</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了Blur2Seq，一种从单张运动模糊图像中联合估计潜在清晰图像和相机运动轨迹的深度学习框架。通过可微模糊生成模块和端到端训练，实现了高性能的去模糊和轨迹估计。</p>
<details>
  <summary>Details</summary>
Motivation: 相机抖动导致的运动模糊（尤其是大幅或旋转运动）是图像恢复的挑战。现有端到端方法在严重或空间变化模糊下表现不佳，需要更有效且可解释的方法。

<p>Contribution: 1. 联合估计清晰图像和相机运动轨迹；2. 通过可微模糊模块实现端到端训练；3. 后优化轨迹提升了结果一致性。</p>
<p>Method: 使用PMBM模型和神经网络预测3D旋转轨迹，结合模型驱动的恢复网络端到端训练。后优化通过reblur损失进一步提升结果。</p>
<p>Result: 在合成和真实数据集上表现最优，尤其针对严重或空间变化模糊的情况。</p>
<p>Insight: 轨迹估计提供了模糊成因的可解释性，同时支持生成模糊图像对应的清晰图像序列。</p>
<p>Abstract: Motion blur caused by camera shake, particularly under large or rotational movements, remains a major challenge in image restoration. We propose a deep learning framework that jointly estimates the latent sharp image and the underlying camera motion trajectory from a single blurry image. Our method leverages the Projective Motion Blur Model (PMBM), implemented efficiently using a differentiable blur creation module compatible with modern networks. A neural network predicts a full 3D rotation trajectory, which guides a model-based restoration network trained end-to-end. This modular architecture provides interpretability by revealing the camera motion that produced the blur. Moreover, this trajectory enables the reconstruction of the sequence of sharp images that generated the observed blurry image. To further refine results, we optimize the trajectory post-inference via a reblur loss, improving consistency between the blurry input and the restored output. Extensive experiments show that our method achieves state-of-the-art performance on both synthetic and real datasets, particularly in cases with severe or spatially variant blur, where end-to-end deblurring networks struggle.   Code and trained models are available at <a target="_blank" rel="noopener" href="https://github.com/GuillermoCarbajal/Blur2Seq/">https://github.com/GuillermoCarbajal/Blur2Seq/</a></p>
</details>


<h3 id="75-Deep-Learning-Powered-Visual-SLAM-Aimed-at-Assisting-Visually-Impaired-Navigation"><a href="#75-Deep-Learning-Powered-Visual-SLAM-Aimed-at-Assisting-Visually-Impaired-Navigation" class="headerlink" title="[75] Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20549">Deep Learning-Powered Visual SLAM Aimed at Assisting Visually Impaired Navigation</a></h3><p><em>Marziyeh Bamdad,Hans-Peter Hutter,Alireza Darvishy</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种基于深度学习的视觉SLAM框架SELM-SLAM3，通过集成SuperPoint和LightGlue技术优化特征提取与匹配，在低纹理、运动模糊等挑战性条件下表现优于传统ORB-SLAM3和其他先进RGB-D SLAM系统。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管SLAM技术有所进步，但在低纹理、运动模糊或复杂光照等挑战性条件下的鲁棒性仍是一个开放性问题。这些问题在视觉辅助导航中尤为突出，影响定位精度和跟踪稳定性，进而降低导航的可靠性和安全性。

<p>Contribution: 论文的主要贡献是提出了SELM-SLAM3框架，通过深度学习方法（SuperPoint和LightGlue）增强了特征提取和匹配的鲁棒性，提高了SLAM在挑战性条件下的性能。</p>
<p>Method: SELM-SLAM3结合了SuperPoint（用于特征提取）和LightGlue（用于特征匹配），优化了视觉SLAM框架。在TUM RGB-D、ICL-NUIM和TartanAir等多样性数据集上进行了测试。</p>
<p>Result: 实验结果表明，SELM-SLAM3在低纹理和快速运动等挑战性场景下表现优越，平均性能超过ORB-SLAM3 87.84%，并优于最先进的RGB-D SLAM系统36.77%。</p>
<p>Insight: 深度学习方法可以显著提升视觉SLAM在复杂环境中的鲁棒性，尤其是特征提取和匹配环节。这表明结合深度学习和传统SLAM技术是实现可靠导航系统的有效途径。</p>
<p>Abstract: Despite advancements in SLAM technologies, robust operation under challenging conditions such as low-texture, motion-blur, or challenging lighting remains an open challenge. Such conditions are common in applications such as assistive navigation for the visually impaired. These challenges undermine localization accuracy and tracking stability, reducing navigation reliability and safety. To overcome these limitations, we present SELM-SLAM3, a deep learning-enhanced visual SLAM framework that integrates SuperPoint and LightGlue for robust feature extraction and matching. We evaluated our framework using TUM RGB-D, ICL-NUIM, and TartanAir datasets, which feature diverse and challenging scenarios. SELM-SLAM3 outperforms conventional ORB-SLAM3 by an average of 87.84% and exceeds state-of-the-art RGB-D SLAM systems by 36.77%. Our framework demonstrates enhanced performance under challenging conditions, such as low-texture scenes and fast motion, providing a reliable platform for developing navigation aids for the visually impaired.</p>
</details>


<h3 id="76-From-Cheap-to-Pro-A-Learning-based-Adaptive-Camera-Parameter-Network-for-Professional-Style-Imaging"><a href="#76-From-Cheap-to-Pro-A-Learning-based-Adaptive-Camera-Parameter-Network-for-Professional-Style-Imaging" class="headerlink" title="[76] From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20550">From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging</a></h3><p><em>Fuchen Li,Yansong Du,Wenbo Cheng,Xiaoxia Zhou,Sen Yin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种轻量级、场景自适应的相机参数调整网络ACamera-Net，直接从RAW输入预测最佳曝光和白平衡，解决了消费级相机在复杂光照条件下图像质量不稳定的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 消费级相机在复杂光照条件下（如低光、高动态范围、背光等）往往表现不佳，导致图像质量下降，影响下游视觉任务的性能。

<p>Contribution: 1. 提出了ACamera-Net框架，包含ACamera-Exposure和ACamera-Color两个模块，分别优化曝光和白平衡。2. 框架轻量且支持实时推理，可直接集成到成像流程中。3. 在多样化真实数据上训练，泛化能力强。</p>
<p>Method: 1. ACamera-Exposure模块预测ISO以缓解曝光不足和对比度损失。2. ACamera-Color模块预测相关色温和增益因子以提高色彩一致性。3. 训练数据为带标注的真实场景RAW图像。</p>
<p>Result: 实验表明，ACamera-Net显著提升了图像质量，稳定了感知输出，优于传统自动模式和轻量级基线方法。</p>
<p>Insight: 直接从RAW数据学习相机参数调整是一种高效且通用的方法，可显著改善图像质量和下游任务性能。</p>
<p>Abstract: Consumer-grade camera systems often struggle to maintain stable image quality under complex illumination conditions such as low light, high dynamic range, and backlighting, as well as spatial color temperature variation. These issues lead to underexposure, color casts, and tonal inconsistency, which degrade the performance of downstream vision tasks. To address this, we propose ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment network that directly predicts optimal exposure and white balance from RAW inputs. The framework consists of two modules: ACamera-Exposure, which estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color, which predicts correlated color temperature and gain factors for improved color consistency. Optimized for real-time inference on edge devices, ACamera-Net can be seamlessly integrated into imaging pipelines. Trained on diverse real-world data with annotated references, the model generalizes well across lighting conditions. Extensive experiments demonstrate that ACamera-Net consistently enhances image quality and stabilizes perception outputs, outperforming conventional auto modes and lightweight baselines without relying on additional image enhancement modules.</p>
</details>


<h3 id="77-EmbodiedBrain-Expanding-Performance-Boundaries-of-Task-Planning-for-Embodied-Intelligence"><a href="#77-EmbodiedBrain-Expanding-Performance-Boundaries-of-Task-Planning-for-Embodied-Intelligence" class="headerlink" title="[77] EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20578">EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</a></h3><p><em>Ding Zou,Feifan Wang,Mengyu Ge,Siyuan Fan,Zongbing Zhang,Wei Chen,Lingfeng Wang,Zhongyou Hu,Wenrui Yan,Zhengwei Gao,Hao Wang,Weizhao Jin,Yu Zhang,Hainan Zhao,Mingliang Zhang,Xianxian Xi,Yaru Zhang,Wenyuan Li,Zhengguang Gao,Yurui Zhu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: EmbodiedBrain是一个新型的视觉语言基础模型，旨在解决当前基于LLM和MLLM的具身智能体在任务规划中的局限性，通过创新的训练方法和评估系统，实现了卓越的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型和多模态语言模型在具身任务中存在设计差距、实时性和性能的权衡问题，以及离线评估的局限性，这些问题阻碍了AGI的实现。

<p>Contribution: 提出了EmbodiedBrain，一个兼具7B和32B参数规模的视觉语言基础模型；设计了代理对齐的数据结构和创新的训练方法（SFT与Step-GRPO结合）；引入了综合奖励系统和新的评估基准。</p>
<p>Method: 采用大规模监督微调（SFT）与Step-GRPO结合的强大训练方法，利用前置步骤作为引导；引入生成奖励模型（GRM）提升训练效率；建立了三部分评估系统。</p>
<p>Result: 实验表明，EmbodiedBrain在所有指标上均表现出色，为具身基础模型设定了新的SOTA。</p>
<p>Insight: 通过创新的训练方法和评估系统，可以有效提升具身智能体的任务规划和执行能力；开源数据和模型为推动具身智能领域的发展提供了重要资源。</p>
<p>Abstract: The realization of Artificial General Intelligence (AGI) necessitates Embodied AI agents capable of robust spatial perception, effective task planning, and adaptive execution in physical environments. However, current large language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks suffer from key limitations, including a significant gap between model design and agent requirements, an unavoidable trade-off between real-time latency and performance, and the use of unauthentic, offline evaluation metrics. To address these challenges, we propose EmbodiedBrain, a novel vision-language foundation model available in both 7B and 32B parameter sizes. Our framework features an agent-aligned data structure and employs a powerful training methodology that integrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group Relative Policy Optimization (Step-GRPO), which boosts long-horizon task success by integrating preceding steps as Guided Precursors. Furthermore, we incorporate a comprehensive reward system, including a Generative Reward Model (GRM) accelerated at the infrastructure level, to improve training efficiency. For enable thorough validation, we establish a three-part evaluation system encompassing General, Planning, and End-to-End Simulation Benchmarks, highlighted by the proposal and open-sourcing of a novel, challenging simulation environment. Experimental results demonstrate that EmbodiedBrain achieves superior performance across all metrics, establishing a new state-of-the-art for embodied foundation models. Towards paving the way for the next generation of generalist embodied agents, we open-source all of our data, model weight, and evaluating methods, which are available at <a target="_blank" rel="noopener" href="https://zterobot.github.io/EmbodiedBrain.github.io">https://zterobot.github.io/EmbodiedBrain.github.io</a>.</p>
</details>


<h3 id="78-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence"><a href="#78-Open-o3-Video-Grounded-Video-Reasoning-with-Explicit-Spatio-Temporal-Evidence" class="headerlink" title="[78] Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20579">Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</a></h3><p><em>Jiahao Meng,Xiangtai Li,Haochen Wang,Yue Tan,Tao Zhang,Lingdong Kong,Yunhai Tong,Anran Wang,Zhiyang Teng,Yujing Wang,Zhuochen Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Open-o3 Video 是一个非代理框架，通过在视频推理中引入显式的时空证据，实现了对动态场景的时空联合跟踪和定位。通过精心构建的数据集和训练策略，该模型在多个视频理解基准测试中取得了最先进的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的视频推理模型通常只生成文本推理轨迹，而无法指出关键证据出现的时间和地点。OpenAI-o3等模型在图像推理中引入了证据中心化的能力，但在视频中扩展这一功能更具挑战性，需要同时处理动态场景的时空跟踪和定位问题。

<p>Contribution: 1. 提出了Open-o3 Video框架，首次将显式时空证据引入视频推理；2. 构建了两个高质量数据集STGR-CoT-30k和STGR-RL-36k，提供统一的时空监督和推理轨迹；3. 提出了冷启动强化学习策略，通过多任务奖励联合优化答案准确性、时间对齐和空间精度。</p>
<p>Method: 1. 数据集构建：精心注释了时空信息（时间戳、对象和边界框）；2. 训练策略：采用冷启动强化学习，设计多种奖励函数；3. 推理框架：生成显式推理轨迹，支持置信度感知验证。</p>
<p>Result: 在V-STAR基准测试中，mAM指标提升14.4%，mLGM提升24.2%；在VideoMME、WorldSense、VideoMMMU等基准测试中均表现出色。推理轨迹还为测试时提供了可扩展的信号。</p>
<p>Insight: 1. 统一的时空监督对视频推理至关重要；2. 多任务奖励的强化学习策略能有效平衡准确性、时间和空间目标；3. 显式推理轨迹不仅提升性能，还为模型的可信度提供了直接支持。</p>
<p>Abstract: Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</p>
</details>


<h3 id="79-GenColorBench-A-Color-Evaluation-Benchmark-for-Text-to-Image-Generation-Models"><a href="#79-GenColorBench-A-Color-Evaluation-Benchmark-for-Text-to-Image-Generation-Models" class="headerlink" title="[79] GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20586">GenColorBench: A Color Evaluation Benchmark for Text-to-Image Generation Models</a></h3><p><em>Muhammad Atif Butt,Alexandra Gomez-Villa,Tao Wu,Javier Vazquez-Corral,Joost Van De Weijer,Kai Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: GenColorBench是首个系统地评估文本到图像生成模型颜色控制能力的基准，包含44K个颜色相关提示，覆盖400多种颜色，揭示了主流模型在颜色生成上的能力和局限性。</p>
<details>
  <summary>Details</summary>
Motivation: 目前文本到图像生成模型的颜色控制能力较弱，缺乏系统性评估。颜色是人类视觉和沟通的核心，在许多应用中至关重要。

<p>Contribution: 提出了首个全面的颜色生成评估基准GenColorBench，基于ISCC-NBS和CSS3&#x2F;X11颜色系统，填补了现有基准的空白。</p>
<p>Method: 构建了44K个颜色相关提示，采用感知和自动化评估方法对主流模型进行了系统性测试。</p>
<p>Result: 评估显示模型的颜色生成能力存在显著差异，揭示了模型对不同颜色约定的理解程度及其失败模式。</p>
<p>Insight: GenColorBench将为提升模型的颜色生成精确性提供指导，推动文本到图像生成技术的发展。</p>
<p>Abstract: Recent years have seen impressive advances in text-to-image generation, with image generative or unified models producing high-quality images from text. Yet these models still struggle with fine-grained color controllability, often failing to accurately match colors specified in text prompts. While existing benchmarks evaluate compositional reasoning and prompt adherence, none systematically assess color precision. Color is fundamental to human visual perception and communication, critical for applications from art to design workflows requiring brand consistency. However, current benchmarks either neglect color or rely on coarse assessments, missing key capabilities such as interpreting RGB values or aligning with human expectations. To this end, we propose GenColorBench, the first comprehensive benchmark for text-to-image color generation, grounded in color systems like ISCC-NBS and CSS3&#x2F;X11, including numerical colors which are absent elsewhere. With 44K color-focused prompts covering 400+ colors, it reveals models’ true capabilities via perceptual and automated assessments. Evaluations of popular text-to-image models using GenColorBench show performance variations, highlighting which color conventions models understand best and identifying failure modes. Our GenColorBench assessments will guide improvements in precise color generation. The benchmark will be made public upon acceptance.</p>
</details>


<h3 id="80-Unsupervised-Domain-Adaptation-via-Similarity-based-Prototypes-for-Cross-Modality-Segmentation"><a href="#80-Unsupervised-Domain-Adaptation-via-Similarity-based-Prototypes-for-Cross-Modality-Segmentation" class="headerlink" title="[80] Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20596">Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</a></h3><p><em>Ziyu Ye,Chen Ju,Chaofan Ma,Xiaoyun Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种基于相似性原型的无监督域适应框架，用于跨模态分割，通过学习类别原型并引入相似性约束，解决了域适应中的类缺失问题，并通过对比学习进一步提升性能。</p>
<details>
  <summary>Details</summary>
Motivation: 深度模型在训练数据上表现优异，但在未见数据上性能骤降。为解决域偏移问题并避免昂贵的未见域标注，论文提出了无监督域适应方法。

<p>Contribution: 1. 提出相似性原型的框架，用于跨模态分割。2. 引入了相似性约束，确保原型对语义类别具有代表性且可分。3. 使用字典存储原型，解决了类缺失问题并支持对比学习。</p>
<p>Method: 1. 在嵌入空间中学习类别原型。2. 引入相似性约束优化原型。3. 使用字典存储原型以支持对比学习。</p>
<p>Result: 实验表明，该方法在跨模态分割任务上优于其他先进方法。</p>
<p>Insight: 通过类别原型和对比学习结合，可以有效缓解域适应中的类缺失问题，提升模型在未见域的泛化能力。</p>
<p>Abstract: Deep learning models have achieved great success on various vision challenges, but a well-trained model would face drastic performance degradation when applied to unseen data. Since the model is sensitive to domain shift, unsupervised domain adaptation attempts to reduce the domain gap and avoid costly annotation of unseen domains. This paper proposes a novel framework for cross-modality segmentation via similarity-based prototypes. In specific, we learn class-wise prototypes within an embedding space, then introduce a similarity constraint to make these prototypes representative for each semantic class while separable from different classes. Moreover, we use dictionaries to store prototypes extracted from different images, which prevents the class-missing problem and enables the contrastive learning of prototypes, and further improves performance. Extensive experiments show that our method achieves better results than other state-of-the-art methods.</p>
</details>


<h3 id="81-OnlineSplatter-Pose-Free-Online-3D-Reconstruction-for-Free-Moving-Objects"><a href="#81-OnlineSplatter-Pose-Free-Online-3D-Reconstruction-for-Free-Moving-Objects" class="headerlink" title="[81] OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20605">OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects</a></h3><p><em>Mark He Huang,Lin Geng Foo,Christian Theobalt,Ying Sun,De Wen Soh</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: OnlineSplatter是一个新颖的在线前馈框架，直接从RGB帧生成高质量的3D高斯表示，无需相机位姿、深度先验或捆绑优化，适用于自由移动物体的在线3D重建。</p>
<details>
  <summary>Details</summary>
Motivation: 自由移动物体的单目视频3D重建面临挑战，特别是在缺乏可靠位姿或深度线索以及物体运动任意的情况下。现有方法需要额外的位姿或深度信息，限制了广泛应用。

<p>Contribution: 提出了OnlineSplatter框架，通过双键记忆模块和密集高斯基元场，实现了无需相机位姿或深度先验的在线3D重建，计算成本与视频序列长度无关。</p>
<p>Method: 框架以第一帧为锚点，通过密集高斯基元场逐步优化物体表示；设计双键记忆模块（隐式外观几何键与显式方向键），结合空间引导记忆读出和稀疏化机制，确保紧凑高效的物体覆盖。</p>
<p>Result: 在真实数据集上评估，OnlineSplatter显著优于现有无位姿重建基线，且随着观测增多持续改进，同时保持恒定的内存和运行时开销。</p>
<p>Insight: 通过空间引导的记忆设计和稀疏化机制，可以有效处理自由移动物体的复杂运动，同时保持重建质量和计算效率的平衡。</p>
<p>Abstract: Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime.</p>
</details>


<h3 id="82-SeViCES-Unifying-Semantic-Visual-Evidence-Consensus-for-Long-Video-Understanding"><a href="#82-SeViCES-Unifying-Semantic-Visual-Evidence-Consensus-for-Long-Video-Understanding" class="headerlink" title="[82] SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20622">SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</a></h3><p><em>Yuan Sheng,Yanbin Hao,Chenxu Li,Shuo Wang,Xiangnan He</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了SeViCES框架，通过语义-视觉一致性证据选择提升长视频理解能力，其训练无关且模型无关，核心是两个模块：SVCFS（语义-视觉帧选择）和ACR（答案一致性优化）。</p>
<details>
  <summary>Details</summary>
Motivation: 长视频内容复杂且时间分散，现有方法难以高效处理。尽管视频大语言模型（Video-LLMs）能处理较长视频，但对真正长序列的计算负担大且推理不一致。需要一种方法选择最有信息量的帧并提供完整的查询相关上下文。

<p>Contribution: 1. 提出SeViCES框架，结合语义和视觉模态的证据选择提升长视频理解；2. 设计SVCFS模块，通过语义分支和视觉分支选择高质量帧；3. 引入ACR模块，融合语义和视觉预测并约束答案空间。</p>
<p>Method: 1. SVCFS模块：包括基于LLM推理的语义分支和聚类引导的视觉分支，通过互信息对齐语义分数和视觉嵌入；2. ACR模块：通过证据融合和答案空间约束优化语义与视觉预测的一致性。</p>
<p>Result: 在长视频理解基准测试中，SeViCES在准确性和鲁棒性上均优于当前最优方法，证明了其有效性。</p>
<p>Insight: 语义和视觉证据的共识选择是提升视频大语言模型推理能力的关键，多模态融合和答案空间约束能显著提高长视频理解的可靠性。</p>
<p>Abstract: Long video understanding remains challenging due to its complex, diverse, and temporally scattered content. Although video large language models (Video-LLMs) can process videos lasting tens of minutes, applying them to truly long sequences is computationally prohibitive and often leads to unfocused or inconsistent reasoning. A promising solution is to select only the most informative frames, yet existing approaches typically ignore temporal dependencies or rely on unimodal evidence, limiting their ability to provide complete and query-relevant context. We propose a Semantic-Visual Consensus Evidence Selection (SeViCES) framework for effective and reliable long video understanding. SeViCES is training-free and model-agnostic, and introduces two key components. The Semantic-Visual Consensus Frame Selection (SVCFS) module selects frames through (1) a temporal-aware semantic branch that leverages LLM reasoning over captions, and (2) a cluster-guided visual branch that aligns embeddings with semantic scores via mutual information. The Answer Consensus Refinement (ACR) module further resolves inconsistencies between semantic- and visual-based predictions by fusing evidence and constraining the answer space. Extensive experiments on long video understanding benchmarks show that SeViCES consistently outperforms state-of-the-art methods in both accuracy and robustness, demonstrating the importance of consensus-driven evidence selection for Video-LLMs.</p>
</details>


<h3 id="83-Deep-Learning-in-Dental-Image-Analysis-A-Systematic-Review-of-Datasets-Methodologies-and-Emerging-Challenges"><a href="#83-Deep-Learning-in-Dental-Image-Analysis-A-Systematic-Review-of-Datasets-Methodologies-and-Emerging-Challenges" class="headerlink" title="[83] Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20634">Deep Learning in Dental Image Analysis: A Systematic Review of Datasets, Methodologies, and Emerging Challenges</a></h3><p><em>Zhenhuan Zhou,Jingbo Zhu,Yuchen Zhang,Xiaohang Guan,Peng Wang,Tao Li</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 这篇系统综述总结了深度学习在牙科图像分析中的应用，涵盖260项研究，详细探讨了公开数据集和算法。文章介绍了牙科影像的基础概念、数据集特性、深度学习技术及其在各项任务中的应用，并分析了当前研究的挑战和未来方向。</p>
<details>
  <summary>Details</summary>
Motivation: 牙科影像分析面临低对比度、金属伪影和投影角度变化等挑战，人工解读耗时且不一致。基于AI的自动分析技术，尤其是深度学习，因其优异的特征提取能力成为解决方案。

<p>Contribution: 1. 系统综述了260项深度学习在牙科图像分析中的研究；2. 总结了49个公开数据集和211种算法；3. 提供了数据集特性、深度学习技术和任务分类的详细分析；4. 讨论了当前挑战和未来研究方向。</p>
<p>Method: 文章通过系统综述分析了260篇论文，分类数据集和算法，总结了深度学习的基础技术和应用方法，并提出了评价指标和性能分析。</p>
<p>Result: 研究展示了深度学习在牙科影像分析中的广泛应用，包括诊断和治疗规划任务，并指出了现有方法的局限性和改进空间。</p>
<p>Insight: 深度学习在牙科影像分析中具有巨大潜力，但仍需解决数据稀缺性、标注主观性和模型泛化性等挑战，未来可能引入更多多模态数据和自监督学习技术。</p>
<p>Abstract: Efficient analysis and processing of dental images are crucial for dentists to achieve accurate diagnosis and optimal treatment planning. However, dental imaging inherently poses several challenges, such as low contrast, metallic artifacts, and variations in projection angles. Combined with the subjectivity arising from differences in clinicians’ expertise, manual interpretation often proves time-consuming and prone to inconsistency. Artificial intelligence (AI)-based automated dental image analysis (DIA) offers a promising solution to these issues and has become an integral part of computer-aided dental diagnosis and treatment. Among various AI technologies, deep learning (DL) stands out as the most widely applied and influential approach due to its superior feature extraction and representation capabilities. To comprehensively summarize recent progress in this field, we focus on the two fundamental aspects of DL research-datasets and models. In this paper, we systematically review 260 studies on DL applications in DIA, including 49 papers on publicly available dental datasets and 211 papers on DL-based algorithms. We first introduce the basic concepts of dental imaging and summarize the characteristics and acquisition methods of existing datasets. Then, we present the foundational techniques of DL and categorize relevant models and algorithms according to different DIA tasks, analyzing their network architectures, optimization strategies, training methods, and performance. Furthermore, we summarize commonly used training and evaluation metrics in the DIA domain. Finally, we discuss the current challenges of existing research and outline potential future directions. We hope that this work provides a valuable and systematic reference for researchers in this field. All supplementary materials and detailed comparison tables will be made publicly available on GitHub.</p>
</details>


<h3 id="84-Better-Tokens-for-Better-3D-Advancing-Vision-Language-Modeling-in-3D-Medical-Imaging"><a href="#84-Better-Tokens-for-Better-3D-Advancing-Vision-Language-Modeling-in-3D-Medical-Imaging" class="headerlink" title="[84] Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20639">Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging</a></h3><p><em>Ibrahim Ethem Hamamci,Sezgin Er,Suprosanna Shit,Hadrien Reynaud,Dong Yang,Pengfei Guo,Marc Edgar,Daguang Xu,Bernhard Kainz,Bjoern Menze</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: BTB3D提出了一种新的因果卷积编码器-解码器架构，通过改进的三维令牌生成方法，显著提升了3D医学影像与语言模型的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 当前3D医学影像的语言模型在处理高分辨率和长序列体积时表现不佳，主要问题包括对比预训练导致的视觉编码器与临床语言不匹配，以及切片级令牌化模糊了精细解剖结构。

<p>Contribution: 提出了BTB3D方法，统一了2D和3D的训练与推理，生成紧凑且频率感知的体积令牌。通过三阶段训练课程，模型能够从短切片样本学习并泛化到超过300切片的扫描。</p>
<p>Method: 采用因果卷积编码器-解码器架构，通过局部重建、重叠窗口平铺和长上下文解码器细化三个阶段进行训练。</p>
<p>Result: 在报告生成任务中，BTB3D提升了BLEU分数并将临床F1提高了40%；在文本到CT合成任务中，FID降低了75%，FVD减半。</p>
<p>Insight: 精确的三维令牌化（而非仅依赖更大的语言模型骨干）是实现可扩展3D医学影像-语言建模的关键。</p>
<p>Abstract: Recent progress in vision-language modeling for 3D medical imaging has been fueled by large-scale computed tomography (CT) corpora with paired free-text reports, stronger architectures, and powerful pretrained models. This has enabled applications such as automated report generation and text-conditioned 3D image synthesis. Yet, current approaches struggle with high-resolution, long-sequence volumes: contrastive pretraining often yields vision encoders that are misaligned with clinical language, and slice-wise tokenization blurs fine anatomy, reducing diagnostic performance on downstream tasks. We introduce BTB3D (Better Tokens for Better 3D), a causal convolutional encoder-decoder that unifies 2D and 3D training and inference while producing compact, frequency-aware volumetric tokens. A three-stage training curriculum enables (i) local reconstruction, (ii) overlapping-window tiling, and (iii) long-context decoder refinement, during which the model learns from short slice excerpts yet generalizes to scans exceeding 300 slices without additional memory overhead. BTB3D sets a new state-of-the-art on two key tasks: it improves BLEU scores and increases clinical F1 by 40% over CT2Rep, CT-CHAT, and Merlin for report generation; and it reduces FID by 75% and halves FVD compared to GenerateCT and MedSyn for text-to-CT synthesis, producing anatomically consistent 512<em>512</em>241 volumes. These results confirm that precise three-dimensional tokenization, rather than larger language backbones alone, is essential for scalable vision-language modeling in 3D medical imaging. The codebase is available at: <a target="_blank" rel="noopener" href="https://github.com/ibrahimethemhamamci/BTB3D">https://github.com/ibrahimethemhamamci/BTB3D</a></p>
</details>


<h3 id="85-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset"><a href="#85-UltraHR-100K-Enhancing-UHR-Image-Synthesis-with-A-Large-Scale-High-Quality-Dataset" class="headerlink" title="[85] UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20661">UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset</a></h3><p><em>Chen Zhao,En Ci,Yunzhe Xu,Tiehan Fan,Shanyan Guan,Yanhao Ge,Jian Yang,Ying Tai</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了大规模高质量UHR数据集UltraHR-100K和一种频率感知的后训练方法，显著提升了超高分辨率（UHR）文本到图像生成的细节质量和整体保真度。</p>
<details>
  <summary>Details</summary>
Motivation: 超高分辨率（UHR）文本到图像生成缺乏大规模高质量数据集，且现有方法在UHR场景下的细粒度细节合成方面表现不足。

<p>Contribution: 1. 提出UltraHR-100K数据集，包含10万张高质量UHR图像；2. 设计频率感知后训练方法（DOTS和SWFR），优化细节生成。</p>
<p>Method: 1. 构建UltraHR-100K数据集；2. 提出DOTS（细节导向时间步采样）和SWFR（软加权频率正则化）方法，增强高频细节保留。</p>
<p>Result: 在UltraHR-eval4K基准测试中，方法显著提升了UHR图像生成的细节质量和整体保真度。</p>
<p>Insight: 高质量数据集和针对高频细节的优化方法对提升UHR图像生成效果至关重要。</p>
<p>Abstract: Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable progress. However, two key challenges remain : 1) the absence of a large-scale high-quality UHR T2I dataset, and (2) the neglect of tailored training strategies for fine-grained detail synthesis in UHR scenarios. To tackle the first challenge, we introduce \textbf{UltraHR-100K}, a high-quality dataset of 100K UHR images with rich captions, offering diverse content and strong visual fidelity. Each image exceeds 3K resolution and is rigorously curated based on detail richness, content complexity, and aesthetic quality. To tackle the second challenge, we propose a frequency-aware post-training method that enhances fine-detail generation in T2I diffusion models. Specifically, we design (i) \textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning on detail-critical denoising steps, and (ii) \textit{Soft-Weighting Frequency Regularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to softly constrain frequency components, encouraging high-frequency detail preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks demonstrate that our approach significantly improves the fine-grained detail quality and overall fidelity of UHR image generation. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/NJU-PCALab/UltraHR-100k%7D%7Bhere%7D">https://github.com/NJU-PCALab/UltraHR-100k}{here}</a>.</p>
</details>


<h3 id="86-HybridSOMSpikeNet-A-Deep-Model-with-Differentiable-Soft-Self-Organizing-Maps-and-Spiking-Dynamics-for-Waste-Classification"><a href="#86-HybridSOMSpikeNet-A-Deep-Model-with-Differentiable-Soft-Self-Organizing-Maps-and-Spiking-Dynamics-for-Waste-Classification" class="headerlink" title="[86] HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20669">HybridSOMSpikeNet: A Deep Model with Differentiable Soft Self-Organizing Maps and Spiking Dynamics for Waste Classification</a></h3><p><em>Debojyoti Ghosh,Adrijit Goswami</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: HybridSOMSpikeNet结合了卷积特征提取、可微分自组织和脉冲神经网络，用于高效准确的废物分类，测试准确率达97.39%，优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 废物分类错误导致环境污染和资源浪费，需要一种智能且高效的分类方法以支持可持续发展和循环经济。

<p>Contribution: 提出了一种混合深度学习框架HybridSOMSpikeNet，集成了可微分软自组织图和脉冲神经动力学，提高了分类准确性和能源效率。</p>
<p>Method: 使用ResNet-152提取特征，通过可微分软自组织图（Soft-SOM）增强聚类和可解释性，最后用脉冲神经头处理时间动态。</p>
<p>Result: 在十类废物数据集上实现了97.39%的测试准确率，优于现有模型，且计算轻量适合实际部署。</p>
<p>Insight: 该框架不仅技术先进，还直接支持可持续发展目标（SDG 11和SDG 12），通过自动化废物分类提升回收效率和减少环境污染。</p>
<p>Abstract: Accurate waste classification is vital for achieving sustainable waste management and reducing the environmental footprint of urbanization. Misclassification of recyclable materials contributes to landfill accumulation, inefficient recycling, and increased greenhouse gas emissions. To address these issues, this study introduces HybridSOMSpikeNet, a hybrid deep learning framework that integrates convolutional feature extraction, differentiable self-organization, and spiking-inspired temporal processing to enable intelligent and energy-efficient waste classification. The proposed model employs a pre-trained ResNet-152 backbone to extract deep spatial representations, followed by a Differentiable Soft Self-Organizing Map (Soft-SOM) that enhances topological clustering and interpretability. A spiking neural head accumulates temporal activations over discrete time steps, improving robustness and generalization. Trained on a ten-class waste dataset, HybridSOMSpikeNet achieved a test accuracy of 97.39%, outperforming several state-of-the-art architectures while maintaining a lightweight computational profile suitable for real-world deployment. Beyond its technical innovations, the framework provides tangible environmental benefits. By enabling precise and automated waste segregation, it supports higher recycling efficiency, reduces contamination in recyclable streams, and minimizes the ecological and operational costs of waste processing. The approach aligns with global sustainability priorities, particularly the United Nations Sustainable Development Goals (SDG 11 and SDG 12), by contributing to cleaner cities, circular economy initiatives, and intelligent environmental management systems.</p>
</details>


<h3 id="87-Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward"><a href="#87-Diagnosing-Visual-Reasoning-Challenges-Insights-and-a-Path-Forward" class="headerlink" title="[87] Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20696">Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward</a></h3><p><em>Jing Bi,Guangyu Sun,Ali Vosoughi,Chen Chen,Chenliang Xu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文系统地分析了多模态大语言模型（MLLMs）在视觉推理中的挑战，提出了一种基于代理的架构，结合轻量级视觉模块以改进推理链，并在多个任务上显著优于基线模型。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管结合视觉和文本推理的多模态大语言模型（MLLMs）通过链式思维（CoT）提示解决复杂视觉任务，但仍然存在视觉幻觉和过度依赖文本先验的问题。作者希望通过系统诊断和改进模型架构来解决这些问题。

<p>Contribution: 论文的主要贡献包括：（1）提出一个三阶段评估框架来分析现有模型的失败模式；（2）设计一种基于代理的架构，结合轻量级视觉模块以改进推理链；（3）在多个任务上展示了显著性能提升（如MMMU +10.3，MathVista +6.0）。</p>
<p>Method: 论文采用了以下方法：（1）构建三阶段评估框架（诊断失败模式）；（2）提出一个基于代理的架构，将LLM的推理能力与轻量级视觉模块结合，支持细粒度分析和迭代优化推理链；（3）通过实验验证方法的有效性。</p>
<p>Result: 实验结果表明，该系统在MMMU和MathVista等任务上显著优于基线模型（7B参数），甚至匹配或超过更大规模的模型性能。</p>
<p>Insight: 论文的核心洞见是未来的视觉推理模型需要结合更多专门工具来分析视觉内容，而轻量级视觉模块与LLM的协同可以有效提升推理能力并减少幻觉问题。</p>
<p>Abstract: Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.</p>
</details>


<h3 id="88-Mixing-Importance-with-Diversity-Joint-Optimization-for-KV-Cache-Compression-in-Large-Vision-Language-Models"><a href="#88-Mixing-Importance-with-Diversity-Joint-Optimization-for-KV-Cache-Compression-in-Large-Vision-Language-Models" class="headerlink" title="[88] Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20707">Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</a></h3><p><em>Xuyang Liu,Xiyan Gui,Yuchao Zhang,Linfeng Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: MixKV提出了一种新颖的KV缓存压缩方法，结合重要性和多样性，优化大型视觉-语言模型中的内存瓶颈问题。</p>
<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在处理多模态序列时，KV缓存的扩展导致内存瓶颈，限制了部署扩展性。现有方法仅关注重要性，忽略了多模态KV缓存中的语义冗余模式。

<p>Contribution: 提出了MixKV方法，首次结合重要性和多样性进行KV缓存压缩，适应注意力头的语义冗余模式，显著提升压缩性能。</p>
<p>Method: MixKV通过分析注意力头间的冗余模式，选择性平衡重要性和多样性，优化KV对的压缩策略。</p>
<p>Result: 实验表明，MixKV在极端压缩条件下（budget&#x3D;64）平均提升了5.1%，在GUI任务中表现尤为突出（8.0%和9.0%增益），同时对LLM也有类似效果。</p>
<p>Insight: KV缓存压缩不仅需要关注重要性，还需考虑多样性以覆盖语义分布，MixKV为多模态模型的高效部署提供了新思路。</p>
<p>Abstract: Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget&#x3D;64), \texttt{MixKV} improves baseline methods by an average of \textbf{5.1%} across five multi-modal understanding benchmarks and achieves remarkable gains of \textbf{8.0%} and \textbf{9.0%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/xuyang-liu16/MixKV%7D%7B/textcolor%7Bciteblue%7D%7Bhttps://github.com/xuyang-liu16/MixKV%7D%7D">https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}</a>.</p>
</details>


<h3 id="89-AutoScape-Geometry-Consistent-Long-Horizon-Scene-Generation"><a href="#89-AutoScape-Geometry-Consistent-Long-Horizon-Scene-Generation" class="headerlink" title="[89] AutoScape: Geometry-Consistent Long-Horizon Scene Generation"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20726">AutoScape: Geometry-Consistent Long-Horizon Scene Generation</a></h3><p><em>Jiacheng Chen,Ziyu Jiang,Mingfu Liang,Bingbing Zhuang,Jong-Chyi Su,Sparsh Garg,Ying Wu,Manmohan Chandraker</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: AutoScape是一个生成长时间驾驶场景的框架，通过RGB-D扩散模型迭代生成几何一致的关键帧，并通过视频扩散模型插值生成连贯视频帧。</p>
<details>
  <summary>Details</summary>
Motivation: 解决长时间驾驶场景生成中几何一致性和真实感的问题。

<p>Contribution: 提出了一个RGB-D扩散模型，联合处理图像和深度信息，并通过几何一致性约束生成高质量关键帧。</p>
<p>Method: 1) 在共享潜在空间中联合处理RGB-D数据；2) 基于已有场景几何条件生成；3) 使用warp-consistent引导采样。</p>
<p>Result: 生成的20秒以上驾驶视频在FID和FVD指标上分别提升48.6%和43.0%。</p>
<p>Insight: 几何一致性是长时间场景生成的关键，联合RGB-D建模显著提升生成质量。</p>
<p>Abstract: This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene’s appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6% and 43.0%, respectively.</p>
</details>


<h3 id="90-ACS-SegNet-An-Attention-Based-CNN-SegFormer-Segmentation-Network-for-Tissue-Segmentation-in-Histopathology"><a href="#90-ACS-SegNet-An-Attention-Based-CNN-SegFormer-Segmentation-Network-for-Tissue-Segmentation-in-Histopathology" class="headerlink" title="[90] ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20754">ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for Tissue Segmentation in Histopathology</a></h3><p><em>Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Diana Mechtcheriakova,Amirreza Mahbod</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种基于注意力机制的双编码器模型ACS-SegNet，结合CNN和ViT的优势，用于组织病理学图像的语义分割任务，并在公开数据集上表现优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 组织病理学图像的自动化分析在计算机辅助诊断中非常重要，现有深度学习方法虽表现优异，但CNN和ViT的结合尚未充分探索，需进一步提升分割性能。

<p>Contribution: 主要贡献是提出了一种新型注意力驱动的双编码器模型ACS-SegNet，通过CNN和ViT的特征融合，显著提升语义分割性能。</p>
<p>Method: 采用了基于注意力的CNN-SegFormer结构，结合CNN的局部特征提取能力和ViT的全局建模能力，实现高效特征融合。</p>
<p>Result: 在GCPS和PUMA数据集上分别取得76.79%&#x2F;86.87%和64.93%&#x2F;76.60%的μIoU&#x2F;μDice分数，优于现有基准方法。</p>
<p>Insight: CNN和ViT的特征融合能够互补优势，注意力机制在多尺度特征整合中发挥关键作用，为病理图像分割提供新思路。</p>
<p>Abstract: Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\mu}IoU&#x2F;{\mu}Dice scores of 76.79%&#x2F;86.87% on the GCPS dataset and 64.93%&#x2F;76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/NimaTorbati/ACS-SegNet">https://github.com/NimaTorbati/ACS-SegNet</a></p>
</details>


<h3 id="91-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion"><a href="#91-DyPE-Dynamic-Position-Extrapolation-for-Ultra-High-Resolution-Diffusion" class="headerlink" title="[91] DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20766">DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion</a></h3><p><em>Noam Issachar,Guy Yariv,Sagie Benaim,Yossi Adi,Dani Lischinski,Raanan Fattal</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: DyPE是一种无需训练的动态位置外推方法，扩展预训练扩散Transformer模型的生成能力至超高分辨率，无需额外采样成本。</p>
<details>
  <summary>Details</summary>
Motivation: 当前扩散Transformer模型在超高分辨率图像生成中计算成本高，DyPE旨在解决这一问题，利用扩散过程的光谱特性动态调整位置编码。

<p>Contribution: 提出了DyPE方法，通过动态调整位置编码的频率谱匹配生成过程各阶段，显著提升超高分辨率图像的生成能力。</p>
<p>Method: DyPE在每个扩散步骤动态调整位置编码的频率谱，匹配当前生成阶段的低频或高频需求，从而支持更高分辨率的生成。</p>
<p>Result: DyPE在多个基准测试中表现优异，最高支持1600万像素的图像生成，且在超高分辨率下的保真度显著提升。</p>
<p>Insight: 扩散过程中的低频和高频特性可以用于动态优化模型生成能力，无需额外训练即可扩展分辨率。</p>
<p>Abstract: Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism’s quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model’s positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at <a target="_blank" rel="noopener" href="https://noamissachar.github.io/DyPE/">https://noamissachar.github.io/DyPE/</a>.</p>
</details>


<h3 id="92-AlphaFlow-Understanding-and-Improving-MeanFlow-Models"><a href="#92-AlphaFlow-Understanding-and-Improving-MeanFlow-Models" class="headerlink" title="[92] AlphaFlow: Understanding and Improving MeanFlow Models"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20771">AlphaFlow: Understanding and Improving MeanFlow Models</a></h3><p><em>Huijie Zhang,Aliaksandr Siarohin,Willi Menapace,Michael Vasilkovsky,Sergey Tulyakov,Qing Qu,Ivan Skorokhodov</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: AlphaFlow提出了一种统一框架，解决了MeanFlow模型中目标和优化的冲突问题，通过动态调整目标实现了更好的收敛性和性能，在ImageNet上取得了新的SOTA结果。</p>
<details>
  <summary>Details</summary>
Motivation: MeanFlow在少步生成建模中展现出强大能力，但其优化过程中目标和梯度存在冲突，导致收敛缓慢。作者希望通过分析MeanFlow的目标分解，改进优化过程。

<p>Contribution: 1. 揭示了MeanFlow目标可分解为轨迹流匹配和轨迹一致性，并发现它们是强负相关；2. 提出了AlphaFlow框架，统一了多种目标；3. 通过动态调整目标提升了收敛性和性能。</p>
<p>Method: 1. 分析MeanFlow的目标分解和优化冲突；2. 提出AlphaFlow框架，支持动态调整目标权重；3. 使用课程学习策略逐步调整目标权重。</p>
<p>Result: 在ImageNet-1K 256x256上，AlphaFlow-XL&#x2F;2+模型取得了FID 2.58（1步）和2.15（2步）的SOTA结果。</p>
<p>Insight: 动态调整优化目标的权重（如课程学习）能有效解决多目标冲突问题，提升生成模型的效率和性能。</p>
<p>Abstract: MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\alpha$-Flow-XL&#x2F;2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).</p>
</details>


<h3 id="93-CUPID-Pose-Grounded-Generative-3D-Reconstruction-from-a-Single-Image"><a href="#93-CUPID-Pose-Grounded-Generative-3D-Reconstruction-from-a-Single-Image" class="headerlink" title="[93] CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20776">CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image</a></h3><p><em>Binbin Huang,Haobin Duan,Yiqun Zhao,Zibo Zhao,Yi Ma,Shenghua Gao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 这篇论文提出了名为Cupid的新方法，从单张2D图像中准确推断出物体的相机位姿、3D形状和纹理。Cupid将3D重建视为从学习到的3D物体分布中进行条件采样的过程，并联合生成体素和像素-体素对应关系，从而在统一的生成框架下实现鲁棒的位姿和形状估计。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的单视图3D重建方法通常在位姿和形状估计的联合优化中表现不佳，且难以生成高保真度的3D结果。本文旨在通过统一的生成框架解决这些问题。

<p>Contribution: 1. 提出Cupid方法，将3D重建建模为条件采样问题；2. 采用两阶段流匹配管道，联合优化位姿和形状；3. 在共享3D潜在空间中表示输入相机位姿和3D形状。</p>
<p>Method: 1. 粗略阶段：生成初步3D几何及其2D投影以恢复位姿；2. 细化阶段：整合位姿对齐的图像特征以提升结构保真度和外观细节。</p>
<p>Result: Cupid在PSNR上超过现有方法3 dB以上，Chamfer Distance降低10%以上，在位姿准确性上与单目估计器相当，同时在视觉保真度上优于基线3D生成模型。</p>
<p>Insight: 通过联合建模位姿和形状的分布，并结合两阶段细化策略，能够显著提升单视图3D重建的准确性和质量。</p>
<p>Abstract: This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit cupid3d.github.io.</p>
</details>


<h3 id="94-Radar-Camera-Fused-Multi-Object-Tracking-Online-Calibration-and-Common-Feature"><a href="#94-Radar-Camera-Fused-Multi-Object-Tracking-Online-Calibration-and-Common-Feature" class="headerlink" title="[94] Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20794">Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common Feature</a></h3><p><em>Lei Cheng,Siyang Cao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了一种融合雷达与摄像头数据的多目标跟踪（MOT）框架，通过在线校准和共同特征提升跟踪效率，减少人工干预。</p>
<details>
  <summary>Details</summary>
Motivation: 传统研究中雷达常被低估，仅作为辅助工具，但其能提供精确的目标深度信息。本文旨在充分发挥雷达的优势，并与摄像头数据融合，提升跟踪性能。

<p>Contribution: 1.提出基于雷达-摄像头融合的MOT框架，支持在线校准；2.利用共同特征推导目标的真实位置；3.引入特征匹配和类别一致性检查，提升传感器关联精度。</p>
<p>Method: 通过在线校准将雷达与摄像头数据融合，利用共同特征进行目标定位，结合特征匹配和类别一致性检查优化关联结果。</p>
<p>Result: 实验表明，该框架能简化传感器映射流程，提升跟踪精度，适用于受控环境和实际交通场景。</p>
<p>Insight: 本研究首次探索了雷达-摄像头共同特征及其在在线校准中的应用，为多传感器融合提供了新思路。</p>
<p>Abstract: This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role–despite its capability to provide accurate range&#x2F;depth information of targets in a world 3D coordinate system–our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/radar-lab/Radar_Camera_MOT">https://github.com/radar-lab/Radar_Camera_MOT</a></p>
</details>


<h3 id="95-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model"><a href="#95-ARGenSeg-Image-Segmentation-with-Autoregressive-Image-Generation-Model" class="headerlink" title="[95] ARGenSeg: Image Segmentation with Autoregressive Image Generation Model"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20803">ARGenSeg: Image Segmentation with Autoregressive Image Generation Model</a></h3><p><em>Xiaolong Wang,Lixiang Ru,Ziyuan Huang,Kaixiang Ji,Dandan Zheng,Jingdong Chen,Jun Zhou</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: ARGenSeg提出了一种基于自回归图像生成模型的新型图像分割范式，利用多模态大语言模型（MLLM）生成视觉令牌，并通过VQ-VAE解码为密集掩码，显著提升了分割精度和推理速度。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法将图像分割集成到MLLM中时，通常依赖于离散表示或任务特定的解码器，导致难以捕获细粒度的视觉细节。ARGenSeg旨在通过图像生成的方式解决这一问题。

<p>Contribution: 1. 提出了一种基于图像生成的分割框架，利用MLLM生成视觉令牌并解码为密集掩码；2. 设计了并行预测策略以减少推理延迟。</p>
<p>Method: 1. 使用MLLM生成视觉令牌；2. 通过VQ-VAE将令牌解码为图像掩码；3. 采用next-scale-prediction策略并行生成令牌以加速推理。</p>
<p>Result: 在多个分割数据集上超越了之前的SOTA方法，同时显著提升了推理速度。</p>
<p>Insight: 将图像分割任务转化为生成任务，可以更自然地捕获细粒度视觉信息，并利用MLLM的强大理解能力。</p>
<p>Abstract: We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.</p>
</details>


<h3 id="96-Video-Prediction-of-Dynamic-Physical-Simulations-With-Pixel-Space-Spatiotemporal-Transformers"><a href="#96-Video-Prediction-of-Dynamic-Physical-Simulations-With-Pixel-Space-Spatiotemporal-Transformers" class="headerlink" title="[96] Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20807">Video Prediction of Dynamic Physical Simulations With Pixel-Space Spatiotemporal Transformers</a></h3><p><em>Dean L Slack,G Thomas Hudson,Thomas Winterbottom,Noura Al Moubayed</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 这篇论文提出了一种基于纯Transformer的自回归视频预测模型，专注于动态物理模拟的因果建模，通过简单的端到端方法在像素空间中实现时空推理。</p>
<details>
  <summary>Details</summary>
Motivation: 受到自回归大型语言模型（LLMs）性能和可扩展性的启发，作者希望将Transformer架构扩展到视频预测领域，尤其是物理动态模拟中的因果建模。现有的视频生成方法在这方面存在不足，因此作者试图通过简单的设计来解决这一问题。

<p>Contribution: 主要贡献包括：1）提出了一个简单而有效的纯Transformer模型，用于自回归视频预测；2）通过连续像素空间表示实现长时程的物理精确预测；3）在无需复杂训练策略或隐特征学习组件的情况下，显著提升了预测时间跨度（相比现有方法达50%）；4）通过可解释性实验揭示了网络区域如何编码PDE模拟参数信息。</p>
<p>Method: 方法的核心是基于Transformer的自回归框架，专注于像素空间的时空建模。作者比较了多种时空自注意布局，并通过物理模拟数据集进行无监督训练。模型通过连续像素表示预测视频帧，并利用因果建模确保时间一致性。</p>
<p>Result: 实验结果表明，该方法在物理模拟场景中显著提升了预测的时间跨度（最高50%），同时在视频质量指标上保持可比性。此外，可解释性实验证明模型能够泛化到分布外模拟参数的估计。</p>
<p>Insight: 研究揭示了纯Transformer架构在视频预测中的潜力，尤其是通过简单设计和像素空间表示，可以实现高效的时空建模和物理动态推理。这为未来基于注意力的时空视频建模提供了一个平台。</p>
<p>Abstract: Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.</p>
</details>


<h3 id="97-Small-Drafts-Big-Verdict-Information-Intensive-Visual-Reasoning-via-Speculation"><a href="#97-Small-Drafts-Big-Verdict-Information-Intensive-Visual-Reasoning-via-Speculation" class="headerlink" title="[97] Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20812">Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation</a></h3><p><em>Yuhan Liu,Lianhui Qin,Shengjie Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出一种无需训练的框架Speculative Verdict (SV)，通过结合多个轻量级专家和大模型实现高效的多模态推理，在信息密集型任务上表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型在信息密集型图像（如密集文本和图形交织的场景）中表现不足，难以准确定位关键线索并进行多跳推理。

<p>Contribution: 提出了SV框架，结合轻量级专家组和强大裁决模型，实现高效推理；引入共识专家选择机制，提升效率和准确性。</p>
<p>Method: SV分两阶段：1) 轻量级专家组生成多样推理路径；2) 裁决模型合成路径输出最终答案。通过共识机制筛选高一致性的路径。</p>
<p>Result: SV在InfographicVQA等挑战性任务上表现优异，相比大型专有模型或训练流程，既纠正错误又节省成本。</p>
<p>Insight: 通过整合部分准确的推理路径，SV实现了错误纠正和高效推理，为密集多模态任务提供了新思路。</p>
<p>Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Tinaliu0123/speculative-verdict">https://github.com/Tinaliu0123/speculative-verdict</a></p>
</details>


<h3 id="98-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives"><a href="#98-HoloCine-Holistic-Generation-of-Cinematic-Multi-Shot-Long-Video-Narratives" class="headerlink" title="[98] HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20822">HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video Narratives</a></h3><p><em>Yihao Meng,Hao Ouyang,Yue Yu,Qiuyu Wang,Wen Wang,Ka Leong Cheng,Hanlin Wang,Yixuan Li,Cheng Chen,Yanhong Zeng,Yujun Shen,Huamin Qu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: HoloCine是一款生成连贯多镜头长视频叙事的模型，解决了当前文本生成视频模型在全局一致性上的不足，通过创新的注意力机制实现了高效且一致的叙事生成。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的文本生成视频模型虽然在生成独立片段上表现优异，但在生成连贯的多镜头叙事（即故事讲述的核心）上存在明显不足，HoloCine旨在填补这一‘叙事鸿沟’。

<p>Contribution: HoloCine的主要贡献在于提出了一种全局一致的场景生成模型，通过Window Cross-Attention和Sparse Inter-Shot Self-Attention机制，实现了高效的跨镜头一致性控制，并具备角色和场景的持久记忆能力。</p>
<p>Method: HoloCine采用了Window Cross-Attention机制将文本提示定位到特定镜头，同时通过Sparse Inter-Shot Self-Attention（镜头内密集、镜头间稀疏）实现高效的长视频生成。</p>
<p>Result: HoloCine在叙事一致性上达到了新的SOTA，并展现了角色和场景的持久记忆以及对电影技术的直观理解能力。</p>
<p>Insight: HoloCine标志着从片段合成到自动化电影制作的重大转变，为端到端的电影创作提供了可行性路径。</p>
<p>Abstract: State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this “narrative gap” with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: <a target="_blank" rel="noopener" href="https://holo-cine.github.io/">https://holo-cine.github.io/</a>.</p>
</details>


<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="99-Kinaema-a-recurrent-sequence-model-for-memory-and-pose-in-motion"><a href="#99-Kinaema-a-recurrent-sequence-model-for-memory-and-pose-in-motion" class="headerlink" title="[99] Kinaema: a recurrent sequence model for memory and pose in motion"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20261">Kinaema: a recurrent sequence model for memory and pose in motion</a></h3><p><em>Mert Bulent Sariyildiz,Philippe Weinzaepfel,Guillaume Bono,Gianluca Monaci,Christian Wolf</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: Kinaema是一个基于循环序列的模型，专注于机器人在连续任务中利用先验信息进行自我定位和高效导航。</p>
<details>
  <summary>Details</summary>
Motivation: 为了解决机器人在大场景中导航时的历史信息利用问题，特别是在连续任务中，如何高效地整合视觉观测流并准确定位。

<p>Contribution: 提出了Kinaema模型，能够在无需显式存储历史观测的情况下，通过隐式潜在记忆和循环更新的Transformer实现高效导航和自我定位。</p>
<p>Method: 采用循环Transformer架构，隐式压缩历史传感器数据为紧凑表示，并通过Mem-Nav任务验证其性能。</p>
<p>Result: Kinaema在大场景导航中表现出色，能够高效定位并导航至目标位置，性能优于传统基于注意力机制的Transformer。</p>
<p>Insight: 循环Transformer框架在机器人导航任务中展现了强大的历史信息整合能力和计算效率。</p>
<p>Abstract: One key aspect of spatially aware robots is the ability to “find their bearings”, ie. to correctly situate themselves in previously seen spaces. In this work, we focus on this particular scenario of continuous robotics operations, where information observed before an actual episode start is exploited to optimize efficiency. We introduce a new model, Kinaema, and agent, capable of integrating a stream of visual observations while moving in a potentially large scene, and upon request, processing a query image and predicting the relative position of the shown space with respect to its current position. Our model does not explicitly store an observation history, therefore does not have hard constraints on context length. It maintains an implicit latent memory, which is updated by a transformer in a recurrent way, compressing the history of sensor readings into a compact representation. We evaluate the impact of this model in a new downstream task we call “Mem-Nav”. We show that our large-capacity recurrent model maintains a useful representation of the scene, navigates to goals observed before the actual episode start, and is computationally efficient, in particular compared to classical transformers with attention over an observation history.</p>
</details>


<h3 id="100-Dino-Diffusion-Modular-Designs-Bridge-the-Cross-Domain-Gap-in-Autonomous-Parking"><a href="#100-Dino-Diffusion-Modular-Designs-Bridge-the-Cross-Domain-Gap-in-Autonomous-Parking" class="headerlink" title="[100] Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20335">Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking</a></h3><p><em>Zixuan Wu,Hengyuan Zhang,Ting-Hsuan Chen,Yuliang Guo,David Paz,Xinyu Huang,Liu Ren</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: 论文提出了一种名为Dino-Diffusion Parking (DDP)的领域无关自动驾驶停车系统，结合视觉基础模型和扩散规划，实现了在分布偏移下的泛化感知和鲁棒运动规划。实验证明其在多种对抗场景中停车成功率超过90%，并展示了良好的仿真到现实迁移能力。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的端到端自动驾驶停车方法在领域内表现良好，但在领域偏移（如天气和光照变化）下的鲁棒性不足。作者旨在提出一种无需额外数据的领域无关解决方案。

<p>Contribution: 1. 提出DDP系统，集成视觉基础模型与扩散规划；2. 在零样本条件下实现跨域泛化；3. 在对抗场景中表现出高成功率；4. 展示了仿真到现实的迁移潜力。</p>
<p>Method: 1. 使用视觉基础模型处理图像输入；2. 基于扩散的规划模块生成鲁棒的运动策略；3. 在CARLA仿真环境中训练，并进行零样本迁移。</p>
<p>Result: 在多种分布偏移场景中，停车成功率超过90%。3D高斯泼溅环境中的测试表明仿真到现实的迁移效果良好。</p>
<p>Insight: 1. 视觉基础模型和扩散规划的结合增强了系统的跨域鲁棒性；2. 零样本迁移证明了方法的泛化能力；3. 仿真到现实的迁移显示了实际应用的潜力。</p>
<p>Abstract: Parking is a critical pillar of driving safety. While recent end-to-end (E2E) approaches have achieved promising in-domain results, robustness under domain shifts (e.g., weather and lighting changes) remains a key challenge. Rather than relying on additional data, in this paper, we propose Dino-Diffusion Parking (DDP), a domain-agnostic autonomous parking pipeline that integrates visual foundation models with diffusion-based planning to enable generalized perception and robust motion planning under distribution shifts. We train our pipeline in CARLA at regular setting and transfer it to more adversarial settings in a zero-shot fashion. Our model consistently achieves a parking success rate above 90% across all tested out-of-distribution (OOD) scenarios, with ablation studies confirming that both the network architecture and algorithmic design significantly enhance cross-domain performance over existing baselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment reconstructed from a real-world parking lot demonstrates promising sim-to-real transfer.</p>
</details>


<h3 id="101-GSWorld-Closed-Loop-Photo-Realistic-Simulation-Suite-for-Robotic-Manipulation"><a href="#101-GSWorld-Closed-Loop-Photo-Realistic-Simulation-Suite-for-Robotic-Manipulation" class="headerlink" title="[101] GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20813">GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation</a></h3><p><em>Guangqi Jiang,Haoran Chang,Ri-Zhao Qiu,Yutong Liang,Mazeyu Ji,Jiyue Zhu,Zhao Dong,Xueyan Zou,Xiaolong Wang</em></p>
<p>Main category: cs.RO</p>
<p>TL;DR: GSWorld是一种结合3D高斯渲染与物理引擎的机器人操作模拟器，支持闭环式策略开发和sim2real训练，无需真实机器人。</p>
<details>
  <summary>Details</summary>
Motivation: 开发一种能够高效支持机器人操作策略开发与训练的仿真环境，以降低成本并提高性能。

<p>Contribution: 提出GSWorld仿真套件，结合3D高斯渲染与物理引擎，支持闭环策略开发和多种sim2real应用。</p>
<p>Method: 采用GSDF（高斯场景描述文件）格式，结合高斯-on-Mesh表示与机器人URDF，实现高质量渲染与仿真。</p>
<p>Result: 构建了包含3种机器人和40多物体的数据库，并实现了多种sim2real应用，如零样本策略学习和基准测试。</p>
<p>Insight: 闭环仿真与高质量渲染的结合可以显著提升机器人操作策略的开发和训练效率。</p>
<p>Abstract: This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates “closing the loop” of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: <a target="_blank" rel="noopener" href="https://3dgsworld.github.io/">https://3dgsworld.github.io/</a>.</p>
</details>


<div id='stat.AP'></div>

<h1 id="stat-AP-Back"><a href="#stat-AP-Back" class="headerlink" title="stat.AP [Back]"></a>stat.AP <a href="#toc">[Back]</a></h1><h3 id="102-AI-Pose-Analysis-and-Kinematic-Profiling-of-Range-of-Motion-Variations-in-Resistance-Training"><a href="#102-AI-Pose-Analysis-and-Kinematic-Profiling-of-Range-of-Motion-Variations-in-Resistance-Training" class="headerlink" title="[102] AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20012">AI Pose Analysis and Kinematic Profiling of Range-of-Motion Variations in Resistance Training</a></h3><p><em>Adam Diamant</em></p>
<p>Main category: stat.AP</p>
<p>TL;DR: 该论文开发了一个基于AI的姿态估计流程，用于量化阻力训练中的运动学特征。通过分析部分范围运动（pROM）和全范围运动（fROM）训练的差异，研究发现pROM的ROM更小且执行时间更短，尤其是在离心阶段。此外，参与者个体差异是变异性主要来源。</p>
<details>
  <summary>Details</summary>
Motivation: 传统阻力训练研究依赖于主观评估或有限的技术手段，缺乏精确的运动学量化。本研究旨在利用AI姿态分析技术，提供更准确的数据支持，以比较pROM和fROM训练的差异。

<p>Contribution: 1. 提出了一个基于AI的姿态估计流程，用于分析阻力训练中的运动学特征；2. 引入新的指标%ROM，用于量化pROM相对于fROM的比例；3. 揭示了pROM和fROM在ROM和执行动力学上的显著差异。</p>
<p>Method: 使用Wolf et al. (2025)的视频数据，通过AI姿态估计提取关节角度轨迹，并进行滤波和平滑处理。采用随机效应Meta分析模型分析变异性。</p>
<p>Result: pROM的ROM更小，执行时间更短（尤其是离心阶段），参与者个体差异是变异性主要来源。%ROM指标在不同练习中表现出相对一致性。</p>
<p>Insight: AI姿态分析技术为阻力训练研究提供了新的量化工具，揭示了pROM和fROM的本质差异，有助于更个性化的训练方案设计。</p>
<p>Abstract: This study develops an AI-based pose estimation pipeline to enable precise quantification of movement kinematics in resistance training. Using video data from Wolf et al. (2025), which compared lengthened partial (pROM) and full range-of-motion (fROM) training across eight upper-body exercises in 26 participants, 280 recordings were processed to extract frame-level joint-angle trajectories. After filtering and smoothing, per-set metrics were derived, including range of motion (ROM), tempo, and concentric&#x2F;eccentric phase durations. A random-effects meta-analytic model was applied to account for within-participant and between-exercise variability. Results show that pROM repetitions were performed with a smaller ROM and shorter overall durations, particularly during the eccentric phase of movement. Variance analyses revealed that participant-level differences, rather than exercise-specific factors, were the primary driver of variation, although there is substantial evidence of heterogeneous treatment effects. We then introduce a novel metric, %ROM, which is the proportion of full ROM achieved during pROM, and demonstrate that this definition of lengthened partials remains relatively consistent across exercises. Overall, these findings suggest that lengthened partials differ from full ROM training not only in ROM, but also in execution dynamics and consistency, highlighting the potential of AI-based methods for advancing research and improving resistance training prescription.</p>
</details>


<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="103-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values"><a href="#103-Every-Question-Has-Its-Own-Value-Reinforcement-Learning-with-Explicit-Human-Values" class="headerlink" title="[103] Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20187">Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values</a></h3><p><em>Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文提出了RLEV方法，通过将人类定义的价值信号直接融入奖励函数，优化大语言模型的训练，使其更符合人类优先级。该方法在多种RL算法和模型规模下优于仅基于正确性的基线方法。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法仅依赖二元正确性奖励训练模型，忽视了任务的重要性差异，导致模型对高价值和低价值任务的响应缺乏区分。RLEV旨在通过显式的人类价值信号解决这一问题。

<p>Contribution: 1. 提出了RLEV方法，将人类价值信号直接整合到奖励函数中；2. 展示了RLEV在价值加权准确性和任务终止策略上的优势；3. 验证了价值对齐与性能提升的因果关系。</p>
<p>Method: RLEV扩展了RLVR框架，利用带有显式价值标签的考试数据训练模型。通过价值加权梯度放大机制，模型学习区分高低价值任务，并动态调整响应策略（高价值任务详尽，低价值任务简洁）。</p>
<p>Result: RLEV在多种RL算法和模型规模下优于基线方法。模型不仅能提高价值加权准确性，还能学习区分高低价值的任务终止策略。</p>
<p>Insight: 显式的人类价值信号可以有效指导模型优化，提升模型对任务优先级的敏感度。即使在噪声价值信号（如基于难度的标签）下，RLEV仍表现出稳健性，说明价值对齐是一种可行的实用路径。</p>
<p>Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method that aligns Large Language Model (LLM) optimization directly with quantifiable human value signals. While Reinforcement Learning with Verifiable Rewards (RLVR) effectively trains models in objective domains using binary correctness rewards, it overlooks that not all tasks are equally significant. RLEV extends this framework by incorporating human-defined value signals directly into the reward function. Using exam-style data with explicit ground-truth value labels, RLEV consistently outperforms correctness-only baselines across multiple RL algorithms and model scales. Crucially, RLEV policies not only improve value-weighted accuracy but also learn a value-sensitive termination policy: concise for low-value prompts, thorough for high-value ones. We demonstrate this behavior stems from value-weighted gradient amplification on end-of-sequence tokens. Ablation studies confirm the gain is causally linked to value alignment. RLEV remains robust under noisy value signals, such as difficulty-based labels, demonstrating that optimizing for an explicit utility function offers a practical path to aligning LLMs with human priorities.</p>
</details>


<h3 id="104-BadGraph-A-Backdoor-Attack-Against-Latent-Diffusion-Model-for-Text-Guided-Graph-Generation"><a href="#104-BadGraph-A-Backdoor-Attack-Against-Latent-Diffusion-Model-for-Text-Guided-Graph-Generation" class="headerlink" title="[104] BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20792">BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation</a></h3><p><em>Liang Ye,Shengqin Chen,Jiazhu Dai</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 这篇论文提出了BadGraph，一种针对文本引导图生成的潜在扩散模型的后门攻击方法，通过插入文本触发器毒化训练数据，实现对目标子图的隐蔽攻击。</p>
<details>
  <summary>Details</summary>
Motivation: 随着图生成技术的快速发展，其安全性问题日益突出，尤其是后门漏洞。目前的研究主要集中在图像扩散和无条件图生成的后门攻击，而对文本引导的图生成攻击研究较少。

<p>Contribution: 1. 提出了一种针对文本引导图生成的潜在扩散模型的后门攻击方法BadGraph；2. 实验证明攻击的有效性和隐蔽性；3. 揭示了潜在扩散模型在文本引导图生成中的安全漏洞。</p>
<p>Method: BadGraph通过插入文本触发器毒化训练数据，后门在VAE和扩散训练阶段被植入。</p>
<p>Result: 在四个基准数据集上，低于10%的毒化率可实现50%的攻击成功率，24%的毒化率可实现80%以上的成功率，且对正常样本的性能影响可忽略。</p>
<p>Insight: 研究揭示了文本引导图生成的潜在扩散模型的安全风险，强调了在药物发现等应用中防御此类攻击的必要性。</p>
<p>Abstract: The rapid progress of graph generation has raised new security concerns, particularly regarding backdoor vulnerabilities. While prior work has explored backdoor attacks in image diffusion and unconditional graph generation, conditional, especially text-guided graph generation remains largely unexamined. This paper proposes BadGraph, a backdoor attack method targeting latent diffusion models for text-guided graph generation. BadGraph leverages textual triggers to poison training data, covertly implanting backdoors that induce attacker-specified subgraphs during inference when triggers appear, while preserving normal performance on clean inputs. Extensive experiments on four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the effectiveness and stealth of the attack: less than 10% poisoning rate can achieves 50% attack success rate, while 24% suffices for over 80% success rate, with negligible performance degradation on benign samples. Ablation studies further reveal that the backdoor is implanted during VAE and diffusion training rather than pretraining. These findings reveal the security vulnerabilities in latent diffusion models of text-guided graph generation, highlight the serious risks in models’ applications such as drug discovery and underscore the need for robust defenses against the backdoor attack in such diffusion models.</p>
</details>


<h3 id="105-Compress-to-Impress-Efficient-LLM-Adaptation-Using-a-Single-Gradient-Step-on-100-Samples"><a href="#105-Compress-to-Impress-Efficient-LLM-Adaptation-Using-a-Single-Gradient-Step-on-100-Samples" class="headerlink" title="[105] Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20800">Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples</a></h3><p><em>Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文提出了一种高效的LLM适应方法，仅需对100个样本进行一次梯度步长计算，通过精心选择的矩阵子集和奇异值梯度分析，显著减少搜索时间，同时提高下游任务准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的LASER方法虽然能够在不进行梯度微调的情况下提升LLM的下游任务准确性，但其逐层搜索的方法效率低下，不适合快速部署。本文旨在消除这一开销，提出更高效的适应算法。

<p>Contribution: 主要贡献包括：(i) 仅需检查少量精心选择的矩阵子集；(ii) 利用矩阵奇异值的梯度定位需要减秩的矩阵；(iii) 扩展因子分解搜索空间以进一步提升准确性；(iv) 发现仅需100个样本即可评估梯度并测量最终准确性。</p>
<p>Method: 方法包括选择关键矩阵子集、通过奇异值梯度分析确定减秩矩阵、扩展因子分解搜索空间（多子空间聚类和分解）以及基于少量样本（100个）的快速评估。最终结合这些方法，提出无需微调的高效适应算法。</p>
<p>Result: 实验结果显示，该方法能将准确性提升高达24.6个百分点，同时显著减少搜索时间，仅需100个样本和一次梯度步长计算即可完成适应。</p>
<p>Insight: 核心发现是下游任务的适应主要由提示风格主导，而非数据集规模，因此少量样本足以支持高效的适应过程。</p>
<p>Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM’s weight matrices can boost downstream accuracy – without any gradient-based fine-tuning. Yet LASER’s exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected – eliminating the layer-by-layer sweep, (ii) The gradient of each matrix’s singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data – both for computing the indicative gradients and for measuring the final accuracy – suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets – entirely without fine-tuning.</p>
</details>


<h3 id="106-Synthetic-Data-for-Robust-Runway-Detection"><a href="#106-Synthetic-Data-for-Robust-Runway-Detection" class="headerlink" title="[106] Synthetic Data for Robust Runway Detection"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20349">Synthetic Data for Robust Runway Detection</a></h3><p><em>Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文探讨了在自动驾驶着陆系统中跑道检测的问题，提出了一种基于商用飞行模拟器的合成图像生成方法，以补充少量标注的真实图像。通过控制生成过程和合成与真实数据的结合，提升了标准目标检测模型的准确性和鲁棒性。</p>
<details>
  <summary>Details</summary>
Motivation: 在关键应用中，如自动驾驶着陆系统，训练数据需要覆盖所有可能场景（包括罕见情况），但真实数据收集和标注成本高昂。合成数据提供了一种低成本且可靠的替代方案，但其与真实数据之间的分布偏移问题需要解决。

<p>Contribution: 1. 提出了一种基于商用飞行模拟器的合成图像生成方法。2. 展示了如何通过控制合成与真实数据的结合来提升模型性能。3. 针对未在真实数据中出现的夜间图像条件，提出了定制化的域适应策略。</p>
<p>Method: 1. 使用商用飞行模拟器生成合成图像。2. 结合少量真实标注图像进行训练。3. 采用定制化的域适应策略处理合成与真实数据之间的分布偏移。</p>
<p>Result: 实验表明，标准目标检测模型在结合合成和真实数据后，能够实现高精度预测，并在夜间图像等未见条件下表现出良好的鲁棒性。</p>
<p>Insight: 合成数据在关键应用中具有潜力，但需结合领域适应技术以解决分布偏移问题，从而提升模型在实际场景中的性能。</p>
<p>Abstract: Deep vision models are now mature enough to be integrated in industrial and possibly critical applications such as autonomous navigation. Yet, data collection and labeling to train such models requires too much efforts and costs for a single company or product. This drawback is more significant in critical applications, where training data must include all possible conditions including rare scenarios. In this perspective, generating synthetic images is an appealing solution, since it allows a cheap yet reliable covering of all the conditions and environments, if the impact of the synthetic-to-real distribution shift is mitigated. In this article, we consider the case of runway detection that is a critical part in autonomous landing systems developed by aircraft manufacturers. We propose an image generation approach based on a commercial flight simulator that complements a few annotated real images. By controlling the image generation and the integration of real and synthetic data, we show that standard object detection models can achieve accurate prediction. We also evaluate their robustness with respect to adverse conditions, in our case nighttime images, that were not represented in the real data, and show the interest of using a customized domain adaptation strategy.</p>
</details>


<h3 id="107-MEIcoder-Decoding-Visual-Stimuli-from-Neural-Activity-by-Leveraging-Most-Exciting-Inputs"><a href="#107-MEIcoder-Decoding-Visual-Stimuli-from-Neural-Activity-by-Leveraging-Most-Exciting-Inputs" class="headerlink" title="[107] MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20762">MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs</a></h3><p><em>Jan Sobotka,Luca Baroni,Ján Antolík</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: MEIcoder是一种基于神经元特异性最兴奋输入（MEIs）的解码方法，结合结构相似性指数损失和对抗训练，能够在小数据集上实现高性能的视觉刺激解码。</p>
<details>
  <summary>Details</summary>
Motivation: 由于在灵长类或人类中获取高通量神经活动数据的挑战性，深度学习解码技术面临数据稀缺的问题。MEIcoder旨在解决这一问题，通过利用MEIs提升解码性能。

<p>Contribution: 1. 提出了MEIcoder方法，结合MEIs、结构相似性损失和对抗训练。2. 在小数据集和少量神经元下实现高性能解码。3. 提出了包含16万样本的统一基准数据集。</p>
<p>Method: MEIcoder利用MEIs提取神经元特异性特征，通过结构相似性指数损失优化重建质量，并采用对抗训练进一步提升生成图像的自然性。</p>
<p>Result: 实验表明，MEIcoder仅需1000-2500个神经元和少于1000个训练样本即可重建高质量自然图像，性能优于现有方法。</p>
<p>Insight: MEIs是解码性能提升的关键因素，该方法为神经科学和神经工程应用提供了实用的解决方案。</p>
<p>Abstract: Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.</p>
</details>


<div id='cs.PL'></div>

<h1 id="cs-PL-Back"><a href="#cs-PL-Back" class="headerlink" title="cs.PL [Back]"></a>cs.PL <a href="#toc">[Back]</a></h1><h3 id="108-Prompt-Decorators-A-Declarative-and-Composable-Syntax-for-Reasoning-Formatting-and-Control-in-LLMs"><a href="#108-Prompt-Decorators-A-Declarative-and-Composable-Syntax-for-Reasoning-Formatting-and-Control-in-LLMs" class="headerlink" title="[108] Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19850">Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs</a></h3><p><em>Mostapha Kalami Heris</em></p>
<p>Main category: cs.PL</p>
<p>TL;DR: 论文提出了Prompt Decorators，一种声明式、可组合的语法，通过紧凑的控制令牌（如+++Reasoning、+++Tone）来控制LLM的行为，提升透明性、可复用性和可解释性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的提示工程依赖冗长的自然语言指令，缺乏一致性、模块化和可解释性，限制了LLM在推理和输出表达上的可控性。

<p>Contribution: 1. 提出Prompt Decorators框架，定义20个核心装饰器；2. 将装饰器分为认知与生成、表达与系统两大功能家族；3. 设计了统一的语法、作用域模型和确定性处理流程。</p>
<p>Method: 通过声明式的控制令牌（装饰器）分离任务意图与执行行为，实现对LLM行为的多维度控制（如推理风格、结构、语气）。</p>
<p>Result: 使用案例显示，该方法提升了推理透明度，降低了提示复杂度，并实现了跨领域的标准化模型行为。</p>
<p>Insight: Prompt Decorators为LLM提供了一个可复用、可解释的接口设计范式，对AI系统的可扩展性和行为一致性具有重要意义。</p>
<p>Abstract: Large Language Models (LLMs) are central to reasoning, writing, and decision-support workflows, yet users lack consistent control over how they reason and express outputs. Conventional prompt engineering relies on verbose natural-language instructions, limiting reproducibility, modularity, and interpretability. This paper introduces Prompt Decorators, a declarative, composable syntax that governs LLM behavior through compact control tokens such as +++Reasoning, +++Tone(style&#x3D;formal), and +++Import(topic&#x3D;”Systems Thinking”). Each decorator modifies a behavioral dimension, such as reasoning style, structure, or tone, without changing task content. The framework formalizes twenty core decorators organized into two functional families (Cognitive &amp; Generative and Expressive &amp; Systemic), each further decomposed into subcategories that govern reasoning, interaction, expression, and session-control. It defines a unified syntax, scoping model, and deterministic processing pipeline enabling predictable and auditable behavior composition. By decoupling task intent from execution behavior, Prompt Decorators create a reusable and interpretable interface for prompt design. Illustrative use cases demonstrate improved reasoning transparency, reduced prompt complexity, and standardized model behavior across domains. The paper concludes with implications for interoperability, behavioral consistency, and the development of declarative interfaces for scalable AI systems.</p>
</details>


<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="109-Empathic-Prompting-Non-Verbal-Context-Integration-for-Multimodal-LLM-Conversations"><a href="#109-Empathic-Prompting-Non-Verbal-Context-Integration-for-Multimodal-LLM-Conversations" class="headerlink" title="[109] Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20743">Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations</a></h3><p><em>Lorenzo Stacchio,Andrea Ubaldi,Alessandro Galdelli,Maurizio Mauri,Emanuele Frontoni,Andrea Gaggioli</em></p>
<p>Main category: cs.HC</p>
<p>TL;DR: Empathic Prompting 是一个新颖的多模态人机交互框架，通过整合非语言上下文（如面部表情）增强LLM对话，提升流畅性和情感对齐。</p>
<details>
  <summary>Details</summary>
Motivation: 传统多模态交互需要显式用户控制，而情感信号在文本交流中往往难以捕捉，尤其是在医疗或教育等领域。

<p>Contribution: 提出了Empathic Prompting框架，通过面部表情识别服务隐式捕捉情感信号，并将其作为上下文嵌入LLM提示中。</p>
<p>Method: 采用模块化架构，集成商用面部表情识别服务，并通过本地部署的DeepSeek实例实现非语言输入与文本输入的融合。</p>
<p>Result: 初步评估（N&#x3D;5）表明系统能一致地将非语言输入整合为连贯的LLM输出，参与者强调了对话流畅性。</p>
<p>Insight: 该框架展示了在情感敏感的领域（如医疗、教育）中，通过非语言信号隐式增强LLM对话的潜力。</p>
<p>Abstract: We present Empathic Prompting, a novel framework for multimodal human-AI interaction that enriches Large Language Model (LLM) conversations with implicit non-verbal context. The system integrates a commercial facial expression recognition service to capture users’ emotional cues and embeds them as contextual signals during prompting. Unlike traditional multimodal interfaces, empathic prompting requires no explicit user control; instead, it unobtrusively augments textual input with affective information for conversational and smoothness alignment. The architecture is modular and scalable, allowing integration of additional non-verbal modules. We describe the system design, implemented through a locally deployed DeepSeek instance, and report a preliminary service and usability evaluation (N&#x3D;5). Results show consistent integration of non-verbal input into coherent LLM outputs, with participants highlighting conversational fluidity. Beyond this proof of concept, empathic prompting points to applications in chatbot-mediated communication, particularly in domains like healthcare or education, where users’ emotional signals are critical yet often opaque in verbal exchanges.</p>
</details>


<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="110-Multimedia-Aware-Question-Answering-A-Review-of-Retrieval-and-Cross-Modal-Reasoning-Architectures"><a href="#110-Multimedia-Aware-Question-Answering-A-Review-of-Retrieval-and-Cross-Modal-Reasoning-Architectures" class="headerlink" title="[110] Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20193">Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures</a></h3><p><em>Rahul Raja,Arpita Vats</em></p>
<p>Main category: cs.IR</p>
<p>TL;DR: 这篇论文综述了多媒体感知问答系统的最新进展，重点分析了检索方法与跨模态推理架构，包括视觉、语言和音频模态的对齐技术。</p>
<details>
  <summary>Details</summary>
Motivation: 随着多媒体内容的快速增长，传统的基于文本的问答系统已无法满足需求，需要整合多模态数据的检索和推理能力以提高问答系统的性能。

<p>Contribution: 论文系统地梳理了多媒体感知问答系统的检索方法、融合技术和答案生成策略，并对基准数据集、评估协议以及性能权衡进行了分析。</p>
<p>Method: 论文通过分类方法（基于检索方法、融合技术和答案生成策略）分析了多种跨模态对齐架构，并探讨了其在不同模态（视觉、语言、音频）中的应用。</p>
<p>Result: 总结了当前多媒体感知问答系统的性能表现，指出了跨模态对齐、延迟与准确性的权衡等关键挑战。</p>
<p>Insight: 未来的研究方向包括提升跨模态语义对齐能力、优化延迟与准确性的平衡，以及构建更健壮、上下文感知的多媒体问答系统。</p>
<p>Abstract: Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.</p>
</details>


<h3 id="111-Automating-Iconclass-LLMs-and-RAG-for-Large-Scale-Classification-of-Religious-Woodcuts"><a href="#111-Automating-Iconclass-LLMs-and-RAG-for-Large-Scale-Classification-of-Religious-Woodcuts" class="headerlink" title="[111] Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19986">Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts</a></h3><p><em>Drew B. Thomas</em></p>
<p>Main category: cs.IR</p>
<p>TL;DR: 该论文提出了一种结合LLMs和RAG的新方法，用于大规模分类早期现代宗教木刻图像，显著提高了分类精度。</p>
<details>
  <summary>Details</summary>
Motivation: 传统基于图像和关键词的分类方法在早期现代宗教图像分类中表现不佳，急需一种能结合视觉与文本信息的自动化方法。

<p>Contribution: 提出了一种结合LLMs、向量数据库和RAG的创新方法，用于生成详细描述并匹配Iconclass代码，提升了分类精度。</p>
<p>Method: 利用LLMs生成结合视觉和文本信息的图像描述，通过混合向量搜索匹配Iconclass代码，实现高精度分类。</p>
<p>Result: 在五个和四个分类级别上分别达到87%和92%的精确度，显著优于传统方法。</p>
<p>Insight: 展示了LLMs和RAG在艺术史和数字人文学科中的潜力，为大规模视觉档案分析提供了有力工具。</p>
<p>Abstract: This paper presents a novel methodology for classifying early modern religious images by using Large Language Models (LLMs) and vector databases in combination with Retrieval-Augmented Generation (RAG). The approach leverages the full-page context of book illustrations from the Holy Roman Empire, allowing the LLM to generate detailed descriptions that incorporate both visual and textual elements. These descriptions are then matched to relevant Iconclass codes through a hybrid vector search. This method achieves 87% and 92% precision at five and four levels of classification, significantly outperforming traditional image and keyword-based searches. By employing full-page descriptions and RAG, the system enhances classification accuracy, offering a powerful tool for large-scale analysis of early modern visual archives. This interdisciplinary approach demonstrates the growing potential of LLMs and RAG in advancing research within art history and digital humanities.</p>
</details>


<div id='quant-ph'></div>

<h1 id="quant-ph-Back"><a href="#quant-ph-Back" class="headerlink" title="quant-ph [Back]"></a>quant-ph <a href="#toc">[Back]</a></h1><h3 id="112-Co-Designing-Quantum-Codes-with-Transversal-Diagonal-Gates-via-Multi-Agent-Systems"><a href="#112-Co-Designing-Quantum-Codes-with-Transversal-Diagonal-Gates-via-Multi-Agent-Systems" class="headerlink" title="[112] Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20728">Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems</a></h3><p><em>Xi He,Sirui Lu,Bei Zeng</em></p>
<p>Main category: quant-ph</p>
<p>TL;DR: 该论文提出了一种多智能体、人在环的工作流，用于协同设计具有规定横向对角门的量子代码。该方法结合了系统枚举和精确分析重建，为量子代码的设计提供了新思路。</p>
<details>
  <summary>Details</summary>
Motivation: 量子代码的设计通常需要满足特定的横向对角门（transversal diagonal gates）条件，传统方法往往难以高效实现这一点。论文旨在通过多智能体系统协同工作，解决这一问题。

<p>Contribution: 1. 提出了一种多智能体协同工作流，结合人在环的设计，实现量子代码与横向对角门的协同设计。2. 基于SSLP框架，开发了新的工具和方法，支持系统化枚举和精确分析。3. 展示了新代码的实现，并证明了方法的可扩展性。</p>
<p>Method: 该方法基于多智能体系统（Synthesis Agent、Search Agent、Audit Agent）协同工作，结合SSLP框架和GPT-5进行问题建模、候选代码搜索和校验。工作流在LaTeX-Python环境中实现，支持代码执行和文档同步。</p>
<p>Result: 系统扫描生成了可实现循环逻辑组的代码表，例如在K&#x3D;3时获得了6量子比特上的16阶逻辑组。此外，还展示了新代码（如（(6,4,2))）的实现。</p>
<p>Insight: 1. 多智能体协同工作流显著提高了量子代码设计的效率和可扩展性。2. SSLP框架支持模块化分析和精确重建，为解决复杂量子问题提供了新思路。3. 结合人在环的设计，增强了方法的灵活性和可靠性。</p>
<p>Abstract: We present a multi-agent, human-in-the-loop workflow that co-designs quantum codes with prescribed transversal diagonal gates. It builds on the Subset-Sum Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL) equalities via small LPs. The workflow is powered by GPT-5 and implemented within TeXRA (<a target="_blank" rel="noopener" href="https://texra.ai)-a/">https://texra.ai)-a</a> multi-agent research assistant platform that supports an iterative tool-use loop agent and a derivation-then-edit workflow reasoning agent. We work in a LaTeX-Python environment where agents reason, edit documents, execute code, and synchronize their work to Git&#x2F;Overleaf. Within this workspace, three roles collaborate: a Synthesis Agent formulates the problem; a Search Agent sweeps&#x2F;screens candidates and exactifies numerics into rationals; and an Audit Agent independently checks all KL equalities and the induced logical action. As a first step we focus on distance $d&#x3D;2$ with nondegenerate residues. For code dimension $K\in{2,3,4}$ and $n\le6$ qubits, systematic sweeps yield certificate-backed tables cataloging attainable cyclic logical groups-all realized by new codes-e.g., for $K&#x3D;3$ we obtain order $16$ at $n&#x3D;6$. From verified instances, Synthesis Agent abstracts recurring structures into closed-form families and proves they satisfy the KL equalities for all parameters. It further demonstrates that SSLP accommodates residue degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts diagonal-transversal feasibility as an analytical pipeline executed at scale, combining systematic enumeration with exact analytical reconstruction. It yields reproducible code constructions, supports targeted extensions to larger $K$ and higher distances, and leads toward data-driven classification.</p>
</details>


<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="113-Branch-and-Browse-Efficient-and-Controllable-Web-Exploration-with-Tree-Structured-Reasoning-and-Action-Memory"><a href="#113-Branch-and-Browse-Efficient-and-Controllable-Web-Exploration-with-Tree-Structured-Reasoning-and-Action-Memory" class="headerlink" title="[113] Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.19838">Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory</a></h3><p><em>Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 本文提出了一种名为Branch-and-Browse的网页探索框架，通过树状结构推理和动作记忆实现了高效且可控的网页探索，显著提升了任务的完成率和执行效率。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的自主网页代理方法在多步推理和回溯方面表现不佳，且计算成本较高。为了解决这些问题，作者提出了Branch-and-Browse框架。

<p>Contribution: 1）引入了树状结构的多分支推理和管理机制；2）通过高效的状态重放和后台推理来引导探索；3）利用页面动作记忆实现了跨会话的动作共享。</p>
<p>Method: 采用树状结构的子任务管理方法，结合网页状态重放和后台推理，实现了高效的多分支推理和可控的网络探索。</p>
<p>Result: 在WebArena基准测试中，任务完成率达35.8%，并将执行时间减少了40.4%，优于现有方法。</p>
<p>Insight: 树状结构的推理和动作记忆是提升网页代理任务效率和可控性的有效手段。</p>
<p>Abstract: Autonomous web agents powered by large language models (LLMs) show strong potential for performing goal-oriented tasks such as information retrieval, report generation, and online transactions. These agents mark a key step toward practical embodied reasoning in open web environments. However, existing approaches remain limited in reasoning depth and efficiency: vanilla linear methods fail at multi-step reasoning and lack effective backtracking, while other search strategies are coarse-grained and computationally costly. We introduce Branch-and-Browse, a fine-grained web agent framework that unifies structured reasoning-acting, contextual memory, and efficient execution. It (i) employs explicit subtask management with tree-structured exploration for controllable multi-branch reasoning, (ii) bootstraps exploration through efficient web state replay with background reasoning, and (iii) leverages a page action memory to share explored actions within and across sessions. On the WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8% and reduces execution time by up to 40.4% relative to state-of-the-art methods. These results demonstrate that Branch-and-Browse is a reliable and efficient framework for LLM-based web agents.</p>
</details>


<h3 id="114-AI-PB-A-Grounded-Generative-Agent-for-Personalized-Investment-Insights"><a href="#114-AI-PB-A-Grounded-Generative-Agent-for-Personalized-Investment-Insights" class="headerlink" title="[114] AI PB: A Grounded Generative Agent for Personalized Investment Insights"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20099">AI PB: A Grounded Generative Agent for Personalized Investment Insights</a></h3><p><em>Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: AI PB是一个面向零售金融的生成式智能体，通过确定性路由、混合检索和多阶段推荐机制，主动生成用户特定的投资建议，并在高风险的金融环境中实现可信赖的AI输出。</p>
<details>
  <summary>Details</summary>
Motivation: 传统被动应答的聊天机器人无法满足高风险金融领域对合规性和用户个性化的需求，因此需要一种能够主动生成可信建议的智能体。

<p>Contribution: 提出了一种结合确定性路由、混合检索和多阶段推荐机制的生产级生成式智能体，专为高风险金融场景设计。</p>
<p>Method: 使用基于组件的编排层（路由内部和外部LLM）、混合检索管道（OpenSearch和金融领域嵌入模型）和多阶段推荐机制（规则启发式、序列行为建模和上下文老虎机）。</p>
<p>Result: 系统在韩国金融法规下运行，通过人类QA和系统指标验证了其在生成可信投资建议方面的有效性。</p>
<p>Insight: 在高风险领域，显式路由和多层次安全措施是实现可信赖生成式AI的关键。</p>
<p>Abstract: We present AI PB, a production-scale generative agent deployed in real retail finance. Unlike reactive chatbots that answer queries passively, AI PB proactively generates grounded, compliant, and user-specific investment insights. It integrates (i) a component-based orchestration layer that deterministically routes between internal and external LLMs based on data sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the finance-domain embedding model, and (iii) a multi-stage recommendation mechanism combining rule heuristics, sequential behavioral modeling, and contextual bandits. Operating fully on-premises under Korean financial regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100 GPUs. Through human QA and system metrics, we demonstrate that grounded generation with explicit routing and layered safety can deliver trustworthy AI insights in high-stakes finance.</p>
</details>


<h3 id="115-What-Defines-Good-Reasoning-in-LLMs-Dissecting-Reasoning-Steps-with-Multi-Aspect-Evaluation"><a href="#115-What-Defines-Good-Reasoning-in-LLMs-Dissecting-Reasoning-Steps-with-Multi-Aspect-Evaluation" class="headerlink" title="[115] What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20603">What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation</a></h3><p><em>Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 该论文提出了一种更细粒度的评估大语言模型（LLM）推理质量的方法，将推理分解为相关性和连贯性两个维度，并引入因果逐步评估（CaSE）方法以避免后见偏差。验证表明，该方法不仅能更准确地评估推理质量，还能通过优化训练数据提升任务表现。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估主要关注最终答案的正确性，但忽略了推理过程的质量。这种粗粒度的评估限制了模型的改进能力。

<p>Contribution: 1. 提出了分解推理质量的框架（相关性和连贯性）；2. 开发了CaSE方法以避免后见偏差；3. 验证了该方法在评估和优化训练数据上的有效性。</p>
<p>Method: 采用因果逐步评估（CaSE），对每个推理步骤仅使用其前文进行评估，避免了后见偏差。通过新标注的数据集（MRa-GSM8K和MRa-MATH）验证该方法。</p>
<p>Result: CaSE方法在评估推理质量上与人类标注一致，且优化训练数据后显著提升了任务的最终表现。</p>
<p>Insight: 细粒度的推理评估不仅有助于分析和调试LLM，还能通过数据优化直接提升模型性能，展示了超越单纯正确性检查的实用价值。</p>
<p>Abstract: Evaluating large language models (LLMs) on final-answer correctness is the dominant paradigm. This approach, however, provides a coarse signal for model improvement and overlooks the quality of the underlying reasoning process. We argue that a more granular evaluation of reasoning offers a more effective path to building robust models. We decompose reasoning quality into two dimensions: relevance and coherence. Relevance measures if a step is grounded in the problem; coherence measures if it follows logically from prior steps. To measure these aspects reliably, we introduce causal stepwise evaluation (CaSE). This method assesses each reasoning step using only its preceding context, which avoids hindsight bias. We validate CaSE against human judgments on our new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we show that curating training data with CaSE-evaluated relevance and coherence directly improves final task performance. Our work provides a scalable framework for analyzing, debugging, and improving LLM reasoning, demonstrating the practical value of moving beyond validity checks.</p>
</details>


<h3 id="116-Real-Deep-Research-for-AI-Robotics-and-Beyond"><a href="#116-Real-Deep-Research-for-AI-Robotics-and-Beyond" class="headerlink" title="[116] Real Deep Research for AI, Robotics and Beyond"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.20809">Real Deep Research for AI, Robotics and Beyond</a></h3><p><em>Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang</em></p>
<p>Main category: cs.AI</p>
<p>TL;DR: 为解决AI和机器人领域研究快速增长带来的信息过载问题，本文提出了Real Deep Research（RDR）框架，用于系统分析研究趋势、跨领域机会，并提供新研究的起点。</p>
<details>
  <summary>Details</summary>
Motivation: AI和机器人领域的研究每年超过1万篇，快速发展的趋势和跨学科需求使研究者难以跟上最新进展。

<p>Contribution: 提出了通用的RDR框架，用于系统分析研究趋势、识别跨领域机会，并为新研究提供具体起点。</p>
<p>Method: 构建了RDR管道，专注于基础模型和机器人技术进步，并将其扩展到其他科学领域。</p>
<p>Result: 附录提供了广泛的分析结果，展示了RDR在不同主题中的应用效果。</p>
<p>Insight: 该框架可帮助研究者更高效地追踪新兴趋势和跨学科机会，促进AI和其他领域的创新。</p>
<p>Abstract: With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one’s expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.</p>
</details>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-10-28/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-10-24/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
