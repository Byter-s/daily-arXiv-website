<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-11-08 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-11-08
            
            
        </div>
        <span class="post-date">
            Nov 8, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 50]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 26]</li>
<li><a href="#cs.MA">cs.MA</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 2]</li>
<li><a href="#cs.SD">cs.SD</a> [Total: 1]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 1]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 5]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 1]</li>
<li><a href="#eess.SY">eess.SY</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 5]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-SILVI-Simple-Interface-for-Labeling-Video-Interactions-cs-CV-q-bio-QMPDF"><a href="#1-SILVI-Simple-Interface-for-Labeling-Video-Interactions-cs-CV-q-bio-QMPDF" class="headerlink" title="[1] SILVI: Simple Interface for Labeling Video Interactions cs.CV | q-bio.QMPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03819">SILVI: Simple Interface for Labeling Video Interactions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | q-bio.QM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03819" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ozan Kanbertay, Richard Vogg, Elif Karakoc, Peter M. Kappeler, Claudia Fichtel</span></p>
<p><strong>TL;DR:</strong> SILVI是一款开源的视频标注工具，专注于标注视频中的行为和个体互动，填补了现有工具无法同时支持行为标注和个体定位的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前计算机视觉方法在大规模视频数据分析中主要关注个体行为检测，而缺乏对互动的标注支持，这对于理解社会化和个体化动物行为至关重要。</p>
<p><strong>Result:</strong> SILVI为生态行为学和计算机视觉的交叉研究提供了工具支持，同时也可用于其他需要动态场景图标注的领域。</p>
<p><strong>Insight:</strong> SILVI的设计理念强调了行为互动的动态性，为细粒度行为分析提供了新工具，其开源特性也促进了更广泛的应用。</p>
<p><strong>Abstract:</strong> Computer vision methods are increasingly used for the automated analysis of large volumes of video data collected through camera traps, drones, or direct observations of animals in the wild. While recent advances have focused primarily on detecting individual actions, much less work has addressed the detection and annotation of interactions – a crucial aspect for understanding social and individualized animal behavior. Existing open-source annotation tools support either behavioral labeling without localization of individuals, or localization without the capacity to capture interactions. To bridge this gap, we present SILVI, an open-source labeling software that integrates both functionalities. SILVI enables researchers to annotate behaviors and interactions directly within video data, generating structured outputs suitable for training and validating computer vision models. By linking behavioral ecology with computer vision, SILVI facilitates the development of automated approaches for fine-grained behavioral analyses. Although developed primarily in the context of animal behavior, SILVI could be useful more broadly to annotate human interactions in other videos that require extracting dynamic scene graphs. The software, along with documentation and download instructions, is available at: <a target="_blank" rel="noopener" href="https://gitlab.gwdg.de/kanbertay/interaction-labelling-app">https://gitlab.gwdg.de/kanbertay/interaction-labelling-app</a>.</p>
  </div>
</details>

<hr>
<h3 id="2-Noise-Injection-Improving-Out-of-Distribution-Generalization-for-Limited-Size-Datasets-cs-CV-cs-AIPDF"><a href="#2-Noise-Injection-Improving-Out-of-Distribution-Generalization-for-Limited-Size-Datasets-cs-CV-cs-AIPDF" class="headerlink" title="[2] Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets cs.CV | cs.AIPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03855">Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03855" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Duong Mai, Lawrence Hall</span></p>
<p><strong>TL;DR:</strong> 本文研究了通过噪声注入技术（如高斯、散斑、泊松和椒盐噪声）提升深度学习模型在有限数据集上的OOD泛化能力，显著缩小了ID与OOD性能差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度学习模型在图像识别中因利用数据集中源特定的捷径特征（如设备或人群相关伪影）而非合理生物标志物，导致在OOD数据上表现不佳，特别是在COVID-19胸部X光检测任务中。本文旨在通过噪声注入增强模型对分布偏移的鲁棒性。</p>
<p><strong>Result:</strong> 噪声注入技术显著缩小了ID与OOD的性能差距，平均性能差距从0.10-0.20降至0.01-0.06。</p>
<p><strong>Insight:</strong> 噪声注入通过干扰模型对捷径特征的依赖，迫使模型学习更具泛化能力的生物标志物，从而提升OOD性能。这一方法简单有效，适用于数据规模有限的任务。</p>
<p><strong>Abstract:</strong> Deep learned (DL) models for image recognition have been shown to fail to generalize to data from different devices, populations, etc. COVID-19 detection from Chest X-rays (CXRs), in particular, has been shown to fail to generalize to out-of-distribution (OOD) data from new clinical sources not covered in the training set. This occurs because models learn to exploit shortcuts - source-specific artifacts that do not translate to new distributions - rather than reasonable biomarkers to maximize performance on in-distribution (ID) data. Rendering the models more robust to distribution shifts, our study investigates the use of fundamental noise injection techniques (Gaussian, Speckle, Poisson, and Salt and Pepper) during training. Our empirical results demonstrate that this technique can significantly reduce the performance gap between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results averaged over ten random seeds across key metrics such as AUC, F1, accuracy, recall and specificity. Our source code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/Duongmai127/Noisy-ood">https://github.com/Duongmai127/Noisy-ood</a></p>
  </div>
</details>

<hr>
<h3 id="3-Investigating-Robot-Control-Policy-Learning-for-Autonomous-X-ray-guided-Spine-Procedures-cs-CV-cs-AI-cs-LG-cs-ROPDF"><a href="#3-Investigating-Robot-Control-Policy-Learning-for-Autonomous-X-ray-guided-Spine-Procedures-cs-CV-cs-AI-cs-LG-cs-ROPDF" class="headerlink" title="[3] Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures cs.CV | cs.AI | cs.LG | cs.ROPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03882">Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03882" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Florence Klitzner, Blanca Inigo, Benjamin D. Killeen, Lalithkumar Seenivasan, Michelle Song</span></p>
<p><strong>TL;DR:</strong> 本研究探讨了模仿学习在X射线引导下脊柱手术机器人控制中的应用，开发了一个高仿真模拟环境，并通过实验验证了该方法的可行性与局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 模仿学习在视频机器人领域受到关注，但尚未在X射线引导的复杂手术（如脊柱手术）中得到验证。研究旨在探索其在这一领域的适用性。</p>
<p><strong>Result:</strong> 策略在68.5%的首次尝试中成功保持了安全的脊柱内轨迹，并能泛化到复杂解剖结构和不同初始条件。</p>
<p><strong>Insight:</strong> 模仿学习在X射线引导手术中具有一定潜力，但需解决入口点精度和闭环控制反馈频率等问题。</p>
<p><strong>Abstract:</strong> Imitation learning-based robot control policies are enjoying renewed interest in video-based robotics. However, it remains unclear whether this approach applies to X-ray-guided procedures, such as spine instrumentation. This is because interpretation of multi-view X-rays is complex. We examine opportunities and challenges for imitation policy learning in bi-plane-guided cannula insertion. We develop an in silico sandbox for scalable, automated simulation of X-ray-guided spine procedures with a high degree of realism. We curate a dataset of correct trajectories and corresponding bi-planar X-ray sequences that emulate the stepwise alignment of providers. We then train imitation learning policies for planning and open-loop control that iteratively align a cannula solely based on visual information. This precisely controlled setup offers insights into limitations and capabilities of this method. Our policy succeeded on the first attempt in 68.5% of cases, maintaining safe intra-pedicular trajectories across diverse vertebral levels. The policy generalized to complex anatomy, including fractures, and remained robust to varied initializations. Rollouts on real bi-planar X-rays further suggest that the model can produce plausible trajectories, despite training exclusively in simulation. While these preliminary results are promising, we also identify limitations, especially in entry point precision. Full closed-look control will require additional considerations around how to provide sufficiently frequent feedback. With more robust priors and domain knowledge, such models may provide a foundation for future efforts toward lightweight and CT-free robotic intra-operative spinal navigation.</p>
  </div>
</details>

<hr>
<h3 id="4-Desert-Waste-Detection-and-Classification-Using-Data-Based-and-Model-Based-Enhanced-YOLOv12-DL-Model-cs-CV-cs-LGPDF"><a href="#4-Desert-Waste-Detection-and-Classification-Using-Data-Based-and-Model-Based-Enhanced-YOLOv12-DL-Model-cs-CV-cs-LGPDF" class="headerlink" title="[4] Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model cs.CV | cs.LGPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03888">Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03888" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abdulmumin Sa’ad, Sulaimon Oyeniyi Adebayo, Abdul Jabbar Siddiqui</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于YOLOv12的增强型实时目标检测框架，用于沙漠环境中的垃圾检测与分类，结合了数据增强和模型优化的方法，显著提升了检测精度和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 全球废物危机日益严重，但传统废物收集方法在偏远地区（如沙漠）效率低下且危险。现有计算机视觉研究多集中于城市环境，忽视了沙漠等特殊地形和有机&#x2F;危险废物的检测。</p>
<p><strong>Result:</strong> 实验结果表明，该模型在精度、召回率和mAP上均有显著提升，同时保持了低延迟和小模型尺寸，适合部署在资源受限的无人机上。</p>
<p><strong>Insight:</strong> 结合数据中心和模型中心的优化方法，可以在复杂环境中实现高效且鲁棒的实时检测，为类似场景提供了实用解决方案。</p>
<p><strong>Abstract:</strong> The global waste crisis is escalating, with solid waste generation expected to increase by 70% by 2050. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking our model against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.</p>
  </div>
</details>

<hr>
<h3 id="5-I-Detect-What-I-Don’t-Know-Incremental-Anomaly-Learning-with-Stochastic-Weight-Averaging-Gaussian-for-Oracle-Free-Medical-Imaging-cs-CV-cs-AIPDF"><a href="#5-I-Detect-What-I-Don’t-Know-Incremental-Anomaly-Learning-with-Stochastic-Weight-Averaging-Gaussian-for-Oracle-Free-Medical-Imaging-cs-CV-cs-AIPDF" class="headerlink" title="[5] I Detect What I Don’t Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging cs.CV | cs.AIPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03912">I Detect What I Don’t Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03912" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nand Kumar Yadav, Rodrigue Rizk, William CW Chen, KC Santosh</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无监督、无标注的增量异常学习框架，通过轻量级适配器和不确定性门控扩展正常样本集，在医学影像中实现高效的异常检测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学影像中未知异常的检测面临标注稀缺和专家监督成本高的问题，亟需一种无需异常标注的自适应方法。</p>
<p><strong>Result:</strong> 在多个医学影像数据集（COVID-CXR、Pneumonia CXR、Brain MRI ND-5）上显著提升了异常检测性能（如ROC-AUC从0.9489提升至0.9982）。</p>
<p><strong>Insight:</strong> 通过适配器和不确定性门控的结合，能够在无标注情况下高效扩展正常样本库，为医学影像中的异常检测提供实用解决方案。</p>
<p><strong>Abstract:</strong> Unknown anomaly detection in medical imaging remains a fundamental challenge due to the scarcity of labeled anomalies and the high cost of expert supervision. We introduce an unsupervised, oracle-free framework that incrementally expands a trusted set of normal samples without any anomaly labels. Starting from a small, verified seed of normal images, our method alternates between lightweight adapter updates and uncertainty-gated sample admission. A frozen pretrained vision backbone is augmented with tiny convolutional adapters, ensuring rapid domain adaptation with negligible computational overhead. Extracted embeddings are stored in a compact coreset enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during incremental expansion is enforced by dual probabilistic gates, a sample is admitted into the normal memory only if its distance to the existing coreset lies within a calibrated z-score threshold, and its SWAG-based epistemic uncertainty remains below a seed-calibrated bound. This mechanism prevents drift and false inclusions without relying on generative reconstruction or replay buffers. Empirically, our system steadily refines the notion of normality as unlabeled data arrive, producing substantial gains over baselines. On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5, ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These results highlight the effectiveness and efficiency of the proposed framework for real-world, label-scarce medical imaging applications.</p>
  </div>
</details>

<hr>
<h3 id="6-Adaptive-Temporal-Refinement-Continuous-Depth-Allocation-and-Distance-Regression-for-Efficient-Action-Localization-cs-CVPDF"><a href="#6-Adaptive-Temporal-Refinement-Continuous-Depth-Allocation-and-Distance-Regression-for-Efficient-Action-Localization-cs-CVPDF" class="headerlink" title="[6] Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization cs.CVPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03943">Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03943" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma</span></p>
<p><strong>TL;DR:</strong> 论文提出两种改进时序动作定位的方法：边界距离回归（BDR）和自适应时序细化（ATR），分别通过距离回归和动态计算分配提升定位精度与效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在处理时序动作边界时采用统一计算，忽视了不同边界的难度差异，导致效率低下。本文旨在通过自适应计算分配和更优的边界回归方法解决这一问题。</p>
<p><strong>Result:</strong> 在THUMOS14上，ATR以162G FLOPs达到56.5% <a href="mailto:&#x6d;&#65;&#80;&#64;&#x30;&#46;&#55;">mAP@0.7</a>，较统一计算（198G FLOPs，53.6%）提升2.9%且计算量减少18%。BDR可无缝适配现有方法，带来1.8-3.1%的mAP提升。</p>
<p><strong>Insight:</strong> 通过自适应计算分配和距离回归，显著提高了时序动作定位的效率与精度，短动作性能提升尤为明显（4.2%）。知识蒸馏进一步降低了训练成本。</p>
<p><strong>Abstract:</strong> Temporal action localization requires precise boundary detection; however, current methods apply uniform computation despite significant variations in difficulty across boundaries. We present two complementary contributions. First, Boundary Distance Regression (BDR) provides information-theoretically optimal localization through signed-distance regression rather than classification, achieving 43% sharper boundary peaks. BDR retrofits to existing methods with approximately 50 lines of code, yielding consistent 1.8 to 3.1% <a href="mailto:&#109;&#x41;&#x50;&#64;&#x30;&#x2e;&#x37;">mAP@0.7</a> improvements across diverse architectures. Second, Adaptive Temporal Refinement (ATR) allocates computation via continuous depth selection $\tau \in [0,1]$, enabling end-to-end differentiable optimization without reinforcement learning. On THUMOS14, ATR achieves 56.5% <a href="mailto:&#109;&#x41;&#x50;&#64;&#x30;&#x2e;&#x37;">mAP@0.7</a> at 162G FLOPs, compared to 53.6% at 198G for uniform processing, providing a 2.9% improvement with 18% less compute. Gains scale with boundary heterogeneity, showing 4.2% improvement on short actions. Training cost is mitigated via knowledge distillation, with lightweight students retaining 99% performance at baseline cost. Results are validated across four benchmarks with rigorous statistical testing.</p>
  </div>
</details>

<hr>
<h3 id="7-Room-Envelopes-A-Synthetic-Dataset-for-Indoor-Layout-Reconstruction-from-Images-cs-CVPDF"><a href="#7-Room-Envelopes-A-Synthetic-Dataset-for-Indoor-Layout-Reconstruction-from-Images-cs-CVPDF" class="headerlink" title="[7] Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03970">Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03970" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sam Bahrami, Dylan Campbell</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种合成数据集Room Envelopes，旨在通过RGB图像和两种点图（可见表面和结构布局表面）来促进室内布局重建的研究。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有场景重建方法通常在部分可见表面的情况下重建不完整，忽略了被遮挡的结构元素（如墙壁、地板和天花板）。由于这些元素通常是平面、重复且简单的，因此更容易预测且成本较低。</p>
<p><strong>Result:</strong> 通过该数据集，能够实现对场景范围及其对象形状和位置的更完整理解。</p>
<p><strong>Insight:</strong> 结构元素的重建可以通过低成本方法实现，因为它们通常简单且重复，无需复杂的生成模型。</p>
<p><strong>Abstract:</strong> Modern scene reconstruction methods are able to accurately recover 3D surfaces that are visible in one or more images. However, this leads to incomplete reconstructions, missing all occluded surfaces. While much progress has been made on reconstructing entire objects given partial observations using generative models, the structural elements of a scene, like the walls, floors and ceilings, have received less attention. We argue that these scene elements should be relatively easy to predict, since they are typically planar, repetitive and simple, and so less costly approaches may be suitable. In this work, we present a synthetic dataset – Room Envelopes – that facilitates progress on this task by providing a set of RGB images and two associated pointmaps for each image: one capturing the visible surface and one capturing the first surface once fittings and fixtures are removed, that is, the structural layout. As we show, this enables direct supervision for feed-forward monocular geometry estimators that predict both the first visible surface and the first layout surface. This confers an understanding of the scene’s extent, as well as the shape and location of its objects.</p>
  </div>
</details>

<hr>
<h3 id="8-Simple-3D-Pose-Features-Support-Human-and-Machine-Social-Scene-Understanding-cs-CV-q-bio-NCPDF"><a href="#8-Simple-3D-Pose-Features-Support-Human-and-Machine-Social-Scene-Understanding-cs-CV-q-bio-NCPDF" class="headerlink" title="[8] Simple 3D Pose Features Support Human and Machine Social Scene Understanding cs.CV | q-bio.NCPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03988">Simple 3D Pose Features Support Human and Machine Social Scene Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | q-bio.NC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03988" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenshuo Qin, Leyla Isik</span></p>
<p><strong>TL;DR:</strong> 该论文研究表明，人类社交互动的理解依赖于3D姿态信息，而现有AI视觉模型缺乏这种能力。通过提取3D关节位置和社交姿态特征，可以显著提升AI模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类能快速从视觉输入中提取社交互动信息，但AI系统在此任务上表现不佳。作者假设人类依赖3D姿态信息，而AI模型缺乏这种能力。</p>
<p><strong>Result:</strong> 3D关节位置特征优于现有AI模型，社交姿态特征与完整关节特征预测能力相当，并能显著提升AI模型的性能。</p>
<p><strong>Insight:</strong> 人类社交场景理解依赖于显式的3D姿态表示，简单的结构化视觉空间特征可以支持AI模型更好地匹配人类的社交判断。</p>
<p><strong>Abstract:</strong> Humans can quickly and effortlessly extract a variety of information about others’ social interactions from visual input, ranging from visuospatial cues like whether two people are facing each other to higher-level information. Yet, the computations supporting these abilities remain poorly understood, and social interaction recognition continues to challenge even the most advanced AI vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose information to make social interaction judgments, which is absent in most AI vision models. To test this, we combined state-of-the-art pose and depth estimation algorithms to extract 3D joint positions of people in short video clips depicting everyday human actions and compared their ability to predict human social interaction judgments with current AI vision models. Strikingly, 3D joint positions outperformed most current AI vision models, revealing that key social information is available in explicit body position but not in the learned features of most vision models, including even the layer-wise embeddings of the pose models used to extract joint positions. To uncover the critical pose features humans use to make social judgments, we derived a compact set of 3D social pose features describing only the 3D position and direction of faces in the videos. We found that these minimal descriptors matched the predictive strength of the full set of 3D joints and significantly improved the performance of off-the-shelf AI vision models when combined with their embeddings. Moreover, the degree to which 3D social pose features were represented in each off-the-shelf AI vision model predicted the model’s ability to match human social judgments. Together, our findings provide strong evidence that human social scene understanding relies on explicit representations of 3D pose and can be supported by simple, structured visuospatial primitives.</p>
  </div>
</details>

<hr>
<h3 id="9-CaRF-Enhancing-Multi-View-Consistency-in-Referring-3D-Gaussian-Splatting-Segmentation-cs-CVPDF"><a href="#9-CaRF-Enhancing-Multi-View-Consistency-in-Referring-3D-Gaussian-Splatting-Segmentation-cs-CVPDF" class="headerlink" title="[9] CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation cs.CVPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03992">CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03992" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuwen Tao, Kanglei Zhou, Xin Tan, Yuan Xie</span></p>
<p><strong>TL;DR:</strong> CaRF是一个用于增强3D高斯泼溅分割中多视图一致性的框架，通过引入相机感知的Gaussian Field Camera Encoding（GFCE）和In Training Paired View Supervision（ITPVS），显著提升了跨视图一致性，并在多个基准测试中优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的Referring 3D Gaussian Splatting Segmentation方法依赖2D渲染伪监督和视图特定特征学习，导致跨视图一致性不足。CaRF旨在直接在3D高斯空间中实现多视图一致性和几何推理。</p>
<p><strong>Result:</strong> 在Ref LERF、LERF OVS和3D OVS基准测试中，CaRF的mIoU分别比现有方法平均提升了16.8%、4.3%和2.0%。</p>
<p><strong>Insight:</strong> CaRF通过直接操作3D高斯空间和显式建模视图依赖关系，显著提升了3D场景理解的可靠性和多视图一致性，对嵌入式AI、AR&#x2F;VR交互和自主感知有潜在应用价值。</p>
<p><strong>Abstract:</strong> Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret free-form language expressions and localize the corresponding 3D regions in Gaussian fields. While recent advances have introduced cross-modal alignment between language and 3D geometry, existing pipelines still struggle with cross-view consistency due to their reliance on 2D rendered pseudo supervision and view specific feature learning. In this work, we present Camera Aware Referring Field (CaRF), a fully differentiable framework that operates directly in the 3D Gaussian space and achieves multi view consistency. Specifically, CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates camera geometry into Gaussian text interactions to explicitly model view dependent variations and enhance geometric reasoning. Building on this, In Training Paired View Supervision (ITPVS) is proposed to align per Gaussian logits across calibrated views during training, effectively mitigating single view overfitting and exposing inter view discrepancies for optimization. Extensive experiments on three representative benchmarks demonstrate that CaRF achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively. Moreover, this work promotes more reliable and view consistent 3D scene understanding, with potential benefits for embodied AI, AR&#x2F;VR interaction, and autonomous perception.</p>
  </div>
</details>

<hr>
<h3 id="10-PhysCorr-Dual-Reward-DPO-for-Physics-Constrained-Text-to-Video-Generation-with-Automated-Preference-Selection-cs-CVPDF"><a href="#10-PhysCorr-Dual-Reward-DPO-for-Physics-Constrained-Text-to-Video-Generation-with-Automated-Preference-Selection-cs-CVPDF" class="headerlink" title="[10] PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection cs.CVPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03997">PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03997" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peiyao Wang, Weining Wang, Qi Li</span></p>
<p><strong>TL;DR:</strong> PhysCorr提出了一个统一框架，通过双奖励模型PhysicsRM和优化方法PhyDPO，解决文本到视频生成中物理一致性问题，显著提升生成的物理合理性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文本到视频生成模型在视觉质量上表现优异，但常违反物理规律（如不合理物体动态、不连贯交互），限制了其在机器人、AI等领域的应用。PhysCorr旨在解决这一问题。</p>
<p><strong>Result:</strong> 多基准测试表明，PhysCorr在物理合理性上显著提升，同时保持视觉质量和语义对齐。</p>
<p><strong>Insight:</strong> 通过显式建模物理约束并优化生成过程，PhysCorr为物理可信的视频生成提供了新思路，尤其在具身AI和机器人领域潜力巨大。</p>
<p><strong>Abstract:</strong> Recent advances in text-to-video generation have achieved impressive perceptual quality, yet generated content often violates fundamental principles of physical plausibility - manifesting as implausible object dynamics, incoherent interactions, and unrealistic motion patterns. Such failures hinder the deployment of video generation models in embodied AI, robotics, and simulation-intensive domains. To bridge this gap, we propose PhysCorr, a unified framework for modeling, evaluating, and optimizing physical consistency in video generation. Specifically, we introduce PhysicsRM, the first dual-dimensional reward model that quantifies both intra-object stability and inter-object interactions. On this foundation, we develop PhyDPO, a novel direct preference optimization pipeline that leverages contrastive feedback and physics-aware reweighting to guide generation toward physically coherent outputs. Our approach is model-agnostic and scalable, enabling seamless integration into a wide range of video diffusion and transformer-based backbones. Extensive experiments across multiple benchmarks demonstrate that PhysCorr achieves significant improvements in physical realism while preserving visual fidelity and semantic alignment. This work takes a critical step toward physically grounded and trustworthy video generation.</p>
  </div>
</details>

<hr>
<h3 id="11-GNN-MoE-Context-Aware-Patch-Routing-using-GNNs-for-Parameter-Efficient-Domain-Generalization-cs-CVPDF"><a href="#11-GNN-MoE-Context-Aware-Patch-Routing-using-GNNs-for-Parameter-Efficient-Domain-Generalization-cs-CVPDF" class="headerlink" title="[11] GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization cs.CVPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04008">GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04008" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mahmoud Soliman, Omar Abdelaziz, Ahmed Radwan, Anand, Mohamed Shehata</span></p>
<p><strong>TL;DR:</strong> 提出GNN-MoE框架，结合GNN和MoE技术，通过动态分配图像块（patch）到专家网络，提升ViT在域泛化任务中的参数效率和性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有PEFT方法在域泛化任务中仍面临参数效率低和泛化能力不足的问题，需要更高效的上下文感知机制来适应域偏移。</p>
<p><strong>Result:</strong> 在多个域泛化基准测试中达到SOTA或接近SOTA的性能，同时保持高参数效率。</p>
<p><strong>Insight:</strong> 图神经网络的路由机制可以有效捕捉局部和全局上下文，提升模型对域偏移的鲁棒性。</p>
<p><strong>Abstract:</strong> Domain generalization (DG) seeks robust Vision Transformer (ViT) performance on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging; standard fine-tuning is costly and can impair generalization. We propose GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT, SAGE) operates on inter-patch graphs to dynamically assign patches to specialized experts. This context-aware GNN routing leverages inter-patch relationships for better adaptation to domain shifts. GNN-MoE achieves state-of-the-art or competitive DG benchmark performance with high parameter efficiency, highlighting the utility of graph-based contextual routing for robust, lightweight DG.</p>
  </div>
</details>

<hr>
<h3 id="12-MedDChest-A-Content-Aware-Multimodal-Foundational-Vision-Model-for-Thoracic-Imaging-cs-CVPDF"><a href="#12-MedDChest-A-Content-Aware-Multimodal-Foundational-Vision-Model-for-Thoracic-Imaging-cs-CVPDF" class="headerlink" title="[12] MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging cs.CVPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04016">MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04016" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mahmoud Soliman, Islam Osman, Mohamed S. Shehata, Rasika Rajapakshe</span></p>
<p><strong>TL;DR:</strong> 论文提出了MedDChest，一种专为胸部影像设计的ViT基础模型，通过大规模领域内预训练和内容感知数据增强策略（Guided Random Resized Crops），显著提升了胸部影像诊断任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉模型在医学影像中表现不佳，原因在于通常采用自然图像预训练的骨干网络进行微调，存在领域差距问题。</p>
<p><strong>Result:</strong> 实验表明MedDChest显著优于基于ImageNet预训练的模型，成为胸部诊断任务的更强特征提取器。</p>
<p><strong>Insight:</strong> 领域内大规模预训练结合针对性的数据增强是医学影像分析的更优路径，模型公开可推动后续研究。</p>
<p><strong>Abstract:</strong> The performance of vision models in medical imaging is often hindered by the prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain natural images. To address this fundamental domain gap, we propose MedDChest, a new foundational Vision Transformer (ViT) model optimized specifically for thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated, multimodal dataset of over 1.2 million images, encompassing different modalities including Chest X-ray and Computed Tomography (CT) compiled from 10 public sources. A core technical contribution of our work is Guided Random Resized Crops, a novel content-aware data augmentation strategy that biases sampling towards anatomically relevant regions, overcoming the inefficiency of standard cropping techniques on medical scans. We validate our model’s effectiveness by fine-tuning it on a diverse set of downstream diagnostic tasks. Comprehensive experiments empirically demonstrate that MedDChest significantly outperforms strong, publicly available ImageNet-pretrained models. By establishing the superiority of large-scale, in-domain pre-training combined with domain-specific data augmentation, MedDChest provides a powerful and robust feature extractor that serves as a significantly better starting point for a wide array of thoracic diagnostic tasks. The model weights will be made publicly available to foster future research and applications.</p>
  </div>
</details>

<hr>
<h3 id="13-A-Hybrid-Deep-Learning-Model-for-Robust-Biometric-Authentication-from-Low-Frame-Rate-PPG-Signals-cs-CV-eess-SPPDF"><a href="#13-A-Hybrid-Deep-Learning-Model-for-Robust-Biometric-Authentication-from-Low-Frame-Rate-PPG-Signals-cs-CV-eess-SPPDF" class="headerlink" title="[13] A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals cs.CV | eess.SPPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04037">A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.SP</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04037" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arfina Rahman, Mahesh Banavar</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于低帧率PPG信号的混合深度学习模型CVT-ConvMixer-LSTM，用于鲁棒的生物特征认证，实现了98%的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> PPG信号因非侵入性、实时活体检测和低成本可穿戴设备的适用性而备受关注，但其易受运动伪影和光照变化影响，因此需要鲁棒的特征提取和分类方法。</p>
<p><strong>Result:</strong> 在CFIHSR数据集上，模型达到了98%的认证准确率，表现出对噪声和受试者间差异的鲁棒性。</p>
<p><strong>Insight:</strong> 混合模型通过结合空间和时间特征显著提升了性能，适用于移动和嵌入式设备的生物识别安全应用。</p>
<p><strong>Abstract:</strong> Photoplethysmography (PPG) signals, which measure changes in blood volume in the skin using light, have recently gained attention in biometric authentication because of their non-invasive acquisition, inherent liveness detection, and suitability for low-cost wearable devices. However, PPG signal quality is challenged by motion artifacts, illumination changes, and inter-subject physiological variability, making robust feature extraction and classification crucial. This study proposes a lightweight and cost-effective biometric authentication framework based on PPG signals extracted from low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The raw PPG signals undergo a standard preprocessing pipeline involving baseline drift removal, motion artifact suppression using Principal Component Analysis (PCA), bandpass filtering, Fourier-based resampling, and amplitude normalization. To generate robust representations, each one-dimensional PPG segment is converted into a two-dimensional time-frequency scalogram via the Continuous Wavelet Transform (CWT), effectively capturing transient cardiovascular dynamics. We developed a hybrid deep learning model, termed CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision Transformer (CVT) and ConvMixer branches with temporal features from a Long Short-Term Memory network (LSTM). The experimental results on 46 subjects demonstrate an authentication accuracy of 98%, validating the robustness of the model to noise and variability between subjects. Due to its efficiency, scalability, and inherent liveness detection capability, the proposed system is well-suited for real-world mobile and embedded biometric security applications.</p>
  </div>
</details>

<hr>
<h3 id="14-Unveiling-Deep-Semantic-Uncertainty-Perception-for-Language-Anchored-Multi-modal-Vision-Brain-Alignment-cs-CVPDF"><a href="#14-Unveiling-Deep-Semantic-Uncertainty-Perception-for-Language-Anchored-Multi-modal-Vision-Brain-Alignment-cs-CVPDF" class="headerlink" title="[14] Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment cs.CVPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04078">Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04078" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zehui Feng, Chenqi Zhang, Mingru Wang, Minuo Wei, Shiwei Cheng</span></p>
<p><strong>TL;DR:</strong> Bratrix提出了一种端到端的多模态语言锚定视觉-大脑对齐框架，通过分解视觉刺激为层次化语义组件，并结合不确定性感知模块，显著提升了EEG、MEG和fMRI数据的检索、重建和描述性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法直接将神经活动与视觉嵌入对齐，但仅依赖视觉表示难以捕捉潜在语义维度，限制了模型的解释性和鲁棒性。Bratrix通过语言锚定和不确定性感知解决了这些问题。</p>
<p><strong>Result:</strong> 在EEG、MEG和fMRI基准测试中，Bratrix在检索、重建和描述任务上表现优于现有方法，如在200-way EEG检索任务中提升14.3%。</p>
<p><strong>Insight:</strong> 通过语言锚定和不确定性感知，Bratrix展示了对神经信号中潜在语义的更好捕捉能力，为跨模态对齐提供了新的研究方向。</p>
<p><strong>Abstract:</strong> Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI remains a fundamental challenge due to subject variability and the entangled nature of visual features. Existing approaches primarily align neural activity directly with visual embeddings, but visual-only representations often fail to capture latent semantic dimensions, limiting interpretability and deep robustness. To address these limitations, we propose Bratrix, the first end-to-end framework to achieve multimodal Language-Anchored Vision-Brain alignment. Bratrix decouples visual stimuli into hierarchical visual and linguistic semantic components, and projects both visual and brain representations into a shared latent space, enabling the formation of aligned visual-language and brain-language embeddings. To emulate human-like perceptual reliability and handle noisy neural signals, Bratrix incorporates a novel uncertainty perception module that applies uncertainty-aware weighting during alignment. By leveraging learnable language-anchored semantic matrices to enhance cross-modal correlations and employing a two-stage training strategy of single-modality pretraining followed by multimodal fine-tuning, Bratrix-M improves alignment precision. Extensive experiments on EEG, MEG, and fMRI benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and captioning performance compared to state-of-the-art methods, specifically surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.</p>
  </div>
</details>

<hr>
<h3 id="15-Adversarial-and-Score-Based-CT-Denoising-CycleGAN-vs-Noise2Score-cs-CVPDF"><a href="#15-Adversarial-and-Score-Based-CT-Denoising-CycleGAN-vs-Noise2Score-cs-CVPDF" class="headerlink" title="[15] Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score cs.CVPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04083">Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04083" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abu Hanif Muhammad Syarubany</span></p>
<p><strong>TL;DR:</strong> 该论文比较了CycleGAN和Noise2Score两种方法在CT图像去噪任务中的表现，发现CycleGAN在最终图像质量上更优，而Noise2Score在无配对数据时表现稳健。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究在无配对和自监督情况下CT图像去噪的效果，比较两种高效训练数据的方法，以找到最佳去噪方案。</p>
<p><strong>Result:</strong> CycleGAN将PSNR从34.66 dB提升至38.913 dB，SSIM从0.9234提升至0.971；Noise2Score在噪声较大时表现突出。</p>
<p><strong>Insight:</strong> CycleGAN在图像质量上更优，适合有配对数据的情况；Noise2Score无需配对数据，适合实际应用中的无监督场景。</p>
<p><strong>Abstract:</strong> We study CT image denoising in the unpaired and self-supervised regimes by evaluating two strong, training-data-efficient paradigms: a CycleGAN-based residual translator and a Noise2Score (N2S) score-matching denoiser. Under a common evaluation protocol, a configuration sweep identifies a simple standard U-Net backbone within CycleGAN (lambda_cycle &#x3D; 30, lambda_iden &#x3D; 2, ngf &#x3D; ndf &#x3D; 64) as the most reliable setting; we then train it to convergence with a longer schedule. The selected CycleGAN improves the noisy input from 34.66 dB &#x2F; 0.9234 SSIM to 38.913 dB &#x2F; 0.971 SSIM and attains an estimated score of 1.9441 and an unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly behind in absolute PSNR &#x2F; SSIM, achieves large gains over very noisy inputs, highlighting its utility when clean pairs are unavailable. Overall, CycleGAN offers the strongest final image quality, whereas Noise2Score provides a robust pair-free alternative with competitive performance. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score">https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score</a>.</p>
  </div>
</details>

<hr>
<h3 id="16-When-Swin-Transformer-Meets-KANs-An-Improved-Transformer-Architecture-for-Medical-Image-Segmentation-cs-CVPDF"><a href="#16-When-Swin-Transformer-Meets-KANs-An-Improved-Transformer-Architecture-for-Medical-Image-Segmentation-cs-CVPDF" class="headerlink" title="[16] When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation cs.CVPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04084">When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04084" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nishchal Sapkota, Haoyan Shi, Yejia Zhang, Xianshi Ma, Bofang Zheng</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为UKAST的新型架构，将Swin Transformer与KANs结合，用于医学图像分割，具有更高的数据效率和性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学图像分割面临复杂解剖结构和有限标注数据的挑战，CNN擅长局部特征但缺乏长距离依赖建模，Transformer虽全局有效但数据需求大且计算昂贵。</p>
<p><strong>Result:</strong> 在四个2D和3D医学图像分割基准测试中达到SOTA，尤其在数据稀缺条件下表现优异，计算开销仅有小幅增加。</p>
<p><strong>Insight:</strong> KAN增强的Transformer为数据高效的医学图像分割提供了新方向，缓解了ViT对数据的依赖性问题。</p>
<p><strong>Abstract:</strong> Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/nsapkota417/UKAST">https://github.com/nsapkota417/UKAST</a></p>
  </div>
</details>

<hr>
<h3 id="17-SpatialLock-Precise-Spatial-Control-in-Text-to-Image-Synthesis-cs-CVPDF"><a href="#17-SpatialLock-Precise-Spatial-Control-in-Text-to-Image-Synthesis-cs-CVPDF" class="headerlink" title="[17] SpatialLock: Precise Spatial Control in Text-to-Image Synthesis cs.CVPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04112">SpatialLock: Precise Spatial Control in Text-to-Image Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04112" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Biao Liu, Yuanzhi Liang</span></p>
<p><strong>TL;DR:</strong> SpatialLock是一种新颖的文本到图像生成框架，通过结合感知信号和定位信息，实现了对生成图像中物体位置的精确控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文本到图像生成方法在物体定位上表现不足，难以精确控制空间布局，因此需要一种更有效的方法来提升生成图像的空间精度。</p>
<p><strong>Result:</strong> 在多个数据集上实现了IOU分数超过0.9，达到最先进的物体定位性能。</p>
<p><strong>Insight:</strong> 结合感知信号和定位信息的双重监督机制是提升文本到图像生成空间精度的有效方法。</p>
<p><strong>Abstract:</strong> Text-to-Image (T2I) synthesis has made significant advancements in recent years, driving applications such as generating datasets automatically. However, precise control over object localization in generated images remains a challenge. Existing methods fail to fully utilize positional information, leading to an inadequate understanding of object spatial layouts. To address this issue, we propose SpatialLock, a novel framework that leverages perception signals and grounding information to jointly control the generation of spatial locations. SpatialLock incorporates two components: Position-Engaged Injection (PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial information through an attention layer, encouraging the model to learn the grounding information effectively. PoG employs perception-based supervision to further refine object localization. Together, these components enable the model to generate objects with precise spatial arrangements and improve the visual quality of the generated images. Experiments show that SpatialLock sets a new state-of-the-art for precise object positioning, achieving IOU scores above 0.9 across multiple datasets.</p>
  </div>
</details>

<hr>
<h3 id="18-Tortoise-and-Hare-Guidance-Accelerating-Diffusion-Model-Inference-with-Multirate-Integration-cs-CVPDF"><a href="#18-Tortoise-and-Hare-Guidance-Accelerating-Diffusion-Model-Inference-with-Multirate-Integration-cs-CVPDF" class="headerlink" title="[18] Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration cs.CVPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04117">Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04117" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yunghee Lee, Byeonghyun Pak, Junwha Hong, Hoseong Kim</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种名为“Tortoise and Hare Guidance (THG)”的训练自由策略，通过多速率积分加速扩散模型推理，同时保持高质量生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的扩散模型推理计算成本高，影响了实时应用。通过分析分类器自由引导（CFG）ODE的多速率特性，作者发现额外的引导分支对数值误差更鲁棒，从而揭示了现有方法的冗余。</p>
<p><strong>Result:</strong> THG在相同计算预算下优于现有CFG加速方法，NFE减少30%，生成质量几乎无损（ΔImageReward≤0.032）。</p>
<p><strong>Insight:</strong> 多速率ODE系统为扩散模型提供了一种高效推理的途径，无需重新训练模型即可实现实时高质量图像合成。</p>
<p><strong>Abstract:</strong> In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward $\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/yhlee-add/THG">https://github.com/yhlee-add/THG</a>.</p>
  </div>
</details>

<hr>
<h3 id="19-Text-to-Sketch-Generation-with-Multi-Styles-cs-CVPDF"><a href="#19-Text-to-Sketch-Generation-with-Multi-Styles-cs-CVPDF" class="headerlink" title="[19] Text to Sketch Generation with Multi-Styles cs.CVPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04123">Text to Sketch Generation with Multi-Styles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04123" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tengjie Li, Shikui Tu, Lei Xu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于扩散模型的免训练框架M3S，支持通过文本提示和参考风格草图实现多风格控制，通过线性平滑和风格-内容引导机制减少内容泄漏，提升生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管视觉语言模型在草图生成方面取得进展，但现有方法缺乏对草图风格的精确控制机制。本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，该方法在风格对齐和生成质量上表现优异，尤其在结构和参考草图相似性较低时效果显著。</p>
<p><strong>Insight:</strong> 通过辅助信息和联合AdaIN模块，多风格生成能灵活协调，且避免了传统风格迁移中的内容泄漏问题。</p>
<p><strong>Abstract:</strong> Recent advances in vision-language models have facilitated progress in sketch generation. However, existing specialized methods primarily focus on generic synthesis and lack mechanisms for precise control over sketch styles. In this work, we propose a training-free framework based on diffusion models that enables explicit style guidance via textual prompts and referenced style sketches. Unlike previous style transfer methods that overwrite key and value matrices in self-attention, we incorporate the reference features as auxiliary information with linear smoothing and leverage a style-content guidance mechanism. This design effectively reduces content leakage from reference sketches and enhances synthesis quality, especially in cases with low structural similarity between reference and target sketches. Furthermore, we extend our framework to support controllable multi-style generation by integrating features from multiple reference sketches, coordinated via a joint AdaIN module. Extensive experiments demonstrate that our approach achieves high-quality sketch generation with accurate style alignment and improved flexibility in style control. The official implementation of M3S is available at <a target="_blank" rel="noopener" href="https://github.com/CMACH508/M3S">https://github.com/CMACH508/M3S</a>.</p>
  </div>
</details>

<hr>
<h3 id="20-Automated-Tennis-Player-and-Ball-Tracking-with-Court-Keypoints-Detection-Hawk-Eye-System-cs-CV-cs-AIPDF"><a href="#20-Automated-Tennis-Player-and-Ball-Tracking-with-Court-Keypoints-Detection-Hawk-Eye-System-cs-CV-cs-AIPDF" class="headerlink" title="[20] Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System) cs.CV | cs.AIPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04126">Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04126" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Venkata Manikanta Desu, Syed Fawaz Ali</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种自动化网球比赛分析系统，结合多深度学习模型实时追踪球员和网球，同时检测场地关键点，为比赛提供详细的分析数据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 通过自动化技术提升网球比赛分析的效率和准确性，为教练、转播方和球员提供实时、可操作的见解。</p>
<p><strong>Result:</strong> 实验显示系统在不同场地条件和比赛场景下表现稳健，输出带注释的视频和详细性能指标。</p>
<p><strong>Insight:</strong> 多模型集成是自动化体育分析的有效方法，深度学习可显著提升实时追踪和目标检测的精度。</p>
<p><strong>Abstract:</strong> This study presents a complete pipeline for automated tennis match analysis. Our framework integrates multiple deep learning models to detect and track players and the tennis ball in real time, while also identifying court keypoints for spatial reference. Using YOLOv8 for player detection, a custom-trained YOLOv5 model for ball tracking, and a ResNet50-based architecture for court keypoint detection, our system provides detailed analytics including player movement patterns, ball speed, shot accuracy, and player reaction times. The experimental results demonstrate robust performance in varying court conditions and match scenarios. The model outputs an annotated video along with detailed performance metrics, enabling coaches, broadcasters, and players to gain actionable insights into the dynamics of the game.</p>
  </div>
</details>

<hr>
<h3 id="21-Learning-from-Online-Videos-at-Inference-Time-for-Computer-Use-Agents-cs-CV-cs-AIPDF"><a href="#21-Learning-from-Online-Videos-at-Inference-Time-for-Computer-Use-Agents-cs-CV-cs-AIPDF" class="headerlink" title="[21] Learning from Online Videos at Inference Time for Computer-Use Agents cs.CV | cs.AIPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04137">Learning from Online Videos at Inference Time for Computer-Use Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04137" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujian Liu, Ze Wang, Hao Chen, Ximeng Sun, Xiaodong Yu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种框架，使计算机使用代理能够在推理时从在线视频中学习，通过检索、过滤视频并将其转化为结构化演示轨迹，动态选择轨迹作为上下文指导，从而提升代理性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 计算机使用代理在需要领域特定程序知识的任务中表现不如人类，人类可以通过观看视频教程快速学习。论文旨在探索如何让代理在推理时有效利用在线视频教程。</p>
<p><strong>Result:</strong> 在两个广泛使用的基准测试中，框架显著优于基础代理和仅使用文本教程的变体，表明视频信息的系统提取对代理性能至关重要。</p>
<p><strong>Insight:</strong> 视频轨迹的分割与选择、动作过滤以及视觉信息的重要性表明，在线视频可以被系统转化为可操作的指导，显著提升计算机使用代理的表现。</p>
<p><strong>Abstract:</strong> Computer-use agents can operate computers and automate laborious tasks, but despite recent rapid progress, they still lag behind human users, especially when tasks require domain-specific procedural knowledge about particular applications, platforms, and multi-step workflows. Humans can bridge this gap by watching video tutorials: we search, skim, and selectively imitate short segments that match our current subgoal. In this paper, we study how to enable computer-use agents to learn from online videos at inference time effectively. We propose a framework that retrieves and filters tutorial videos, converts them into structured demonstration trajectories, and dynamically selects trajectories as in-context guidance during execution. Particularly, using a VLM, we infer UI actions, segment videos into short subsequences of actions, and assign each subsequence a textual objective. At inference time, a two-stage selection mechanism dynamically chooses a single trajectory to add in context at each step, focusing the agent on the most helpful local guidance for its next decision. Experiments on two widely used benchmarks show that our framework consistently outperforms strong base agents and variants that use only textual tutorials or transcripts. Analyses highlight the importance of trajectory segmentation and selection, action filtering, and visual information, suggesting that abundant online videos can be systematically distilled into actionable guidance that improves computer-use agents at inference time. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/video_demo">https://github.com/UCSB-NLP-Chang/video_demo</a>.</p>
  </div>
</details>

<hr>
<h3 id="22-Seeing-Straight-Document-Orientation-Detection-for-Efficient-OCR-cs-CV-cs-CLPDF"><a href="#22-Seeing-Straight-Document-Orientation-Detection-for-Efficient-OCR-cs-CV-cs-CLPDF" class="headerlink" title="[22] Seeing Straight: Document Orientation Detection for Efficient OCR cs.CV | cs.CLPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04161">Seeing Straight: Document Orientation Detection for Efficient OCR</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04161" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Suranjan Goswami, Abhinav Ravi, Raja Kolla, Ali Faraz, Shaharukh Khan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了OCR-Rotation-Bench（ORB）基准测试，用于评估OCR对图像旋转的鲁棒性，并开发了一种轻量化的旋转分类方法，显著提高了OCR性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 文档方向检测是OCR预处理中的关键步骤，但现有方法在真实场景中因用户错误（如相机方向错误）表现不佳。</p>
<p><strong>Result:</strong> 在ORB-En和ORB-Indic上分别达到96%和92%的准确率，显著提升OCR性能（闭源模型提升14%，开源模型提升4倍）。</p>
<p><strong>Insight:</strong> 准确的旋转矫正对OCR性能至关重要，轻量化的分类方法在真实场景中具有高效性和鲁棒性。</p>
<p><strong>Abstract:</strong> Despite significant advances in document understanding, determining the correct orientation of scanned or photographed documents remains a critical pre-processing step in the real world settings. Accurate rotation correction is essential for enhancing the performance of downstream tasks such as Optical Character Recognition (OCR) where misalignment commonly arises due to user errors, particularly incorrect base orientations of the camera during capture. In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from rotation-transformed structured and free-form English OCR datasets, and (ii) ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource languages. We also present a fast, robust and lightweight rotation classification pipeline built on the vision encoder of Phi-3.5-Vision model with dynamic image cropping, fine-tuned specifically for 4-class rotation task in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy on identifying the rotations respectively on both the datasets. Beyond classification, we demonstrate the critical role of our module in boosting OCR performance: closed-source (up to 14%) and open-weights models (up to 4x) in the simulated real-world setting.</p>
  </div>
</details>

<hr>
<h3 id="23-Systematic-Evaluation-of-Preprocessing-Techniques-for-Accurate-Image-Registration-in-Digital-Pathology-cs-CV-cs-AIPDF"><a href="#23-Systematic-Evaluation-of-Preprocessing-Techniques-for-Accurate-Image-Registration-in-Digital-Pathology-cs-CV-cs-AIPDF" class="headerlink" title="[23] Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology cs.CV | cs.AIPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04171">Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04171" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fatemehzahra Darzi, Rodrigo Escobar Diaz Guerrero, Thomas Bocklitz</span></p>
<p><strong>TL;DR:</strong> 这篇论文系统地评估了不同预处理技术对数字病理学中多模图像配准的影响，发现CycleGAN颜色变换能显著降低配准误差。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 数字病理学中，不同染色或成像模态的图像配准对于信息整合和分析至关重要，但如何通过预处理技术提高配准精度尚不明确。</p>
<p><strong>Result:</strong> CycleGAN颜色变换在两种测试场景下均表现最佳，显著降低了配准误差（MMrTRE和AMrTRE）。</p>
<p><strong>Insight:</strong> 预处理步骤（特别是颜色变换）在多模态图像配准中起关键作用，选择合适的预处理方法可显著提升配准精度和分析可靠性。</p>
<p><strong>Abstract:</strong> Image registration refers to the process of spatially aligning two or more images by mapping them into a common coordinate system, so that corresponding anatomical or tissue structures are matched across images. In digital pathology, registration enables direct comparison and integration of information from different stains or imaging modalities, sup-porting applications such as biomarker analysis and tissue reconstruction. Accurate registration of images from different modalities is an essential step in digital pathology. In this study, we investigated how various color transformation techniques affect image registration between hematoxylin and eosin (H&amp;E) stained images and non-linear multimodal images. We used a dataset of 20 tissue sample pairs, with each pair undergoing several preprocessing steps, including different color transformation (CycleGAN, Macenko, Reinhard, Vahadane), inversion, contrast adjustment, intensity normalization, and denoising. All images were registered using the VALIS registration method, which first applies rigid registration and then performs non-rigid registration in two steps on both low and high-resolution images. Registration performance was evaluated using the relative Target Registration Error (rTRE). We reported the median of median rTRE values (MMrTRE) and the average of median rTRE values (AMrTRE) for each method. In addition, we performed a custom point-based evaluation using ten manually selected key points. Registration was done separately for two scenarios, using either the original or inverted multimodal images. In both scenarios, CycleGAN color transformation achieved the lowest registration errors, while the other methods showed higher errors. These findings show that applying color transformation before registration improves alignment between images from different modalities and supports more reliable analysis in digital pathology.</p>
  </div>
</details>

<hr>
<h3 id="24-Covariance-Descriptors-Meet-General-Vision-Encoders-Riemannian-Deep-Learning-for-Medical-Image-Classification-cs-CVPDF"><a href="#24-Covariance-Descriptors-Meet-General-Vision-Encoders-Riemannian-Deep-Learning-for-Medical-Image-Classification-cs-CVPDF" class="headerlink" title="[24] Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification cs.CVPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04190">Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04190" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Josef Mayr, Anna Reithmeir, Maxime Di Folco, Julia A. Schnabel</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了协方差描述符（Covariance Descriptors）在医学图像分类中的有效性，结合预训练的通用视觉编码器（GVE）特征，并与手工特征对比，结果显示GVE衍生的协方差描述符性能更优，且与SPDNet结合时效果超越现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 协方差描述符在通用计算机视觉任务中表现优异，但在医学图像分析中研究不足。作者希望验证其在医学图像分类中的潜力，尤其是与预训练通用视觉编码器的结合效果。</p>
<p><strong>Result:</strong> 在MedMNSIT基准测试的11个数据集上，GVE衍生的协方差描述符表现优于手工特征，且SPDNet结合DINOv2特征时性能最佳。</p>
<p><strong>Insight:</strong> 预训练的通用视觉编码器能为医学图像分析提供更强大的特征表示，而协方差描述符与这些特征结合可进一步提升分类性能。</p>
<p><strong>Abstract:</strong> Covariance descriptors capture second-order statistics of image features. They have shown strong performance in general computer vision tasks, but remain underexplored in medical imaging. We investigate their effectiveness for both conventional and learning-based medical image classification, with a particular focus on SPDNet, a classification network specifically designed for symmetric positive definite (SPD) matrices. We propose constructing covariance descriptors from features extracted by pre-trained general vision encoders (GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and MedSAM - are evaluated across eleven binary and multi-class datasets from the MedMNSIT benchmark. Our results show that covariance descriptors derived from GVE features consistently outperform those derived from handcrafted features. Moreover, SPDNet yields superior performance to state-of-the-art methods when combined with DINOv2 features. Our findings highlight the potential of combining covariance descriptors with powerful pretrained vision encoders for medical image analysis.</p>
  </div>
</details>

<hr>
<h3 id="25-MedSapiens-Taking-a-Pose-to-Rethink-Medical-Imaging-Landmark-Detection-cs-CV-cs-AI-cs-LGPDF"><a href="#25-MedSapiens-Taking-a-Pose-to-Rethink-Medical-Imaging-Landmark-Detection-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[25] MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection cs.CV | cs.AI | cs.LGPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04255">MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04255" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marawan Elbatel, Anbang Wang, Keyuan Liu, Kaouther Mouheb, Enrique Almar-Munoz</span></p>
<p><strong>TL;DR:</strong> 该论文提出MedSapiens模型，通过将人体姿态估计的基础模型Sapiens适配到医学影像中的解剖标志点检测任务，取得了优于现有方法的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统医学影像标志点检测依赖领域专用模型，而大规模预训练视觉模型的涌现为跨领域应用提供了新机会。论文探索了人体姿态估计模型在医学领域的潜力。</p>
<p><strong>Result:</strong> MedSapiens在多个数据集上达到新SOTA，分别比通用和专用模型提高5.26%和21.81%的检测成功率（SDR）；在少样本设置中比现有方法高2.69%。</p>
<p><strong>Insight:</strong> 人体姿态优化的基础模型在医学影像任务中具备未被充分利用的潜力，跨领域适配是提升表现的有效途径。</p>
<p><strong>Abstract:</strong> This paper does not introduce a novel architecture; instead, it revisits a fundamental yet overlooked baseline: adapting human-centric foundation models for anatomical landmark detection in medical imaging. While landmark detection has traditionally relied on domain-specific models, the emergence of large-scale pre-trained vision models presents new opportunities. In this study, we investigate the adaptation of Sapiens, a human-centric foundation model designed for pose estimation, to medical imaging through multi-dataset pretraining, establishing a new state of the art across multiple datasets. Our proposed model, MedSapiens, demonstrates that human-centric foundation models, inherently optimized for spatial pose localization, provide strong priors for anatomical landmark detection, yet this potential has remained largely untapped. We benchmark MedSapiens against existing state-of-the-art models, achieving up to 5.26% improvement over generalist models and up to 21.81% improvement over specialist models in the average success detection rate (SDR). To further assess MedSapiens adaptability to novel downstream tasks with few annotations, we evaluate its performance in limited-data settings, achieving 2.69% improvement over the few-shot state of the art in SDR. Code and model weights are available at <a target="_blank" rel="noopener" href="https://github.com/xmed-lab/MedSapiens">https://github.com/xmed-lab/MedSapiens</a> .</p>
  </div>
</details>

<hr>
<h3 id="26-Proto-LeakNet-Towards-Signal-Leak-Aware-Attribution-in-Synthetic-Human-Face-Imagery-cs-CV-cs-AIPDF"><a href="#26-Proto-LeakNet-Towards-Signal-Leak-Aware-Attribution-in-Synthetic-Human-Face-Imagery-cs-CV-cs-AIPDF" class="headerlink" title="[26] Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery cs.CV | cs.AIPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04260">Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04260" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Claudio Giusti, Luca Guarnera, Sebastiano Battiato</span></p>
<p><strong>TL;DR:</strong> Proto-LeakNet提出了一种信号泄漏感知的图像溯源框架，利用扩散模型潜在空间的统计特征，通过部分前向扩散和时序注意力编码器实现高精度分类，同时在未训练生成器上表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着合成图像和深度伪造技术的进步，溯源和真实性验证成为关键挑战。研究发现扩散模型在输出中留下统计痕迹（信号泄漏），这为图像溯源提供了新思路。</p>
<p><strong>Result:</strong> 在闭集数据上训练，Macro AUC达98.13%，对未见生成器表现优异，且对后处理鲁棒。</p>
<p><strong>Insight:</strong> 扩散模型的潜在空间中存在信号泄漏，可通过建模其潜在几何实现可靠且可解释的图像溯源和深度伪造检测。</p>
<p><strong>Abstract:</strong> The growing sophistication of synthetic image and deepfake generation models has turned source attribution and authenticity verification into a critical challenge for modern computer vision systems. Recent studies suggest that diffusion pipelines unintentionally imprint persistent statistical traces, known as signal leaks, within their outputs, particularly in latent representations. Building on this observation, we propose Proto-LeakNet, a signal-leak-aware and interpretable attribution framework that integrates closed-set classification with a density-based open-set evaluation on the learned embeddings, enabling analysis of unseen generators without retraining. Operating in the latent domain of diffusion models, our method re-simulates partial forward diffusion to expose residual generator-specific cues. A temporal attention encoder aggregates multi-step latent features, while a feature-weighted prototype head structures the embedding space and enables transparent attribution. Trained solely on closed data and achieving a Macro AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under post-processing, surpassing state-of-the-art methods, and achieves strong separability between known and unseen generators. These results demonstrate that modeling signal-leak bias in latent space enables reliable and interpretable AI-image and deepfake forensics. The code for the whole work will be available upon submission.</p>
  </div>
</details>

<hr>
<h3 id="27-DINOv2-Driven-Gait-Representation-Learning-for-Video-Based-Visible-Infrared-Person-Re-identification-cs-CVPDF"><a href="#27-DINOv2-Driven-Gait-Representation-Learning-for-Video-Based-Visible-Infrared-Person-Re-identification-cs-CVPDF" class="headerlink" title="[27] DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification cs.CVPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04281">DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04281" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujie Yang, Shuang Li, Jun Ye, Neng Dong, Fan Li</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新的视频基于可见光-红外跨模态行人重识别方法，通过结合DINOv2的视觉先验与步态特征学习，显著提升了模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的行人重识别方法主要关注模态不变的视觉特征，而忽略了步态特征的跨模态不变性和时间动态特性，导致跨模态视频匹配的时空一致性建模不足。</p>
<p><strong>Result:</strong> 在HITSZ-VCM和BUPT数据集上，DinoGRL显著优于现有最优方法。</p>
<p><strong>Insight:</strong> 步态特征具有跨模态不变性和时间动态性，结合语义先验可以显著提升跨模态行人重识别的表现。</p>
<p><strong>Abstract:</strong> Video-based Visible-Infrared person re-identification (VVI-ReID) aims to retrieve the same pedestrian across visible and infrared modalities from video sequences. Existing methods tend to exploit modality-invariant visual features but largely overlook gait features, which are not only modality-invariant but also rich in temporal dynamics, thus limiting their ability to model the spatiotemporal consistency essential for cross-modal video matching. To address these challenges, we propose a DINOv2-Driven Gait Representation Learning (DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn gait features complementary to appearance cues, facilitating robust sequence-level representations for cross-modal retrieval. Specifically, we introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which generates and enhances silhouette representations with general-purpose semantic priors from DINOv2 and jointly optimizes them with the ReID objective to achieve semantically enriched and task-adaptive gait feature learning. Furthermore, we develop a Progressive Bidirectional Multi-Granularity Enhancement (PBMGE) module, which progressively refines feature representations by enabling bidirectional interactions between gait and appearance streams across multiple spatial granularities, fully leveraging their complementarity to enhance global representations with rich local details and produce highly discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets demonstrate the superiority of our approach, significantly outperforming existing state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="28-Vision-Foundation-Models-in-Agriculture-Toward-Domain-Specific-Adaptation-for-Weed-Herbicide-Trials-Assessment-cs-CVPDF"><a href="#28-Vision-Foundation-Models-in-Agriculture-Toward-Domain-Specific-Adaptation-for-Weed-Herbicide-Trials-Assessment-cs-CVPDF" class="headerlink" title="[28] Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment cs.CVPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04288">Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04288" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Leire Benito-Del-Valle, Artzai Picón, Daniel Mugica, Manuel Ramos, Eva Portillo</span></p>
<p><strong>TL;DR:</strong> 该论文通过自监督学习方法，将通用视觉基础模型适应于农业领域的除草剂试验评估，显著提高了物种识别和损伤分类的性能，并在未见条件下展现出更强的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 农业领域的除草剂试验需要精确的植物物种识别和损伤评估，而通用视觉基础模型在农业细粒度任务中表现有限，因此需要针对特定领域进行优化。</p>
<p><strong>Result:</strong> 领域特定模型在物种识别（F1从0.91提升至0.94）和损伤分类（F1从0.26提升至0.33）中显著优于通用模型，且在未见条件下表现更优（物种识别从0.56到0.66；损伤分类从0.17到0.27）。</p>
<p><strong>Insight:</strong> 领域特定的预训练不仅能提升模型性能，还能显著减少标注需求（80%的标注样本节省），为农业领域的自动化解决方案提供了可能性。</p>
<p><strong>Abstract:</strong> Herbicide field trials require accurate identification of plant species and assessment of herbicide-induced damage across diverse environments. While general-purpose vision foundation models have shown promising results in complex visual domains, their performance can be limited in agriculture, where fine-grained distinctions between species and damage types are critical.   In this work, we adapt a general-purpose vision foundation model to herbicide trial characterization. Trained using a self-supervised learning approach on a large, curated agricultural dataset, the model learns rich and transferable representations optimized for herbicide trials images.   Our domain-specific model significantly outperforms the best general-purpose foundation model in both species identification (F1 score improvement from 0.91 to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions (new locations and other time), it achieves even greater gains (species identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In domain-shift scenarios, such as drone imagery, it maintains strong performance (species classification from 0.49 to 0.60).   Additionally, we show that domain-specific pretraining enhances segmentation accuracy, particularly in low-annotation regimes. An annotation-efficiency analysis reveals that, under unseen conditions, the domain-specific model achieves 5.4% higher F1 score than the general-purpose model, while using 80% fewer labeled samples.   These results demonstrate the generalization capabilities of domain-specific foundation models and their potential to significantly reduce manual annotation efforts, offering a scalable and automated solution for herbicide trial analysis.</p>
  </div>
</details>

<hr>
<h3 id="29-Deep-learning-based-object-detection-of-offshore-platforms-on-Sentinel-1-Imagery-and-the-impact-of-synthetic-training-data-cs-CV-cs-AI-eess-IVPDF"><a href="#29-Deep-learning-based-object-detection-of-offshore-platforms-on-Sentinel-1-Imagery-and-the-impact-of-synthetic-training-data-cs-CV-cs-AI-eess-IVPDF" class="headerlink" title="[29] Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data cs.CV | cs.AI | eess.IVPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04304">Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04304" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Robin Spanier, Thorsten Hoeser, Claudia Kuenzer</span></p>
<p><strong>TL;DR:</strong> 论文研究利用合成的训练数据结合真实Sentinel-1卫星图像训练YOLOv10模型，以提升海上平台检测的性能和地理迁移能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 海洋基础设施（如海上风电场、油气平台等）的快速扩张需要有效的监测系统，但由于数据稀缺，特别是对于少数类别和不常见形状的对象，现有模型表现不佳。</p>
<p><strong>Result:</strong> 模型F1分数从0.85提升到0.90，检测到3,529个海上平台，证明了合成数据对不平衡类别和模型性能的提升。</p>
<p><strong>Insight:</strong> 合成数据能够有效解决遥感任务中的数据不平衡问题，支持全球可迁移的检测模型。</p>
<p><strong>Abstract:</strong> The recent and ongoing expansion of marine infrastructure, including offshore wind farms, oil and gas platforms, artificial islands, and aquaculture facilities, highlights the need for effective monitoring systems. The development of robust models for offshore infrastructure detection relies on comprehensive, balanced datasets, but falls short when samples are scarce, particularly for underrepresented object classes, shapes, and sizes. By training deep learning-based YOLOv10 object detection models with a combination of synthetic and real Sentinel-1 satellite imagery acquired in the fourth quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of Guinea, and Coast of Brazil), this study investigates the use of synthetic training data to enhance model performance. We evaluated this approach by applying the model to detect offshore platforms in three unseen regions (Gulf of Mexico, North Sea, Persian Gulf) and thereby assess geographic transferability. This region-holdout evaluation demonstrated that the model generalises beyond the training areas. In total, 3,529 offshore platforms were detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and 1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which improved to 0.90 upon incorporating synthetic data. We analysed how synthetic data enhances the representation of unbalanced classes and overall model performance, taking a first step toward globally transferable detection of offshore infrastructure. This study underscores the importance of balanced datasets and highlights synthetic data generation as an effective strategy to address common challenges in remote sensing, demonstrating the potential of deep learning for scalable, global offshore infrastructure monitoring.</p>
  </div>
</details>

<hr>
<h3 id="30-RISE-T2V-Rephrasing-and-Injecting-Semantics-with-LLM-for-Expansive-Text-to-Video-Generation-cs-CVPDF"><a href="#30-RISE-T2V-Rephrasing-and-Injecting-Semantics-with-LLM-for-Expansive-Text-to-Video-Generation-cs-CVPDF" class="headerlink" title="[30] RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation cs.CVPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04317">RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04317" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangjun Zhang, Litong Gong, Yinglin Zheng, Yansong Liu, Wentao Jiang</span></p>
<p><strong>TL;DR:</strong> RISE-T2V提出了一种将提示重述和语义特征提取集成为一步的新框架，显著提升了文本到视频(T2V)生成的质量和对用户意图的匹配度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有T2V扩散模型依赖预训练文本编码器进行语义对齐，但对简洁提示的理解不足，且无法在线重述提示以更好地匹配用户意图。</p>
<p><strong>Result:</strong> 实验表明RISE-T2V适用于多种视频扩散模型架构，显著提升了生成视频的质量和对用户意图的匹配度。</p>
<p><strong>Insight:</strong> 利用LLM的强大语义理解能力可以弥补T2V模型的不足，提示重述和语义提取的结合是提升性能的关键。</p>
<p><strong>Abstract:</strong> Most text-to-video(T2V) diffusion models depend on pre-trained text encoders for semantic alignment, yet they often fail to maintain video quality when provided with concise prompts rather than well-designed ones. The primary issue lies in their limited textual semantics understanding. Moreover, these text encoders cannot rephrase prompts online to better align with user intentions, which limits both the scalability and usability of the models, To address these challenges, we introduce RISE-T2V, which uniquely integrates the processes of prompt rephrasing and semantic feature extraction into a single and seamless step instead of two separate steps. RISE-T2V is universal and can be applied to various pre-trained LLMs and video diffusion models(VDMs), significantly enhancing their capabilities for T2V tasks. We propose an innovative module called the Rephrasing Adapter, enabling diffusion models to utilize text hidden states during the next token prediction of the LLM as a condition for video generation. By employing a Rephrasing Adapter, the video generation model can implicitly rephrase basic prompts into more comprehensive representations that better match the user’s intent. Furthermore, we leverage the powerful capabilities of LLMs to enable video generation models to accomplish a broader range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a versatile framework applicable to different video diffusion model architectures, significantly enhancing the ability of T2V models to generate high-quality videos that align with user intent. Visual results are available on the webpage at <a target="_blank" rel="noopener" href="https://rise-t2v.github.io/">https://rise-t2v.github.io</a>.</p>
  </div>
</details>

<hr>
<h3 id="31-Comparative-Study-of-CNN-Architectures-for-Binary-Classification-of-Horses-and-Motorcycles-in-the-VOC-2008-Dataset-cs-CVPDF"><a href="#31-Comparative-Study-of-CNN-Architectures-for-Binary-Classification-of-Horses-and-Motorcycles-in-the-VOC-2008-Dataset-cs-CVPDF" class="headerlink" title="[31] Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset cs.CVPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04344">Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04344" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhammad Annas Shaikh, Hamza Zaman, Arbaz Asif</span></p>
<p><strong>TL;DR:</strong> 论文比较了九种CNN架构在VOC 2008数据集上对马和摩托车进行二分类的性能，重点解决了类别不平衡问题，并通过实验验证了数据增强的显著效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是探索不同CNN架构在类别不平衡的二分类任务中的表现，并量化数据增强对性能的影响。</p>
<p><strong>Result:</strong> 结果显示，ConvNeXT-Tiny表现最佳（马的AP为95.53%，摩托车的AP为89.12%），数据增强显著改善了少数类的检测性能，尤其对深层架构效果更明显。</p>
<p><strong>Insight:</strong> 研究发现：1) 数据增强对处理类别不平衡至关重要；2) 架构选择对性能影响显著；3) 深层架构在数据增强下表现更好。</p>
<p><strong>Abstract:</strong> This paper presents a comprehensive evaluation of nine convolutional neural network architectures for binary classification of horses and motorcycles in the VOC 2008 dataset. We address the significant class imbalance problem by implementing minority-class augmentation techniques. Our experiments compare modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and Vision Transformer across multiple performance metrics. Results demonstrate substantial performance variations, with ConvNeXt-Tiny achieving the highest Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle detection. We observe that data augmentation significantly improves minority class detection, particularly benefiting deeper architectures. This study provides insights into architecture selection for imbalanced binary classification tasks and quantifies the impact of data augmentation strategies in mitigating class imbalance issues in object detection.</p>
  </div>
</details>

<hr>
<h3 id="32-Evaluating-the-Impact-of-Weather-Induced-Sensor-Occlusion-on-BEVFusion-for-3D-Object-Detection-cs-CVPDF"><a href="#32-Evaluating-the-Impact-of-Weather-Induced-Sensor-Occlusion-on-BEVFusion-for-3D-Object-Detection-cs-CVPDF" class="headerlink" title="[32] Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection cs.CVPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04347">Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04347" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sanjay Kumar, Tim Brophy, Eoin Martino Grua, Ganesh Sistu, Valentina Donzella</span></p>
<p><strong>TL;DR:</strong> 该论文研究了天气导致的传感器遮挡对BEVFusion架构在3D物体检测中的影响，发现相机遮挡对仅依赖相机的检测影响显著，而LiDAR在重度遮挡下性能下降明显。多传感器融合时，LiDAR的遮挡影响更大。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动车辆在复杂环境中需要精确的3D物体检测，而BEV（鸟瞰图）表示通过多传感器数据融合提供了强大的感知能力。然而，传感器因天气或被遮挡对检测性能的影响尚未充分研究。</p>
<p><strong>Result:</strong> 实验结果显示：1) 相机遮挡导致仅依赖相机的检测mAP下降41.3%；2) LiDAR在重度遮挡下mAP下降47.3%；3) 融合设置中，LiDAR遮挡影响更大（mAP下降26.8%）。</p>
<p><strong>Insight:</strong> 论文揭示了多模态融合中LiDAR的关键作用，并指出未来需开发更鲁棒的传感器融合技术以应对部分传感器失效或环境干扰。</p>
<p><strong>Abstract:</strong> Accurate 3D object detection is essential for automated vehicles to navigate safely in complex real-world environments. Bird’s Eye View (BEV) representations, which project multi-sensor data into a top-down spatial format, have emerged as a powerful approach for robust perception. Although BEV-based fusion architectures have demonstrated strong performance through multimodal integration, the effects of sensor occlusions, caused by environmental conditions such as fog, haze, or physical obstructions, on 3D detection accuracy remain underexplored. In this work, we investigate the impact of occlusions on both camera and Light Detection and Ranging (LiDAR) outputs using the BEVFusion architecture, evaluated on the nuScenes dataset. Detection performance is measured using mean Average Precision (mAP) and the nuScenes Detection Score (NDS). Our results show that moderate camera occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is based only on the camera. On the other hand, LiDAR sharply drops in performance only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%), with a severe impact on long-range detection. In fused settings, the effect depends on which sensor is occluded: occluding the camera leads to a minor 4.1% drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8% drop (to 50.1%), revealing the model’s stronger reliance on LiDAR for the task of 3D object detection. Our results highlight the need for future research into occlusion-aware evaluation methods and improved sensor fusion techniques that can maintain detection accuracy in the presence of partial sensor failure or degradation due to adverse environmental conditions.</p>
  </div>
</details>

<hr>
<h3 id="33-A-MATLAB-tutorial-on-deep-feature-extraction-combined-with-chemometrics-for-analytical-applications-cs-CVPDF"><a href="#33-A-MATLAB-tutorial-on-deep-feature-extraction-combined-with-chemometrics-for-analytical-applications-cs-CVPDF" class="headerlink" title="[33] A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications cs.CVPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04349">A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04349" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Puneet Mishra, Martijntje Vollebregt, Yizhou Ma, Maria Font-i-Furnols</span></p>
<p><strong>TL;DR:</strong> 这篇教程论文旨在为分析化学领域的研究人员提供一个使用MATLAB结合深度学习和化学计量学提取成像数据深度特征的逐步指南，填补了现有深度学习模型在分析化学中应用的结构化指导的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 分析化学中成像数据的空间信息提取和分析仍面临挑战，传统化学计量学方法难以高效处理。尽管深度学习在图像处理方面进展显著，但因缺乏结构化实施指南，其在分析化学中的应用受限。</p>
<p><strong>Result:</strong> 教程提供了可重复的演示代码，研究人员可以按照步骤在自己的数据集上实施深度特征提取。</p>
<p><strong>Insight:</strong> 通过利用现有深度学习模型提取深度特征，分析化学领域可以更高效地处理复杂的成像数据，而无需从头训练模型。</p>
<p><strong>Abstract:</strong> Background In analytical chemistry, spatial information about materials is commonly captured through imaging techniques, such as traditional color cameras or with advanced hyperspectral cameras and microscopes. However, efficiently extracting and analyzing this spatial information for exploratory and predictive purposes remains a challenge, especially when using traditional chemometric methods. Recent advances in deep learning and artificial intelligence have significantly enhanced image processing capabilities, enabling the extraction of multiscale deep features that are otherwise challenging to capture with conventional image processing techniques. Despite the wide availability of open-source deep learning models, adoption in analytical chemistry remains limited because of the absence of structured, step-by-step guidance for implementing these models.   Results This tutorial aims to bridge this gap by providing a step-by-step guide for applying deep learning approaches to extract spatial information from imaging data and integrating it with other data sources, such as spectral information. Importantly, the focus of this work is not on training deep learning models for image processing but on using existing open source models to extract deep features from imaging data.   Significance The tutorial provides MATLAB code tutorial demonstrations, showcasing the processing of imaging data from various imaging modalities commonly encountered in analytical chemistry. Readers must run the tutorial steps on their own datasets using the codes presented in this tutorial.</p>
  </div>
</details>

<hr>
<h3 id="34-Multi-Task-Learning-for-Visually-Grounded-Reasoning-in-Gastrointestinal-VQA-cs-CV-cs-LGPDF"><a href="#34-Multi-Task-Learning-for-Visually-Grounded-Reasoning-in-Gastrointestinal-VQA-cs-CV-cs-LGPDF" class="headerlink" title="[34] Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA cs.CV | cs.LGPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04384">Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04384" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Itbaan Safwan, Muhammad Annas Shaikh, Muhammad Haaris, Ramail Khan, Muhammad Atif Tahir</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于LoRA调优Florence-2模型的多任务学习框架，用于医学视诊问答（VQA）、解释生成和视觉定位，结合了三个数据集实现联合学习，显著优于单任务基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决医学视诊问答任务中单一任务学习的局限性，通过多任务学习提升模型的视觉定位、推理和解释能力，实现更准确和可解释的回答。</p>
<p><strong>Result:</strong> 实验表明，该方法在答案准确性和视觉定位方面显著优于单任务基线。</p>
<p><strong>Insight:</strong> 多任务学习在医学VQA任务中具有显著优势，尤其是结合视觉定位和解释生成时，能够提升模型的解释性和准确性。</p>
<p><strong>Abstract:</strong> We present a multi-task framework for the MediaEval Medico 2025 challenge, leveraging a LoRA-tuned Florence-2 model for simultaneous visual question answering (VQA), explanation generation, and visual grounding. The proposed system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer learning, (2) a synthetically enriched explanation dataset offering structured medical reasoning, and (3) text-to-region pairs linking visual features with segmentation masks. This multi-task setup enables the model to jointly learn visual grounding, reasoning, and interpretation, producing responses that are both accurate and interpretable. Extensive evaluation demonstrates that our approach substantially improves over single-task baselines in both answer accuracy and visual localization, highlighting the effectiveness of grounded multi-task learning for medical VQA applications.</p>
  </div>
</details>

<hr>
<h3 id="35-HideAndSeg-an-AI-based-tool-with-automated-prompting-for-octopus-segmentation-in-natural-habitats-cs-CVPDF"><a href="#35-HideAndSeg-an-AI-based-tool-with-automated-prompting-for-octopus-segmentation-in-natural-habitats-cs-CVPDF" class="headerlink" title="[35] HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats cs.CVPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04426">HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04426" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alan de Aguiar, Michaella Pereira Andrade, Charles Morphy D. Santos, João Paulo Gois</span></p>
<p><strong>TL;DR:</strong> HideAndSeg是一个基于AI的工具，结合了SAM2和YOLOv11，用于自动分割自然栖息地中的章鱼视频。通过自动化提示和无监督指标，减少了人工干预，提高了分割质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 章鱼在自然栖息地中由于其伪装能力、皮肤纹理和颜色的快速变化、非刚性变形和频繁遮挡，分析难度大，且缺乏大规模标注数据集。</p>
<p><strong>Result:</strong> HideAndSeg减少了分割噪声，在完全遮挡的场景下表现优于人工提示方法。</p>
<p><strong>Insight:</strong> 无监督指标可用于指导无真实标签数据下的分割优化，自动化流程显著提升了野外行为研究的效率。</p>
<p><strong>Abstract:</strong> Analyzing octopuses in their natural habitats is challenging due to their camouflage capability, rapid changes in skin texture and color, non-rigid body deformations, and frequent occlusions, all of which are compounded by variable underwater lighting and turbidity. Addressing the lack of large-scale annotated datasets, this paper introduces HideAndSeg, a novel, minimally supervised AI-based tool for segmenting videos of octopuses. It establishes a quantitative baseline for this task. HideAndSeg integrates SAM2 with a custom-trained YOLOv11 object detector. First, the user provides point coordinates to generate the initial segmentation masks with SAM2. These masks serve as training data for the YOLO model. After that, our approach fully automates the pipeline by providing a bounding box prompt to SAM2, eliminating the need for further manual intervention. We introduce two unsupervised metrics - temporal consistency $DICE_t$ and new component count $NC_t$ - to quantitatively evaluate segmentation quality and guide mask refinement in the absence of ground-truth data, i.e., real-world information that serves to train, validate, and test AI models. Results show that HideAndSeg achieves satisfactory performance, reducing segmentation noise compared to the manually prompted approach. Our method can re-identify and segment the octopus even after periods of complete occlusion in natural environments, a scenario in which the manually prompted model fails. By reducing the need for manual analysis in real-world scenarios, this work provides a practical tool that paves the way for more efficient behavioral studies of wild cephalopods.</p>
  </div>
</details>

<hr>
<h3 id="36-Solving-Convex-Partition-Visual-Jigsaw-Puzzles-cs-CVPDF"><a href="#36-Solving-Convex-Partition-Visual-Jigsaw-Puzzles-cs-CVPDF" class="headerlink" title="[36] Solving Convex Partition Visual Jigsaw Puzzles cs.CVPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04450">Solving Convex Partition Visual Jigsaw Puzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04450" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yaniv Ohayon, Ofir Itzhak Shahar, Ohad Ben-Shahar</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种解决凸分割视觉拼图的贪婪求解器，扩展了计算处理的拼图类型，结合了几何和图像兼容性，并提供了一个新的基准数据集。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有拼图求解器主要针对方形拼图，实用性有限。凸分割拼图是主要的多边形拼图子集，但因复杂度高未被充分研究。</p>
<p><strong>Result:</strong> 论文报告了多种性能指标，并提供了新的基准数据集。</p>
<p><strong>Insight:</strong> 凸分割拼图的解决在实际应用中具有潜在影响，尤其是在图像处理和计算机视觉领域。</p>
<p><strong>Abstract:</strong> Jigsaw puzzle solving requires the rearrangement of unordered pieces into their original pose in order to reconstruct a coherent whole, often an image, and is known to be an intractable problem. While the possible impact of automatic puzzle solvers can be disruptive in various application domains, most of the literature has focused on developing solvers for square jigsaw puzzles, severely limiting their practical use. In this work, we significantly expand the types of puzzles handled computationally, focusing on what is known as Convex Partitions, a major subset of polygonal puzzles whose pieces are convex. We utilize both geometrical and pictorial compatibilities, introduce a greedy solver, and report several performance measures next to the first benchmark dataset of such puzzles.</p>
  </div>
</details>

<hr>
<h3 id="37-V-Thinker-Interactive-Thinking-with-Images-cs-CVPDF"><a href="#37-V-Thinker-Interactive-Thinking-with-Images-cs-CVPDF" class="headerlink" title="[37] V-Thinker: Interactive Thinking with Images cs.CVPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04460">V-Thinker: Interactive Thinking with Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04460" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Runqi Qiao, Qiuna Tan, Minghan Yang, Guanting Dong, Peiqing Yang</span></p>
<p><strong>TL;DR:</strong> V-Thinker是一种通用的多模态推理助手，通过端到端强化学习实现视觉中心的交互式思考，解决了现有大型多模态模型（LMMs）在图像交互与长时序推理中的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LMMs在图像交互与长时序推理中的能力不足，且视觉工具空间和任务特定设计限制了其发展。V-Thinker旨在通过强化学习提升模型在交互式视觉思考中的表现。</p>
<p><strong>Result:</strong> V-Thinker在通用和交互式推理任务中表现优于现有LMMs基准模型。</p>
<p><strong>Insight:</strong> 通过合成数据和渐进训练，V-Thinker展示了提升LMMs在视觉交互推理任务中的潜力，为未来研究提供了新方向。</p>
<p><strong>Abstract:</strong> Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising “Thinking with Images” paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.</p>
  </div>
</details>

<hr>
<h3 id="38-Landslide-Hazard-Mapping-with-Geospatial-Foundation-Models-Geographical-Generalizability-Data-Scarcity-and-Band-Adaptability-cs-CVPDF"><a href="#38-Landslide-Hazard-Mapping-with-Geospatial-Foundation-Models-Geographical-Generalizability-Data-Scarcity-and-Band-Adaptability-cs-CVPDF" class="headerlink" title="[38] Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability cs.CVPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04474">Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04474" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenwen Li, Sizhe Wang, Hyunho Lee, Chenyan Lu, Sujit Roy</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个基于地理空间基础模型（GeoFMs）的框架，用于解决滑坡灾害映射中的传感器、标签和领域适应性问题。相比传统深度学习方法，GeoFMs在光谱变化、标签稀缺和跨区域泛化方面表现更优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 滑坡灾害预测和映射的准确性对灾害响应至关重要，但传统深度学习方法在跨传感器、跨区域和训练数据稀缺的情况下表现不佳。本研究旨在通过GeoFMs解决这些问题。</p>
<p><strong>Result:</strong> GeoFMs（如Prithvi-EO-2.0）在滑坡映射任务中优于U-Net、Segformer等传统模型，且在光谱变化、标签稀缺和跨区域泛化中表现更稳健。</p>
<p><strong>Insight:</strong> 地理空间基础模型为灾害映射提供了更鲁棒和可扩展的解决方案，但计算成本和AI-ready数据的不足仍是挑战。</p>
<p><strong>Abstract:</strong> Landslides cause severe damage to lives, infrastructure, and the environment, making accurate and timely mapping essential for disaster preparedness and response. However, conventional deep learning models often struggle when applied across different sensors, regions, or under conditions of limited training data. To address these challenges, we present a three-axis analytical framework of sensor, label, and domain for adapting geospatial foundation models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a series of experiments, we show that it consistently outperforms task-specific CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other GeoFMs (TerraMind, SatMAE). The model, built on global pretraining, self-supervision, and adaptable fine-tuning, proved resilient to spectral variation, maintained accuracy under label scarcity, and generalized more reliably across diverse datasets and geographic settings. Alongside these strengths, we also highlight remaining challenges such as computational cost and the limited availability of reusable AI-ready training data for landslide research. Overall, our study positions GeoFMs as a step toward more robust and scalable approaches for landslide risk reduction and environmental monitoring.</p>
  </div>
</details>

<hr>
<h3 id="39-THEval-Evaluation-Framework-for-Talking-Head-Video-Generation-cs-CVPDF"><a href="#39-THEval-Evaluation-Framework-for-Talking-Head-Video-Generation-cs-CVPDF" class="headerlink" title="[39] THEval. Evaluation Framework for Talking Head Video Generation cs.CVPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04520">THEval. Evaluation Framework for Talking Head Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04520" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nabyl Quignon, Baptiste Chopin, Yaohui Wang, Antitza Dantcheva</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新的评估框架THEval，专注于说话头部视频生成的三个维度（质量、自然度和同步性），包含8个高效且符合人类偏好的指标，填补了当前评估指标的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有说话头部视频生成的评估方法主要依赖有限指标（如视频质量、唇同步）和用户研究，无法全面反映生成视频的表现。作者旨在设计一个更全面的评估框架。</p>
<p><strong>Result:</strong> 实验表明，尽管许多模型在唇同步上表现良好，但在生成表情丰富和无伪影的细节方面仍存在挑战。</p>
<p><strong>Insight:</strong> 现有算法在面部表情和细节生成上仍有改进空间，高效且全面的评估框架对推动领域进展至关重要。</p>
<p><strong>Abstract:</strong> Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.</p>
  </div>
</details>

<hr>
<h3 id="40-Learning-from-Single-Timestamps-Complexity-Estimation-in-Laparoscopic-Cholecystectomy-cs-CVPDF"><a href="#40-Learning-from-Single-Timestamps-Complexity-Estimation-in-Laparoscopic-Cholecystectomy-cs-CVPDF" class="headerlink" title="[40] Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy cs.CVPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04525">Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04525" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dimitrios Anastasiou, Santiago Barbarisi, Lucy Culshaw, Jayna Patel, Evangelos B. Mazomenos</span></p>
<p><strong>TL;DR:</strong> STC-Net是一种新颖的单时间戳复杂性评估框架，用于腹腔镜胆囊切除术（LC），通过Parkland分级量表（PGS）在弱时间监督下实现全视频分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 腹腔镜胆囊切除术中，准确评估手术复杂度对术后分析和培训至关重要。传统PGS虽临床有效，但自动化分析未被充分探索，尤其是在未修剪的全视频场景中。</p>
<p><strong>Result:</strong> 在1859个LC视频中，STC-Net准确率达62.11%，F1分数为61.42%，相比基线提升10%以上。</p>
<p><strong>Insight:</strong> 弱监督方法在手术复杂性评估中有效，STC-Net有望支持术后分析和培训。</p>
<p><strong>Abstract:</strong> Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.</p>
  </div>
</details>

<hr>
<h3 id="41-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm-cs-CV-cs-CLPDF"><a href="#41-Thinking-with-Video-Video-Generation-as-a-Promising-Multimodal-Reasoning-Paradigm-cs-CV-cs-CLPDF" class="headerlink" title="[41] Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm cs.CV | cs.CLPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04570">Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04570" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang</span></p>
<p><strong>TL;DR:</strong> 论文引入了“Thinking with Video”新范式，利用视频生成模型（如Sora-2）结合视觉与文本推理，克服传统多模态推理的局限性。通过VideoThinkBench验证，Sora-2在视觉和文本任务中表现优异，展示视频生成模型作为统一多模态理解与生成模型的潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统“Thinking with Text”和“Thinking with Images”范式无法捕捉动态过程或统一视觉与文本推理。因此，论文提出“Thinking with Video”范式，以视频生成模型解决这些问题。</p>
<p><strong>Result:</strong> Sora-2在视觉任务中与SOTA视觉语言模型相当，部分任务（如Eyeballing Games）表现更优；在文本任务中，MATH准确率达92%，MMMU达75.53%。</p>
<p><strong>Insight:</strong> 视频生成模型有望成为统一多模态理解与生成的核心工具，通过时间动态性弥补静态图像的不足，实现更高效的多模态推理。</p>
<p><strong>Abstract:</strong> “Thinking with Text” and “Thinking with Images” paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce “Thinking with Video”, a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2’s performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions “thinking with video” as a unified multimodal reasoning paradigm.</p>
  </div>
</details>

<hr>
<h3 id="42-UniSplat-Unified-Spatio-Temporal-Fusion-via-3D-Latent-Scaffolds-for-Dynamic-Driving-Scene-Reconstruction-cs-CVPDF"><a href="#42-UniSplat-Unified-Spatio-Temporal-Fusion-via-3D-Latent-Scaffolds-for-Dynamic-Driving-Scene-Reconstruction-cs-CVPDF" class="headerlink" title="[42] UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction cs.CVPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04595">UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04595" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chen Shi, Shaoshuai Shi, Xiaoyang Lyu, Chunyang Liu, Kehua Sheng</span></p>
<p><strong>TL;DR:</strong> UniSplat提出了一种动态驾驶场景重建的统一框架，通过3D潜在支架实现时空信息融合，解决了稀疏、非重叠视角和复杂动态场景的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在稀疏、非重叠视角和复杂动态场景的重建中表现不佳，UniSplat旨在解决这些问题。</p>
<p><strong>Result:</strong> 在真实数据集上的实验表明，UniSplat在新视角合成任务中表现最优，并能提供高质量渲染，即使视角超出原始相机覆盖范围。</p>
<p><strong>Insight:</strong> 通过统一的潜在支架和高效融合机制，UniSplat在动态场景重建中实现了鲁棒性和高精度，展示了时空建模的重要性。</p>
<p><strong>Abstract:</strong> Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.</p>
  </div>
</details>

<hr>
<h3 id="43-PixCLIP-Achieving-Fine-grained-Visual-Language-Understanding-via-Any-granularity-Pixel-Text-Alignment-Learning-cs-CV-cs-MMPDF"><a href="#43-PixCLIP-Achieving-Fine-grained-Visual-Language-Understanding-via-Any-granularity-Pixel-Text-Alignment-Learning-cs-CV-cs-MMPDF" class="headerlink" title="[43] PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning cs.CV | cs.MMPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04601">PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04601" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yicheng Xiao, Yu Chen, Haoxuan Ma, Jiale Hong, Caorui Li</span></p>
<p><strong>TL;DR:</strong> PixCLIP通过像素级对齐学习和长文本处理提升CLIP模型的细粒度视觉语言理解能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管CLIP在多模态任务中表现出色，但其在细粒度图像-文本对齐方面的能力仍有提升空间。现有工作多关注视觉信息的细粒度处理，但CLIP的文本编码器限制了其处理长文本的能力。</p>
<p><strong>Result:</strong> PixCLIP在像素级交互和长文本处理方面取得突破，性能达到SOTA。</p>
<p><strong>Insight:</strong> 视觉和文本信息的细粒度协同处理是提升多模态模型能力的关键。</p>
<p><strong>Abstract:</strong> While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model’s fine-grained vision-language alignment. However, the inherent token length limitation of CLIP’s text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP’s original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.</p>
  </div>
</details>

<hr>
<h3 id="44-NovisVQ-A-Streaming-Convolutional-Neural-Network-for-No-Reference-Opinion-Unaware-Frame-Quality-Assessment-cs-CVPDF"><a href="#44-NovisVQ-A-Streaming-Convolutional-Neural-Network-for-No-Reference-Opinion-Unaware-Frame-Quality-Assessment-cs-CVPDF" class="headerlink" title="[44] NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment cs.CVPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04628">NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04628" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kylie Cancilla, Alexander Moore, Amar Saini, Carmen Carrano</span></p>
<p><strong>TL;DR:</strong> NovisVQ是一个基于流的卷积神经网络，用于无参考且无需人工评分的视频质量评估（VQA）。它通过合成DAVIS数据集的退化视频训练，直接预测FR指标（如LPIPS、PSNR、SSIM），且无需参考视频。该模型在时间建模方面表现优异，超越了图像基线和传统方法BRISQUE。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有VQA方法存在局限性：FR指标需要参考视频，NR方法依赖昂贵的人工评分数据。此外，大多数无视评分的NR方法是图像基础的，忽略了视频任务中关键的时间上下文信息。</p>
<p><strong>Result:</strong> 模型在多种退化案例中表现优于图像基线，且与FR指标的相关性高于传统方法BRISQUE，验证了时间建模的有效性和模型的实用性。</p>
<p><strong>Insight:</strong> 时间建模对视频质量评估至关重要，合成数据训练的无参考方法可以高效替代依赖人工评分的传统方法，适用于实际视觉系统。</p>
<p><strong>Abstract:</strong> Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.</p>
  </div>
</details>

<hr>
<h3 id="45-Polarization-resolved-imaging-improves-eye-tracking-cs-CV-physics-opticsPDF"><a href="#45-Polarization-resolved-imaging-improves-eye-tracking-cs-CV-physics-opticsPDF" class="headerlink" title="[45] Polarization-resolved imaging improves eye tracking cs.CV | physics.opticsPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04652">Polarization-resolved imaging improves eye tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | physics.optics</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04652" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mantas Žurauskas, Tom Bu, Sanaz Alali, Beyza Kalkanli, Derek Shi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于偏振分辨近红外成像的眼球追踪技术（PET），通过测量眼部组织反射光的偏振状态和强度，显著提升了追踪性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统眼球追踪技术仅依赖光强度信息，容易受到眼睑遮挡、瞳孔变化等因素的影响，限制了其鲁棒性和准确性。</p>
<p><strong>Result:</strong> 在346名参与者的实验中，PET系统将95%绝对凝视误差的中位数降低了10-16%，尤其是在遮挡和瞳孔变化等复杂条件下表现更优。</p>
<p><strong>Insight:</strong> 偏振分辨成像为眼球追踪提供了额外的光学对比信息，显著提高了系统的鲁棒性和准确性，有望应用于未来可穿戴设备。</p>
<p><strong>Abstract:</strong> Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization–filter–array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10–16% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light–tissue polarization effects to practical gains in human–computer interaction and position PET as a simple, robust sensing modality for future wearable devices.</p>
  </div>
</details>

<hr>
<h3 id="46-Benchmark-Designers-Should-“Train-on-the-Test-Set”-to-Expose-Exploitable-Non-Visual-Shortcuts-cs-CVPDF"><a href="#46-Benchmark-Designers-Should-“Train-on-the-Test-Set”-to-Expose-Exploitable-Non-Visual-Shortcuts-cs-CVPDF" class="headerlink" title="[46] Benchmark Designers Should “Train on the Test Set” to Expose Exploitable Non-Visual Shortcuts cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04655">Benchmark Designers Should “Train on the Test Set” to Expose Exploitable Non-Visual Shortcuts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04655" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ellis Brown, Jihan Yang, Shusheng Yang, Rob Fergus, Saining Xie</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种诊断和去偏方法，用于揭示和减少多模态大语言模型（MLLMs）基准测试中的非视觉捷径问题，通过‘训练测试集’和‘迭代偏差修剪’技术改进基准设计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的多模态基准测试容易被模型通过非视觉捷径（如语言偏见和表面模式）绕过，导致评估不准确。论文旨在帮助基准设计者提前发现并消除这些捷径。</p>
<p><strong>Result:</strong> 在四个基准测试中发现了普遍的非视觉偏见，并成功创建了去偏版本VSI-Bench-Debiased，减少了非视觉可解性，增大了视觉盲区性能差距。</p>
<p><strong>Insight:</strong> 基准设计者应主动‘攻击’自己的测试集，通过诊断和去偏技术提升评估的鲁棒性；非视觉捷径的揭示对多模态模型的实际能力评估至关重要。</p>
<p><strong>Abstract:</strong> Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to <code>game&#39;&#39; their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly </code>training on the test set’’ – probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a <code>Test-set Stress-Test&#39;&#39; (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an </code>Iterative Bias Pruning’’ (IBP) procedure. Applying this framework to four benchmarks – VSI-Bench, CV-Bench, MMMU, and VideoMME – we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.</p>
  </div>
</details>

<hr>
<h3 id="47-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding-cs-CVPDF"><a href="#47-SIMS-V-Simulated-Instruction-Tuning-for-Spatial-Video-Understanding-cs-CVPDF" class="headerlink" title="[47] SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding cs.CVPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04668">SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04668" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种利用3D模拟器生成空间丰富的视频训练数据的方法SIMS-V，通过系统化的问题类型、混合和规模消融，发现了三种最有效的问题类别，显著提升了多模态语言模型的空间推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管多模态语言模型在高级视频理解方面表现出色，但在跨时空的空间推理上仍有不足，且当前依赖真实视频数据的方法面临数据多样性和精确标注的瓶颈。</p>
<p><strong>Result:</strong> 7B参数的视频LLM在仅25K模拟数据上微调后，性能超过72B基线，并在真实世界空间推理任务中表现出色，同时保持通用视频理解能力。</p>
<p><strong>Insight:</strong> 模拟数据可以高效提升空间推理能力，少量但关键的问题类别比全覆盖更有效，且模拟训练能良好迁移到真实世界任务。</p>
<p><strong>Abstract:</strong> Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V – a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.</p>
  </div>
</details>

<hr>
<h3 id="48-Cambrian-S-Towards-Spatial-Supersensing-in-Video-cs-CVPDF"><a href="#48-Cambrian-S-Towards-Spatial-Supersensing-in-Video-cs-CVPDF" class="headerlink" title="[48] Cambrian-S: Towards Spatial Supersensing in Video cs.CVPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04670">Cambrian-S: Towards Spatial Supersensing in Video</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04670" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang</span></p>
<p><strong>TL;DR:</strong> 论文提出了空间超感知（spatial supersensing）的概念，作为多模态智能发展的新范式，并设计了VSI-SUPER基准测试和Cambrian-S模型以推动这一领域的研究。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的多模态系统主要集中在短上下文和任务驱动的反应式模式，缺乏对持续事件、3D空间认知和世界建模的综合能力。因此，需要一种更广泛的感知范式（即超感知）来推动智能系统的进步。</p>
<p><strong>Result:</strong> Cambrian-S在VSI-Bench上实现了30%的绝对性能提升，但在VSI-SUPER基准上的表现仍有局限。预测感知方法显著优于现有基线模型，表明需要更智能的感知机制来组织经验。</p>
<p><strong>Insight:</strong> 1. 空间超感知需要超越简单的视觉任务，实现持续事件理解和预测能力。2. 数据扩展虽然能提升部分性能，但无法完全解决空间超感知的复杂性。3. 预测感知为模型提供了一种主动组织和筛选信息的新途径。</p>
<p><strong>Abstract:</strong> We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.</p>
  </div>
</details>

<hr>
<h3 id="49-InfinityStar-Unified-Spacetime-AutoRegressive-Modeling-for-Visual-Generation-cs-CVPDF"><a href="#49-InfinityStar-Unified-Spacetime-AutoRegressive-Modeling-for-Visual-Generation-cs-CVPDF" class="headerlink" title="[49] InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation cs.CVPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04675">InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04675" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinlai Liu, Jian Han, Bin Yan, Hui Wu, Fengda Zhu</span></p>
<p><strong>TL;DR:</strong> InfinityStar提出了一种统一的时空自回归框架，支持高分辨率图像和动态视频生成，通过联合建模时空依赖关系，在多种生成任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基于自回归模型在视觉和语言领域的成功，研究旨在开发一个统一的框架，能够高效生成高质量的视频内容，解决现有方法在分辨率和效率上的局限性。</p>
<p><strong>Result:</strong> 在VBench上取得83.74的高分，超过所有自回归模型和部分扩散模型，生成速度比领先的扩散方法快10倍。</p>
<p><strong>Insight:</strong> 统一的时空自回归设计不仅简化了多任务生成流程，还为高效高质量的视频生成提供了新的思路。</p>
<p><strong>Abstract:</strong> We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.</p>
  </div>
</details>

<hr>
<h3 id="50-Tracking-and-Understanding-Object-Transformations-cs-CVPDF"><a href="#50-Tracking-and-Understanding-Object-Transformations-cs-CVPDF" class="headerlink" title="[50] Tracking and Understanding Object Transformations cs.CVPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04678">Tracking and Understanding Object Transformations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04678" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yihong Sun, Xinyu Yang, Jennifer J. Sun, Bharath Hariharan</span></p>
<p><strong>TL;DR:</strong> 论文提出了跟踪物体状态变化的任务（Track Any State），并介绍了TubeletGraph系统和新数据集VOST-TAS。系统通过语义和邻近性先验恢复丢失的物体，生成状态变化图，实现了SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实中的物体经常发生状态变化（如苹果被切块），现有跟踪方法在物体外观显著变化时容易丢失目标。为此，论文提出跟踪和理解物体状态变化的任务。</p>
<p><strong>Result:</strong> TubeletGraph在物体状态变化下的跟踪性能达到SOTA，并展示了在时序定位和语义推理方面的潜力。</p>
<p><strong>Insight:</strong> 结合语义和邻近性先验的图表示方法能有效理解复杂物体状态变化，为零样本场景提供新思路。</p>
<p><strong>Abstract:</strong> Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at <a target="_blank" rel="noopener" href="https://tubelet-graph.github.io/">https://tubelet-graph.github.io</a>.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="51-TextualVerifier-Verify-TextGrad-Step-by-Step-cs-CLPDF"><a href="#51-TextualVerifier-Verify-TextGrad-Step-by-Step-cs-CLPDF" class="headerlink" title="[51] TextualVerifier: Verify TextGrad Step-by-Step cs.CLPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03739">TextualVerifier: Verify TextGrad Step-by-Step</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03739" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eugenius Mario Situmorang, Adila Alfa Krisnadhi, Ari Wibisono</span></p>
<p><strong>TL;DR:</strong> TextualVerifier是一个验证框架，通过链式思维分解、变体生成、多数投票和共识聚合四个阶段，为TextGrad提供自验证机制，显著提升了基于文本的优化推理的可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> TextGrad作为一种基于文本的自动微分方法，缺乏自我验证机制，无法保证文本决策中的推理有效性。因此，本研究旨在填补这一验证空白。</p>
<p><strong>Result:</strong> 实验显示，在PRM800K上推理步骤有效性提升29%；与TextGrad集成后，GPQA-Diamond等基准测试中的准确率显著提升（p &lt;0.001）。</p>
<p><strong>Insight:</strong> 通过LLM技术实现的自验证为文本优化开辟了新方向，验证了其在提升推理可靠性方面的潜力。</p>
<p><strong>Abstract:</strong> TextGrad is a novel approach to text-based automatic differentiation that enables composite AI systems to perform optimization without explicit numerical equations. However, it currently lacks self-verification mechanisms that ensure reasoning validity in text-based decision making. This research introduces TextualVerifier, a verification framework that leverages chain-of-thought reasoning and majority voting with large language models to address this verification gap. TextualVerifier implements a four-stage workflow: chain-of-thought decomposition, variant generation, majority voting, and consensus aggregation. It integrates non-invasively with TextGrad at both the loss function and optimization result verification stages. Experimental evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1) standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically significant improvements (p &lt; 0.001). In phase one, TextualVerifier improves the validity of reasoning steps by 29 percent. In phase two, integration into TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4 percent with a moderate overhead of 5.9 LLM calls on average. Further evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92 percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively. TextualVerifier thus presents the first self-verification framework for TextGrad through LLM-based techniques without requiring numerical gradients, enabling more reliable reasoning and opening new directions for verification in text-based optimization.</p>
  </div>
</details>

<hr>
<h3 id="52-PLLuM-A-Family-of-Polish-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#52-PLLuM-A-Family-of-Polish-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[52] PLLuM: A Family of Polish Large Language Models cs.CL | cs.AIPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03823">PLLuM: A Family of Polish Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03823" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jan Kocoń, Maciej Piasecki, Arkadiusz Janz, Teddy Ferdinan, Łukasz Radliński</span></p>
<p><strong>TL;DR:</strong> PLLuM是首个专门为波兰语设计的大型开源语言模型家族，填补了非英语语言模型的空白，并强调数据治理和AI责任。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大型语言模型主要集中于英语，缺乏对波兰语等语言的高质量支持，PLLuM旨在解决这一问题并推动波兰本土AI技术发展。</p>
<p><strong>Result:</strong> PLLuM在公共管理任务中展示了实用性，并开源以促进波兰AI研究。</p>
<p><strong>Insight:</strong> 多语言模型需关注数据质量和文化相关性，责任AI框架是提升模型透明度和安全性的关键。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) play a central role in modern artificial intelligence, yet their development has been primarily focused on English, resulting in limited support for other languages. We present PLLuM (Polish Large Language Model), the largest open-source family of foundation models tailored specifically for the Polish language. Developed by a consortium of major Polish research institutions, PLLuM addresses the need for high-quality, transparent, and culturally relevant language models beyond the English-centric commercial landscape. We describe the development process, including the construction of a new 140-billion-token Polish text corpus for pre-training, a 77k custom instructions dataset, and a 100k preference optimization dataset. A key component is a Responsible AI framework that incorporates strict data governance and a hybrid module for output correction and safety filtering. We detail the models’ architecture, training procedures, and alignment techniques for both base and instruction-tuned variants, and demonstrate their utility in a downstream task within public administration. By releasing these models publicly, PLLuM aims to foster open research and strengthen sovereign AI technologies in Poland.</p>
  </div>
</details>

<hr>
<h3 id="53-STARS-Segment-level-Token-Alignment-with-Rejection-Sampling-in-Large-Language-Models-cs-CLPDF"><a href="#53-STARS-Segment-level-Token-Alignment-with-Rejection-Sampling-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[53] STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models cs.CLPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03827">STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03827" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammad Atif Quamar, Mohammad Areeb, Mikhail Kuznetsov, Muslum Ozgur Ozmen, Z. Berkay Celik</span></p>
<p><strong>TL;DR:</strong> STARS提出了一种在解码阶段通过分段的令牌对齐和拒绝采样来优化大语言模型生成的算法，显著提高了计算效率和对齐质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型对齐方法（如微调）计算成本高且效果有限，而推断时间方法（如Best-of-N采样）则计算不可行。STARS旨在克服这些限制。</p>
<p><strong>Result:</strong> 实验表明，STARS在六个大语言模型上显著优于监督微调和直接偏好优化，并与Best-of-N基线竞争。</p>
<p><strong>Insight:</strong> STARS证明了细粒度的奖励引导采样是一种通用、鲁棒且高效的对齐替代方案。</p>
<p><strong>Abstract:</strong> Aligning large language models with human values is crucial for their safe deployment; however, existing methods, such as fine-tuning, are computationally expensive and suboptimal. In contrast, inference-time approaches like Best-of-N sampling require practically infeasible computation to achieve optimal alignment. We propose STARS: Segment-level Token Alignment with Rejection Sampling, a decoding-time algorithm that steers model generation by iteratively sampling, scoring, and rejecting&#x2F;accepting short, fixed-size token segments. This allows for early correction of the generation path, significantly improving computational efficiency and boosting alignment quality. Across a suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT) by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up to 4.3 percentage points on win-rates, while remaining highly competitive with strong Best-of-N baselines. Our work establishes granular, reward-guided sampling as a generalizable, robust, and efficient alternative to traditional fine-tuning and full-sequence ranking methods for aligning LLMs.</p>
  </div>
</details>

<hr>
<h3 id="54-Context-informs-pragmatic-interpretation-in-vision-language-models-cs-CLPDF"><a href="#54-Context-informs-pragmatic-interpretation-in-vision-language-models-cs-CLPDF" class="headerlink" title="[54] Context informs pragmatic interpretation in vision-language models cs.CLPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03908">Context informs pragmatic interpretation in vision-language models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03908" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alvin Wei Ming Tan, Ben Prystawski, Veronica Boyce, Michael C. Frank</span></p>
<p><strong>TL;DR:</strong> 论文研究了视觉语言模型在多轮语言环境中进行上下文敏感语用推理的能力，通过迭代参考游戏测试模型和人类的表现。模型在缺乏相关上下文时表现较差，但在上下文相关时表现显著提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是评估视觉语言模型在复杂对话环境中能否像人类一样进行上下文敏感的语用推理，尤其是在多轮迭代参考游戏中。</p>
<p><strong>Result:</strong> 结果显示，模型在缺乏相关上下文时表现较差，但在上下文相关时表现接近人类；抽象指代任务仍然是模型的难点。</p>
<p><strong>Insight:</strong> 研究启示是上下文信息对视觉语言模型的语用推理能力至关重要，未来需要进一步提升模型在抽象指代任务中的表现。</p>
<p><strong>Abstract:</strong> Iterated reference games - in which players repeatedly pick out novel referents using language - present a test case for agents’ ability to perform context-sensitive pragmatic reasoning in multi-turn linguistic environments. We tested humans and vision-language models on trials from iterated reference games, varying the given context in terms of amount, order, and relevance. Without relevant context, models were above chance but substantially worse than humans. However, with relevant context, model performance increased dramatically over trials. Few-shot reference games with abstract referents remain a difficult task for machine learning models.</p>
  </div>
</details>

<hr>
<h3 id="55-The-Human-Flourishing-Geographic-Index-A-County-Level-Dataset-for-the-United-States-2013–2023-cs-CL-cs-CY-stat-APPDF"><a href="#55-The-Human-Flourishing-Geographic-Index-A-County-Level-Dataset-for-the-United-States-2013–2023-cs-CL-cs-CY-stat-APPDF" class="headerlink" title="[55] The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013–2023 cs.CL | cs.CY | stat.APPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03915">The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013–2023</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY | stat.AP</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03915" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Stefano M. Iacus, Devika Jain, Andrea Nasuto, Giuseppe Porro, Marcello Carammia</span></p>
<p><strong>TL;DR:</strong> 文章提出了一个名为‘人类繁荣地理指数’（HFGI）的数据集，通过分析2013-2023年间26亿条美国地理标注推文，并结合精细调优的大语言模型，量化了48项与繁荣相关的指标，为多学科研究提供了高分辨率的社会福祉分析工具。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的人类福祉测量工具通常缺乏精细的空间和时间分辨率，影响了对其动态变化的理解。本文旨在填补这一空白，通过社交媒体数据捕捉多维度的繁荣指标。</p>
<p><strong>Result:</strong> HFGI数据集能够准确反映人类繁荣的多维度指标，并与已有指标表现出预期的相关性，支持高分辨率的福祉动态分析。</p>
<p><strong>Insight:</strong> 社交媒体数据可以成为一种强大的工具，用于量化和分析人类福祉的多维度特征，尤其是在时间和空间上的高分辨率需求场景。</p>
<p><strong>Abstract:</strong> Quantifying human flourishing, a multidimensional construct including happiness, health, purpose, virtue, relationships, and financial stability, is critical for understanding societal well-being beyond economic indicators. Existing measures often lack fine spatial and temporal resolution. Here we introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned large language models to classify expressions across 48 indicators aligned with Harvard’s Global Flourishing Study framework plus attitudes towards migration and perception of corruption. The dataset offers monthly and yearly county- and state-level indicators of flourishing-related discourse, validated to confirm that the measures accurately represent the underlying constructs and show expected correlations with established indicators. This resource enables multidisciplinary analyses of well-being, inequality, and social change at unprecedented resolution, offering insights into the dynamics of human flourishing as reflected in social media discourse across the United States over the past decade.</p>
  </div>
</details>

<hr>
<h3 id="56-Abductive-Inference-in-Retrieval-Augmented-Language-Models-Generating-and-Validating-Missing-Premises-cs-CL-cs-AIPDF"><a href="#56-Abductive-Inference-in-Retrieval-Augmented-Language-Models-Generating-and-Validating-Missing-Premises-cs-CL-cs-AIPDF" class="headerlink" title="[56] Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises cs.CL | cs.AIPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04020">Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04020" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shiyin Lin</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个框架，将溯因推理（abductive inference）引入检索增强的语言模型（RAG），以填补证据不完整导致的推理空白，并通过生成和验证缺失前提来提高答案准确性和推理可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有检索增强生成（RAG）系统在证据不完全时表现不佳，导致推理过程出现空白。为了解决这一问题，论文提出利用溯因推理生成合理的缺失前提，以增强模型的鲁棒性和可解释性。</p>
<p><strong>Result:</strong> 在溯因推理和多跳问答基准测试中，该方法显著提高了答案准确性和推理的可靠性。</p>
<p><strong>Insight:</strong> 溯因推理是增强RAG系统鲁棒性和可解释性的有效途径，尤其是在处理不完整证据时。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) enhanced with retrieval – commonly referred to as Retrieval-Augmented Generation (RAG) – have demonstrated strong performance in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved evidence is incomplete, leaving gaps in the reasoning process. In such cases, \emph{abductive inference} – the process of generating plausible missing premises to explain observations – offers a principled approach to bridge these gaps. In this paper, we propose a framework that integrates abductive inference into retrieval-augmented LLMs. Our method detects insufficient evidence, generates candidate missing premises, and validates them through consistency and plausibility checks. Experimental results on abductive reasoning and multi-hop QA benchmarks show that our approach improves both answer accuracy and reasoning faithfulness. This work highlights abductive inference as a promising direction for enhancing the robustness and explainability of RAG systems.</p>
  </div>
</details>

<hr>
<h3 id="57-WST-Weakly-Supervised-Transducer-for-Automatic-Speech-Recognition-cs-CLPDF"><a href="#57-WST-Weakly-Supervised-Transducer-for-Automatic-Speech-Recognition-cs-CLPDF" class="headerlink" title="[57] WST: Weakly Supervised Transducer for Automatic Speech Recognition cs.CLPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04035">WST: Weakly Supervised Transducer for Automatic Speech Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04035" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongji Gao, Chenda Liao, Changliang Liu, Matthew Wiesner, Leibny Paola Garcia</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种弱监督Transducer（WST），用于降低自动语音识别（ASR）中对高质量标注数据的依赖，能够在转录错误率高达70%的情况下保持性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的RNN-T模型在ASR任务中依赖大量高质量标注数据，但标注成本高且难以获取。论文旨在通过弱监督学习解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，即使在转录错误率达70%的情况下，WST仍能保持性能，优于现有的CTC类弱监督方法。</p>
<p><strong>Insight:</strong> WST展示了在现实ASR场景中的实用性和鲁棒性，为低资源ASR提供了一种有效解决方案。</p>
<p><strong>Abstract:</strong> The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily on large-scale, high-quality annotated data, which are often costly and difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised Transducer (WST), which integrates a flexible training graph designed to robustly handle errors in the transcripts without requiring additional confidence estimation or auxiliary pre-trained models. Empirical evaluations on synthetic and industrial datasets reveal that WST effectively maintains performance even with transcription error rates of up to 70%, consistently outperforming existing Connectionist Temporal Classification (CTC)-based weakly supervised approaches, such as Bypass Temporal Classification (BTC) and Omni-Temporal Classification (OTC). These results demonstrate the practical utility and robustness of WST in realistic ASR settings. The implementation will be publicly available.</p>
  </div>
</details>

<hr>
<h3 id="58-T-FIX-Text-Based-Explanations-with-Features-Interpretable-to-eXperts-cs-CLPDF"><a href="#58-T-FIX-Text-Based-Explanations-with-Features-Interpretable-to-eXperts-cs-CLPDF" class="headerlink" title="[58] T-FIX: Text-Based Explanations with Features Interpretable to eXperts cs.CLPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04070">T-FIX: Text-Based Explanations with Features Interpretable to eXperts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04070" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shreya Havaldar, Helen Jin, Chaehyeon Kim, Anton Xue, Weiqiu You</span></p>
<p><strong>TL;DR:</strong> T-FIX是一个新的基准测试，旨在评估LLM生成的解释是否与专家直觉一致，涵盖七个知识密集领域，并开发了新指标以衡量对齐性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于LLM在知识密集型领域（如手术、天文学、治疗）中的应用，用户（通常是领域专家）不仅需要答案，还需要与其专业直觉一致的解释。现有的评估方法主要关注解释的合理性和内部一致性，无法捕捉解释内容是否真正符合专家直觉。</p>
<p><strong>Result:</strong> T-FIX提供了一个标准的评估框架，帮助验证LLM解释的专业性和可靠性。</p>
<p><strong>Insight:</strong> 研究强调了在知识密集型领域中，LLM解释需要与专家直觉一致的重要性，而不仅仅是表面的合理性。</p>
<p><strong>Abstract:</strong> As LLMs are deployed in knowledge-intensive settings (e.g., surgery, astronomy, therapy), users expect not just answers, but also meaningful explanations for those answers. In these settings, users are often domain experts (e.g., doctors, astrophysicists, psychologists) who require explanations that reflect expert-level reasoning. However, current evaluation schemes primarily emphasize plausibility or internal faithfulness of the explanation, which fail to capture whether the content of the explanation truly aligns with expert intuition. We formalize expert alignment as a criterion for evaluating explanations with T-FIX, a benchmark spanning seven knowledge-intensive domains. In collaboration with domain experts, we develop novel metrics to measure the alignment of LLM explanations with expert judgment.</p>
  </div>
</details>

<hr>
<h3 id="59-Plan-of-Knowledge-Retrieval-Augmented-Large-Language-Models-for-Temporal-Knowledge-Graph-Question-Answering-cs-CLPDF"><a href="#59-Plan-of-Knowledge-Retrieval-Augmented-Large-Language-Models-for-Temporal-Knowledge-Graph-Question-Answering-cs-CLPDF" class="headerlink" title="[59] Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering cs.CLPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04072">Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04072" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinying Qian, Ying Zhang, Yu Zhao, Baohang Zhou, Xuhui Sui</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为PoK的框架，通过结合知识计划和对比性时间检索器，提升大语言模型在时间知识图谱问答任务中的表现，显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在时间知识图谱问答（TKGQA）任务中未能充分理解时间约束的复杂语义信息，而大语言模型（LLMs）虽具备强大的语义理解和推理能力，但其时间推理能力有限且存在幻觉和知识缺乏问题。</p>
<p><strong>Result:</strong> 在四个TKGQA基准数据集上，PoK显著提升了LLMs的检索精度和推理准确率，最多超越现有方法56.0%。</p>
<p><strong>Insight:</strong> 结合结构化规划与时间知识检索能有效弥补LLMs在时间推理上的不足，增强模型的解释性和事实一致性。</p>
<p><strong>Abstract:</strong> Temporal Knowledge Graph Question Answering (TKGQA) aims to answer time-sensitive questions by leveraging factual information from Temporal Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG embeddings or graph neural networks to inject temporal knowledge, they fail to fully understand the complex semantic information of time constraints. Recently, Large Language Models (LLMs) have shown remarkable progress, benefiting from their strong semantic understanding and reasoning generalization capabilities. However, their temporal reasoning ability remains limited. LLMs frequently suffer from hallucination and a lack of knowledge. To address these limitations, we propose the Plan of Knowledge framework with a contrastive temporal retriever, which is named PoK. Specifically, the proposed Plan of Knowledge module decomposes a complex temporal question into a sequence of sub-objectives from the pre-defined tools, serving as intermediate guidance for reasoning exploration. In parallel, we construct a Temporal Knowledge Store (TKS) with a contrastive retrieval framework, enabling the model to selectively retrieve semantically and temporally aligned facts from TKGs. By combining structured planning with temporal knowledge retrieval, PoK effectively enhances the interpretability and factual consistency of temporal reasoning. Extensive experiments on four benchmark TKGQA datasets demonstrate that PoK significantly improves the retrieval precision and reasoning accuracy of LLMs, surpassing the performance of the state-of-the-art TKGQA methods by 56.0% at most.</p>
  </div>
</details>

<hr>
<h3 id="60-Improving-the-Performance-of-Radiology-Report-De-identification-with-Large-Scale-Training-and-Benchmarking-Against-Cloud-Vendor-Methods-cs-CLPDF"><a href="#60-Improving-the-Performance-of-Radiology-Report-De-identification-with-Large-Scale-Training-and-Benchmarking-Against-Cloud-Vendor-Methods-cs-CLPDF" class="headerlink" title="[60] Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods cs.CLPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04079">Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04079" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eva Prakash, Maayane Attias, Pierre Chambon, Justin Xu, Steven Truong</span></p>
<p><strong>TL;DR:</strong> 该论文通过大规模训练数据微调transformer模型，提升放射学报告的自动去识别性能，并在PHI检测上超越商业云服务系统。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的PHI去识别方法在多中心泛化性和鲁棒性方面表现不足，尤其是对合成PHI的处理能力有限。本文旨在通过大规模数据集和引入新PHI类别(AGE)解决这些问题。</p>
<p><strong>Result:</strong> 模型在斯坦福和宾大数据集上的F1分数分别为0.996和0.973，显著优于商业系统（F1：0.960 vs. 0.632-0.754）。合成PHI检测一致性高（F1：0.959）。</p>
<p><strong>Insight:</strong> 大规模多模态训练显著提升模型的泛化能力和鲁棒性；合成PHI生成实现了隐私保护与数据实用性的平衡；transformer模型在PHI检测领域潜力巨大。</p>
<p><strong>Abstract:</strong> Objective: To enhance automated de-identification of radiology reports by scaling transformer-based models through extensive training datasets and benchmarking performance against commercial cloud vendor systems for protected health information (PHI) detection. Materials and Methods: In this retrospective study, we built upon a state-of-the-art, transformer-based, PHI de-identification pipeline by fine-tuning on two large annotated radiology corpora from Stanford University, encompassing chest X-ray, chest CT, abdomen&#x2F;pelvis CT, and brain MR reports and introducing an additional PHI category (AGE) into the architecture. Model performance was evaluated on test sets from Stanford and the University of Pennsylvania (Penn) for token-level PHI detection. We further assessed (1) the stability of synthetic PHI generation using a “hide-in-plain-sight” method and (2) performance against commercial systems. Precision, recall, and F1 scores were computed across all PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining the previous state-of-the-art model performance. Synthetic PHI evaluation showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50 independently de-identified Penn datasets. Our model outperformed all vendor systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754). Discussion: Large-scale, multimodal training improved cross-institutional generalization and robustness. Synthetic PHI generation preserved data utility while ensuring privacy. Conclusion: A transformer-based de-identification model trained on diverse radiology datasets outperforms prior academic and commercial systems in PHI detection and establishes a new benchmark for secure clinical text processing.</p>
  </div>
</details>

<hr>
<h3 id="61-Batch-Prompting-Suppresses-Overthinking-Reasoning-Under-Constraint-How-Batch-Prompting-Suppresses-Overthinking-in-Reasoning-Models-cs-CLPDF"><a href="#61-Batch-Prompting-Suppresses-Overthinking-Reasoning-Under-Constraint-How-Batch-Prompting-Suppresses-Overthinking-in-Reasoning-Models-cs-CLPDF" class="headerlink" title="[61] Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models cs.CLPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04108">Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04108" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenmo Qiu, Saurabh Srivastava</span></p>
<p><strong>TL;DR:</strong> 批处理提示（batch prompting）不仅能优化推理模型的吞吐量，还能显著抑制模型在多步推理中的过度思考现象，提高准确率并减少token使用量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究批处理提示对大规模推理模型（LRMs）行为的影响，探索其在推理过程中的正则化效果，特别是在抑制过度思考和提升效率方面的潜力。</p>
<p><strong>Result:</strong> 批处理提示在提高准确率的同时，显著减少了推理token的使用量，并抑制了过度思考和犹豫现象。</p>
<p><strong>Insight:</strong> 批处理提示不仅是吞吐量优化的工具，还能作为推理时的正则化手段，提升模型的效率和可靠性。</p>
<p><strong>Abstract:</strong> Recent work has explored batch prompting as a strategy to amortize inference cost in large language models (LLMs). In this paper, we show that batching offers an additional, underappreciated benefit: it regularizes model behavior during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a comprehensive study across 13 diverse benchmarks and observe that batching improves accuracy while substantially reducing reasoning token usage, often by 3x-5x. Through detailed behavioral analysis, we find that batching suppresses overthinking, reduces hedging language (e.g., repetitive self-corrections), and encourages more decisive answers. Surprisingly, we also observe emergent collective effects in batched inference: models often generalize patterns from earlier examples to solve harder ones in the same batch. These findings position batching not just as a throughput optimization, but as a powerful inference-time regularizer for more efficient and reliable LLM reasoning.</p>
  </div>
</details>

<hr>
<h3 id="62-RIDE-Difficulty-Evolving-Perturbation-with-Item-Response-Theory-for-Mathematical-Reasoning-cs-CLPDF"><a href="#62-RIDE-Difficulty-Evolving-Perturbation-with-Item-Response-Theory-for-Mathematical-Reasoning-cs-CLPDF" class="headerlink" title="[62] RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning cs.CLPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04120">RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04120" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinyuan Li, Murong Xu, Wenbiao Tao, Hanlun Zhu, Yike Zhao</span></p>
<p><strong>TL;DR:</strong> RIDE是一个基于IRT理论的对抗性问题重写框架，用于生成难度可控的数学问题，以评估LLMs的真实数学推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大规模语言模型在数学推理任务上的高表现可能源于数据泄露或浅层模式匹配，而非真正的推理能力，因此需要更严格的评估方法。</p>
<p><strong>Result:</strong> 在竞赛级数学基准上，RIDE生成的问题使LLM平均性能下降21.73%，揭示了其推理能力的局限性。</p>
<p><strong>Insight:</strong> IRT理论在动态调整问题难度方面的应用是有效的，为LLM评估提供了新思路。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) achieve high performance on mathematical reasoning, but these results can be inflated by training data leakage or superficial pattern matching rather than genuine reasoning. To this end, an adversarial perturbation-based evaluation is needed to measure true mathematical reasoning ability. Current rule-based perturbation methods often generate ill-posed questions and impede the systematic evaluation of question difficulty and the evolution of benchmarks. To bridge this gap, we propose RIDE, a novel adversarial question-rewriting framework that leverages Item Response Theory (IRT) to rigorously measure question difficulty and to generate intrinsically more challenging, well-posed variations of mathematical problems. We employ 35 LLMs to simulate students and build a difficulty ranker from their responses. This ranker provides a reward signal during reinforcement learning and guides a question-rewriting model to reformulate existing questions across difficulty levels. Applying RIDE to competition-level mathematical benchmarks yields perturbed versions that degrade advanced LLM performance, with experiments showing an average 21.73% drop across 26 models, thereby exposing limited robustness in mathematical reasoning and confirming the validity of our evaluation approach.</p>
  </div>
</details>

<hr>
<h3 id="63-CantoASR-Prosody-Aware-ASR-LALM-Collaboration-for-Low-Resource-Cantonese-cs-CL-cs-SDPDF"><a href="#63-CantoASR-Prosody-Aware-ASR-LALM-Collaboration-for-Low-Resource-Cantonese-cs-CL-cs-SDPDF" class="headerlink" title="[63] CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese cs.CL | cs.SDPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04139">CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.SD</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04139" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dazhong Chen, Yi-Cheng Lin, Yuchen Huang, Ziwei Gong, Di Jiang</span></p>
<p><strong>TL;DR:</strong> CantoASR：一个结合了语音识别（ASR）和大音频语言模型（LALM）的协作框架，通过融合声学特征和上下文推理，显著提升了低资源粤语的识别准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 粤语作为一种低资源语言，面临标注数据有限、六种声调、变调及口音变异的挑战，现有的ASR模型（如Whisper）表现不佳。</p>
<p><strong>Result:</strong> 在自发粤语数据上测试，CER显著优于Whisper-Large-V3。</p>
<p><strong>Insight:</strong> 声学特征与LALM的上下文推理结合，为低资源声调方言的ASR提供了一种可扩展的解决方案。</p>
<p><strong>Abstract:</strong> Automatic speech recognition (ASR) is critical for language accessibility, yet low-resource Cantonese remains challenging due to limited annotated data, six lexical tones, tone sandhi, and accent variation. Existing ASR models, such as Whisper, often suffer from high word error rates. Large audio-language models (LALMs), in contrast, can leverage broader contextual reasoning but still require explicit tonal and prosodic acoustic cues. We introduce CantoASR, a collaborative ASR-LALM error correction framework that integrates forced alignment for acoustic feature extraction, a LoRA-finetuned Whisper for improved tone discrimination, and an instruction-tuned Qwen-Audio for prosody-aware correction. Evaluations on spontaneous Cantonese data show substantial CER gains over Whisper-Large-V3. These findings suggest that integrating acoustic cues with LALM reasoning provides a scalable strategy for low-resource tonal and dialectal ASR.</p>
  </div>
</details>

<hr>
<h3 id="64-BAPPA-Benchmarking-Agents-Plans-and-Pipelines-for-Automated-Text-to-SQL-Generation-cs-CL-cs-AI-cs-DB-cs-MAPDF"><a href="#64-BAPPA-Benchmarking-Agents-Plans-and-Pipelines-for-Automated-Text-to-SQL-Generation-cs-CL-cs-AI-cs-DB-cs-MAPDF" class="headerlink" title="[64] BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation cs.CL | cs.AI | cs.DB | cs.MAPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04153">BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.DB | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04153" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fahim Ahmed, Md Mubtasim Ahasan, Jahir Sadik Monon, Muntasir Wahed, M Ashraful Amin</span></p>
<p><strong>TL;DR:</strong> 论文探讨了三种多智能体LLM流水线，旨在提升文本到SQL生成任务的性能，尤其关注小型高效模型的表现。通过系统性能评测，发现多智能体讨论和规划器-编码器流水线能显著提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大型语言模型（LLM）在从自然语言指令生成SQL时面临大规模模式和复杂推理的挑战。此前工作多关注复杂但低效的流水线，而小型高效模型被忽视。本文旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验显示，多智能体讨论可将Qwen2.5-7b-Instruct的执行准确率提升10.6%；规划器-编码器流水线表现最佳，DeepSeek-R1-32B和QwQ-32B规划器将Gemma 3 27B IT的准确率从52.4%提升至56.4%。</p>
<p><strong>Insight:</strong> 多智能体协作和分步规划能有效提升小型模型在复杂文本到SQL任务中的表现，为实际应用提供了高效解决方案。</p>
<p><strong>Abstract:</strong> Text-to-SQL systems provide a natural language interface that can enable even laymen to access information stored in databases. However, existing Large Language Models (LLM) struggle with SQL generation from natural instructions due to large schema sizes and complex reasoning. Prior work often focuses on complex, somewhat impractical pipelines using flagship models, while smaller, efficient models remain overlooked. In this work, we explore three multi-agent LLM pipelines, with systematic performance benchmarking across a range of small to large open-source models: (1) Multi-agent discussion pipeline, where agents iteratively critique and refine SQL queries, and a judge synthesizes the final answer; (2) Planner-Coder pipeline, where a thinking model planner generates stepwise SQL generation plans and a coder synthesizes queries; and (3) Coder-Aggregator pipeline, where multiple coders independently generate SQL queries, and a reasoning agent selects the best query. Experiments on the Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines, the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%. Codes are available at <a target="_blank" rel="noopener" href="https://github.com/treeDweller98/bappa-sql">https://github.com/treeDweller98/bappa-sql</a>.</p>
  </div>
</details>

<hr>
<h3 id="65-LLM-as-a-Judge-is-Bad-Based-on-AI-Attempting-the-Exam-Qualifying-for-the-Member-of-the-Polish-National-Board-of-Appeal-cs-CLPDF"><a href="#65-LLM-as-a-Judge-is-Bad-Based-on-AI-Attempting-the-Exam-Qualifying-for-the-Member-of-the-Polish-National-Board-of-Appeal-cs-CLPDF" class="headerlink" title="[65] LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal cs.CLPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04205">LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04205" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Michał Karp, Anna Kubaszewska, Magdalena Król, Robert Król, Aleksander Smywiński-Pohl</span></p>
<p><strong>TL;DR:</strong> 该研究通过实证评估发现，当前大型语言模型（LLMs）无法通过波兰国家上诉委员会成员资格考试的实践部分，且‘LLM-as-a-judge’方法的评估与官方评委存在显著差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机在于验证LLMs在专业法律考试中的表现，尤其是能否作为评委或考生参与高标准的法律资格考试。</p>
<p><strong>Result:</strong> 结果显示，LLMs在选择题知识测试中表现尚可，但实践写作部分未达到通过标准，且模型评委的评估与官方评委存在显著差异。</p>
<p><strong>Insight:</strong> 研究揭示了LLMs在法律领域的局限性，如易产生幻觉、错误引用法律条款、逻辑论证不足等，强调了法律专家与技术团队协作的重要性。</p>
<p><strong>Abstract:</strong> This study provides an empirical assessment of whether current large language models (LLMs) can pass the official qualifying examination for membership in Poland’s National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors examine two related ideas: using LLM as actual exam candidates and applying the ‘LLM-as-a-judge’ approach, in which model-generated answers are automatically evaluated by other models. The paper describes the structure of the exam, which includes a multiple-choice knowledge test on public procurement law and a written judgment, and presents the hybrid information recovery and extraction pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4 Sonnet and Bielik-11B-v2.6) were tested in closed-book and various Retrieval-Augmented Generation settings. The results show that although the models achieved satisfactory scores in the knowledge test, none met the passing threshold in the practical written part, and the evaluations of the ‘LLM-as-a-judge’ often diverged from the judgments of the official examining committee. The authors highlight key limitations: susceptibility to hallucinations, incorrect citation of legal provisions, weaknesses in logical argumentation, and the need for close collaboration between legal experts and technical teams. The findings indicate that, despite rapid technological progress, current LLMs cannot yet replace human judges or independent examiners in Polish public procurement adjudication.</p>
  </div>
</details>

<hr>
<h3 id="66-SSPO-Subsentence-level-Policy-Optimization-cs-CLPDF"><a href="#66-SSPO-Subsentence-level-Policy-Optimization-cs-CLPDF" class="headerlink" title="[66] SSPO: Subsentence-level Policy Optimization cs.CLPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04256">SSPO: Subsentence-level Policy Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04256" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kun Yang, Zikang chen, Yanmeng Wang, Zhigen Li</span></p>
<p><strong>TL;DR:</strong> SSPO是一种新的强化学习优化方法，通过在子句级别计算重要性比率，平衡了GRPO和GSPO的优点，避免了训练崩溃和高方差问题，同时提高了采样数据的利用率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的RLVR算法（如GRPO和GSPO）存在训练不稳定或数据利用率低的问题，SSPO旨在解决这些问题，进一步提升大语言模型的推理能力。</p>
<p><strong>Result:</strong> 在五个数据集上平均得分为46.57，优于GRPO（43.01）和GSPO（44.42），并在三个数据集上达到最先进性能。</p>
<p><strong>Insight:</strong> 子句级别的优化能更精细地平衡稳定性和数据利用率，而动态剪裁机制进一步提升了模型的探索能力。</p>
<p><strong>Abstract:</strong> As a significant part of post-training of the Large Language Models (LLMs), Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs’ reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative Policy Optimization) and GSPO (Group Sequence Policy Optimization), are observed to suffer from unstable policy updates and low usage of sampling data, respectively. The importance ratio of GRPO is calculated at the token level, which focuses more on optimizing a single token. This will be easily affected by outliers, leading to model training collapse. GSPO proposed the calculation of the response level importance ratio, which solves the problem of high variance and training noise accumulation in the calculation of the GRPO importance ratio. However, since all the response tokens share a common importance ratio, extreme values can easily raise or lower the overall mean, leading to the entire response being mistakenly discarded, resulting in a decrease in the utilization of sampled data. This paper introduces SSPO, which applies sentence-level importance ratio, taking the balance between GRPO and GSPO. SSPO not only avoids training collapse and high variance, but also prevents the whole response tokens from being abandoned by the clipping mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily adjust the clipping bounds, encouraging high-entropy tokens to explore and narrow the clipping range of low-entropy tokens. In particular, SSPO achieves an average score of 46.57 across five datasets, surpassing GRPO (43.01) and GSPO (44.42), and wins state-of-the-art performance on three datasets. These results highlight SSPO’s effectiveness in leveraging generated data by taking the essence of GSPO but rejecting its shortcomings.</p>
  </div>
</details>

<hr>
<h3 id="67-Dynamic-Jointly-Batch-Selection-for-Data-Efficient-Machine-Translation-Fine-Tuning-cs-CLPDF"><a href="#67-Dynamic-Jointly-Batch-Selection-for-Data-Efficient-Machine-Translation-Fine-Tuning-cs-CLPDF" class="headerlink" title="[67] Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning cs.CLPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04406">Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04406" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammad Amin Ghanizadeh, Mohammad Javad Dousti</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种数据选择方法，通过结合学习模型和预训练参考模型的协同效应，定义可学习性评分并动态选择批次，显著提高了机器翻译微调的数据效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 机器翻译模型的数据质量和有效选择对性能至关重要，但传统方法往往忽视数据点间的依赖关系。本文旨在通过动态批次选择优化数据效率和训练效果。</p>
<p><strong>Result:</strong> 在英波等语言对上，数据效率提高了5倍，计算效率提升24倍（缓存嵌入时），且翻译性能优于随机选择基线。</p>
<p><strong>Insight:</strong> 数据选择不仅关注单个样本的质量，还需考虑批次内样本的协同效应，动态批次选择是提升微调效率的有效途径。</p>
<p><strong>Abstract:</strong> Data quality and its effective selection are fundamental to improving the performance of machine translation models, serving as cornerstones for achieving robust and reliable translation systems. This paper presents a data selection methodology specifically designed for fine-tuning machine translation systems, which leverages the synergy between a learner model and a pre-trained reference model to enhance overall training effectiveness. By defining a learnability score, our approach systematically evaluates the utility of data points for training, ensuring that only the most relevant and impactful examples contribute to the fine-tuning process. Furthermore, our method employs a batch selection strategy which considers interdependencies among data points, optimizing the efficiency of the training process while maintaining a focus on data relevance. Experiments on English to Persian and several other language pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that our method can achieve up to a fivefold improvement in data efficiency compared to an iid baseline. Experimental results indicate that our approach improves computational efficiency by 24 when utilizing cached embeddings, as it requires fewer training data points. Additionally, it enhances generalization, resulting in superior translation performance compared to random selection method.</p>
  </div>
</details>

<hr>
<h3 id="68-If-I-Could-Turn-Back-Time-Temporal-Reframing-as-a-Historical-Reasoning-Task-for-LLMs-cs-CLPDF"><a href="#68-If-I-Could-Turn-Back-Time-Temporal-Reframing-as-a-Historical-Reasoning-Task-for-LLMs-cs-CLPDF" class="headerlink" title="[68] If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs cs.CLPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04432">If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04432" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lars Bungum, Charles Yijia Huang, Abeer Kashar</span></p>
<p><strong>TL;DR:</strong> 该研究探讨了大型语言模型（LLMs）在时间推理任务中的表现，通过模拟1940年的挪威书籍问答，测试模型在不同语言和规模下的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是评估LLMs在时间推理和历史背景理解方面的能力，尤其是当问题涉及过去时间点时。</p>
<p><strong>Result:</strong> 结果显示，英文提示效果更好，大模型表现更优，但即使专为挪威语设计的最大LLM也未超越英文提示的效果。</p>
<p><strong>Insight:</strong> 研究提示语言选择对LLMs任务表现的重要性，以及模型规模与性能的正相关关系。</p>
<p><strong>Abstract:</strong> In this study, we experiment with the ability of LLMs to do temporal reasoning. Using a Norwegian book from 1940 containing trivia questions, we prompt the LLMs to answer the questions as if it were 1940. We also pose the questions in both English and Norwegian. Correct answers are often presented as sentences, and grading is done by means of LLM-as-judge, with sampled checks by a native speaker. Prompting in English consistently gave better results than in Norwegian, an unexpected result. In contrast, using larger LLMs improved results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families, and also the largest available LLM especially crafted for Norwegian.</p>
  </div>
</details>

<hr>
<h3 id="69-ThaiOCRBench-A-Task-Diverse-Benchmark-for-Vision-Language-Understanding-in-Thai-cs-CLPDF"><a href="#69-ThaiOCRBench-A-Task-Diverse-Benchmark-for-Vision-Language-Understanding-in-Thai-cs-CLPDF" class="headerlink" title="[69] ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai cs.CLPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04479">ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04479" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan</span></p>
<p><strong>TL;DR:</strong> 论文提出了首个针对泰语的视觉-语言理解基准ThaiOCRBench，填补了现有基准在泰语和多模态任务中的空白，评估了多种VLMs的性能并揭示了开源模型的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉-语言模型（VLMs）基准主要集中在高资源语言，泰语等低资源语言和多模态任务（尤其是文档结构理解）缺乏代表性基准。</p>
<p><strong>Result:</strong> 专有模型（如Gemini 2.5 Pro）表现优于开源模型，开源模型在细粒度文本识别和手写内容提取上表现较差。</p>
<p><strong>Insight:</strong> 揭示了语言偏见、结构不匹配和幻觉内容是VLMs在泰语任务中的主要挑战，为改进低资源语言文档理解提供了方向。</p>
<p><strong>Abstract:</strong> We present ThaiOCRBench, the first comprehensive benchmark for evaluating vision-language models (VLMs) on Thai text-rich visual understanding tasks. Despite recent progress in multimodal modeling, existing benchmarks predominantly focus on high-resource languages, leaving Thai underrepresented, especially in tasks requiring document structure understanding. ThaiOCRBench addresses this gap by offering a diverse, human-annotated dataset comprising 2,808 samples across 13 task categories. We evaluate a wide range of state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and open-source systems. Results show a significant performance gap, with proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source counterparts. Notably, fine-grained text recognition and handwritten content extraction exhibit the steepest performance drops among open-source models. Through detailed error analysis, we identify key challenges such as language bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a standardized framework for assessing VLMs in low-resource, script-complex settings, and provides actionable insights for improving Thai-language document understanding.</p>
  </div>
</details>

<hr>
<h3 id="70-RUST-BENCH-Benchmarking-LLM-Reasoning-on-Unstructured-Text-within-Structured-Tables-cs-CL-cs-AI-cs-DB-cs-IR-cs-LGPDF"><a href="#70-RUST-BENCH-Benchmarking-LLM-Reasoning-on-Unstructured-Text-within-Structured-Tables-cs-CL-cs-AI-cs-DB-cs-IR-cs-LGPDF" class="headerlink" title="[70] RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables cs.CL | cs.AI | cs.DB | cs.IR | cs.LGPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04491">RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.DB | cs.IR | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04491" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta</span></p>
<p><strong>TL;DR:</strong> RUST-BENCH是一个新的基准测试，旨在评估大型语言模型（LLMs）在真实世界复杂表格数据上的推理能力，填补了现有基准在小规模、单一表格测试上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有表格推理基准测试多为小规模、单一结构的表格，无法反映真实世界中长表、异构和领域特定数据的复杂性，无法全面评估LLMs的推理能力。</p>
<p><strong>Result:</strong> 实验表明，LLMs在处理异构模式和复杂推理时表现不佳，揭示了当前模型结构和提示策略的局限性。</p>
<p><strong>Insight:</strong> RUST-BENCH为表格推理研究提供了一个更具挑战性的测试平台，凸显了现有技术的改进空间。</p>
<p><strong>Abstract:</strong> Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models’ (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.</p>
  </div>
</details>

<hr>
<h3 id="71-OUNLP-at-TSAR-2025-Shared-Task-Multi-Round-Text-Simplifier-via-Code-Generation-cs-CL-cs-AIPDF"><a href="#71-OUNLP-at-TSAR-2025-Shared-Task-Multi-Round-Text-Simplifier-via-Code-Generation-cs-CL-cs-AIPDF" class="headerlink" title="[71] OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation cs.CL | cs.AIPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04495">OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04495" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cuong Huynh, Jie Cao</span></p>
<p><strong>TL;DR:</strong> 本文介绍了OUNLP团队为TSAR-2025共享任务设计的基于LLM提示生成的文本简化系统，发现文本简化性能与源CEFR和目标CEFR级别之间的差距高度相关，并提出了两种多轮简化方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基于CEFR级别的文本简化需求，探索LLM在多轮简化中的潜力。</p>
<p><strong>Result:</strong> 系统在20个团队中排名第7，MRS-Joint进一步提升了性能。</p>
<p><strong>Insight:</strong> 文本简化性能与CEFR级别差距相关，多轮简化结合LLM提示可优化结果。</p>
<p><strong>Abstract:</strong> This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task (Alva-Manchego et al., 2025), designed for readability-controlled text simplification using LLM-prompting-based generation. Based on the analysis of prompt-based text simplification methods, we discovered an interesting finding that text simplification performance is highly related to the gap between the source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by this finding, we propose two multi-round simplification methods and generate them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams. Later improvements with MRS-Joint show that taking the LLM simplified candidates as the starting point could further boost the multi-round simplification performance.</p>
  </div>
</details>

<hr>
<h3 id="72-Modeling-Clinical-Uncertainty-in-Radiology-Reports-from-Explicit-Uncertainty-Markers-to-Implicit-Reasoning-Pathways-cs-CLPDF"><a href="#72-Modeling-Clinical-Uncertainty-in-Radiology-Reports-from-Explicit-Uncertainty-Markers-to-Implicit-Reasoning-Pathways-cs-CLPDF" class="headerlink" title="[72] Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways cs.CLPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04506">Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04506" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Paloma Rabaey, Jong Hak Moon, Jung-Oh Lee, Min Gwan Kim, Hangyul Yoon</span></p>
<p><strong>TL;DR:</strong> 论文提出一个两部分框架来量化放射报告中显性和隐性不确定性，并通过专家验证和大语言模型（LLM）排名显性不确定性标记，同时通过系统性扩展方法建模隐性不确定性，最终发布了一个包含不确定性信息的结构化数据集Lunguage++。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 放射报告中的不确定性对临床决策和自动化分析至关重要，但现有方法难以准确量化显性和隐性不确定性。</p>
<p><strong>Result:</strong> 发布了Lunguage++数据集，支持不确定性感知的图像分类和诊断推理。</p>
<p><strong>Insight:</strong> 显性和隐性不确定性的量化方法有助于提高放射报告的自动化分析能力，并为临床不确定性研究提供新工具。</p>
<p><strong>Abstract:</strong> Radiology reports are invaluable for clinical decision-making and hold great potential for automated analysis when structured into machine-readable formats. These reports often contain uncertainty, which we categorize into two distinct types: (i) Explicit uncertainty reflects doubt about the presence or absence of findings, conveyed through hedging phrases. These vary in meaning depending on the context, making rule-based systems insufficient to quantify the level of uncertainty for specific findings; (ii) Implicit uncertainty arises when radiologists omit parts of their reasoning, recording only key findings or diagnoses. Here, it is often unclear whether omitted findings are truly absent or simply unmentioned for brevity. We address these challenges with a two-part framework. We quantify explicit uncertainty by creating an expert-validated, LLM-based reference ranking of common hedging phrases, and mapping each finding to a probability value based on this reference. In addition, we model implicit uncertainty through an expansion framework that systematically adds characteristic sub-findings derived from expert-defined diagnostic pathways for 14 common diagnoses. Using these methods, we release Lunguage++, an expanded, uncertainty-aware version of the Lunguage benchmark of fine-grained structured radiology reports. This enriched resource enables uncertainty-aware image classification, faithful diagnostic reasoning, and new investigations into the clinical impact of diagnostic uncertainty.</p>
  </div>
</details>

<hr>
<h3 id="73-Are-language-models-aware-of-the-road-not-taken-Token-level-uncertainty-and-hidden-state-dynamics-cs-CL-cs-AIPDF"><a href="#73-Are-language-models-aware-of-the-road-not-taken-Token-level-uncertainty-and-hidden-state-dynamics-cs-CL-cs-AIPDF" class="headerlink" title="[73] Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics cs.CL | cs.AIPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04527">Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04527" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amir Zur, Atticus Geiger, Ekdeep Singh Lubana, Eric Bigelow</span></p>
<p><strong>TL;DR:</strong> 该论文研究了语言模型在生成文本时是否意识到未被选择的路径，并通过隐状态动态分析了模型的token级不确定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探索语言模型在推理过程中是否隐含地表示可能的不同路径，从而帮助量化不确定性。</p>
<p><strong>Result:</strong> 实验结果显示，模型的不确定性与隐状态操控的有效性显著相关，且隐状态能预测模型的未来输出分布。</p>
<p><strong>Insight:</strong> 洞察是语言模型在决策过程中隐含表示多种路径的能力，这种能力在不确定性高时最为显著。</p>
<p><strong>Abstract:</strong> When a language model generates text, the selection of individual tokens might lead it down very different reasoning paths, making uncertainty difficult to quantify. In this work, we consider whether reasoning language models represent the alternate paths that they could take during generation. To test this hypothesis, we use hidden activations to control and predict a language model’s uncertainty during chain-of-thought reasoning. In our experiments, we find a clear correlation between how uncertain a model is at different tokens, and how easily the model can be steered by controlling its activations. This suggests that activation interventions are most effective when there are alternate paths available to the model – in other words, when it has not yet committed to a particular final answer. We also find that hidden activations can predict a model’s future outcome distribution, demonstrating that models implicitly represent the space of possible paths.</p>
  </div>
</details>

<hr>
<h3 id="74-BanglaMedQA-and-BanglaMMedBench-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Bangla-Biomedical-Question-Answering-cs-CLPDF"><a href="#74-BanglaMedQA-and-BanglaMMedBench-Evaluating-Retrieval-Augmented-Generation-Strategies-for-Bangla-Biomedical-Question-Answering-cs-CLPDF" class="headerlink" title="[74] BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering cs.CLPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04560">BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04560" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sadia Sultana, Saiyma Sittul Muna, Mosammat Zannatul Samarukh, Ajwad Abrar, Tareque Mohmud Chowdhury</span></p>
<p><strong>TL;DR:</strong> 论文介绍了首个大规模孟加拉语生物医学多选题数据集BanglaMedQA和BanglaMMedBench，并评估了多种检索增强生成（RAG）策略在提升医学QA准确性中的作用。Agentic RAG方法表现最佳，准确率达89.54%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 低资源语言的生物医学QA系统发展不足，限制了可靠医学知识的公平获取。本文旨在填补孟加拉语领域的研究空白，并探索RAG方法在此类任务中的潜力。</p>
<p><strong>Result:</strong> Agentic RAG在openai&#x2F;gpt-oss-120b模型上实现了89.54%的最高准确率，优于其他策略配置，并在推理质量上表现突出。</p>
<p><strong>Insight:</strong> RAG方法能够显著提升低资源语言医学QA的可靠性，动态策略选择（Agentic RAG）是关键创新点。这为多语言医学AI研究提供了新方向。</p>
<p><strong>Abstract:</strong> Developing accurate biomedical Question Answering (QA) systems in low-resource languages remains a major challenge, limiting equitable access to reliable medical knowledge. This paper introduces BanglaMedQA and BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical artificial intelligence (AI). The study applies and benchmarks several Retrieval-Augmented Generation (RAG) strategies, including Traditional, Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining textbook-based and web retrieval with generative reasoning to improve factual accuracy. A key novelty lies in integrating a Bangla medical textbook corpus through Optical Character Recognition (OCR) and implementing an Agentic RAG pipeline that dynamically selects between retrieval and reasoning strategies. Experimental results show that the Agentic RAG achieved the highest accuracy 89.54% with openai&#x2F;gpt-oss-120b, outperforming other configurations and demonstrating superior rationale quality. These findings highlight the potential of RAG-based methods to enhance the reliability and accessibility of Bangla medical QA, establishing a foundation for future research in multilingual medical artificial intelligence.</p>
  </div>
</details>

<hr>
<h3 id="75-When-retrieval-outperforms-generation-Dense-evidence-retrieval-for-scalable-fake-news-detection-cs-CLPDF"><a href="#75-When-retrieval-outperforms-generation-Dense-evidence-retrieval-for-scalable-fake-news-detection-cs-CLPDF" class="headerlink" title="[75] When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection cs.CLPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04643">When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04643" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alamgir Munir Qazi, John P. McCrae, Jamal Abdul Nasir</span></p>
<p><strong>TL;DR:</strong> DeReC提出了一种轻量级的密集检索分类框架，通过结合密集检索和专用分类，显著提升了虚假新闻检测的效率和准确性，优于生成式LLM方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 虚假新闻泛滥需要高效的事实验证系统，而当前基于LLM生成解释性理由的方法存在计算成本高和幻觉风险的问题。</p>
<p><strong>Result:</strong> DeReC在RAWFC数据集上F1分数达65.58%，优于L-Defense的61.20%，同时运行时减少95%。</p>
<p><strong>Insight:</strong> 经过精心设计的检索系统可以在特定任务中匹配或超越LLM性能，同时更适用于实际部署。</p>
<p><strong>Abstract:</strong> The proliferation of misinformation necessitates robust yet computationally efficient fact verification systems. While current state-of-the-art approaches leverage Large Language Models (LLMs) for generating explanatory rationales, these methods face significant computational barriers and hallucination risks in real-world deployments. We present DeReC (Dense Retrieval Classification), a lightweight framework that demonstrates how general-purpose text embeddings can effectively replace autoregressive LLM-based approaches in fact verification tasks. By combining dense retrieval with specialized classification, our system achieves better accuracy while being significantly more efficient. DeReC outperforms explanation-generating LLMs in efficiency, reducing runtime by 95% on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92% on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds), showcasing its effectiveness across varying dataset sizes. On the RAWFC dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art method L-Defense (61.20%). Our results demonstrate that carefully engineered retrieval-based systems can match or exceed LLM performance in specialized tasks while being significantly more practical for real-world deployment.</p>
  </div>
</details>

<hr>
<h3 id="76-Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning-cs-CLPDF"><a href="#76-Logit-Entropy-Adaptive-Stopping-Heuristic-for-Efficient-Chain-of-Thought-Reasoning-cs-CLPDF" class="headerlink" title="[76] Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning cs.CLPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04654">Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04654" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammad Atif Quamar, Mohammad Areeb</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为LEASH的自适应停止启发式方法，用于在链式思维推理中高效停止生成过程，减少计算浪费。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 链式思维推理（CoT）在大型语言模型中需要生成固定长度的推理过程，这会浪费计算资源和增加延迟。</p>
<p><strong>Result:</strong> 在GSM8K和AQuA-RAT基准测试中，LEASH减少了30-35%的token生成和27%的延迟，但准确率下降了10个百分点。</p>
<p><strong>Insight:</strong> LEASH是一种无需训练的模型无关方法，为链式思维推理提供了一种高效的替代方案。</p>
<p><strong>Abstract:</strong> Chain-of-Thought (CoT) prompting is a key technique for enabling complex reasoning in large language models. However, generating full, fixed-length rationales is computationally wasteful, inflating both token usage and latency. We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free decoding algorithm that adaptively halts rationale generation. LEASH monitors two intrinsic signals: the slope of token-level entropy and the improvement in the top-logit margin. It terminates the generation once both signals plateau, indicating the model has reached a stable reasoning state. Across four instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces average token generation by 30–35% and latency by 27%, while incurring a 10 p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no additional training or supervision, offering a simple and efficient alternative to CoT decoding.</p>
  </div>
</details>

<hr>
<div id='cs.MA'></div>

<h1 id="cs-MA-Back"><a href="#cs-MA-Back" class="headerlink" title="cs.MA [Back]"></a>cs.MA <a href="#toc">[Back]</a></h1><h3 id="77-Multi-Agent-Collaborative-Framework-For-Math-Problem-Generation-cs-MA-cs-CL-cs-HC-I-2-11-I-2-6-K-3-1PDF"><a href="#77-Multi-Agent-Collaborative-Framework-For-Math-Problem-Generation-cs-MA-cs-CL-cs-HC-I-2-11-I-2-6-K-3-1PDF" class="headerlink" title="[77] Multi-Agent Collaborative Framework For Math Problem Generation cs.MA | cs.CL | cs.HC | I.2.11; I.2.6; K.3.1PDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03958">Multi-Agent Collaborative Framework For Math Problem Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MA | cs.CL | cs.HC | I.2.11; I.2.6; K.3.1</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03958" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kia Karbasi, Kevin Hong, Mohammad Amin Samadi, Gregory Pottie</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种多智能体协作框架，用于生成数学题目，通过在推理时引入多轮迭代优化来平衡问题的复杂性和认知需求，提升了自动生成题目的质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的预训练语言模型在自动生成数学题目时难以精确控制问题的复杂性和认知需求，影响了教育内容的质量和实用性。</p>
<p><strong>Result:</strong> 初步评估表明，该框架生成的题目在教育相关性和认知挑战之间实现了更好的平衡，提升了教育内容的质量。</p>
<p><strong>Insight:</strong> 多智能体协作工作流可以为自动生成教育内容提供更精细的控制，有望推动自适应学习环境的发展。</p>
<p><strong>Abstract:</strong> Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system’s ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="78-DR-WELL-Dynamic-Reasoning-and-Learning-with-Symbolic-World-Model-for-Embodied-LLM-Based-Multi-Agent-Collaboration-cs-AI-cs-CL-cs-LG-cs-MAPDF"><a href="#78-DR-WELL-Dynamic-Reasoning-and-Learning-with-Symbolic-World-Model-for-Embodied-LLM-Based-Multi-Agent-Collaboration-cs-AI-cs-CL-cs-LG-cs-MAPDF" class="headerlink" title="[78] DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration cs.AI | cs.CL | cs.LG | cs.MAPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04646">DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04646" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Narjes Nourzad, Hanqing Yang, Shiyu Chen, Carlee Joe-Wong</span></p>
<p><strong>TL;DR:</strong> DR. WELL是一个去中心化的神经符号框架，通过符号规划和动态世界模型实现多智能体协作，避免轨迹级别的脆弱对齐，提升任务的完成率和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多智能体协作中，轨迹级别的协调容易因小偏差导致冲突，而符号规划可以通过抽象化和同步最小动作词汇来解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，DR. WELL提高了任务完成率和效率，但也引入了时间开销。</p>
<p><strong>Insight:</strong> 符号规划和动态世界模型可以显著提升多智能体协作的可重用性和可解释性。</p>
<p><strong>Abstract:</strong> Cooperative multi-agent planning requires agents to make joint decisions with partial information and limited communication. Coordination at the trajectory level often fails, as small deviations in timing or movement cascade into conflicts. Symbolic planning mitigates this challenge by raising the level of abstraction and providing a minimal vocabulary of actions that enable synchronization and collective progress. We present DR. WELL, a decentralized neurosymbolic framework for cooperative multi-agent planning. Cooperation unfolds through a two-phase negotiation protocol: agents first propose candidate roles with reasoning and then commit to a joint allocation under consensus and environment constraints. After commitment, each agent independently generates and executes a symbolic plan for its role without revealing detailed trajectories. Plans are grounded in execution outcomes via a shared world model that encodes the current state and is updated as agents act. By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids brittle step-level alignment and enables higher-level operations that are reusable, synchronizable, and interpretable. Experiments on cooperative block-push tasks show that agents adapt across episodes, with the dynamic world model capturing reusable patterns and improving task completion rates and efficiency. Experiments on cooperative block-push tasks show that our dynamic world model improves task completion and efficiency through negotiation and self-refinement, trading a time overhead for evolving, more efficient collaboration strategies.</p>
  </div>
</details>

<hr>
<h3 id="79-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks-cs-AI-cs-CLPDF"><a href="#79-VeriCoT-Neuro-symbolic-Chain-of-Thought-Validation-via-Logical-Consistency-Checks-cs-AI-cs-CLPDF" class="headerlink" title="[79] VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks cs.AI | cs.CLPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04662">VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04662" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Feng, Nathaniel Weir, Kaj Bostrom, Sam Bayless, Darion Cassel</span></p>
<p><strong>TL;DR:</strong> VeriCoT提出了一种神经符号方法，通过形式逻辑验证CoT推理的合理性，结合自动化求解器和人类审查，有效识别逻辑缺陷，并在推理时和微调阶段提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLMs在多步推理（如Chain-of-Thought）中无法可靠验证其逻辑合理性，即使答案正确，推理过程也可能存在漏洞，这在高风险场景中尤为危险。</p>
<p><strong>Result:</strong> 在ProofWriter、LegalBench和BioASQ数据集上，VeriCoT能有效识别错误推理，并显著预测最终答案的正确性。还能通过微调提升模型推理能力。</p>
<p><strong>Insight:</strong> 结合神经与符号方法的优势，可以显著提升LLM推理的可信度，并为推理时优化和模型微调提供了新思路。</p>
<p><strong>Abstract:</strong> LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but they cannot reliably verify their own logic. Even when they reach correct answers, the underlying reasoning may be flawed, undermining trust in high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a neuro-symbolic method that extracts and verifies formal logical arguments from CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order logic and identifies premises that ground the argument in source context, commonsense knowledge, or prior reasoning steps. The symbolic representation enables automated solvers to verify logical validity while the NL premises allow humans and systems to identify ungrounded or fallacious reasoning steps. Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT effectively identifies flawed reasoning, and serves as a strong predictor of final answer correctness. We also leverage VeriCoT’s verification signal for (1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct preference optimization (DPO) using verification-based pairwise rewards, further improving reasoning validity and accuracy.</p>
  </div>
</details>

<hr>
<div id='cs.SD'></div>

<h1 id="cs-SD-Back"><a href="#cs-SD-Back" class="headerlink" title="cs.SD [Back]"></a>cs.SD <a href="#toc">[Back]</a></h1><h3 id="80-MIDI-LLM-Adapting-Large-Language-Models-for-Text-to-MIDI-Music-Generation-cs-SD-cs-CL-cs-MMPDF"><a href="#80-MIDI-LLM-Adapting-Large-Language-Models-for-Text-to-MIDI-Music-Generation-cs-SD-cs-CL-cs-MMPDF" class="headerlink" title="[80] MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation cs.SD | cs.CL | cs.MMPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03942">MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SD | cs.CL | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03942" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shih-Lun Wu, Yoon Kim, Cheng-Zhi Anna Huang</span></p>
<p><strong>TL;DR:</strong> MIDI-LLM将大型语言模型(LLM)扩展为能够从自由文本提示生成多轨MIDI音乐的系统，通过两阶段训练实现文本到MIDI的能力，并在质量和速度上优于Text2midi模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文本到音乐的生成模型在控制性和推理速度上存在不足，而MIDI-LLM旨在通过扩展LLM的能力并保留其参数结构来解决这些问题。</p>
<p><strong>Result:</strong> 实验表明，MIDI-LLM在生成质量、文本控制和推理速度上优于Text2midi模型。</p>
<p><strong>Insight:</strong> 通过保留LLM原有结构并扩展其能力，可以在多模态任务中高效利用现有的语言模型基础设施。</p>
<p><strong>Abstract:</strong> We present MIDI-LLM, an LLM for generating multitrack MIDI music from free-form text prompts. Our approach expands a text LLM’s vocabulary to include MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI abilities. By preserving the original LLM’s parameter structure, we can directly leverage the vLLM library for accelerated inference. Experiments show that MIDI-LLM achieves higher quality, better text control, and faster inference compared to the recent Text2midi model. Live demo at <a target="_blank" rel="noopener" href="https://midi-llm-demo.vercel.app/">https://midi-llm-demo.vercel.app</a>.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="81-Shape-Deformation-Networks-for-Automated-Aortic-Valve-Finite-Element-Meshing-from-3D-CT-Images-eess-IV-cs-CV-cs-LGPDF"><a href="#81-Shape-Deformation-Networks-for-Automated-Aortic-Valve-Finite-Element-Meshing-from-3D-CT-Images-eess-IV-cs-CV-cs-LGPDF" class="headerlink" title="[81] Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images eess.IV | cs.CV | cs.LGPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03890">Shape Deformation Networks for Automated Aortic Valve Finite Element Meshing from 3D CT Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03890" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Linchen Qian, Jiasong Chen, Ruonan Gong, Wei Sun, Minliang Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于深度神经网络的模板拟合流程，用于从3D CT图像生成主动脉瓣的结构化四边形网格，以提高网格质量和一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的主动脉瓣几何建模方法生成的三角网格拓扑不规则，容易出现形状不佳的元素和患者间不一致的对应关系，这对生物力学分析和术前规划造成了挑战。</p>
<p><strong>Result:</strong> 实验结果表明，该方法生成的主动脉瓣表面网格具有更高的平滑性和形状质量，同时比传统方法需要更少的显式正则化项。</p>
<p><strong>Insight:</strong> 使用结构化四边形网格作为模板和训练目标，不仅确保了网格的一致性和质量，还简化了训练过程，提高了建模的效率和效果。</p>
<p><strong>Abstract:</strong> Accurate geometric modeling of the aortic valve from 3D CT images is essential for biomechanical analysis and patient-specific simulations to assess valve health or make a preoperative plan. However, it remains challenging to generate aortic valve meshes with both high-quality and consistency across different patients. Traditional approaches often produce triangular meshes with irregular topologies, which can result in poorly shaped elements and inconsistent correspondence due to inter-patient anatomical variation. In this work, we address these challenges by introducing a template-fitting pipeline with deep neural networks to generate structured quad (i.e., quadrilateral) meshes from 3D CT images to represent aortic valve geometries. By remeshing aortic valves of all patients with a common quad mesh template, we ensure a uniform mesh topology with consistent node-to-node and element-to-element correspondence across patients. This consistency enables us to simplify the learning objective of the deep neural networks, by employing a loss function with only two terms (i.e., a geometry reconstruction term and a smoothness regularization term), which is sufficient to preserve mesh smoothness and element quality. Our experiments demonstrate that the proposed approach produces high-quality aortic valve surface meshes with improved smoothness and shape quality, while requiring fewer explicit regularization terms compared to the traditional methods. These results highlight that using structured quad meshes for the template and neural network training not only ensures mesh correspondence and quality but also simplifies the training process, thus enhancing the effectiveness and efficiency of aortic valve modeling.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="82-RLHF-A-comprehensive-Survey-for-Cultural-Multimodal-and-Low-Latency-Alignment-Methods-cs-LG-cs-AI-cs-CLPDF"><a href="#82-RLHF-A-comprehensive-Survey-for-Cultural-Multimodal-and-Low-Latency-Alignment-Methods-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[82] RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods cs.LG | cs.AI | cs.CLPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03939">RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03939" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Raghav Sharma, Manan Mehta, Sai Tiger Raina</span></p>
<p><strong>TL;DR:</strong> 这篇调查论文系统地总结了强化学习人类反馈（RLHF）在多模态对齐、文化公平和低延迟优化方面的最新进展，并提出了未来研究方向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> RLHF目前主要应用于文本领域，但在多模态对齐、文化公平性和低延迟优化方面存在研究空白。本文旨在填补这些空白，并为构建更健壮、高效和公平的AI系统提供指导。</p>
<p><strong>Result:</strong> 通过综合分析，论文指出当前RLHF在这些新兴领域的研究进展和不足，并提出了未来需要解决的问题。</p>
<p><strong>Insight:</strong> 多模态和文化公平性是RLHF研究的新方向，未来的AI系统需要在这些方面进一步优化，以提高其鲁棒性和公平性。</p>
<p><strong>Abstract:</strong> Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.</p>
  </div>
</details>

<hr>
<h3 id="83-The-Illusion-of-Certainty-Uncertainty-quantification-for-LLMs-fails-under-ambiguity-cs-LG-cs-CLPDF"><a href="#83-The-Illusion-of-Certainty-Uncertainty-quantification-for-LLMs-fails-under-ambiguity-cs-LG-cs-CLPDF" class="headerlink" title="[83] The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity cs.LG | cs.CLPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04418">The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04418" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tim Tomov, Dominik Fuchsgruber, Tom Wollschläger, Stephan Günnemann</span></p>
<p><strong>TL;DR:</strong> 这篇论文揭示了当前大型语言模型（LLMs）不确定性量化（UQ）方法在模糊性问题上的表现接近随机，提出了首个模糊问答数据集，并指出现有方法的理论局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 实际语言具有模糊性，但现有的不确定性量化方法通常在无模糊性任务上测试，导致其在真实场景中表现不佳。</p>
<p><strong>Result:</strong> 研究表明，现有UQ方法在模糊性问题上的表现接近随机，且理论分析表明其存在根本性局限。</p>
<p><strong>Insight:</strong> 需要重新思考当前LLMs的不确定性量化建模范式，以适应真实世界的语言模糊性。</p>
<p><strong>Abstract:</strong> Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is critical for trustworthy deployment. While real-world language is inherently ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically benchmarked against tasks with no ambiguity. In this work, we demonstrate that while current uncertainty estimators perform well under the restrictive assumption of no ambiguity, they degrade to close-to-random performance on ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first ambiguous question-answering (QA) datasets equipped with ground-truth answer distributions estimated from factual co-occurrence. We find this performance deterioration to be consistent across different estimation paradigms: using the predictive distribution itself, internal representations throughout the model, and an ensemble of models. We show that this phenomenon can be theoretically explained, revealing that predictive-distribution and ensemble-based estimators are fundamentally limited under ambiguity. Overall, our study reveals a key shortcoming of current UQ methods for LLMs and motivates a rethinking of current modeling paradigms.</p>
  </div>
</details>

<hr>
<h3 id="84-What’s-in-Common-Multimodal-Models-Hallucinate-When-Reasoning-Across-Scenes-cs-LG-cs-CVPDF"><a href="#84-What’s-in-Common-Multimodal-Models-Hallucinate-When-Reasoning-Across-Scenes-cs-LG-cs-CVPDF" class="headerlink" title="[84] What’s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes cs.LG | cs.CVPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03768">What’s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03768" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Candace Ross, Florian Bordes, Adina Williams, Polina Kirichenko, Mark Ibrahim</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为Common-O的新基准测试，用于评估多模态语言模型在跨场景推理时的幻觉问题，发现当前模型在复杂推理任务上表现不佳，尤其在相似物体存在的场景中更容易产生幻觉。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管多模态语言模型在开放词汇任务上表现优异，但在真实世界场景中的跨场景推理时仍存在严重的幻觉问题。论文旨在填补现有感知基准测试与真实世界推理之间的差距。</p>
<p><strong>Result:</strong> 尽管模型在感知任务上表现饱和，但在Common-O上的最佳模型仅达到35%的准确率，而在更复杂的Common-O Complex任务上仅为1%。模型在相似物体存在的场景中更容易产生幻觉。</p>
<p><strong>Insight:</strong> 论文揭示了多模态语言模型在跨场景推理中的局限性，指出多图像训练可能是未来改进的方向。同时，提出了公开基准测试以推动相关研究。</p>
<p><strong>Abstract:</strong> Multimodal language models possess a remarkable ability to handle an open-vocabulary’s worth of objects. Yet the best models still suffer from hallucinations when reasoning about scenes in the real world, revealing a gap between their seemingly strong performance on existing perception benchmarks that are saturating and their reasoning in the real world. To address this gap, we build a novel benchmark of in-the-wild scenes that we call Common-O. With more than 10.5k examples using exclusively new images not found in web training data to avoid contamination, Common-O goes beyond just perception, inspired by cognitive tests for humans, to probe reasoning across scenes by asking “what’s in common?”. We evaluate leading multimodal language models, including models specifically trained to perform chain-of-thought reasoning. We find that perceiving objects in single images is tractable for most models, yet reasoning across scenes is very challenging even for the best models, including reasoning models. Despite saturating many leaderboards focusing on perception, the best performing model only achieves 35% on Common-O – and on Common-O Complex, consisting of more complex scenes, the best model achieves only 1%. Curiously, we find models are more prone to hallucinate when similar objects are present in the scene, suggesting models may be relying on object co-occurrence seen during training. Among the models we evaluated, we found scale can provide modest improvements while models explicitly trained with multi-image inputs show bigger improvements, suggesting scaled multi-image training may offer promise. We make our benchmark publicly available to spur research into the challenge of hallucination when reasoning across scenes.</p>
  </div>
</details>

<hr>
<h3 id="85-NVIDIA-Nemotron-Nano-V2-VL-cs-LG-cs-AI-cs-CVPDF"><a href="#85-NVIDIA-Nemotron-Nano-V2-VL-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[85] NVIDIA Nemotron Nano V2 VL cs.LG | cs.AI | cs.CVPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03929">NVIDIA Nemotron Nano V2 VL</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03929" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">NVIDIA, :, Amala Sanjay Deshmukh, Kateryna Chumachenko, Tuomas Rintamaki</span></p>
<p><strong>TL;DR:</strong> NVIDIA发布了Nemotron Nano V2 VL，这是Nemotron视觉-语言系列的最新型号，专为现实世界文档理解、长视频理解和推理任务设计，性能优于前代模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 提升视觉-语言模型在文档理解和长视频场景中的性能，同时优化推理效率。</p>
<p><strong>Result:</strong> 模型性能显著优于前代Llama-3.1-Nemotron-Nano-VL-8B，并在多个视觉和文本任务中表现优异。</p>
<p><strong>Insight:</strong> 结合Mamba和Transformer的混合架构以及高效的token缩减技术，可能是未来长序列场景中视觉-语言模型的发展方向。</p>
<p><strong>Abstract:</strong> We introduce Nemotron Nano V2 VL, the latest model of the Nemotron vision-language series designed for strong real-world document understanding, long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers significant improvements over our previous model, Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major enhancements in model architecture, datasets, and training recipes. Nemotron Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and innovative token reduction techniques to achieve higher inference throughput in long document and video scenarios. We are releasing model checkpoints in BF16, FP8, and FP4 formats and sharing large parts of our datasets, recipes and training code.</p>
  </div>
</details>

<hr>
<h3 id="86-Distribution-Aware-Tensor-Decomposition-for-Compression-of-Convolutional-Neural-Networks-cs-LG-cs-CVPDF"><a href="#86-Distribution-Aware-Tensor-Decomposition-for-Compression-of-Convolutional-Neural-Networks-cs-LG-cs-CVPDF" class="headerlink" title="[86] Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks cs.LG | cs.CVPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04494">Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04494" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alper Kalle, Theo Rudkiewicz, Mohamed-Oumar Ouerfelli, Mohamed Tamaazousti</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于数据感知的张量分解方法，用于卷积神经网络（CNN）的压缩，目标是减小计算和内存开销，同时保持模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统方法在权重空间中最小化各向同性范数（如Frobenius范数）进行压缩，但忽略了输入数据分布的影响，可能导致性能下降。本文旨在通过数据感知的范数优化压缩过程。</p>
<p><strong>Result:</strong> 在ResNet-18&#x2F;50、GoogLeNet等CNN架构和ImageNet、Cifar10&#x2F;100等多个数据集上验证了方法的有效性，无需微调即可保持高性能。</p>
<p><strong>Insight:</strong> 1. 数据感知的压缩方法优于传统权重空间优化；2. 协方差信息可跨数据集迁移，增强了方法的通用性；3. 为CNN压缩提供了一种无需微调的高效解决方案。</p>
<p><strong>Abstract:</strong> Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer’s output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1&#x2F;2}\rVert_F$ where $\Sigma^{1&#x2F;2}$ is the square root of the covariance matrix of the layer’s input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18&#x2F;50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="87-Black-Box-Guardrail-Reverse-engineering-Attack-cs-CR-cs-CLPDF"><a href="#87-Black-Box-Guardrail-Reverse-engineering-Attack-cs-CR-cs-CLPDF" class="headerlink" title="[87] Black-Box Guardrail Reverse-engineering Attack cs.CR | cs.CLPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04215">Black-Box Guardrail Reverse-engineering Attack</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04215" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongwei Yao, Yun Xia, Shuo Shao, Haoran Shi, Tong Qiao</span></p>
<p><strong>TL;DR:</strong> 该论文首次研究了黑盒大型语言模型（LLM）护栏的反向工程攻击，提出了基于强化学习的框架GRA，通过遗传算法驱动的数据增强逼近目标护栏的决策策略，实现了高精度的规则匹配。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLM广泛采用护栏机制以约束输出，这些机制暴露了新的安全漏洞，研究目标是通过反向工程攻击揭示其决策逻辑，从而评估其安全风险。</p>
<p><strong>Result:</strong> 在ChatGPT、DeepSeek和Qwen3三个商业化系统上的实验表明，GRA规则匹配率超过0.92，且API成本低于85美元。</p>
<p><strong>Insight:</strong> 研究揭示了当前LLM护栏设计的关键漏洞，强调了在实际部署中需要更鲁棒的防御机制。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) increasingly employ guardrails to enforce ethical, legal, and application-specific constraints on their outputs. While effective at mitigating harmful responses, these guardrails introduce a new class of vulnerabilities by exposing observable decision patterns. In this work, we present the first study of black-box LLM guardrail reverse-engineering attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement learning-based framework that leverages genetic algorithm-driven data augmentation to approximate the decision-making policy of victim guardrails. By iteratively collecting input-output pairs, prioritizing divergence cases, and applying targeted mutations and crossovers, our method incrementally converges toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3, and demonstrate that it achieves an rule matching rate exceeding 0.92 while requiring less than $85 in API costs. These findings underscore the practical feasibility of guardrail extraction and highlight significant security risks for current LLM safety mechanisms. Our findings expose critical vulnerabilities in current guardrail designs and highlight the urgent need for more robust defense mechanisms in LLM deployment.</p>
  </div>
</details>

<hr>
<div id='eess.SY'></div>

<h1 id="eess-SY-Back"><a href="#eess-SY-Back" class="headerlink" title="eess.SY [Back]"></a>eess.SY <a href="#toc">[Back]</a></h1><h3 id="88-A-convolutional-neural-network-deep-learning-method-for-model-class-selection-eess-SY-cs-AI-cs-CV-cs-LG-cs-SY-eess-SP-68T05-Learning-and-adaptive-systems-93C95-Neural-networks-in"><a href="#88-A-convolutional-neural-network-deep-learning-method-for-model-class-selection-eess-SY-cs-AI-cs-CV-cs-LG-cs-SY-eess-SP-68T05-Learning-and-adaptive-systems-93C95-Neural-networks-in" class="headerlink" title="[88] A convolutional neural network deep learning method for model class selection eess.SY | cs.AI | cs.CV | cs.LG | cs.SY | eess.SP | 68T05 (Learning and adaptive systems) 93C95 (Neural networks in"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.03743">A convolutional neural network deep learning method for model class selection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.SY | cs.AI | cs.CV | cs.LG | cs.SY | eess.SP | 68T05 (Learning and adaptive systems) 93C95 (Neural networks in</h3><p>  control theory) | I.2.6; I.2.8</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.03743" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span><br><span style="color: #7f8c8d; font-style: italic;">Marios Impraimakis</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于卷积神经网络（CNN）的深度学习方法，用于模型类别的选择，无需系统输入信息或完整的系统识别，适用于结构健康监测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统模型类别选择方法通常需要系统输入信息或完整的系统识别过程，这在某些实际应用中可能不切实际。因此，作者提出了一种仅基于响应的深度学习方法。</p>
<p><strong>Result:</strong> 实验表明，该方法能够从微小的信号变化中准确选择模型类别，包括阻尼行为或滞后行为的影响，且适用于复杂系统。</p>
<p><strong>Insight:</strong> 该方法展示了深度学习在结构健康监测中的潜力，尤其是在缺乏完整系统信息时的实用性。</p>
<p><strong>Abstract:</strong> The response-only model class selection capability of a novel deep convolutional neural network method is examined herein in a simple, yet effective, manner. Specifically, the responses from a unique degree of freedom along with their class information train and validate a one-dimensional convolutional neural network. In doing so, the network selects the model class of new and unlabeled signals without the need of the system input information, or full system identification. An optional physics-based algorithm enhancement is also examined using the Kalman filter to fuse the system response signals using the kinematics constraints of the acceleration and displacement data. Importantly, the method is shown to select the model class in slight signal variations attributed to the damping behavior or hysteresis behavior on both linear and nonlinear dynamic systems, as well as on a 3D building finite element model, providing a powerful tool for structural health monitoring applications.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="89-GraSP-VLA-Graph-based-Symbolic-Action-Representation-for-Long-Horizon-Planning-with-VLA-Policies-cs-RO-cs-CVPDF"><a href="#89-GraSP-VLA-Graph-based-Symbolic-Action-Representation-for-Long-Horizon-Planning-with-VLA-Policies-cs-RO-cs-CVPDF" class="headerlink" title="[89] GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies cs.RO | cs.CVPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04357">GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04357" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maëlic Neau, Zoe Falomir, Paulo E. Santos, Anne-Gwenn Bosser, Cédric Buche</span></p>
<p><strong>TL;DR:</strong> GraSP-VLA提出了一种结合神经符号方法的框架，通过连续场景图生成符号化动作表示，解决长期规划任务中VLA模型缺乏高层规划能力和符号方法泛化不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有VLA模型缺乏高层符号规划能力，而符号方法在泛化和扩展性上受限，无法满足长期任务需求。</p>
<p><strong>Result:</strong> 实验表明，GraSP-VLA在自动规划领域生成任务中表现有效，并在实际长期任务中展示了协调低层VLA策略的潜力。</p>
<p><strong>Insight:</strong> 通过神经符号结合，GraSP-VLA在长期任务中实现了更高层规划和低层执行的协调，为机器人学习提供了新思路。</p>
<p><strong>Abstract:</strong> Deploying autonomous robots that can learn new skills from demonstrations is an important challenge of modern robotics. Existing solutions often apply end-to-end imitation learning with Vision-Language Action (VLA) models or symbolic approaches with Action Model Learning (AML). On the one hand, current VLA models are limited by the lack of high-level symbolic planning, which hinders their abilities in long-horizon tasks. On the other hand, symbolic approaches in AML lack generalization and scalability perspectives. In this paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that uses a Continuous Scene Graph representation to generate a symbolic representation of human demonstrations. This representation is used to generate new planning domains during inference and serves as an orchestrator for low-level VLA policies, scaling up the number of actions that can be reproduced in a row. Our results show that GraSP-VLA is effective for modeling symbolic representations on the task of automatic planning domain generation from observations. In addition, results on real-world experiments show the potential of our Continuous Scene Graph representation to orchestrate low-level VLA policies in long-horizon tasks.</p>
  </div>
</details>

<hr>
<h3 id="90-Evo-1-Lightweight-Vision-Language-Action-Model-with-Preserved-Semantic-Alignment-cs-RO-cs-CVPDF"><a href="#90-Evo-1-Lightweight-Vision-Language-Action-Model-with-Preserved-Semantic-Alignment-cs-RO-cs-CVPDF" class="headerlink" title="[90] Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment cs.RO | cs.CVPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04555">Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04555" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tao Lin, Yilei Zhong, Yuxin Du, Jingjing Zhang, Jiting Liu</span></p>
<p><strong>TL;DR:</strong> Evo-1 是一个轻量级的视觉-语言-动作（VLA）模型，通过优化架构和训练范式，减少了计算成本和部署负担，同时保持了高性能，无需机器人数据预训练。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的 VLA 模型通常参数量巨大且依赖大规模机器人数据预训练，计算成本高且部署效率低。此外，训练方式常损害视觉-语言模型的感知表示，导致泛化能力差。Evo-1 旨在解决这些问题。</p>
<p><strong>Result:</strong> 1. 在 Meta-World 和 RoboTwin 上超越 SOTA 12.4% 和 6.9%；2. LIBERO 任务达到 94.8%；3. 实际应用中成功率 78%，推理频率高且内存占用低。</p>
<p><strong>Insight:</strong> 轻量化和高效架构设计是关键，同时保持视觉-语言模型的语义对齐能力是提升泛化性能的重要策略。</p>
<p><strong>Abstract:</strong> Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.</p>
  </div>
</details>

<hr>
<h3 id="91-Real-to-Sim-Robot-Policy-Evaluation-with-Gaussian-Splatting-Simulation-of-Soft-Body-Interactions-cs-RO-cs-CV-cs-LGPDF"><a href="#91-Real-to-Sim-Robot-Policy-Evaluation-with-Gaussian-Splatting-Simulation-of-Soft-Body-Interactions-cs-RO-cs-CV-cs-LGPDF" class="headerlink" title="[91] Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions cs.RO | cs.CV | cs.LGPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04665">Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04665" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaifeng Zhang, Shuo Sha, Hanxiao Jiang, Matthew Loper, Hyunjong Song</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个基于3D高斯点云的从真实到仿真（real-to-sim）的策略评估框架，用于高效评估机器人策略在涉及可变形物体任务中的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 直接在真实世界中评估机器人策略成本高、耗时长且难以重复，尤其是涉及可变形物体的任务。现有仿真器难以同时捕捉视觉和物理复杂性。</p>
<p><strong>Result:</strong> 在填充玩具、绳索路由和T形块推送等任务中，仿真结果与真实世界表现高度相关，并能揭示策略的关键行为模式。</p>
<p><strong>Insight:</strong> 物理信息的重建与高质量渲染结合，可提供可重复、可扩展且准确的机器人策略评估方法。</p>
<p><strong>Abstract:</strong> Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: <a target="_blank" rel="noopener" href="https://real2sim-eval.github.io/">https://real2sim-eval.github.io/</a></p>
  </div>
</details>

<hr>
<h3 id="92-X-Diffusion-Training-Diffusion-Policies-on-Cross-Embodiment-Human-Demonstrations-cs-RO-cs-AI-cs-CVPDF"><a href="#92-X-Diffusion-Training-Diffusion-Policies-on-Cross-Embodiment-Human-Demonstrations-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[92] X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations cs.RO | cs.AI | cs.CVPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04671">X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04671" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maximus A. Pace, Prithwish Dan, Chuanruo Ning, Atiksh Bhardwaj, Audrey Du</span></p>
<p><strong>TL;DR:</strong> X-Diffusion利用扩散过程在人类和机器人的动作差异中提取有用信息，通过噪声分类器区分动作来源，从而最大化利用人类数据训练机器人策略。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类视频数据丰富，但与机器人动作执行存在差异，直接使用会导致无效动作学习。需要一种方法在不引入动态不可行动作的情况下利用人类数据。</p>
<p><strong>Result:</strong> 在五个任务中，X-Diffusion平均成功率比基线高16%，证明其在利用人类数据上的有效性。</p>
<p><strong>Insight:</strong> 噪声扩散过程能自然过滤低级执行差异，保留高级任务信息，为跨实体学习提供了新思路。</p>
<p><strong>Abstract:</strong> Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at <a target="_blank" rel="noopener" href="https://portal-cornell.github.io/X-Diffusion/">https://portal-cornell.github.io/X-Diffusion/</a>.</p>
  </div>
</details>

<hr>
<h3 id="93-GentleHumanoid-Learning-Upper-body-Compliance-for-Contact-rich-Human-and-Object-Interaction-cs-RO-cs-CV-cs-HCPDF"><a href="#93-GentleHumanoid-Learning-Upper-body-Compliance-for-Contact-rich-Human-and-Object-Interaction-cs-RO-cs-CV-cs-HCPDF" class="headerlink" title="[93] GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction cs.RO | cs.CV | cs.HCPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.04679">GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.04679" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qingzhou Lu, Yao Feng, Baiyu Shi, Michael Piseno, Zhenan Bao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为GentleHumanoid的框架，通过将阻抗控制与全身运动跟踪策略结合，实现上半身的柔顺性控制，从而在人与物体交互中减少峰值接触力并保持任务成功。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在人类中心环境中，人形机器人需要安全自然的物理交互，但现有强化学习策略多强调刚性跟踪，缺乏对柔顺性的支持。</p>
<p><strong>Result:</strong> 在仿真和Unitree G1人形机器人上验证，任务中峰值接触力显著降低，交互更自然流畅。</p>
<p><strong>Insight:</strong> 通过统一的柔顺性控制框架，能够在复杂交互中平衡安全性与任务完成效果，为人形机器人的实际应用提供支持。</p>
<p><strong>Abstract:</strong> Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-11-11/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-11-07/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
