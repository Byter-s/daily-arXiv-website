<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-12-10 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-12-10
            
            
        </div>
        <span class="post-date">
            Dec 10, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 166]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 40]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 3]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 14]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 1]</li>
<li><a href="#cs.MA">cs.MA</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 8]</li>
<li><a href="#cs.CY">cs.CY</a> [Total: 1]</li>
<li><a href="#cs.CE">cs.CE</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 5]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 1]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Video-Models-Start-to-Solve-Chess-Maze-Sudoku-Mental-Rotation-and-Raven’-Matrices-cs-CV-cs-AIPDF"><a href="#1-Video-Models-Start-to-Solve-Chess-Maze-Sudoku-Mental-Rotation-and-Raven’-Matrices-cs-CV-cs-AIPDF" class="headerlink" title="[1] Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven’ Matrices cs.CV | cs.AIPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05969">Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven’ Matrices</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05969" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hokin Deng</span></p>
<p><strong>TL;DR:</strong> 论文展示了视频生成模型在推理任务（如象棋、迷宫、数独、心理旋转和Raven矩阵）上的能力，成功率可达60%。作者提出了一种“任务对”设计的实验范式，并开发了支持该范式的代码框架VMEvalKit，其自动评估结果与人类判断高度相关。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索视频生成模型是否能够执行复杂的推理任务，并通过建立可扩展的实验范式推动这一领域的发展。</p>
<p><strong>Result:</strong> 模型在象棋、迷宫等任务上的成功率可达60%，且自动评估与人类判断强相关。</p>
<p><strong>Insight:</strong> 视频生成模型已初步具备推理能力，通过VMEvalKit框架的开放和标准化，有望进一步推动视频模型的推理能力提升。</p>
<p><strong>Abstract:</strong> We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven’s Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the “Task Pair” design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\href{<a target="_blank" rel="noopener" href="https://grow-ai-like-a-child.com/video-reason/%7D%7Bresults%7D$">https://grow-ai-like-a-child.com/video-reason/}{results}$</a> and our $\href{<a target="_blank" rel="noopener" href="https://github.com/hokindeng/VMEvalKit%7D%7BVMEvalKit%7D$">https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$</a> codebase.</p>
  </div>
</details>

<hr>
<h3 id="2-Adaptive-Dataset-Quantization-A-New-Direction-for-Dataset-Pruning-cs-CV-cs-AIPDF"><a href="#2-Adaptive-Dataset-Quantization-A-New-Direction-for-Dataset-Pruning-cs-CV-cs-AIPDF" class="headerlink" title="[2] Adaptive Dataset Quantization: A New Direction for Dataset Pruning cs.CV | cs.AIPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05987">Adaptive Dataset Quantization: A New Direction for Dataset Pruning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05987" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenyue Yu, Jianyu Yu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新的数据集量化方法，通过减少样本内的冗余来降低存储和通信成本，适用于资源受限的边缘设备。与传统方法不同，该方法通过自适应量化分配算法，在不同精度需求的样本间分配不同的量化比例，实现了高效的数据集压缩。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模数据集在资源受限的边缘设备上存储和传输成本高，传统方法主要解决样本间的冗余问题，而忽略了样本内的冗余。作者提出了一种新的数据集量化方法，以解决这一挑战。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在保持模型训练性能的同时，显著压缩了数据集，优于传统量化和数据集剪枝方法。</p>
<p><strong>Insight:</strong> 样本内冗余是降低数据集存储和传输成本的新方向，自适应量化分配为实现高效压缩提供了新的思路。</p>
<p><strong>Abstract:</strong> This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method’s effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.</p>
  </div>
</details>

<hr>
<h3 id="3-VG3T-Visual-Geometry-Grounded-Gaussian-Transformer-cs-CV-cs-AI-cs-LGPDF"><a href="#3-VG3T-Visual-Geometry-Grounded-Gaussian-Transformer-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[3] VG3T: Visual Geometry Grounded Gaussian Transformer cs.CV | cs.AI | cs.LGPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05988">VG3T: Visual Geometry Grounded Gaussian Transformer</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05988" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junho Kim, Seongwon Lee</span></p>
<p><strong>TL;DR:</strong> VG3T提出了一种新颖的多视角前馈网络，通过3D高斯表示预测3D语义占据，解决了现有方法在多视角融合中的碎片化和不一致性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多视角图像生成3D场景表征时存在的碎片化和不一致性问题，提升多视角融合的质量和效率。</p>
<p><strong>Result:</strong> 在nuScenes基准测试中，VG3T的mIoU提升了1.7%，同时减少了46%的基元数量，展现出更高的效率和性能。</p>
<p><strong>Insight:</strong> 通过联合多视角信息和高斯表示的直接优化，VG3T为3D场景理解提供了一种更统一和高效的方法。</p>
<p><strong>Abstract:</strong> Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.</p>
  </div>
</details>

<hr>
<h3 id="4-EmoDiffTalk-Emotion-aware-Diffusion-for-Editable-3D-Gaussian-Talking-Head-cs-CVPDF"><a href="#4-EmoDiffTalk-Emotion-aware-Diffusion-for-Editable-3D-Gaussian-Talking-Head-cs-CVPDF" class="headerlink" title="[4] EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head cs.CVPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05991">EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05991" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chang Liu, Tianjiao Jing, Chengcheng Ma, Xuanqi Zhou, Zhengxuan Lian</span></p>
<p><strong>TL;DR:</strong> EmoDiffTalk提出了一种新型的可编辑3D高斯说话头部模型，通过情感感知的高斯扩散方法和多模态控制，实现了细腻和动态的情感编辑。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于3D高斯飞溅的说话头部模型在情感表达上存在不足，尤其是缺乏精细和动态的情绪编辑能力。</p>
<p><strong>Result:</strong> 在EmoTalk3D和RenderMe-360数据集上表现优异，情感细腻度、唇同步和可控性均超越前人工作。</p>
<p><strong>Insight:</strong> EmoDiffTalk为高质量、多模态可编辑的3D说话头部合成提供了新思路，尤其是AU空间的连续情感编辑。</p>
<p><strong>Abstract:</strong> Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.</p>
  </div>
</details>

<hr>
<h3 id="5-FishDetector-R1-Unified-MLLM-Based-Framework-with-Reinforcement-Fine-Tuning-for-Weakly-Supervised-Fish-Detection-Segmentation-and-Counting-cs-CV-cs-CY-cs-RO-eess-IVPDF"><a href="#5-FishDetector-R1-Unified-MLLM-Based-Framework-with-Reinforcement-Fine-Tuning-for-Weakly-Supervised-Fish-Detection-Segmentation-and-Counting-cs-CV-cs-CY-cs-RO-eess-IVPDF" class="headerlink" title="[5] FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting cs.CV | cs.CY | cs.RO | eess.IVPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05996">FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CY | cs.RO | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05996" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi Liu, Jingyu Song, Vedanth Kallakuri, Katherine A. Skinner</span></p>
<p><strong>TL;DR:</strong> FishDetector-R1是一个基于MLLM的框架，通过强化学习微调和弱监督实现了鱼类检测、分割和计数的统一解决方案，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 水下鱼类图像分析对生态监测至关重要，但受限于视觉退化和昂贵标注，提出了一种弱监督的解决方案。</p>
<p><strong>Result:</strong> 在DeepFish数据集上，AP提升20%，mIoU提升10%，MAE和GAME分别降低30%和35%。</p>
<p><strong>Insight:</strong> 验证了奖励设计的有效性，展示了强跨域鲁棒性，为弱监督海洋视觉理解提供可靠方案。</p>
<p><strong>Abstract:</strong> Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is <a target="_blank" rel="noopener" href="https://umfieldrobotics.github.io/FishDetector-R1">https://umfieldrobotics.github.io/FishDetector-R1</a>.</p>
  </div>
</details>

<hr>
<h3 id="6-Simple-Agents-Outperform-Experts-in-Biomedical-Imaging-Workflow-Optimization-cs-CV-cs-AIPDF"><a href="#6-Simple-Agents-Outperform-Experts-in-Biomedical-Imaging-Workflow-Optimization-cs-CV-cs-AIPDF" class="headerlink" title="[6] Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization cs.CV | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06006">Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06006" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuefei, Wang, Kai A. Horstmann, Ethan Lin, Jonathan Chen</span></p>
<p><strong>TL;DR:</strong> 简单AI代理在生物医学影像工作流优化中超越专家表现。研究发现，复杂代理架构并非普适有效，提出了实用的代理设计路线图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 将生产级计算机视觉工具适配到特定科学数据集是‘最后一公里’瓶颈。当前解决方案（如精细调参或手动编码）耗时耗力，科学家缺乏标注数据。</p>
<p><strong>Result:</strong> 简单代理生成的代码在适配任务中表现优于专家手动解决方案。</p>
<p><strong>Insight:</strong> 复杂代理架构未必在所有任务中有效，简单设计可能更实用，为实际应用提供了清晰的代理设计指导。</p>
<p><strong>Abstract:</strong> Adapting production-level computer vision tools to bespoke scientific datasets is a critical “last mile” bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.</p>
  </div>
</details>

<hr>
<h3 id="7-Fast-and-Flexible-Robustness-Certificates-for-Semantic-Segmentation-cs-CVPDF"><a href="#7-Fast-and-Flexible-Robustness-Certificates-for-Semantic-Segmentation-cs-CVPDF" class="headerlink" title="[7] Fast and Flexible Robustness Certificates for Semantic Segmentation cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06010">Fast and Flexible Robustness Certificates for Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06010" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thomas Massena, Corentin Friedrich, Franck Mamalet, Mathieu Serrurier</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新型的可证明鲁棒的语义分割网络，基于Lipschitz约束，实现了高效的训练和实时认证，显著提升了计算效率，性能优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的鲁棒性研究主要集中在分类任务上，语义分割的高效认证方法较少；论文旨在填补这一空白，提供实时兼容的鲁棒认证框架。</p>
<p><strong>Result:</strong> 在Cityscapes等数据集上取得了具有竞争力的像素精度，认证速度显著提升，且认证结果与最先进的对抗攻击方法相当。</p>
<p><strong>Insight:</strong> Lipschitz约束不仅为实现可证明鲁棒性提供了理论保证，还为高效的实时认证奠定了基础，适合工业应用。</p>
<p><strong>Abstract:</strong> Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\ell_2$ attacks of radius $ε$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.</p>
  </div>
</details>

<hr>
<h3 id="8-High-Throughput-Unsupervised-Profiling-of-the-Morphology-of-316L-Powder-Particles-for-Use-in-Additive-Manufacturing-cs-CVPDF"><a href="#8-High-Throughput-Unsupervised-Profiling-of-the-Morphology-of-316L-Powder-Particles-for-Use-in-Additive-Manufacturing-cs-CVPDF" class="headerlink" title="[8] High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing cs.CVPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06012">High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06012" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Emmanuel Akeweje, Conall Kirk, Chi-Wai Chan, Denis Dowling, Mimi Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于机器学习的自动化框架，用于高通量分析和聚类316L金属粉末的形态，以提升选择性激光熔化（SLM）技术的原料质量控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统粉末表征方法效率低、定性化，无法捕捉工业规模粉末批次的异质性，影响SLM技术的零件质量。</p>
<p><strong>Result:</strong> 傅里叶描述符+k均值方法表现最佳，运行速度快，适用于大规模数据集（126,000张粉末图像）。</p>
<p><strong>Insight:</strong> 该框架为未来的粉末流动性和SLM零件质量研究提供了形态分类基础，同时支持实时原料监控。</p>
<p><strong>Abstract:</strong> Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.</p>
  </div>
</details>

<hr>
<h3 id="9-VAT-Vision-Action-Transformer-by-Unlocking-Full-Representation-of-ViT-cs-CV-cs-ROPDF"><a href="#9-VAT-Vision-Action-Transformer-by-Unlocking-Full-Representation-of-ViT-cs-CV-cs-ROPDF" class="headerlink" title="[9] VAT: Vision Action Transformer by Unlocking Full Representation of ViT cs.CV | cs.ROPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06013">VAT: Vision Action Transformer by Unlocking Full Representation of ViT</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06013" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenhao Li, Chengwei Ma, Weixin Mao</span></p>
<p><strong>TL;DR:</strong> VAT提出了一种基于ViT的新型架构Vision Action Transformer，充分利用ViT的多层特征表示，通过动作令牌与视觉特征的深度融合，显著提升了机器人模仿学习的性能，在多个基准测试中创下新记录。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有机器人学习方法中，Vision Transformers（ViTs）通常仅使用最终层的特征，而忽略了其他层的丰富信息。作者认为这种简化的表示不足以支持复杂的机器人任务。</p>
<p><strong>Result:</strong> VAT在LIBERO基准测试中实现了98.15%的平均成功率，超越了OpenVLA-OFT等现有方法，刷新了State-of-the-art。</p>
<p><strong>Insight:</strong> 研究表明，充分利用ViT的完整“表示轨迹”（representation trajectory）对提升机器人策略的性能至关重要，揭示了视觉模型中多层次特征的潜在价值。</p>
<p><strong>Abstract:</strong> In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer’s features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ‘’representation trajectory’’ of vision models to advance robotic policy. The GitHub URL for the project code is <a target="_blank" rel="noopener" href="https://github.com/sellerbubble/VAT">https://github.com/sellerbubble/VAT</a>.</p>
  </div>
</details>

<hr>
<h3 id="10-Benchmarking-CXR-Foundation-Models-With-Publicly-Available-MIMIC-CXR-and-NIH-CXR14-Datasets-cs-CVPDF"><a href="#10-Benchmarking-CXR-Foundation-Models-With-Publicly-Available-MIMIC-CXR-and-NIH-CXR14-Datasets-cs-CVPDF" class="headerlink" title="[10] Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets cs.CVPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06014">Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06014" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiho Shin, Dominic Marshall, Matthieu Komorowski</span></p>
<p><strong>TL;DR:</strong> 该论文比较了两种大规模胸部X光(CXR)基础模型(CXR-Foundation和MedImagelnsight)在MIMIC-CR和NIH ChestX-ray14数据集上的性能，提供了标准化评估的基准。MedImagelnsight在多数任务中表现略优，而CXR-Foundation展现了更强的跨数据集稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医疗基础模型在医学图像表征学习中表现优异，但其在不同数据集上的比较行为尚未充分研究。</p>
<p><strong>Result:</strong> MedImagelnsight在多数任务中表现略优，CXR-Foundation具有更强的跨数据集稳定性。无监督聚类显示MedImagelnsight嵌入具有疾病特异性结构。</p>
<p><strong>Insight:</strong> 标准化评估对医疗基础模型至关重要，该研究为未来多模态和临床整合研究提供了可复现的基线。</p>
<p><strong>Abstract:</strong> Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.</p>
  </div>
</details>

<hr>
<h3 id="11-PrefGen-Multimodal-Preference-Learning-for-Preference-Conditioned-Image-Generation-cs-CV-cs-AIPDF"><a href="#11-PrefGen-Multimodal-Preference-Learning-for-Preference-Conditioned-Image-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[11] PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation cs.CV | cs.AIPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06020">PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06020" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenyi Mo, Tianyu Zhang, Yalong Bai, Ligong Han, Ying Ba</span></p>
<p><strong>TL;DR:</strong> PrefGen提出了一种多模态偏好学习框架，利用多模态大语言模型（MLLMs）提取用户偏好表征，并将其注入到基于扩散模型的图像生成中，显著提升了图像质量和个性化对齐效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在捕捉用户细微偏好或编码个性化视觉信号方面表现不足。PrefGen旨在解决这一问题，通过多模态学习实现更精准的偏好条件生成。</p>
<p><strong>Result:</strong> 在图像质量和偏好对齐方面显著优于基线方法，验证了表征提取和对齐的有效性。</p>
<p><strong>Insight:</strong> 多模态表征对齐与条件化生成是提升个性化图像生成的关键，而偏好任务的细化设计能够有效捕捉用户意图。</p>
<p><strong>Abstract:</strong> Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.</p>
  </div>
</details>

<hr>
<h3 id="12-Neural-reconstruction-of-3D-ocean-wave-hydrodynamics-from-camera-sensing-cs-CV-physics-flu-dynPDF"><a href="#12-Neural-reconstruction-of-3D-ocean-wave-hydrodynamics-from-camera-sensing-cs-CV-physics-flu-dynPDF" class="headerlink" title="[12] Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing cs.CV | physics.flu-dynPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06024">Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | physics.flu-dyn</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06024" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiabin Liu, Zihao Zhou, Jialei Yan, Anxin Guo, Alvise Benetazzo</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于神经网络的3D海浪自由表面重构方法，结合注意力机制和多尺度金字塔架构，解决了海浪长期观测中计算成本高和遮挡问题，实现了高精度海浪高程和速度场的重构。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了解决海浪长期观测中密集视觉重构的高计算成本和遮挡问题，论文提出了一种基于视觉的神经网络方法，旨在精确重构3D海浪自由表面及其速度场。</p>
<p><strong>Result:</strong> 实验表明，该方法能实现毫米级海浪高程预测、0.01 Hz以下的主频误差、高频谱功率定律的精确估计，并在遮挡情况下表现优异。</p>
<p><strong>Insight:</strong> 通过结合物理约束和全局多尺度注意力，该方法不仅能高效重构3D海浪动态，还能在遮挡条件下保持鲁棒性，展示了神经网络在海洋物理研究中的潜力。</p>
<p><strong>Abstract:</strong> Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.</p>
  </div>
</details>

<hr>
<h3 id="13-The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family-Why-Prompt-Based-Expertise-Fails-in-Concept-Driven-Image-Segmentation-cs-CV-cs-AIPDF"><a href="#13-The-SAM2-to-SAM3-Gap-in-the-Segment-Anything-Model-Family-Why-Prompt-Based-Expertise-Fails-in-Concept-Driven-Image-Segmentation-cs-CV-cs-AIPDF" class="headerlink" title="[13] The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation cs.CV | cs.AIPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06032">The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06032" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ranjan Sapkota, Konstantinos I. Roumeliotis, Manoj Karkee</span></p>
<p><strong>TL;DR:</strong> 本文研究了Segment Anything Model（SAM）家族中SAM2和SAM3之间的根本性差异，解释了SAM2基于提示的分割方法在SAM3多模态概念驱动的范式中失效的原因，并提出了五个核心分析点。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> SAM2和SAM3在分割任务中存在显著差异，但现有研究未充分分析其根本性变化。本文旨在填补这一空白，探讨为何SAM2的提示分割方法无法适应SAM3的多模态概念驱动框架。</p>
<p><strong>Result:</strong> 研究表明，SAM3通过多模态融合和开放词汇推理能力，超越了SAM2的纯几何分割方法，成为概念驱动分割的新范式。</p>
<p><strong>Insight:</strong> 多模态和概念驱动的分割方法是未来趋势，SAM3的成功为相关研究提供了新的方向。</p>
<p><strong>Abstract:</strong> This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.</p>
  </div>
</details>

<hr>
<h3 id="14-Representation-Learning-for-Point-Cloud-Understanding-cs-CVPDF"><a href="#14-Representation-Learning-for-Point-Cloud-Understanding-cs-CVPDF" class="headerlink" title="[14] Representation Learning for Point Cloud Understanding cs.CVPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06058">Representation Learning for Point Cloud Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06058" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siming Yan</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了点云理解中的表示学习，提出了一种将预训练的2D模型集成到3D网络训练中的方法，显著提升了3D理解能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着3D数据获取技术的发展，3D数据在各个领域的应用日益广泛，但由于其复杂性，如何高效理解和处理点云数据成为关键问题。</p>
<p><strong>Result:</strong> 实验表明，所提方法在点云表示学习中表现优异，展示了其在3D理解中的潜力。</p>
<p><strong>Insight:</strong> 通过有效整合2D知识，可以显著提升3D数据的表示学习能力，这为解决3D数据的复杂性提供了新思路。</p>
<p><strong>Abstract:</strong> With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.</p>
  </div>
</details>

<hr>
<h3 id="15-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing-cs-CV-cs-AIPDF"><a href="#15-EgoEdit-Dataset-Real-Time-Streaming-Model-and-Benchmark-for-Egocentric-Video-Editing-cs-CV-cs-AIPDF" class="headerlink" title="[15] EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing cs.CV | cs.AIPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06065">EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06065" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Runjia Li, Moayed Haji-Ali, Ashkan Mirzaei, Chaoyang Wang, Arpit Sahni</span></p>
<p><strong>TL;DR:</strong> 论文提出了EgoEdit系统，用于解决第一人称视频编辑中的独特挑战，包括快速运动和手部-物体交互；同时实现实时流式推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有AI视频编辑工具在第三人称视频上表现良好，但在第一人称视频中存在领域差距，且离线编辑延迟高，难以支持实时交互。</p>
<p><strong>Result:</strong> EgoEdit在第一人称编辑任务上显著优于现有方法，同时在通用编辑任务上保持竞争力。</p>
<p><strong>Insight:</strong> 第一人称视频编辑需要专门的数据集和模型设计，尤其要关注手部交互的保留和运动稳定性。</p>
<p><strong>Abstract:</strong> We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at <a target="_blank" rel="noopener" href="https://snap-research.github.io/EgoEdit">https://snap-research.github.io/EgoEdit</a></p>
  </div>
</details>

<hr>
<h3 id="16-Shoot-Bounce-3D-Single-Shot-Occlusion-Aware-3D-from-Lidar-by-Decomposing-Two-Bounce-Light-cs-CVPDF"><a href="#16-Shoot-Bounce-3D-Single-Shot-Occlusion-Aware-3D-from-Lidar-by-Decomposing-Two-Bounce-Light-cs-CVPDF" class="headerlink" title="[16] Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light cs.CVPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06080">Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06080" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tzofi Klinghoffer, Siddharth Somasundaram, Xiaoyu Xiang, Yuchen Fan, Christian Richardt</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种数据驱动的方法，利用单光子激光雷达（lidar）的多重反射光信息，从单次测量中恢复密集深度、遮挡几何和材料属性，解决了遮挡和镜面反射等复杂场景下的3D重建问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在3D场景重建中，遮挡区域和镜面材料（如镜子）的存在增加了单次测量的挑战性。传统方法需要逐点扫描，而论文旨在通过多光源同时照明和多反射光信息，实现更高效的3D重建。</p>
<p><strong>Result:</strong> 实验结果表明，该方法能够从单次测量中恢复具有遮挡和镜面反射的3D场景几何结构。</p>
<p><strong>Insight:</strong> 通过利用多重反射光的信息，可以显著提升单光子激光雷达在复杂场景中的3D重建能力，为实际应用提供了新的可能性。</p>
<p><strong>Abstract:</strong> 3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at <a target="_blank" rel="noopener" href="https://shoot-bounce-3d.github.io/">https://shoot-bounce-3d.github.io</a>.</p>
  </div>
</details>

<hr>
<h3 id="17-BeLLA-End-to-End-Birds-Eye-View-Large-Language-Assistant-for-Autonomous-Driving-cs-CVPDF"><a href="#17-BeLLA-End-to-End-Birds-Eye-View-Large-Language-Assistant-for-Autonomous-Driving-cs-CVPDF" class="headerlink" title="[17] BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving cs.CVPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06096">BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06096" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Karthik Mohan, Sonam Singh, Amit Arvind Kale</span></p>
<p><strong>TL;DR:</strong> BeLLA是一个端到端的架构，将360°鸟瞰图（BEV）表示与大型语言模型结合，用于自动驾驶中的问答任务，显著提升了需要空间推理的任务性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视觉-语言模型在自动驾驶研究中多依赖单视角或多视角特征聚合，缺乏统一的空间表示，难以处理涉及空间关系和场景理解的任务。</p>
<p><strong>Result:</strong> BeLLA在涉及相对定位和行为理解的问答任务中表现显著优于现有方法，提升幅度达9.3%。</p>
<p><strong>Insight:</strong> 统一的空间表示对提升自动驾驶场景理解任务的性能至关重要，多模态模型结合空间信息能显著增强推理能力。</p>
<p><strong>Abstract:</strong> The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360° BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.</p>
  </div>
</details>

<hr>
<h3 id="18-SpectraIrisPAD-Leveraging-Vision-Foundation-Models-for-Spectrally-Conditioned-Multispectral-Iris-Presentation-Attack-Detection-cs-CVPDF"><a href="#18-SpectraIrisPAD-Leveraging-Vision-Foundation-Models-for-Spectrally-Conditioned-Multispectral-Iris-Presentation-Attack-Detection-cs-CVPDF" class="headerlink" title="[18] SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection cs.CVPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06103">SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06103" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Raghavendra Ramachandra, Sushma Venkatesh</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为SpectraIrisPAD的多光谱虹膜表示攻击检测框架，利用DINOv2 ViT骨干网络和新型谱位置编码技术，结合对比学习，有效区分真实虹膜样本与伪造攻击。此外，作者还发布了一个新的多光谱虹膜数据集MSIrPAD，包含多种攻击类型。实验表明，该方法在未见攻击场景下表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 虹膜识别虽然在生物特征识别中精度极高，但对表示攻击（PAs）的脆弱性限制了其实际应用。传统的近红外（NIR）虹膜系统在多光谱成像中缺乏充分利用，这使得开发通用的表示攻击检测（PAD）方法成为必要。</p>
<p><strong>Result:</strong> SpectraIrisPAD在多种未见攻击场景下显著优于现有基线方法，展示了更高的检测精度和泛化能力。</p>
<p><strong>Insight:</strong> 多光谱信息在虹膜PAD任务中具有重要价值，而ViT模型结合谱位置编码能够有效捕捉不同波段的区分性特征。此外，对比学习进一步提升了特征的鲁棒性。</p>
<p><strong>Abstract:</strong> Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800,nm, 830,nm, 850,nm, 870,nm, and 980,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.</p>
  </div>
</details>

<hr>
<h3 id="19-Explainable-Melanoma-Diagnosis-with-Contrastive-Learning-and-LLM-based-Report-Generation-cs-CV-cs-AIPDF"><a href="#19-Explainable-Melanoma-Diagnosis-with-Contrastive-Learning-and-LLM-based-Report-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[19] Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation cs.CV | cs.AIPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06105">Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06105" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junwen Zheng, Xinran Xu, Li Rong Wang, Chang Cai, Lucinda Siyun Tan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于对比学习和LLM的双模态可解释性框架CEFM，用于黑色素瘤诊断，通过将临床ABC标准映射到ViT嵌入空间并结合文本生成，实现了高准确性和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度学习在黑色素瘤分类中表现出色，但模型的不透明性和缺乏可解释性阻碍了临床应用，医生难以信任黑盒模型的决策过程。</p>
<p><strong>Result:</strong> 在公开数据集上达到92.79%准确率和0.961 AUC，可解释性指标显著提升，定性分析显示嵌入空间与临床ABC规则一致。</p>
<p><strong>Insight:</strong> 通过多模态对齐和文本生成，成功弥合了高性能分类与临床信任之间的鸿沟，为可解释性AI在医疗领域的应用提供了范例。</p>
<p><strong>Abstract:</strong> Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians’ application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.</p>
  </div>
</details>

<hr>
<h3 id="20-Tracking-Guided-4D-Generation-Foundation-Tracker-Motion-Priors-for-3D-Model-Animation-cs-CVPDF"><a href="#20-Tracking-Guided-4D-Generation-Foundation-Tracker-Motion-Priors-for-3D-Model-Animation-cs-CVPDF" class="headerlink" title="[20] Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation cs.CVPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06158">Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06158" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Su Sun, Cheng Zhao, Himangi Mittal, Gaurav Mittal, Rohith Kukkala</span></p>
<p><strong>TL;DR:</strong> 论文提出Track4DGen框架，通过结合多视角视频扩散模型与基础点追踪技术，生成动态4D对象，解决了跨视角和时间的外观一致性难题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法因仅依赖像素或潜在空间的视频扩散损失，缺乏显式的时间感知特征级追踪指导，导致视角差异和时序漂移问题。</p>
<p><strong>Result:</strong> Track4DGen在多项基准测试中超越基线方法，生成时序稳定的可编辑4D资产。</p>
<p><strong>Insight:</strong> 显式引入追踪器和运动先验，能有效提高4D生成的外观和时序一致性。</p>
<p><strong>Abstract:</strong> Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.</p>
  </div>
</details>

<hr>
<h3 id="21-Automated-Annotation-of-Shearographic-Measurements-Enabling-Weakly-Supervised-Defect-Detection-cs-CVPDF"><a href="#21-Automated-Annotation-of-Shearographic-Measurements-Enabling-Weakly-Supervised-Defect-Detection-cs-CVPDF" class="headerlink" title="[21] Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection cs.CVPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06171">Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06171" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jessica Plassmann, Nicolas Schuler, Michael Schuth, Georg von Freymann</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种自动化标注方法，利用深度学习生成剪切成像测量中的缺陷标注，支持弱监督训练，减少人工标注需求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 剪切成像技术在检测缺陷方面具有高敏感性，但其工业应用受限，原因是高质量标注数据集缺乏且人工标注费时费力、难以标准化。</p>
<p><strong>Result:</strong> 实验表明该方法生成的标注准确性足以支持弱监督训练，显著减少了人工标注的工作量。</p>
<p><strong>Insight:</strong> 自动化标注方法可以提升剪切成像数据集的规模和质量，加速工业缺陷检测模型的开发与应用。</p>
<p><strong>Abstract:</strong> Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.</p>
  </div>
</details>

<hr>
<h3 id="22-Physics-Grounded-Shadow-Generation-from-Monocular-3D-Geometry-Priors-and-Approximate-Light-Direction-cs-CVPDF"><a href="#22-Physics-Grounded-Shadow-Generation-from-Monocular-3D-Geometry-Priors-and-Approximate-Light-Direction-cs-CVPDF" class="headerlink" title="[22] Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction cs.CVPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06174">Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06174" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shilin Hu, Jingyi Xu, Akshat Dave, Dimitris Samaras, Hieu Le</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种结合物理建模和深度学习的阴影生成框架，通过单目3D几何先验和近似光源方向生成物理真实且视觉一致的阴影。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有深度学习方法在阴影生成中很少利用显式的物理建模（几何和光照），导致生成的阴影在几何复杂或光照模糊的场景中表现不佳。</p>
<p><strong>Result:</strong> 在DESOBAV2数据集上的实验表明，该模型生成的阴影在复杂几何或模糊光照场景中优于现有方法。</p>
<p><strong>Insight:</strong> 结合物理建模和深度学习可以显著提升阴影生成的真实性和一致性，尤其在复杂场景中。</p>
<p><strong>Abstract:</strong> Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.</p>
  </div>
</details>

<hr>
<h3 id="23-Physics-Grounded-Attached-Shadow-Detection-Using-Approximate-3D-Geometry-and-Light-Direction-cs-CVPDF"><a href="#23-Physics-Grounded-Attached-Shadow-Detection-Using-Approximate-3D-Geometry-and-Light-Direction-cs-CVPDF" class="headerlink" title="[23] Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction cs.CVPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06179">Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06179" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shilin Hu, Jingyi Xu, Sagnik Das, Dimitris Samaras, Hieu Le</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个基于物理的框架，联合检测物体表面上的附着阴影和投射阴影，通过光照和几何关系的迭代推理，显著提升了附着阴影的检测效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前阴影检测方法主要集中在投射阴影上，缺乏专门针对附着阴影的数据集和模型。附着阴影对理解物体的三维结构和场景理解至关重要，因此需要填补这一空白。</p>
<p><strong>Result:</strong> 实验结果显示，该方法显著改善了附着阴影的检测（BER降低至少33%），同时保持了良好的投射阴影检测性能。</p>
<p><strong>Insight:</strong> 通过光照和几何关系的联合推理，可以显著提升阴影检测的准确性，尤其是对于附着阴影这种依赖物体表面几何形状的阴影类型。</p>
<p><strong>Abstract:</strong> Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.</p>
  </div>
</details>

<hr>
<h3 id="24-Multi-Modal-Zero-Shot-Prediction-of-Color-Trajectories-in-Food-Drying-cs-CV-cs-AI-cs-LG-eess-IVPDF"><a href="#24-Multi-Modal-Zero-Shot-Prediction-of-Color-Trajectories-in-Food-Drying-cs-CV-cs-AI-cs-LG-eess-IVPDF" class="headerlink" title="[24] Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying cs.CV | cs.AI | cs.LG | eess.IVPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06190">Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06190" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shichen Li, Ahmadreza Eslaminia, Chenhui Shao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种多模态方法，用于预测食品干燥过程中未见条件下的颜色轨迹，结合高维时间颜色信息和干燥参数，显著提高了预测准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有食品干燥颜色变化研究方法依赖低维特征，无法捕捉复杂动态变化，且缺乏对未见条件的泛化能力。</p>
<p><strong>Result:</strong> 在未见条件下，模型对饼干和苹果干燥的RMSE分别为2.12和1.29，比基线模型误差降低90%以上。</p>
<p><strong>Insight:</strong> 高维时间颜色信息与干燥参数的多模态融合显著提升模型性能，为食品质量控制提供了新方法。</p>
<p><strong>Abstract:</strong> Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model’s superior accuracy, robustness, and broad applicability.</p>
  </div>
</details>

<hr>
<h3 id="25-Opinion-Learning-Intuitive-Physics-May-Require-More-than-Visual-Data-cs-CV-cs-LGPDF"><a href="#25-Opinion-Learning-Intuitive-Physics-May-Require-More-than-Visual-Data-cs-CV-cs-LGPDF" class="headerlink" title="[25] Opinion: Learning Intuitive Physics May Require More than Visual Data cs.CV | cs.LGPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06232">Opinion: Learning Intuitive Physics May Require More than Visual Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06232" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ellen Su, Solim Legris, Todd M. Gureckis, Mengye Ren</span></p>
<p><strong>TL;DR:</strong> 研究表明，仅靠视觉数据（即使符合发展现实的分布）可能不足以让当前架构学习支持直觉物理的表征，数据量和分布的变化可能不是实现人工直觉物理的充分条件。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类通过直觉物理模型高效导航世界，而当前深度学习模型在大规模视频数据训练后仍无法达到人类水平。论文探讨数据分布而非数据量是否是学习直觉物理的关键。</p>
<p><strong>Result:</strong> 结果表明，即使数据分布符合发展现实，模型性能仍无显著提升，说明现有架构和数据策略不足以支持直觉物理学习。</p>
<p><strong>Insight:</strong> 直觉物理的学习可能需要多模态数据或更复杂的架构设计，而非仅依赖视觉数据。</p>
<p><strong>Abstract:</strong> Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children’s everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.</p>
  </div>
</details>

<hr>
<h3 id="26-NexusFlow-Unifying-Disparate-Tasks-under-Partial-Supervision-via-Invertible-Flow-Networks-cs-CVPDF"><a href="#26-NexusFlow-Unifying-Disparate-Tasks-under-Partial-Supervision-via-Invertible-Flow-Networks-cs-CVPDF" class="headerlink" title="[26] NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks cs.CVPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06251">NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06251" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fangzhou Lin, Yuping Wang, Yuliang Guo, Zixun Huang, Xinyu Huang</span></p>
<p><strong>TL;DR:</strong> NexusFlow 是一个轻量级的模块化框架，通过可逆耦合层统一异构任务的特征分布，解决部分监督多任务学习中任务结构差异的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在多任务学习中，现有方法主要关注同构密集预测任务，而忽略了异构任务的结构差异和部分监督问题。NexusFlow 旨在解决这一限制。</p>
<p><strong>Result:</strong> 在 nuScenes 数据集上达到 SOTA 性能，并在 NYUv2 上验证了方法的通用性。</p>
<p><strong>Insight:</strong> 可逆性设计是关键，既能对齐异构任务特征，又能保持模型的表达力。</p>
<p><strong>Abstract:</strong> Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.</p>
  </div>
</details>

<hr>
<h3 id="27-Language-driven-Fine-grained-Retrieval-cs-CVPDF"><a href="#27-Language-driven-Fine-grained-Retrieval-cs-CVPDF" class="headerlink" title="[27] Language-driven Fine-grained Retrieval cs.CVPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06255">Language-driven Fine-grained Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06255" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shijie Wang, Xin Yu, Yadan Luo, Zijian Wang, Pengfei Zhang</span></p>
<p><strong>TL;DR:</strong> LaFG是一个基于语言的细粒度检索框架，通过大语言模型（LLMs）和视觉语言模型（VLMs）将类别名称转换为属性级监督，以解决现有细粒度图像检索（FGIR）方法在泛化到未见类别时的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的FGIR方法使用稀疏的一热标签作为监督，忽略了类别名称中丰富的语义信息，限制了模型对跨类别细节的比较能力，从而影响了泛化性能。</p>
<p><strong>Result:</strong> LaFG能够有效捕捉类别名称中的语义信息，提升对未见类别的检索性能。</p>
<p><strong>Insight:</strong> 将类别名称转化为属性级监督是一种有效的方法，能够弥补传统稀疏标签的不足，提升细粒度检索的泛化能力。</p>
<p><strong>Abstract:</strong> Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer</p>
  </div>
</details>

<hr>
<h3 id="28-Knowing-the-Answer-Isn’t-Enough-Fixing-Reasoning-Path-Failures-in-LVLMs-cs-CVPDF"><a href="#28-Knowing-the-Answer-Isn’t-Enough-Fixing-Reasoning-Path-Failures-in-LVLMs-cs-CVPDF" class="headerlink" title="[28] Knowing the Answer Isn’t Enough: Fixing Reasoning Path Failures in LVLMs cs.CVPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06258">Knowing the Answer Isn’t Enough: Fixing Reasoning Path Failures in LVLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06258" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chaoyang Wang, Yangfan He, Yiyang Zhou, Yixuan Wang, Jiaqi Liu</span></p>
<p><strong>TL;DR:</strong> 论文揭示了大型视觉语言模型（LVLM）的关键缺陷：即使模型知道正确答案，也常通过错误的推理路径得出。核心问题在于路径选择偏差，而非知识缺乏。作者提出PSO框架，通过两阶段优化提升推理性能和稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> LVLM在推理过程中频繁选择不稳定或不一致的路径，导致结果不可靠。尽管有能力生成正确轨迹，但路径选择偏差显著影响了性能。</p>
<p><strong>Result:</strong> 实验表明PSO显著提升推理准确性（平均7.4%）并增强推理稳定性。</p>
<p><strong>Insight:</strong> LVLM的性能瓶颈更多源于推理路径选择而非知识缺失，动态优化和负样本管理是关键改进方向。</p>
<p><strong>Abstract:</strong> We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/aiming-lab/PSO">https://github.com/aiming-lab/PSO</a>.</p>
  </div>
</details>

<hr>
<h3 id="29-FacePhys-State-of-the-Heart-Learning-cs-CVPDF"><a href="#29-FacePhys-State-of-the-Heart-Learning-cs-CVPDF" class="headerlink" title="[29] FacePhys: State of the Heart Learning cs.CVPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06275">FacePhys: State of the Heart Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06275" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kegang Wang, Jiankai Tang, Yuntao Wang, Xin Liu, Yuxuan Fan</span></p>
<p><strong>TL;DR:</strong> FacePhys提出了一种内存高效的心率测量算法，通过时空状态空间对偶性解决了模型可扩展性、跨数据集泛化和实时操作的难题，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的远程光电容积描记术（rPPG）在计算和信号传输方面存在局限性，导致实际部署困难。FacePhys旨在解决这一问题。</p>
<p><strong>Result:</strong> FacePhys实现了实时推理，内存占用仅为3.6 MB，每帧延迟9.46 ms，性能超越现有方法83%至99%。</p>
<p><strong>Insight:</strong> 时空状态空间的利用为高效实时心率测量提供了新思路，为远程健康监测的实际部署铺平了道路。</p>
<p><strong>Abstract:</strong> Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms – surpassing existing methods by 83% to 99%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at <a target="_blank" rel="noopener" href="https://www.facephys.com/">https://www.facephys.com/</a>.</p>
  </div>
</details>

<hr>
<h3 id="30-RefBench-PRO-Perceptual-and-Reasoning-Oriented-Benchmark-for-Referring-Expression-Comprehension-cs-CV-cs-AIPDF"><a href="#30-RefBench-PRO-Perceptual-and-Reasoning-Oriented-Benchmark-for-Referring-Expression-Comprehension-cs-CV-cs-AIPDF" class="headerlink" title="[30] RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension cs.CV | cs.AIPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06276">RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06276" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianyi Gao, Hao Li, Han Fang, Xin Wei, Xiaodong Dong</span></p>
<p><strong>TL;DR:</strong> 本文提出了RefBench-PRO，一个面向感知和推理的引用表达式理解（REC）基准测试，将任务分解为六个逐步挑战的子任务，并提出了一种自动数据生成方法和改进的RL学习框架Ref-R1。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有REC基准测试主要关注感知能力，缺乏对不同认知能力的多模态大语言模型（MLLM）的可解释评分机制，因此无法全面评估其接地能力。</p>
<p><strong>Result:</strong> 实验表明，RefBench-PRO能够对MLLM进行可解释的评估，并在感知和推理方面提供更大挑战。</p>
<p><strong>Insight:</strong> 通过分解任务和自动生成数据，可以更全面地评估MLLM的认知能力，尤其是推理部分；动态IoU的引入有助于提升复杂条件下的性能。</p>
<p><strong>Abstract:</strong> Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.</p>
  </div>
</details>

<hr>
<h3 id="31-Unleashing-the-Intrinsic-Visual-Representation-Capability-of-Multimodal-Large-Language-Models-cs-CV-cs-AIPDF"><a href="#31-Unleashing-the-Intrinsic-Visual-Representation-Capability-of-Multimodal-Large-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[31] Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models cs.CV | cs.AIPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06281">Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06281" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hengzhuang Li, Xinsong Zhang, Qiming Peng, Bin Luo, Han Hu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种称为Latent Visual Reconstruction (LaVer)的新训练框架，通过掩码图像建模在LLM的联合潜在语义空间中，解决了多模态大语言模型（MLLMs）中视觉信息利用不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> MLLMs在训练中主要依赖文本生成任务，导致深层网络中视觉信息利用不足，出现视觉表现退化或幻觉问题。</p>
<p><strong>Result:</strong> 实验表明LaVer在多种基准测试中表现优异，尤其是在需要密集视觉能力的场景中。</p>
<p><strong>Insight:</strong> 在MLLMs中引入直接视觉监督可以有效缓解模态不平衡问题，提升视觉信息的利用率。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at <a target="_blank" rel="noopener" href="https://github.com/Fir-lat/LaVer">https://github.com/Fir-lat/LaVer</a>.</p>
  </div>
</details>

<hr>
<h3 id="32-A-Sleep-Monitoring-System-Based-on-Audio-Video-and-Depth-Information-cs-CV-cs-MMPDF"><a href="#32-A-Sleep-Monitoring-System-Based-on-Audio-Video-and-Depth-Information-cs-CV-cs-MMPDF" class="headerlink" title="[32] A Sleep Monitoring System Based on Audio, Video and Depth Information cs.CV | cs.MMPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06282">A Sleep Monitoring System Based on Audio, Video and Depth Information</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06282" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lyn Chao-ling Chen, Kuan-Wen Chen, Yi-Ping Hung</span></p>
<p><strong>TL;DR:</strong> 本文提出一种基于音频、视频和深度信息的非侵入式睡眠监测系统，通过事件检测方法对睡眠干扰进行分类和量化评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 睡眠干扰的量化评估对提高睡眠质量至关重要。现有方法多为侵入式或功能单一，无法全面捕捉多种干扰因素。</p>
<p><strong>Result:</strong> 系统在睡眠环境中测试，验证了其可靠性和有效性。</p>
<p><strong>Insight:</strong> 多模态传感器结合事件检测方法可显著提升非侵入式睡眠监测的准确性，为居家睡眠监测提供了可行的解决方案。</p>
<p><strong>Abstract:</strong> For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on&#x2F;off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.</p>
  </div>
</details>

<hr>
<h3 id="33-StrokeNet-Unveiling-How-to-Learn-Fine-Grained-Interactions-in-Online-Handwritten-Stroke-Classification-cs-CVPDF"><a href="#33-StrokeNet-Unveiling-How-to-Learn-Fine-Grained-Interactions-in-Online-Handwritten-Stroke-Classification-cs-CVPDF" class="headerlink" title="[33] StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification cs.CVPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06290">StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06290" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiheng Huang, Shuang She, Zewei Wei, Jianmin Lin, Ming Yang</span></p>
<p><strong>TL;DR:</strong> StrokeNet提出了一种新方法，通过动态选择参考点并利用其顺序表示笔画，结合Inline Sequence Attention模块和Cross-Ellipse Query机制，显著提升了在线手写笔画分类的精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在线手写笔画分类的挑战主要在于笔画间的细粒度语义关系难以建模，现有深度学习方法难以捕捉这种局部化的交互。</p>
<p><strong>Result:</strong> 在多个公开数据集上实现最佳性能，CASIA-onDo数据集上的准确率从93.81%提升至95.54%。</p>
<p><strong>Insight:</strong> 通过参考点的选择和序列化表示，可以有效捕捉笔画间的细粒度交互，解决了冗余问题并提升了分类精度。</p>
<p><strong>Abstract:</strong> Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$%$ to 95.54$%$, demonstrating the effectiveness and robustness of our approach.</p>
  </div>
</details>

<hr>
<h3 id="34-Exploiting-Spatiotemporal-Properties-for-Efficient-Event-Driven-Human-Pose-Estimation-cs-CV-cs-AIPDF"><a href="#34-Exploiting-Spatiotemporal-Properties-for-Efficient-Event-Driven-Human-Pose-Estimation-cs-CV-cs-AIPDF" class="headerlink" title="[34] Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation cs.CV | cs.AIPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06306">Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06306" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoxian Zhou, Chuanzhi Xu, Langyi Chen, Haodong Chen, Yuk Ying Chung</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于点云的事件驱动方法，利用时空特性改进人体姿态估计，避免了传统方法中事件信号的高时间分辨率损失问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统方法通常将事件流转换为密集的事件帧，这会引入额外计算并牺牲事件信号的高时间分辨率。论文旨在利用事件流的时空特性，直接在点云框架下高效地进行人体姿态估计。</p>
<p><strong>Result:</strong> 在DHP19数据集上，所提方法在PointNet、DGCNN和Point Transformer三种骨干网络上均表现出性能提升。</p>
<p><strong>Insight:</strong> 直接在点云框架下处理事件流，结合时空建模和边缘增强技术，能够在保持高时间分辨率的同时提升稀疏事件的姿态估计效果。</p>
<p><strong>Abstract:</strong> Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.</p>
  </div>
</details>

<hr>
<h3 id="35-ReCAD-Reinforcement-Learning-Enhanced-Parametric-CAD-Model-Generation-with-Vision-Language-Models-cs-CVPDF"><a href="#35-ReCAD-Reinforcement-Learning-Enhanced-Parametric-CAD-Model-Generation-with-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[35] ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models cs.CVPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06328">ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06328" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiahao Li, Yusheng Luo, Yunzhong Lou, Xiangdong Zhou</span></p>
<p><strong>TL;DR:</strong> ReCAD提出了一种基于强化学习的框架，结合预训练大模型（PLM）和视觉语言模型（VLM），从多模态输入生成精确的参数化CAD模型，显著提升了几何精度和语义保真度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通常依赖监督微调（SFT），难以支持复杂的编辑操作，且未能充分利用预训练大模型的生成能力。ReCAD旨在解决这一问题，通过强化学习增强模型生成和编辑CAD模型的能力。</p>
<p><strong>Result:</strong> 在图像到CAD任务中，ReCAD将平均Chamfer距离从73.47降至29.61（同分布）和272.06降至80.23（非同分布），表现显著优于基线方法。</p>
<p><strong>Insight:</strong> 通过RL与预训练模型的结合，可以有效提升复杂任务的生成能力；参数化代码的引入为模型提供了更强的结构和编辑能力支持。</p>
<p><strong>Abstract:</strong> We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model’s reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.</p>
  </div>
</details>

<hr>
<h3 id="36-S2WMamba-A-Spectral-Spatial-Wavelet-Mamba-for-Pansharpening-cs-CVPDF"><a href="#36-S2WMamba-A-Spectral-Spatial-Wavelet-Mamba-for-Pansharpening-cs-CVPDF" class="headerlink" title="[36] S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening cs.CVPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06330">S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06330" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoyu Zhang, Junhan Luo, Yugang Cao, Siran Peng, Jie Huang</span></p>
<p><strong>TL;DR:</strong> S2WMamba 提出了一种基于谱-空间小波变换和 Mamba 结构的全色锐化方法，通过分离频域信息实现高效跨模态交互，显著提升了图像融合质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 全色锐化中，如何同时保留空间细节和光谱保真度是一个关键挑战。传统方法容易将两类信息耦合，导致性能受限。</p>
<p><strong>Result:</strong> 在 WV3、GF2 和 QB 数据集上，S2WMamba 在 PSNR 和 HQNR 指标上均优于现有基线，PSNR 提升最高达 0.23 dB，HQNR 达 0.956。</p>
<p><strong>Insight:</strong> 1) 频域分解能有效解耦空间与光谱信息；2) Mamba 的长程建模能力对跨模态交互至关重要；3) 动态门控能自适应调整特征贡献。</p>
<p><strong>Abstract:</strong> Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel’s spectrum as a 1D signal to separate low&#x2F;high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D&#x2F;1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/KagUYa66/S2WMamba">https://github.com/KagUYa66/S2WMamba</a>.</p>
  </div>
</details>

<hr>
<h3 id="37-CryoHype-Reconstructing-a-thousand-cryo-EM-structures-with-transformer-based-hypernetworks-cs-CVPDF"><a href="#37-CryoHype-Reconstructing-a-thousand-cryo-EM-structures-with-transformer-based-hypernetworks-cs-CVPDF" class="headerlink" title="[37] CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks cs.CVPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06332">CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06332" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jeffrey Gu, Minkyu Jeon, Ambri Ma, Serena Yeung-Levy, Ellen D. Zhong</span></p>
<p><strong>TL;DR:</strong> 论文提出CryoHype，一种基于Transformer的超网络方法，用于从冷冻电镜图像中同时重建多个生物分子结构，解决了传统方法在解析大量不同分子物种时的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 冷冻电镜（cryo-EM）通常用于单一分子结构的解析，但实际应用中常需要对多种分子混合物进行高通量结构解析，传统方法在建模多个分子物种时能力有限。</p>
<p><strong>Result:</strong> 在包含100个结构的基准数据集上达到SOTA结果，并成功扩展到从无标记图像中重建1,000个结构。</p>
<p><strong>Insight:</strong> CryoHype展示了超网络在高通量冷冻电镜结构解析中的潜力，为复杂生物混合物的分析提供了新工具。</p>
<p><strong>Abstract:</strong> Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.</p>
  </div>
</details>

<hr>
<h3 id="38-Beyond-Hallucinations-A-Multimodal-Guided-Task-Aware-Generative-Image-Compression-for-Ultra-Low-Bitrate-cs-CVPDF"><a href="#38-Beyond-Hallucinations-A-Multimodal-Guided-Task-Aware-Generative-Image-Compression-for-Ultra-Low-Bitrate-cs-CVPDF" class="headerlink" title="[38] Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate cs.CVPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06344">Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06344" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaile Wang, Lijun He, Haisheng Fu, Haixia Bi, Fan Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个多模态引导的任务感知生成图像压缩框架（MTGC），通过在超低码率下结合文本、压缩图像和任务相关语义伪词来增强语义一致性，减少了生成幻觉问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生成图像压缩在超低码率下常因生成幻觉导致语义偏差，限制了其在6G语义通信中的可靠应用，因此需要解决这一问题。</p>
<p><strong>Result:</strong> 实验表明MTGC显著提升了语义一致性（如DISTS降低10.59%）和感知质量，同时在超低码率下保持了像素级保真度。</p>
<p><strong>Insight:</strong> 多模态引导可以有效减少生成幻觉，任务相关语义提取和协同注入是实现高效生成图像压缩的关键。</p>
<p><strong>Abstract:</strong> Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp &lt; 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model’s powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.</p>
  </div>
</details>

<hr>
<h3 id="39-CLUENet-Cluster-Attention-Makes-Neural-Networks-Have-Eyes-cs-CVPDF"><a href="#39-CLUENet-Cluster-Attention-Makes-Neural-Networks-Have-Eyes-cs-CVPDF" class="headerlink" title="[39] CLUENet: Cluster Attention Makes Neural Networks Have Eyes cs.CVPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06345">CLUENet: Cluster Attention Makes Neural Networks Have Eyes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06345" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangshuai Song, Jun-Jie Huang, Tianrui Liu, Ke Liang, Chang Tang</span></p>
<p><strong>TL;DR:</strong> CLUENet 是一种基于聚类的注意力网络，旨在解决传统卷积和注意力模型在视觉任务中的局限性。它通过三种创新方法提升了模型的分类性能和可解释性，并在 CIFAR-100 和 Mini-ImageNet 上展示了优越的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统卷积和注意力模型在视觉任务中存在固定的感受野和复杂结构的问题，限制了其对不规则空间模式的建模能力，同时缺乏可解释性。聚类方法虽然具有更好的可解释性和灵活性，但精度和效率较低，训练中容易出现梯度消失。CLUENet 旨在结合聚类的优势，提升模型的性能和透明度。</p>
<p><strong>Result:</strong> 在 CIFAR-100 和 Mini-ImageNet 数据集上，CLUENet 在分类性能和可解释性方面均优于现有聚类方法和主流视觉模型。</p>
<p><strong>Insight:</strong> CLUENet 展示了聚类方法与深度学习结合的潜力，尤其在提升模型可解释性的同时保持高性能。未来可以探索其在其他视觉任务中的应用。</p>
<p><strong>Abstract:</strong> Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.</p>
  </div>
</details>

<hr>
<h3 id="40-TreeQ-Pushing-the-Quantization-Boundary-of-Diffusion-Transformer-via-Tree-Structured-Mixed-Precision-Search-cs-CVPDF"><a href="#40-TreeQ-Pushing-the-Quantization-Boundary-of-Diffusion-Transformer-via-Tree-Structured-Mixed-Precision-Search-cs-CVPDF" class="headerlink" title="[40] TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search cs.CVPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06353">TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06353" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaicheng Yang, Kaisen Yang, Baiting Wu, Xun Zhang, Qianrui Yang</span></p>
<p><strong>TL;DR:</strong> TreeQ 提出了一种针对扩散变换器（DiT）的量化解方案，通过树结构混合精度搜索、环境噪声引导和通用 Monarch 分支，显著提升了低比特量化的性能，首次在 DiT 上实现了接近无损的 4 比特 PTQ 效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散变换器（DiT）在图像生成中表现出色，但其高计算和内存需求限制了实际部署。现有混合精度量化（MPQ）对 DiT 的研究不足，TreeQ 旨在填补这一空白并解决 DiT 量化的核心挑战。</p>
<p><strong>Result:</strong> 在 DiT-XL&#x2F;2 上实现了 W3A3 和 W4A4 PTQ&#x2F;PEFT 的最先进性能，首次实现 4 比特 PTQ 接近无损。</p>
<p><strong>Insight:</strong> TreeQ 的创新设计（TSS、ENG、GMB）为解决 DiT 量化瓶颈提供了通用框架，为低比特量化在图像生成任务中的应用开辟了新方向。</p>
<p><strong>Abstract:</strong> Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture’s linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL&#x2F;2 under W3A3 and W4A4 PTQ&#x2F;PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/racoonykc/TreeQ">https://github.com/racoonykc/TreeQ</a></p>
  </div>
</details>

<hr>
<h3 id="41-Human3R-Incorporating-Human-Priors-for-Better-3D-Dynamic-Reconstruction-from-Monocular-Videos-cs-CVPDF"><a href="#41-Human3R-Incorporating-Human-Priors-for-Better-3D-Dynamic-Reconstruction-from-Monocular-Videos-cs-CVPDF" class="headerlink" title="[41] Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos cs.CVPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06368">Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06368" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weitao Xiong, Zhiyuan Yuan, Jiahao Lu, Chengfeng Zhao, Peng Li</span></p>
<p><strong>TL;DR:</strong> Human3R通过结合SMPL人体模型和单目深度估计，引入混合几何先验，提升单目视频动态人体3D重建质量，解决了现有方法的几何不一致性和分辨率退化问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 单目动态视频重建在动态人体场景中面临几何不一致和分辨率退化的挑战，现有方法缺乏对人体结构的理解，导致结果失真。</p>
<p><strong>Result:</strong> 在TUM Dynamics和GTA-IM数据集上表现出优越的动态人体重建性能。</p>
<p><strong>Insight:</strong> 结合结构化人体先验和单目深度估计，可以有效提升动态场景的3D重建质量，尤其在几何一致性和细节保留方面。</p>
<p><strong>Abstract:</strong> Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.</p>
  </div>
</details>

<hr>
<h3 id="42-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning-cs-CVPDF"><a href="#42-VG-Refiner-Towards-Tool-Refined-Referring-Grounded-Reasoning-via-Agentic-Reinforcement-Learning-cs-CVPDF" class="headerlink" title="[42] VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning cs.CVPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06373">VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06373" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuji Wang, Wenlong Liu, Jingxuan Niu, Haoji Zhang, Yansong Tang</span></p>
<p><strong>TL;DR:</strong> VG-Refiner 是一个针对工具精细化引用接地任务的框架，通过两阶段思考和重新思考机制应对工具输出的不可靠性，并引入精细化奖励改进模型的修正能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有工具集成视觉推理（TiVR）方法在处理不可靠或错误的工具输出时缺乏有效响应机制，导致引用和接地任务中的误导性推理。</p>
<p><strong>Result:</strong> 在引用和推理接地基准测试中显著提升了准确性和修正能力，同时保持了预训练模型的通用能力。</p>
<p><strong>Insight:</strong> 工具的不可靠输出是一个关键问题，通过显式分析和响应工具反馈可以有效提升推理的鲁棒性和准确性。</p>
<p><strong>Abstract:</strong> Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.</p>
  </div>
</details>

<hr>
<h3 id="43-Are-AI-Generated-Driving-Videos-Ready-for-Autonomous-Driving-A-Diagnostic-Evaluation-Framework-cs-CVPDF"><a href="#43-Are-AI-Generated-Driving-Videos-Ready-for-Autonomous-Driving-A-Diagnostic-Evaluation-Framework-cs-CVPDF" class="headerlink" title="[43] Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework cs.CVPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06376">Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06376" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinhao Xiang, Abhijeet Rastogi, Jiawei Zhang</span></p>
<p><strong>TL;DR:</strong> 论文提出一个诊断框架，评估AI生成的驾驶视频（AIGV）是否可用于自动驾驶模型训练与测试，并通过ADGV-Bench基准和ADGVE评估器验证其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> AI生成的驾驶视频提供了一种低成本、规模化替代真实或模拟数据的方法，但其是否可靠支持自动驾驶模型的训练和评估仍需验证。</p>
<p><strong>Result:</strong> 实验表明，未经处理的AIGV会损害感知性能，而通过ADGVE过滤的视频能提高质量和下游模型表现，成为真实数据的补充。</p>
<p><strong>Insight:</strong> AIGV在自动驾驶中具有潜力，但需严格过滤和处理以避免负面影响，同时提供了一种新的数据扩展途径。</p>
<p><strong>Abstract:</strong> Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.</p>
  </div>
</details>

<hr>
<h3 id="44-Perceptual-Region-Driven-Infrared-Visible-Co-Fusion-for-Extreme-Scene-Enhancement-cs-CVPDF"><a href="#44-Perceptual-Region-Driven-Infrared-Visible-Co-Fusion-for-Extreme-Scene-Enhancement-cs-CVPDF" class="headerlink" title="[44] Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement cs.CVPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06400">Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06400" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jing Tao, Yonghong Zong, Banglei Guana, Pengju Sun, Taihang Lei</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于区域感知的红外-可见光融合框架，适用于极端场景增强。该方法结合多曝光和多模态成像，解决了现有方法在极端条件下牺牲可见光图像质量的问题，并通过对比度增强和结构相似性补偿优化融合效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在极端环境下，红外和可见光光谱的准确融合面临挑战，现有方法常牺牲可见光图像质量，影响测量精度。因此，需要一种既能保留几何保真度又能整合热辐射的方法。</p>
<p><strong>Result:</strong> 在合成和真实数据上的实验表明，该方法在图像清晰度和性能上优于现有方法，定量和视觉评估均证实了其优越性。</p>
<p><strong>Insight:</strong> 区域感知和多模态融合的结合是优化极端场景下红外-可见光图像融合的有效途径，结构相似性补偿机制为光谱整合提供了新思路。</p>
<p><strong>Abstract:</strong> In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.</p>
  </div>
</details>

<hr>
<h3 id="45-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation-cs-CV-cs-AI-cs-CL-cs-LGPDF"><a href="#45-Rethinking-Training-Dynamics-in-Scale-wise-Autoregressive-Generation-cs-CV-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[45] Rethinking Training Dynamics in Scale-wise Autoregressive Generation cs.CV | cs.AI | cs.CL | cs.LGPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06421">Rethinking Training Dynamics in Scale-wise Autoregressive Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06421" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gengze Zhou, Chongjian Ge, Hao Tan, Feng Liu, Yicong Hong</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了Self-Autoregressive Refinement (SAR)方法，通过Stagger-Scale Rollout (SSR)和Contrastive Student-Forcing Loss (CSFL)解决尺度自回归生成模型中暴露偏差和尺度学习不平衡问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尺度自回归生成模型在推断过程中存在暴露偏差问题，影响生成质量，原因在于训练-测试不匹配和尺度间学习难度不平衡。</p>
<p><strong>Result:</strong> SAR显著提升生成质量（如FID降低5.2%），计算开销小（5小时32xA100 GPU）。</p>
<p><strong>Insight:</strong> 通过动态训练分析和对模型自身预测的引入，SAR为尺度自回归生成提供了一种高效的后训练解决方案。</p>
<p><strong>Abstract:</strong> Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.</p>
  </div>
</details>

<hr>
<h3 id="46-A-Perception-CNN-for-Facial-Expression-Recognition-cs-CVPDF"><a href="#46-A-Perception-CNN-for-Facial-Expression-Recognition-cs-CVPDF" class="headerlink" title="[46] A Perception CNN for Facial Expression Recognition cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06422">A Perception CNN for Facial Expression Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06422" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chunwei Tian, Jingyuan Xie, Lingjun Li, Wangmeng Zuo, Yanning Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种感知CNN（PCNN）用于面部表情识别（FER），通过并行网络学习局部面部特征并结合全局结构特征，实现了高性能的表情识别。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的CNN可能在FER任务中忽略了面部区域的局部特征，尤其是眼睛、脸颊和嘴巴等器官对表情变化的敏感性。</p>
<p><strong>Result:</strong> 在CK+、JAFFE、FER2013等多个数据集上表现优异。</p>
<p><strong>Insight:</strong> 局部器官特征的敏感性对FER至关重要，融合局部与全局特征可以显著提升识别性能。</p>
<p><strong>Abstract:</strong> Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at <a target="_blank" rel="noopener" href="https://github.com/hellloxiaotian/PCNN">https://github.com/hellloxiaotian/PCNN</a>.</p>
  </div>
</details>

<hr>
<h3 id="47-DragMesh-Interactive-3D-Generation-Made-Easy-cs-CVPDF"><a href="#47-DragMesh-Interactive-3D-Generation-Made-Easy-cs-CVPDF" class="headerlink" title="[47] DragMesh: Interactive 3D Generation Made Easy cs.CVPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06424">DragMesh: Interactive 3D Generation Made Easy</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06424" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianshan Zhang, Zeyu Zhang, Hao Tang</span></p>
<p><strong>TL;DR:</strong> DragMesh提出了一种实时交互式3D生成框架，通过解耦的运动生成核心实现高效且符合运动学约束的3D物体运动。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在生成静态3D内容上表现优秀，但对物体运动和交互响应的理解仍是一个挑战。现有方法要么速度慢，要么违反运动学约束。DragMesh旨在解决这一问题。</p>
<p><strong>Result:</strong> DragMesh实现了实时性能，无需重新训练即可在未见物体上生成合理的运动轨迹。</p>
<p><strong>Insight:</strong> 解耦设计和双四元数表示是实现高效且符合运动学约束的3D运动生成的关键。</p>
<p><strong>Abstract:</strong> While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE’s non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/DragMesh">https://github.com/AIGeeksGroup/DragMesh</a>. Website: <a target="_blank" rel="noopener" href="https://aigeeksgroup.github.io/DragMesh">https://aigeeksgroup.github.io/DragMesh</a>.</p>
  </div>
</details>

<hr>
<h3 id="48-When-Gender-is-Hard-to-See-Multi-Attribute-Support-for-Long-Range-Recognition-cs-CV-cs-AIPDF"><a href="#48-When-Gender-is-Hard-to-See-Multi-Attribute-Support-for-Long-Range-Recognition-cs-CV-cs-AIPDF" class="headerlink" title="[48] When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition cs.CV | cs.AIPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06426">When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06426" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nzakiese Mbongo, Kailash A. Hambarde, Hugo Proença</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种双路径Transformer框架，结合视觉和属性驱动线索，实现远距离性别识别，超越了现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 远距离性别识别因分辨率低、视角多变和面部线索缺失而具有挑战性，需要结合多属性线索提升识别效果。</p>
<p><strong>Result:</strong> 在多种指标（macro-F1、准确率、AUC）上超越现有方法，且对距离、角度和高度变化具有鲁棒性。</p>
<p><strong>Insight:</strong> 语言引导的双路径学习为远距离性别识别提供了可扩展且可解释的解决方案。</p>
<p><strong>Abstract:</strong> Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.</p>
  </div>
</details>

<hr>
<h3 id="49-AGORA-Adversarial-Generation-Of-Real-time-Animatable-3D-Gaussian-Head-Avatars-cs-CVPDF"><a href="#49-AGORA-Adversarial-Generation-Of-Real-time-Animatable-3D-Gaussian-Head-Avatars-cs-CVPDF" class="headerlink" title="[49] AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars cs.CVPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06438">AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06438" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ramazan Fazylov, Sergey Zagoruyko, Aleksandr Parkin, Stamatis Lefkimmiatis, Ivan Laptev</span></p>
<p><strong>TL;DR:</strong> AGORA提出了一种基于3D高斯飞溅和对抗生成网络的可动画化3D头部位模型框架，通过轻量级变形分支实现高保真、实时渲染，并在CPU上首次实现实用的3D高斯飞溅动画合成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法（如NeRF）在渲染速度和动态一致性上存在不足，而3D高斯飞溅方法仅适用于静态头部生成。AGORA旨在填补这一技术空白，提供高保真且可实时控制的3D头像。</p>
<p><strong>Result:</strong> 1. 在表情精度上超越NeRF方法，单GPU渲染速度达250+FPS。2. 在CPU上首次实现约9FPS的3D高斯飞溅动画合成。3. 生成的头像既逼真又可精确控制。</p>
<p><strong>Insight:</strong> AGORA通过结合3D高斯飞溅的高效渲染与GAN的动态生成能力，为实用高性能数字人提供了新思路，尤其在实时性和跨平台（CPU）部署上表现突出。</p>
<p><strong>Abstract:</strong> The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: <a target="_blank" rel="noopener" href="https://ramazan793.github.io/AGORA/">https://ramazan793.github.io/AGORA/</a></p>
  </div>
</details>

<hr>
<h3 id="50-Towards-Stable-Cross-Domain-Depression-Recognition-under-Missing-Modalities-cs-CVPDF"><a href="#50-Towards-Stable-Cross-Domain-Depression-Recognition-under-Missing-Modalities-cs-CVPDF" class="headerlink" title="[50] Towards Stable Cross-Domain Depression Recognition under Missing Modalities cs.CVPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06447">Towards Stable Cross-Domain Depression Recognition under Missing Modalities</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06447" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiuyi Chen, Mingkui Tan, Haifeng Lu, Qiuna Xu, Zhihua Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个稳定跨领域抑郁症识别的统一框架SCD-MLLM，解决了多模态抑郁症识别中数据异构性和模态缺失的问题，表现优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 抑郁症的及时筛查对公共健康至关重要，但现有多模态抑郁症识别方法缺乏统一框架，且对模态缺失的稳定性不足，无法适应真实场景需求。</p>
<p><strong>Result:</strong> 在五个公开数据集上的实验表明，SCD-MLLM在完整和部分模态情况下均优于SOTA模型和商用LLM（如Gemini和GPT），展示了卓越的跨领域泛化能力和模态缺失稳定性。</p>
<p><strong>Insight:</strong> 统一的多模态框架和自适应融合策略能显著提升抑郁症识别的实用性和鲁棒性，尤其是在真实场景中模态缺失的挑战下。</p>
<p><strong>Abstract:</strong> Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.</p>
  </div>
</details>

<hr>
<h3 id="51-Sanvaad-A-Multimodal-Accessibility-Framework-for-ISL-Recognition-and-Voice-Based-Interaction-cs-CVPDF"><a href="#51-Sanvaad-A-Multimodal-Accessibility-Framework-for-ISL-Recognition-and-Voice-Based-Interaction-cs-CVPDF" class="headerlink" title="[51] Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction cs.CVPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06485">Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06485" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kush Revankar, Shreyas Deshpande, Araham Sayeed, Ansh Tandale, Sarika Bobde</span></p>
<p><strong>TL;DR:</strong> Sanvaad是一个轻量级的多模态无障碍框架，旨在通过实时双向沟通工具支持聋哑用户和视障用户之间的交流。它包括基于MediaPipe地标的ISL识别模块和语音到手势的转换组件，同时还为视障用户提供了基于语音的界面。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决聋哑用户与视障用户之间沟通工具单向性的问题，提供一个双向交互的实用框架，以促进包容性交流。</p>
<p><strong>Result:</strong> Sanvaad能够在边缘设备上高效运行，无需专用硬件，实现了实时、双向的无障碍沟通。</p>
<p><strong>Insight:</strong> 轻量级技术和多模态集成的结合可以有效解决特殊人群的沟通需求，同时强调了边缘计算的潜力。</p>
<p><strong>Abstract:</strong> Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.</p>
  </div>
</details>

<hr>
<h3 id="52-Method-of-UAV-Inspection-of-Photovoltaic-Modules-Using-Thermal-and-RGB-Data-Fusion-cs-CV-cs-AI-cs-ROPDF"><a href="#52-Method-of-UAV-Inspection-of-Photovoltaic-Modules-Using-Thermal-and-RGB-Data-Fusion-cs-CV-cs-AI-cs-ROPDF" class="headerlink" title="[52] Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion cs.CV | cs.AI | cs.ROPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06504">Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06504" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Andrii Lysyi, Anatoliy Sachenko, Pavlo Radiuk, Mykola Lysyi, Oleksandr Melnychenko</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种无人机光伏组件检测方法，融合热成像和RGB数据，通过智能自动化框架解决了传统方法的热调色板偏差、数据冗余和高带宽问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统光伏组件检测方法存在热调色板偏差、数据冗余和高带宽需求等问题，亟需一种更高效、精确的自动化解决方案。</p>
<p><strong>Result:</strong> 1. 在PVF-10基准上<a href="mailto:&#109;&#65;&#80;&#64;&#x30;&#46;&#x35;">mAP@0.5</a>达到0.903（提升12-15%）；2. 召回率为96%；3. 去重减少15-20%误报，数据传输减少60-70%。</p>
<p><strong>Insight:</strong> 多模态数据融合和智能去重设计显著提升了光伏检测的效率和准确性，适用于大规模光伏电站的自动化巡检。</p>
<p><strong>Abstract:</strong> The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (<a href="mailto:&#x6d;&#65;&#x50;&#64;&#48;&#46;&#x35;">mAP@0.5</a>) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system’s readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.</p>
  </div>
</details>

<hr>
<h3 id="53-ShadowWolf-–-Automatic-Labelling-Evaluation-and-Model-Training-Optimised-for-Camera-Trap-Wildlife-Images-cs-CV-cs-AIPDF"><a href="#53-ShadowWolf-–-Automatic-Labelling-Evaluation-and-Model-Training-Optimised-for-Camera-Trap-Wildlife-Images-cs-CV-cs-AIPDF" class="headerlink" title="[53] ShadowWolf – Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images cs.CV | cs.AIPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06521">ShadowWolf – Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06521" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jens Dede, Anna Förster</span></p>
<p><strong>TL;DR:</strong> ShadowWolf是一个用于相机陷阱野生动物图像的自动标注、评估和模型训练的统一框架，通过动态模型重训练适应环境变化，提升野生动物监测的准确性和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 全球人口增长导致人类栖息地扩张，野生动物空间减少，人类与野生动物互动增加。传统AI训练方法因环境多样性（如地形、天气、光照）面临挑战，ShadowWolf旨在解决这些问题。</p>
<p><strong>Result:</strong> ShadowWolf提高了野生动物监测系统的准确性和效率，支持更有效的保护工作。</p>
<p><strong>Insight:</strong> 通过统一的框架和动态重训练，ShadowWolf解决了野生动物监测中的环境多样性问题，为AI在保护生物学中的应用提供了新思路。</p>
<p><strong>Abstract:</strong> The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.   In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.</p>
  </div>
</details>

<hr>
<h3 id="54-MedGRPO-Multi-Task-Reinforcement-Learning-for-Heterogeneous-Medical-Video-Understanding-cs-CVPDF"><a href="#54-MedGRPO-Multi-Task-Reinforcement-Learning-for-Heterogeneous-Medical-Video-Understanding-cs-CVPDF" class="headerlink" title="[54] MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding cs.CVPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06581">MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06581" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuhao Su, Anwesa Choudhuri, Zhongpai Gao, Benjamin Planche, Van Nguyen Nguyen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了MedVidBench基准和MedGRPO框架，通过跨数据集奖励归一化和医学LLM评估器提升多任务医疗视频理解能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大视觉语言模型在医疗视频理解任务中表现不佳，需要解决空间精度、时序推理和临床语义等问题。</p>
<p><strong>Result:</strong> 在MedVidBench上微调的Qwen2.5-VL-7B显著优于GPT-4.1和Gemini-2.5-Flash，MedGRPO进一步提升了基准性能。</p>
<p><strong>Insight:</strong> 通过临床维度评估和多数据集平衡训练，可有效提升医疗视频理解任务的性能。</p>
<p><strong>Abstract:</strong> Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \emph{cross-dataset reward normalization} that maps each dataset’s median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench’s efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at <a target="_blank" rel="noopener" href="https://yuhaosu.github.io/MedGRPO/">https://yuhaosu.github.io/MedGRPO/</a>.</p>
  </div>
</details>

<hr>
<h3 id="55-From-Remote-Sensing-to-Multiple-Time-Horizons-Forecasts-Transformers-Model-for-CyanoHAB-Intensity-in-Lake-Champlain-cs-CVPDF"><a href="#55-From-Remote-Sensing-to-Multiple-Time-Horizons-Forecasts-Transformers-Model-for-CyanoHAB-Intensity-in-Lake-Champlain-cs-CVPDF" class="headerlink" title="[55] From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain cs.CVPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06598">From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06598" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhammad Adil, Patrick J. Clemins, Andrew W. Schroth, Panagiotis D. Oikonomou, Donna M. Rizzo</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于Transformer和BiLSTM的远程感知框架，用于提前14天预测蓝藻水华（CyanoHABs）的强度，结果显示模型在多时间范围预测中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 蓝藻水华对水生生态系统和公共健康构成威胁，Lake Champlain地区尤为严重。传统的现场监测方法覆盖范围有限，而远程感知提供了可扩展的解决方案。</p>
<p><strong>Result:</strong> 模型在1天、2天和3天预测中的F1分数分别为89.5%、86.4%和85.5%；14天预测的F1分数为78.9%，AUC为82.6%。</p>
<p><strong>Insight:</strong> 远程感知结合深度学习模型能够有效解决稀疏数据问题，并为蓝藻水华的早期预警提供可靠支持。</p>
<p><strong>Abstract:</strong> Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model’s ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.</p>
  </div>
</details>

<hr>
<h3 id="56-Learning-Relative-Gene-Expression-Trends-from-Pathology-Images-in-Spatial-Transcriptomics-cs-CVPDF"><a href="#56-Learning-Relative-Gene-Expression-Trends-from-Pathology-Images-in-Spatial-Transcriptomics-cs-CVPDF" class="headerlink" title="[56] Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics cs.CVPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06612">Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06612" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kazuya Nishimura, Haruka Hirose, Ryoma Bise, Kaito Shiku, Yasuhiro Kojima</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种从病理图像中学习基因相对表达趋势的方法，通过关注相对表达模式而非绝对表达值，有效减少RNA测序成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统方法通过点损失函数预测绝对基因表达值，但由于测序技术的复杂性和细胞间变异性，结果存在噪声和批次效应。论文提出学习相对表达模式以避开这些问题。</p>
<p><strong>Result:</strong> 在合成数据集和真实数据集上的实验验证了方法的有效性。</p>
<p><strong>Insight:</strong> 相对表达模式的稳定性优于绝对表达值，为基因表达分析提供新视角。</p>
<p><strong>Abstract:</strong> Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at <a target="_blank" rel="noopener" href="https://github.com/naivete5656/STRank">https://github.com/naivete5656/STRank</a>.</p>
  </div>
</details>

<hr>
<h3 id="57-Hierarchical-Deep-Learning-for-Diatom-Image-Classification-A-Multi-Level-Taxonomic-Approach-cs-CVPDF"><a href="#57-Hierarchical-Deep-Learning-for-Diatom-Image-Classification-A-Multi-Level-Taxonomic-Approach-cs-CVPDF" class="headerlink" title="[57] Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach cs.CVPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06613">Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06613" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yueying Ke</span></p>
<p><strong>TL;DR:</strong> 通过将分类学层次结构嵌入神经网络架构，提出了一种分层卷积网络，用于硅藻图像的多级分类，显著提升分类精度并减少错误传播。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统硅藻分类方法依赖专家，而现有深度学习模型多为扁平分类，无法充分利用分类学层次结构。</p>
<p><strong>Result:</strong> 分层模型在物种级别与基线模型相当（69.4%），但在上层分类学级别表现更优，错误传播显著减少（92.5% vs 67.2%）。</p>
<p><strong>Insight:</strong> 层次约束和共享特征的联合作用（自上而下与自下而上）提升了模型的鲁棒性和生物学对齐性。</p>
<p><strong>Abstract:</strong> Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.   We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.   The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).   Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.</p>
  </div>
</details>

<hr>
<h3 id="58-Masked-Autoencoder-Pretraining-on-Strong-Lensing-Images-for-Joint-Dark-Matter-Model-Classification-and-Super-Resolution-cs-CV-astro-ph-CO-astro-ph-IM-cs-AI-cs-LGPDF"><a href="#58-Masked-Autoencoder-Pretraining-on-Strong-Lensing-Images-for-Joint-Dark-Matter-Model-Classification-and-Super-Resolution-cs-CV-astro-ph-CO-astro-ph-IM-cs-AI-cs-LGPDF" class="headerlink" title="[58] Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution cs.CV | astro-ph.CO | astro-ph.IM | cs.AI | cs.LGPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06642">Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | astro-ph.CO | astro-ph.IM | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06642" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种在强透镜图像上使用掩码自编码器（MAE）的预训练策略，用于暗物质模型分类和超分辨率任务，结果显示其优于从头训练的ViT模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 强引力透镜图像的低分辨率和噪声特性使得分析暗物质影响具有挑战性。MAE预训练策略旨在解决这一问题。</p>
<p><strong>Result:</strong> 在90%掩码比例下，分类任务AUC达0.968，准确率88.65%；超分辨率PSNR达33 dB，SSIM 0.961。</p>
<p><strong>Insight:</strong> MAE预训练在物理模拟数据上表现优异，掩码比例需要在分类和重建任务间权衡。</p>
<p><strong>Abstract:</strong> Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.</p>
  </div>
</details>

<hr>
<h3 id="59-TextMamba-Scene-Text-Detector-with-Mamba-cs-CV-cs-AIPDF"><a href="#59-TextMamba-Scene-Text-Detector-with-Mamba-cs-CV-cs-AIPDF" class="headerlink" title="[59] TextMamba: Scene Text Detector with Mamba cs.CV | cs.AIPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06657">TextMamba: Scene Text Detector with Mamba</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06657" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiyan Zhao, Yue Yan, Da-Han Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了基于Mamba的新型场景文本检测器TextMamba，结合选择机制与注意力层，解决了传统Transformer方法在长距离依赖建模中的信息遗忘和无关表征问题。通过Top_k算法选择关键信息，设计双尺度前馈网络和嵌入金字塔增强模块，在多个基准测试中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管基于Transformer的场景文本检测方法解决了传统CNN的全局特征提取局限性，但其在跨域建模长距离依赖时存在信息遗忘或关注无关表征的问题。近期提出的状态空间模型Mamba能通过线性复杂度选择机制更好地建模长距离依赖，因此尝试将其引入文本检测领域以提升性能。</p>
<p><strong>Result:</strong> 在CTW1500（F1&#x3D;89.7%）、TotalText（F1&#x3D;89.2%）和ICDAR19ArT（F1&#x3D;78.5%）等基准测试中达到SOTA或竞争性性能。</p>
<p><strong>Insight:</strong> 状态空间模型（如Mamba）在长距离依赖建模中的高效性可被迁移到文本检测任务，通过显式选择机制和多尺度特征融合可以进一步提升性能。</p>
<p><strong>Abstract:</strong> In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder’s ability to extract relevant information from long sequences. We adopt the Top_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7%, 89.2%, and 78.5% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.</p>
  </div>
</details>

<hr>
<h3 id="60-Personalized-Image-Descriptions-from-Attention-Sequences-cs-CVPDF"><a href="#60-Personalized-Image-Descriptions-from-Attention-Sequences-cs-CVPDF" class="headerlink" title="[60] Personalized Image Descriptions from Attention Sequences cs.CVPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06662">Personalized Image Descriptions from Attention Sequences</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06662" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruoyu Xue, Hieu Le, Jingyi Xu, Sounak Mondal, Abe Leite</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种名为DEPER的方法，通过建模个性化的视觉行为（注意力序列）和语言风格来生成个性化的图像描述，显著提升了描述的质量和与人类偏好的一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的个性化图像描述模型仅关注语言风格，忽略了人类观看图像时的注意力模式差异。这种差异会导致描述的巨大变异性，因此作者希望通过建模个性化的视觉行为来提升描述的准确性和个性化。</p>
<p><strong>Result:</strong> 在四个数据集上的实验表明，DEPER平均提升了24%的性能，生成的描述更符合人类偏好且质量更高。</p>
<p><strong>Insight:</strong> 论文揭示了人类观看行为的多样性是描述变异性的一大来源，建模这种感知多样性可以显著提升多模态系统的性能和与人类的对齐度。</p>
<p><strong>Abstract:</strong> People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.</p>
  </div>
</details>

<hr>
<h3 id="61-CoT4Det-A-Chain-of-Thought-Framework-for-Perception-Oriented-Vision-Language-Tasks-cs-CVPDF"><a href="#61-CoT4Det-A-Chain-of-Thought-Framework-for-Perception-Oriented-Vision-Language-Tasks-cs-CVPDF" class="headerlink" title="[61] CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks cs.CVPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06663">CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06663" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Qi, Yumeng Zhang, Chenting Gong, Xiao Tan, Weiming Zhang</span></p>
<p><strong>TL;DR:</strong> CoT4Det 提出了一种链式思维框架，将感知任务分解为分类、计数和定位三步，显著提升了大型视觉语言模型在感知任务上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型视觉语言模型（LVLMs）在感知任务（如目标检测）上表现不佳，远不及专用模型。本文旨在通过改进推理步骤来提升其性能。</p>
<p><strong>Result:</strong> CoT4Det 在 COCO2017 val 上 mAP 从 19% 提升到 33%，在 RefCOCO 和 Flickr30k 等基准上也表现优异。</p>
<p><strong>Insight:</strong> 通过任务分解和对齐 LVLMs 的推理能力，可以显著提升感知任务的性能，同时不影响模型的其他视觉语言能力。</p>
<p><strong>Abstract:</strong> Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks – such as object detection, semantic segmentation, and depth estimation – remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding – each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.</p>
  </div>
</details>

<hr>
<h3 id="62-1-1-2-Detector-Empowered-Video-Large-Language-Model-for-Spatio-Temporal-Grounding-and-Reasoning-cs-CVPDF"><a href="#62-1-1-2-Detector-Empowered-Video-Large-Language-Model-for-Spatio-Temporal-Grounding-and-Reasoning-cs-CVPDF" class="headerlink" title="[62] 1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning cs.CVPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06673">1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06673" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shida Gao, Feng Xue, Xiangfeng Wang, Anlong Ming, Teng Long</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合视频大语言模型（Video LLM）和开放词汇检测器（OVD）的方法DEViL，通过参考语义令牌（RST）和时序一致性正则化（TTReg），显著提升了时空定位和推理任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通过文本令牌自回归生成边界框，导致长输出序列和时空误差累积。为了解决这些问题，作者提出了结合检测器和语言模型的方法，以实现更准确的时空定位和推理。</p>
<p><strong>Result:</strong> 实验表明DEViL在STVG和GroundedVQA等任务中表现优异。</p>
<p><strong>Insight:</strong> 结合检测器和语言模型能够有效解决自回归定位中的误差累积问题，提升时空定位和推理任务的性能。</p>
<p><strong>Abstract:</strong> Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD’s text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on <a target="_blank" rel="noopener" href="https://github.com/gaostar123/DeViL">https://github.com/gaostar123/DeViL</a>.</p>
  </div>
</details>

<hr>
<h3 id="63-RunawayEvil-Jailbreaking-the-Image-to-Video-Generative-Models-cs-CVPDF"><a href="#63-RunawayEvil-Jailbreaking-the-Image-to-Video-Generative-Models-cs-CVPDF" class="headerlink" title="[63] RunawayEvil: Jailbreaking the Image-to-Video Generative Models cs.CVPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06674">RunawayEvil: Jailbreaking the Image-to-Video Generative Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06674" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Songping Wang, Rufan Qian, Yueming Lyu, Qinglong Liu, Linzhuang Zou</span></p>
<p><strong>TL;DR:</strong> RunawayEvil 是第一个针对图像到视频（I2V）生成模型的多模态越狱框架，具备动态进化能力，通过强化学习和LLM驱动的策略自定义，显著提高了攻击成功率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究I2V生成模型的安全性漏洞，尤其是其易受越狱攻击的特性，目前缺乏深入探索。RunawayEvil填补了这一空白，提供了一种动态自适应的攻击框架。</p>
<p><strong>Result:</strong> 在Open-Sora 2.0和CogVideoX等商业I2V模型上取得了显著攻击成功率，比现有方法高出58.5%至79%（COCO2017数据集）。</p>
<p><strong>Insight:</strong> 揭示了I2V模型的潜在安全风险，为未来开发更健壮的视频生成系统提供了重要参考。</p>
<p><strong>Abstract:</strong> Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a “Strategy-Tactic-Action” paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.</p>
  </div>
</details>

<hr>
<h3 id="64-The-Role-of-Entropy-in-Visual-Grounding-Analysis-and-Optimization-cs-CV-cs-AI-cs-CLPDF"><a href="#64-The-Role-of-Entropy-in-Visual-Grounding-Analysis-and-Optimization-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[64] The Role of Entropy in Visual Grounding: Analysis and Optimization cs.CV | cs.AI | cs.CLPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06726">The Role of Entropy in Visual Grounding: Analysis and Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06726" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuo Li, Jiajun Sun, Zhihao Zhang, Xiaoran Fan, Senjie Jin</span></p>
<p><strong>TL;DR:</strong> 论文探讨了熵在视觉定位任务中的作用及其优化方法，提出了一种名为ECVGPO的算法以更好地平衡探索与利用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管在强化学习微调多模态大语言模型（MLLMs）方面取得了进展，但熵在感知导向任务（如视觉定位）中的作用及其控制策略仍未充分研究。</p>
<p><strong>Result:</strong> 实验表明，ECVGPO在各种基准测试和模型中均实现了广泛改进。</p>
<p><strong>Insight:</strong> 熵的控制对于视觉定位任务的成功至关重要，ECVGPO通过显式调节熵值，提升了任务性能。</p>
<p><strong>Abstract:</strong> Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.</p>
  </div>
</details>

<hr>
<h3 id="65-Graph-Convolutional-Long-Short-Term-Memory-Attention-Network-for-Post-Stroke-Compensatory-Movement-Detection-Based-on-Skeleton-Data-cs-CVPDF"><a href="#65-Graph-Convolutional-Long-Short-Term-Memory-Attention-Network-for-Post-Stroke-Compensatory-Movement-Detection-Based-on-Skeleton-Data-cs-CVPDF" class="headerlink" title="[65] Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data cs.CVPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06736">Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06736" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaxing Fan, Jiaojiao Liu, Wenkong Wang, Yang Zhang, Xin Ma</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于骨架数据的图卷积长短期记忆注意力网络（GCN-LSTM-ATT），用于检测中风后的代偿性运动，准确率达0.8580，显著优于传统机器学习方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 中风患者普遍存在上肢运动功能障碍，康复训练中的代偿性运动对长期恢复不利，因此需要高效检测方法。</p>
<p><strong>Result:</strong> GCN-LSTM-ATT模型的检测准确率为0.8580，显著优于其他方法；消融实验验证了各模块的有效性。</p>
<p><strong>Insight:</strong> 结合时空特征的深度学习模型在医疗动作检测中表现优越，为康复训练策略优化提供了新工具。</p>
<p><strong>Abstract:</strong> Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients’ long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.</p>
  </div>
</details>

<hr>
<h3 id="66-FedSCAl-Leveraging-Server-and-Client-Alignment-for-Unsupervised-Federated-Source-Free-Domain-Adaptation-cs-CVPDF"><a href="#66-FedSCAl-Leveraging-Server-and-Client-Alignment-for-Unsupervised-Federated-Source-Free-Domain-Adaptation-cs-CVPDF" class="headerlink" title="[66] FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation cs.CVPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06738">FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06738" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">M Yashwanth, Sampath Koti, Arunabh Singh, Shyam Marjit, Anirban Chakraborty</span></p>
<p><strong>TL;DR:</strong> FedSCAl提出了一种联邦学习框架，通过服务器-客户端对齐（SCAl）机制解决无监督联邦源自由领域自适应（FFreeDA）问题，缓解客户端漂移并提升伪标签准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 联邦学习在无监督环境下因客户端数据分布差异大（领域差距）导致伪标签不可靠，现有方法难以解决客户端漂移问题。</p>
<p><strong>Result:</strong> 在基准视觉数据集上，FedSCAl在分类任务中优于现有联邦学习方法。</p>
<p><strong>Insight:</strong> 服务器和客户端的预测对齐在无监督联邦学习中具有重要意义，可有效缓解数据异质性带来的挑战。</p>
<p><strong>Abstract:</strong> We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients’ domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients’ and server model’s predictions. We observe an improvement in the clients’ pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.</p>
  </div>
</details>

<hr>
<h3 id="67-Task-Model-Alignment-A-Simple-Path-to-Generalizable-AI-Generated-Image-Detection-cs-CV-cs-AIPDF"><a href="#67-Task-Model-Alignment-A-Simple-Path-to-Generalizable-AI-Generated-Image-Detection-cs-CV-cs-AIPDF" class="headerlink" title="[67] Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection cs.CV | cs.AIPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06746">Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06746" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruoxin Chen, Jiahui Gao, Kaiqing Lin, Keyue Zhang, Yandan Zhao</span></p>
<p><strong>TL;DR:</strong> 该论文提出任务-模型对齐原则，通过将AI生成图像检测分为语义一致性和像素伪影检测两个互补任务，显著提升了泛化性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型（VLM）用于AI生成图像检测时资源消耗大且存在严重幻觉问题，核心问题是任务与模型的不匹配。</p>
<p><strong>Result:</strong> 在五个真实场景基准测试中，AlignGemini实现了平均准确率9.5%的提升。</p>
<p><strong>Insight:</strong> 任务与模型的匹配性是提升检测泛化能力的关键，互补性任务分工能有效减少系统性盲区。</p>
<p><strong>Abstract:</strong> Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs’ underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks–semantic consistency checking and pixel-artifact detection–and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.</p>
  </div>
</details>

<hr>
<h3 id="68-UARE-A-Unified-Vision-Language-Model-for-Image-Quality-Assessment-Restoration-and-Enhancement-cs-CVPDF"><a href="#68-UARE-A-Unified-Vision-Language-Model-for-Image-Quality-Assessment-Restoration-and-Enhancement-cs-CVPDF" class="headerlink" title="[68] UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement cs.CVPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06750">UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06750" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weiqi Li, Xuanyu Zhang, Bin Chen, Jingfen Xie, Yan Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了UARE模型，首次统一了图像质量评估（IQA）、修复和增强任务。通过两阶段训练和多任务协同学习，UARE利用IQA信号指导修复任务，取得了显著效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究通常将图像质量评估（IQA）和图像修复任务分开处理，忽视了二者的紧密联系。受多模态统一模型的启发，作者希望通过一个统一模型结合IQA和修复，并探索IQA如何指导修复任务。</p>
<p><strong>Result:</strong> 在多个IQA、修复和增强任务上的实验表明，UARE表现出色。</p>
<p><strong>Insight:</strong> 通过统一任务和多任务学习，IQA信号可以有效指导图像修复和增强任务，这为低层视觉任务的设计提供了新思路。</p>
<p><strong>Abstract:</strong> Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at <a target="_blank" rel="noopener" href="https://github.com/lwq20020127/UARE">https://github.com/lwq20020127/UARE</a>.</p>
  </div>
</details>

<hr>
<h3 id="69-VisChainBench-A-Benchmark-for-Multi-Turn-Multi-Image-Visual-Reasoning-Beyond-Language-Priors-cs-CV-cs-AIPDF"><a href="#69-VisChainBench-A-Benchmark-for-Multi-Turn-Multi-Image-Visual-Reasoning-Beyond-Language-Priors-cs-CV-cs-AIPDF" class="headerlink" title="[69] VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors cs.CV | cs.AIPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06759">VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06759" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenbo Lyu, Yingjun Du, Jinglin Zhao, Xianton Zhen, Ling Shao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了VisChainBench，一个用于评估大型视觉语言模型（LVLMs）在多轮、多图像视觉推理任务中能力的基准测试。它突出了超越语言先验的视觉推理挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基准测试主要关注静态或水平比较，依赖语言线索，忽视了多轮、上下文依赖的视觉推理能力。</p>
<p><strong>Result:</strong> 提供了基准的数据和代码，以促进对LVLMs在多轮、多图像视觉推理能力的评估。</p>
<p><strong>Insight:</strong> 突出了视觉推理中超越语言先验的重要性，并为未来研究提供了新的评估方向。</p>
<p><strong>Abstract:</strong> Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons – e.g., spotting visual differences or assessing appropriateness – while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs’ ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/eyehole/VisChainBench">https://huggingface.co/datasets/eyehole/VisChainBench</a></p>
  </div>
</details>

<hr>
<h3 id="70-MMDuet2-Enhancing-Proactive-Interaction-of-Video-MLLMs-with-Multi-Turn-Reinforcement-Learning-cs-CV-cs-CLPDF"><a href="#70-MMDuet2-Enhancing-Proactive-Interaction-of-Video-MLLMs-with-Multi-Turn-Reinforcement-Learning-cs-CV-cs-CLPDF" class="headerlink" title="[70] MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning cs.CV | cs.CLPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06810">MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06810" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yueqian Wang, Songxiang Liu, Disong Wang, Nuo Xu, Guanglu Wan</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新颖的视频多模态大语言模型（Video MLLM）MMDuet2，通过多轮强化学习（RL）方法实现了模型在视频播放过程中的主动交互能力，避免了手动调整阈值或标注精确回复时间的需求，并在ProactiveVideoQA基准上取得了最优性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频多模态大语言模型通常采用基于回合的交互方式，而视频播放过程中的实时主动交互具有挑战性，但能为实际应用带来显著优势。</p>
<p><strong>Result:</strong> MMDuet2在回复时机和质量上优于现有基线模型，在ProactiveVideoQA基准上达到了最先进的性能。</p>
<p><strong>Insight:</strong> 通过RL方法可以克服传统方法中手动调整和标注的不足，为视频多模态大语言模型的实时交互提供了一种可行的训练范式。</p>
<p><strong>Abstract:</strong> Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.</p>
  </div>
</details>

<hr>
<h3 id="71-JOCA-Task-Driven-Joint-Optimisation-of-Camera-Hardware-and-Adaptive-Camera-Control-Algorithms-cs-CVPDF"><a href="#71-JOCA-Task-Driven-Joint-Optimisation-of-Camera-Hardware-and-Adaptive-Camera-Control-Algorithms-cs-CVPDF" class="headerlink" title="[71] JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms cs.CVPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06763">JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06763" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chengyang Yan, Mitch Bryson, Donald G. Dansereau</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种联合优化相机硬件和自适应相机控制算法的新方法JOCA，通过统一框架结合梯度优化和无导数方法，显著提升下游视觉任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 相机捕获的图像质量直接影响下游感知任务的效果。现有方法多专注于优化固定参数，而忽略了运行时自适应控制的需求。JOCA旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验表明，JOCA在低光和快速运动场景下显著优于单独优化静态或动态参数的基线方法。</p>
<p><strong>Insight:</strong> 联合优化硬件和自适应控制算法能显著提升感知性能，为任务驱动的相机系统设计提供了新思路。</p>
<p><strong>Abstract:</strong> The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.</p>
  </div>
</details>

<hr>
<h3 id="72-Less-Is-More-but-Where-Dynamic-Token-Compression-via-LLM-Guided-Keyframe-Prior-cs-CV-cs-AI-cs-CL-cs-LGPDF"><a href="#72-Less-Is-More-but-Where-Dynamic-Token-Compression-via-LLM-Guided-Keyframe-Prior-cs-CV-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[72] Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior cs.CV | cs.AI | cs.CL | cs.LGPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06866">Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06866" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yulin Li, Haokun Gui, Ziyang Fan, Junjie Wang, Bin Kang</span></p>
<p><strong>TL;DR:</strong> 论文提出了DyToK方法，利用VLLM的注意力机制动态压缩视频中的视觉令牌，避免传统关键帧方法的额外计算开销，实现了高效且准确的长视频理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频大型语言模型（VLLM）在处理长视频时，因视觉令牌序列过长导致计算成本二次增长。传统的关键帧采样方法虽然能提升效率，但引入了额外开销，且帧选择范式不够灵活。</p>
<p><strong>Result:</strong> 实验表明，DyToK在保持准确性的同时，实现了4.3倍的推理加速，并在多种VLLM（如LLaVA-OneVision和Qwen2.5-VL）上表现出色。</p>
<p><strong>Insight:</strong> VLLM的注意力机制本身隐含着关键帧选择的能力，DyToK巧妙利用了这一特性，为视频理解的高效动态压缩提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs’ inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at <a target="_blank" rel="noopener" href="https://github.com/yu-lin-li/DyToK">https://github.com/yu-lin-li/DyToK</a> .</p>
  </div>
</details>

<hr>
<h3 id="73-Stitch-and-Tell-A-Structured-Multimodal-Data-Augmentation-Method-for-Spatial-Understanding-cs-CV-cs-AIPDF"><a href="#73-Stitch-and-Tell-A-Structured-Multimodal-Data-Augmentation-Method-for-Spatial-Understanding-cs-CV-cs-AIPDF" class="headerlink" title="[73] Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding cs.CV | cs.AIPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06769">Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06769" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hang Yin, Xiaomin He, PeiWen Yuan, Yiwei Li, Jiayi Shi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Stitch and Tell (SiTe)的结构化多模态数据增强方法，通过拼接图像并生成空间感知的文本描述，提升了视觉语言模型的空间理解能力，且无需额外标注或人工干预。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视觉语言模型在描述图像中物体相对位置时存在空间幻觉问题，主要原因是图像与文本之间的不对称性。为了解决这一问题，需要增强模型的空间理解能力。</p>
<p><strong>Result:</strong> 实验表明SiTe在空间理解任务（如MME_Position和Spatial-MM）上显著提升，同时在通用视觉语言基准（如COCO-QA和MMBench）上保持或改进性能。</p>
<p><strong>Insight:</strong> 显式地将空间感知结构注入训练数据可以有效缓解空间幻觉问题，提升模型的空间理解能力，同时不影响其通用视觉语言性能。</p>
<p><strong>Abstract:</strong> Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\text{MME}_{\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.</p>
  </div>
</details>

<hr>
<h3 id="74-NeuroABench-A-Multimodal-Evaluation-Benchmark-for-Neurosurgical-Anatomy-Identification-cs-CV-cs-AI-cs-CLPDF"><a href="#74-NeuroABench-A-Multimodal-Evaluation-Benchmark-for-Neurosurgical-Anatomy-Identification-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[74] NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification cs.CV | cs.AI | cs.CLPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06921">NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06921" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziyang Song, Zelin Zang, Xiaofan Ye, Boqiang Xu, Long Bai</span></p>
<p><strong>TL;DR:</strong> 论文介绍了NeuroABench，首个用于评估神经外科领域解剖学理解的多模态基准测试，包含9小时的标注视频和68个临床解剖结构。实验表明，目前最先进的MLLMs在解剖识别任务中表现有限，最优模型仅达到40.87%的准确率，落后于人类表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究主要关注手术流程和工作流，而忽视了解剖学理解在手术视频中的关键作用。临床实践中，解剖学知识对正确解读和学习手术视频至关重要。</p>
<p><strong>Result:</strong> 实验显示，最先进的MLLMs在解剖识别任务中准确率仅40.87%，且显著低于人类表现（学生平均46.5%）。</p>
<p><strong>Insight:</strong> MLLMs在解剖学理解上虽取得进展，但与人类水平仍有较大差距，需进一步优化模型和标注方法。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group’s average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.</p>
  </div>
</details>

<hr>
<h3 id="75-Physics-Informed-Human-Posture-Estimation-Based-on-3D-Landmarks-from-Monocular-RGB-Videos-cs-CVPDF"><a href="#75-Physics-Informed-Human-Posture-Estimation-Based-on-3D-Landmarks-from-Monocular-RGB-Videos-cs-CVPDF" class="headerlink" title="[75] Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos cs.CVPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06783">Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06783" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tobias Leuthold, Michele Xiloyannis, Yves Zimmermann</span></p>
<p><strong>TL;DR:</strong> 提出了一种基于BlazePose的实时后处理算法，通过加权优化结合3D和2D估计，引入骨骼长度和生物力学模型的物理约束，显著提升了人体姿态估计的准确性和一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的人体姿态估计模型（如BlazePose）缺乏解剖学约束，导致估计结果可能不符合真实的生物力学规律。通过引入物理知识，可以改进姿态估计的准确性和鲁棒性。</p>
<p><strong>Result:</strong> 在Physio2.2M数据集上，3D MPJPE降低了10.2%，身体段间角度误差减少了16.6%，同时保持了实时性能。</p>
<p><strong>Insight:</strong> 引入物理约束（如骨骼长度和生物力学模型）可以显著提升姿态估计的准确性和鲁棒性，同时保持实时性，适用于移动设备和消费级硬件。</p>
<p><strong>Abstract:</strong> Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.</p>
  </div>
</details>

<hr>
<h3 id="76-Generalized-Geometry-Encoding-Volume-for-Real-time-Stereo-Matching-cs-CVPDF"><a href="#76-Generalized-Geometry-Encoding-Volume-for-Real-time-Stereo-Matching-cs-CVPDF" class="headerlink" title="[76] Generalized Geometry Encoding Volume for Real-time Stereo Matching cs.CVPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06793">Generalized Geometry Encoding Volume for Real-time Stereo Matching</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06793" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaxin Liu, Gangwei Xu, Xianqi Wang, Chengliang Zhang, Xin Yang</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种名为GGEV的实时立体匹配网络，通过在深度感知特征的引导下动态聚合成本，解决了实时方法与泛化能力之间的权衡问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的实时立体匹配方法主要关注域内性能，而忽略了泛化能力的重要性；而基于单目基础模型的方法虽有较好的泛化性，但推理延迟较高。作者旨在设计一种同时具备实时性和强泛化能力的立体匹配网络。</p>
<p><strong>Result:</strong> 在KITTI 2012、KITTI 2015和ETH3D基准测试中，GGEV的零样本泛化能力优于所有现有实时方法，并达到SOTA性能。</p>
<p><strong>Insight:</strong> 通过结合域不变的结构先验和动态成本聚合，可以在不牺牲实时性的情况下显著提升立体匹配模型的泛化能力。</p>
<p><strong>Abstract:</strong> Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="77-VDOT-Efficient-Unified-Video-Creation-via-Optimal-Transport-Distillation-cs-CVPDF"><a href="#77-VDOT-Efficient-Unified-Video-Creation-via-Optimal-Transport-Distillation-cs-CVPDF" class="headerlink" title="[77] VDOT: Efficient Unified Video Creation via Optimal Transport Distillation cs.CVPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06802">VDOT: Efficient Unified Video Creation via Optimal Transport Distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06802" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yutong Wang, Haiyu Zhang, Tianfan Xue, Yu Qiao, Yaohui Wang</span></p>
<p><strong>TL;DR:</strong> VDOT提出了一种高效的统一视频生成模型，通过最优传输蒸馏技术解决了现有模型生成速度慢或条件单一的问题，同时提升了生成质量和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频生成模型要么仅支持少数特定条件，要么因复杂推理导致生成时间长，难以实际应用。为解决这些问题，VDOT旨在开发一种高效统一的视频生成方法。</p>
<p><strong>Result:</strong> 实验表明，4步VDOT在性能上匹配或超越其他需要100步去噪的基线模型。</p>
<p><strong>Insight:</strong> 最优传输技术通过几何约束避免了KL蒸馏中的梯度崩溃问题，适用于少步生成场景；统一数据标注和评测基准为视频生成研究提供了标准化工具。</p>
<p><strong>Abstract:</strong> The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.</p>
  </div>
</details>

<hr>
<h3 id="78-Think-Reflect-Revise-A-Policy-Guided-Reflective-Framework-for-Safety-Alignment-in-Large-Vision-Language-Models-cs-CV-cs-CLPDF"><a href="#78-Think-Reflect-Revise-A-Policy-Guided-Reflective-Framework-for-Safety-Alignment-in-Large-Vision-Language-Models-cs-CV-cs-CLPDF" class="headerlink" title="[78] Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models cs.CV | cs.CLPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07141">Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07141" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fenghua Weng, Chaochao Lu, Xia Hu, Wenqi Shao, Wenjie Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出Think-Reflect-Revise（TRR）框架，通过三阶段训练（思考-反思-修订）增强大型视觉语言模型（LVLM）的安全对齐能力，显著提升安全性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的单次思考-回答范式易受上下文或视觉攻击，可能导致输出忽略有害内容。TRR框架通过利用一次推理中暴露的有害信号，实现自我修正。</p>
<p><strong>Result:</strong> TRR将Qwen2.5-VL-7B的安全响应率从42.8%提升至87.7%，同时在通用基准测试中保持稳定。</p>
<p><strong>Insight:</strong> 反思机制能有效利用一次推理中的负面信号实现自我修正，提升模型安全性，同时不影响通用性能。</p>
<p><strong>Abstract:</strong> As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at <a target="_blank" rel="noopener" href="https://think-reflect-revise.github.io/">https://think-reflect-revise.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="79-RMAdapter-Reconstruction-based-Multi-Modal-Adapter-for-Vision-Language-Models-cs-CV-cs-AI-cs-LG-cs-MMPDF"><a href="#79-RMAdapter-Reconstruction-based-Multi-Modal-Adapter-for-Vision-Language-Models-cs-CV-cs-AI-cs-LG-cs-MMPDF" class="headerlink" title="[79] RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models cs.CV | cs.AI | cs.LG | cs.MMPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06811">RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06811" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiang Lin, Weixin Li, Shu Guo, Lihong Wang, Di Huang</span></p>
<p><strong>TL;DR:</strong> RMAdapter是一种基于重构的多模态适配器，用于视觉语言模型（VLMs）的少样本微调，通过双分支架构动态平衡任务特定知识和通用知识，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 预训练的视觉语言模型（如CLIP）在多模态迁移学习中表现优异，但在少样本场景下微调时，任务特定适应与泛化能力之间的平衡成为挑战。现有的研究主要集中于基于提示的方法，而基于适配器的方法尚未充分探索，且性能差距较大。</p>
<p><strong>Result:</strong> 在三种代表性任务（新类别泛化、新目标数据集泛化和领域泛化）评估中，RMAdapter在所有指标上均优于现有方法。</p>
<p><strong>Insight:</strong> 双分支架构的设计能够有效平衡任务特定和通用知识，同时轻量化的优化策略确保了计算效率，为多模态适配器的研究提供了新思路。</p>
<p><strong>Abstract:</strong> Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.</p>
  </div>
</details>

<hr>
<h3 id="80-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes-cs-CVPDF"><a href="#80-MeshSplatting-Differentiable-Rendering-with-Opaque-Meshes-cs-CVPDF" class="headerlink" title="[80] MeshSplatting: Differentiable Rendering with Opaque Meshes cs.CVPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06818">MeshSplatting: Differentiable Rendering with Opaque Meshes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06818" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jan Held, Sanghyun Son, Renaud Vandeghen, Daniel Rebain, Matheus Gadelha</span></p>
<p><strong>TL;DR:</strong> MeshSplatting提出了一种基于网格的渲染方法，通过可微分渲染联合优化几何和外观，解决了点基表示与网格管线兼容性问题，实现了高效的实时渲染和质量提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于基元（如3D高斯）的渲染方法虽然实现了实时新视角合成，但与AR&#x2F;VR和游戏引擎中的网格管线不兼容。MeshSplatting旨在解决这一问题。</p>
<p><strong>Result:</strong> 在Mip-NeRF360数据集上，MeshSplatting的PSNR比当前最佳方法MiLo高0.69dB，训练速度快2倍，内存使用少2倍。</p>
<p><strong>Insight:</strong> MeshSplatting成功连接了神经渲染与交互式3D图形，为实时场景交互提供了无缝解决方案。</p>
<p><strong>Abstract:</strong> Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR&#x2F;VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at <a target="_blank" rel="noopener" href="https://meshsplatting.github.io/">https://meshsplatting.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="81-Generating-Storytelling-Images-with-Rich-Chains-of-Reasoning-cs-CV-cs-CLPDF"><a href="#81-Generating-Storytelling-Images-with-Rich-Chains-of-Reasoning-cs-CV-cs-CLPDF" class="headerlink" title="[81] Generating Storytelling Images with Rich Chains-of-Reasoning cs.CV | cs.CLPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07198">Generating Storytelling Images with Rich Chains-of-Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07198" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiujie Song, Qi Jia, Shota Watanabe, Xiaoyi Pang, Ruijie Chen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种生成具有复杂推理链条（CoRs）的叙事图像的方法，结合了大型语言模型（LLMs）和文本到图像（T2I）模型的能力，并开发了专门的评估框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 叙事图像通过视觉线索传达复杂的故事，但由于其语义复杂性，生成此类图像具有挑战性，因此需要探索生成AI模型的潜力来解决这一问题。</p>
<p><strong>Result:</strong> 实验证明了方法的可行性和有效性，能够生成语义丰富且多样化的叙事图像。</p>
<p><strong>Insight:</strong> 通过结合语言和视觉模型的多模态能力，可以高效生成复杂的叙事图像，为视觉故事叙述提供了新的工具。</p>
<p><strong>Abstract:</strong> An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at <a target="_blank" rel="noopener" href="https://github.com/xiujiesong/StorytellingImageGeneration">https://github.com/xiujiesong/StorytellingImageGeneration</a>.</p>
  </div>
</details>

<hr>
<h3 id="82-CADE-Continual-Weakly-supervised-Video-Anomaly-Detection-with-Ensembles-cs-CVPDF"><a href="#82-CADE-Continual-Weakly-supervised-Video-Anomaly-Detection-with-Ensembles-cs-CVPDF" class="headerlink" title="[82] CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles cs.CVPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06840">CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06840" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Satoshi Hashimoto, Tatsuya Konishi, Tomoya Kaichi, Kazunori Matsumoto, Mori Kurokawa</span></p>
<p><strong>TL;DR:</strong> CADE是首个结合持续学习和弱监督视频异常检测的方法，通过双生成器和多判别器集成解决数据不平衡和遗忘问题，显著提升了多场景数据集上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有弱监督视频异常检测方法主要针对静态数据集，忽略了数据域的动态变化。持续学习视角可解决域偏移导致的遗忘问题。</p>
<p><strong>Result:</strong> 在ShanghaiTech和Charlotte Anomaly等数据集上，CADE显著优于现有方法。</p>
<p><strong>Insight:</strong> 遗忘会导致模型偏向特定异常模式，多判别器集成能有效缓解这一问题，提升检测多样性。</p>
<p><strong>Abstract:</strong> Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the “incompleteness’’ where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.</p>
  </div>
</details>

<hr>
<h3 id="83-Pseudo-Anomalies-Are-All-You-Need-Diffusion-Based-Generation-for-Weakly-Supervised-Video-Anomaly-Detection-cs-CVPDF"><a href="#83-Pseudo-Anomalies-Are-All-You-Need-Diffusion-Based-Generation-for-Weakly-Supervised-Video-Anomaly-Detection-cs-CVPDF" class="headerlink" title="[83] Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection cs.CVPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06845">Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06845" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Satoshi Hashimoto, Hitoshi Nishimura, Yanan Wang, Mori Kurokawa</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于生成的弱监督视频异常检测方法PA-VAD，通过合成伪异常视频而非真实异常数据训练模型，避免了真实异常数据稀缺的问题，并在多个数据集上取得了领先性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 实际应用中的视频异常检测受限于真实异常数据的稀缺和采集成本。</p>
<p><strong>Result:</strong> 在ShanghaiTech和UCF-Crime数据集上分别达到98.2%和82.5%的性能，超越了使用真实异常数据的方法。</p>
<p><strong>Insight:</strong> 高质量异常检测可通过合成伪异常实现，为实际部署提供了可扩展的路径。</p>
<p><strong>Abstract:</strong> Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.</p>
  </div>
</details>

<hr>
<h3 id="84-Hide-and-Seek-Attribution-Weakly-Supervised-Segmentation-of-Vertebral-Metastases-in-CT-cs-CV-cs-LGPDF"><a href="#84-Hide-and-Seek-Attribution-Weakly-Supervised-Segmentation-of-Vertebral-Metastases-in-CT-cs-CV-cs-LGPDF" class="headerlink" title="[84] Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT cs.CV | cs.LGPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06849">Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06849" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matan Atad, Alexander W. Marka, Lisa Steinhelfer, Anna Curto-Vilalta, Yannik Leonhardt</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种弱监督方法，仅通过脊椎级别的健康&#x2F;恶性肿瘤标签训练，无需病灶掩码，用于CT中脊柱转移瘤的分割。该方法结合了扩散自编码器和选择性遮挡技术，显著提升了分割性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> CT中脊柱转移瘤的精确分割在临床上很重要，但由于体素级标注稀缺且恶性与良性病变相似，难以大规模应用。</p>
<p><strong>Result:</strong> 在保留的放射科医师标注上，取得了显著的性能提升（F1: 0.91&#x2F;0.85; Dice: 0.87&#x2F;0.78），远超基线方法（F1: 0.79&#x2F;0.67; Dice: 0.74&#x2F;0.55）。</p>
<p><strong>Insight:</strong> 研究表明，仅通过脊椎级别标签训练的生成编辑结合选择性遮挡技术，能够实现高精度的弱监督分割，为临床应用提供了可行方案。</p>
<p><strong>Abstract:</strong> Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy&#x2F;malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic&#x2F;lytic performance despite no mask supervision (F1: 0.91&#x2F;0.85; Dice: 0.87&#x2F;0.78), exceeding baselines (F1: 0.79&#x2F;0.67; Dice: 0.74&#x2F;0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.</p>
  </div>
</details>

<hr>
<h3 id="85-Toward-More-Reliable-Artificial-Intelligence-Reducing-Hallucinations-in-Vision-Language-Models-cs-CV-cs-AI-cs-CL-cs-LGPDF"><a href="#85-Toward-More-Reliable-Artificial-Intelligence-Reducing-Hallucinations-in-Vision-Language-Models-cs-CV-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[85] Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models cs.CV | cs.AI | cs.CL | cs.LGPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07564">Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07564" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kassoum Sanogo, Renzo Ardiccioni</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无训练的自校正框架，通过不确定性引导的视觉重关注减少视觉语言模型（VLMs）中的幻觉内容，实验证明其显著降低了幻觉率并提升了准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉语言模型在生成内容时经常产生看似合理但实际错误的幻觉内容，亟需一种无需额外训练的方法来提升其可靠性。</p>
<p><strong>Result:</strong> 在POPE和MMHAL BENCH基准上，幻觉率降低了9.8个百分点，对抗性分割中的对象存在准确性提升了4.7点。</p>
<p><strong>Insight:</strong> 不确定性引导的视觉重关注可以有效纠正标准解码失败的内容，提升模型的可靠性。</p>
<p><strong>Abstract:</strong> Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.</p>
  </div>
</details>

<hr>
<h3 id="86-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training-cs-CVPDF"><a href="#86-Boosting-Unsupervised-Video-Instance-Segmentation-with-Automatic-Quality-Guided-Self-Training-cs-CVPDF" class="headerlink" title="[86] Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training cs.CVPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06864">Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06864" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaixuan Lu, Mehmet Onurcan Kaya, Dim P. Papadopoulos</span></p>
<p><strong>TL;DR:</strong> AutoQ-VIS提出了一种新的无监督视频实例分割框架，通过质量引导的自训练方法解决合成数据与真实视频之间的领域差距，实现了先进性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视频实例分割（VIS）需要像素级掩码和时间一致性标签，标注成本高。现有无监督方法依赖合成数据，但受限于合成与真实数据之间的领域差距。</p>
<p><strong>Result:</strong> 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA方法VideoCutLER提升了4.4%。</p>
<p><strong>Insight:</strong> 质量感知的自训练可以有效缩小合成与真实视频之间的领域差距，为无监督VIS提供了一种可行方案。</p>
<p><strong>Abstract:</strong> Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\text{AP}_{50}$ on YouTubeVIS-2019 $\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at <a target="_blank" rel="noopener" href="https://github.com/wcbup/AutoQ-VIS">https://github.com/wcbup/AutoQ-VIS</a>.</p>
  </div>
</details>

<hr>
<h3 id="87-Towards-Robust-Pseudo-Label-Learning-in-Semantic-Segmentation-An-Encoding-Perspective-cs-CVPDF"><a href="#87-Towards-Robust-Pseudo-Label-Learning-in-Semantic-Segmentation-An-Encoding-Perspective-cs-CVPDF" class="headerlink" title="[87] Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective cs.CVPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06870">Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06870" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wangkai Li, Rui Sun, Zhaoyang Li, Tianzhu Zhang</span></p>
<p><strong>TL;DR:</strong> ECOCSeg提出了一种基于纠错输出编码（ECOC）的新方法，用于提升语义分割中伪标签学习的鲁棒性，通过细粒度编码和位级标签去噪机制，显著提升了无监督域适应（UDA）和半监督学习（SSL）的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前语义分割中的伪标签学习方法因使用独热编码而容易产生错误的伪标签，且在训练过程中会被进一步放大。为了解决这一问题，作者提出了ECOCSeg，通过ECOC编码改进伪标签的鲁棒性。</p>
<p><strong>Result:</strong> 在多个UDA和SSL基准测试中，ECOCSeg与现有方法结合后均表现出显著性能提升，验证了其有效性。</p>
<p><strong>Insight:</strong> ECOCSeg通过引入ECOC编码和位级去噪，为语义分割中的伪标签学习提供了一种更鲁棒和可扩展的解决方案，尤其适用于标签稀缺的场景。</p>
<p><strong>Abstract:</strong> Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Woof6/ECOCSeg">https://github.com/Woof6/ECOCSeg</a>.</p>
  </div>
</details>

<hr>
<h3 id="88-SceneMixer-Exploring-Convolutional-Mixing-Networks-for-Remote-Sensing-Scene-Classification-cs-CVPDF"><a href="#88-SceneMixer-Exploring-Convolutional-Mixing-Networks-for-Remote-Sensing-Scene-Classification-cs-CVPDF" class="headerlink" title="[88] SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification cs.CVPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06877">SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06877" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammed Q. Alkhatib, Ali Jamali, Swalpa Kumar Roy</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于卷积混合范式（convolutional mixer）的轻量级架构SceneMixer，用于遥感场景分类。模型通过多尺度深度卷积和点操作交替提取局部和上下文信息，在AID和EuroSAT数据集上表现优异，平衡了准确性和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 遥感场景分类面临空间分辨率、视角、方向和背景条件等变化的挑战，现有模型的泛化能力受限。本文旨在设计一个轻量且高效的模型，以解决这些问题。</p>
<p><strong>Result:</strong> 在AID数据集上达到74.7%的总体准确率，EuroSAT数据集上达到93.9%的总体准确率，优于其他CNN和Transformer模型。</p>
<p><strong>Insight:</strong> 卷积混合网络在遥感场景分类中是一种高效的替代方案，能够在较低计算成本下实现高精度。</p>
<p><strong>Abstract:</strong> Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: <a target="_blank" rel="noopener" href="https://github.com/mqalkhatib/SceneMixer">https://github.com/mqalkhatib/SceneMixer</a></p>
  </div>
</details>

<hr>
<h3 id="89-JoPano-Unified-Panorama-Generation-via-Joint-Modeling-cs-CV-cs-AIPDF"><a href="#89-JoPano-Unified-Panorama-Generation-via-Joint-Modeling-cs-CV-cs-AIPDF" class="headerlink" title="[89] JoPano: Unified Panorama Generation via Joint Modeling cs.CV | cs.AIPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06885">JoPano: Unified Panorama Generation via Joint Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06885" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wancheng Feng, Chen An, Zhenliang He, Meina Kan, Shiguang Shan</span></p>
<p><strong>TL;DR:</strong> JoPano通过联合建模统一了全景图生成任务，采用DiT基模型和联合面适配器解决视觉质量和冗余问题，并引入新指标评估接缝一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的全景图生成方法受限于U-Net架构的视觉质量和任务独立建模的冗余，JoPano旨在通过联合建模提升生成质量与效率。</p>
<p><strong>Result:</strong> JoPano在FID、CLIP-FID、IS和CLIP-Score等指标上达到SOTA，生成高质量全景图。</p>
<p><strong>Insight:</strong> 联合建模和DiT架构的结合显著提升了全景图生成的视觉质量和任务统一性，接缝问题的解决是一个关键改进。</p>
<p><strong>Abstract:</strong> Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.</p>
  </div>
</details>

<hr>
<h3 id="90-Overcoming-Small-Data-Limitations-in-Video-Based-Infant-Respiration-Estimation-cs-CVPDF"><a href="#90-Overcoming-Small-Data-Limitations-in-Video-Based-Infant-Respiration-Estimation-cs-CVPDF" class="headerlink" title="[90] Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation cs.CVPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06888">Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06888" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liyang Song, Hardik Bishnoi, Sai Kumar Reddy Manne, Sarah Ostadabbas, Briana J. Taylor</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个新的婴儿呼吸数据集（AIR-400），并开发了首个可复现的婴儿呼吸估计方法，填补了该领域的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 婴儿呼吸监测对于早期发现和治疗呼吸异常至关重要，但目前缺乏公开的婴儿呼吸数据集和有效的算法。</p>
<p><strong>Result:</strong> 论文提出了首个可复现的婴儿呼吸估计基准，并展示了其方法的有效性。</p>
<p><strong>Insight:</strong> 婴儿呼吸监测需要专门的数据集和算法，而非直接沿用成人数据的方法。</p>
<p><strong>Abstract:</strong> The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.</p>
  </div>
</details>

<hr>
<h3 id="91-Scaling-Zero-Shot-Reference-to-Video-Generation-cs-CVPDF"><a href="#91-Scaling-Zero-Shot-Reference-to-Video-Generation-cs-CVPDF" class="headerlink" title="[91] Scaling Zero-Shot Reference-to-Video Generation cs.CVPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06905">Scaling Zero-Shot Reference-to-Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06905" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zijian Zhou, Shikun Liu, Haozhe Liu, Haonan Qiu, Zhaochong An</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Saber的零样本参考视频生成框架，无需依赖昂贵的参考图像-视频-文本三元组数据，仅通过视频-文本对实现可扩展的生成效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的参考视频生成方法依赖昂贵的显式三元组数据，难以扩展。Saber旨在克服这一瓶颈，通过零样本学习实现高效且可扩展的视频生成。</p>
<p><strong>Result:</strong> 在OpenS2V-Eval基准测试中，Saber优于依赖R2V数据的方法，展示了出色的泛化能力和生成效果。</p>
<p><strong>Insight:</strong> 通过零样本学习和掩码训练策略，Saber不仅降低了数据需求，还提升了生成质量，为参考视频生成的可扩展性提供了新思路。</p>
<p><strong>Abstract:</strong> Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.</p>
  </div>
</details>

<hr>
<h3 id="92-Selective-Masking-based-Self-Supervised-Learning-for-Image-Semantic-Segmentation-cs-CV-cs-LGPDF"><a href="#92-Selective-Masking-based-Self-Supervised-Learning-for-Image-Semantic-Segmentation-cs-CV-cs-LGPDF" class="headerlink" title="[92] Selective Masking based Self-Supervised Learning for Image Semantic Segmentation cs.CV | cs.LGPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06981">Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06981" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuemin Wang, Ian Stavness</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于选择性掩码的自监督学习方法，用于图像语义分割，通过选择性掩码图像重建作为预训练任务，显著提升了下游分割任务的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的随机掩码方法在掩码图像建模预训练中表现不佳，无法充分利用模型的已有知识。为了改进这一问题，作者提出了选择性掩码方法，旨在通过迭代步骤选择性地掩码重建损失最高的图像块，从而提升预训练效果。</p>
<p><strong>Result:</strong> 在Pascal VOC和Cityscapes上选择性掩码方法比传统方法提升了2.9%的准确性，在杂草数据集上提升了2.5%。尤其对低性能类别提升显著。</p>
<p><strong>Insight:</strong> 选择性掩码方法能够动态利用模型知识优化预训练，尤其适用于计算资源受限的场景，为端到端语义分割提供了高效解决方案。</p>
<p><strong>Abstract:</strong> This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model’s knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.</p>
  </div>
</details>

<hr>
<h3 id="93-Power-of-Boundary-and-Reflection-Semantic-Transparent-Object-Segmentation-using-Pyramid-Vision-Transformer-with-Transparent-Cues-cs-CV-cs-AIPDF"><a href="#93-Power-of-Boundary-and-Reflection-Semantic-Transparent-Object-Segmentation-using-Pyramid-Vision-Transformer-with-Transparent-Cues-cs-CV-cs-AIPDF" class="headerlink" title="[93] Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues cs.CV | cs.AIPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07034">Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07034" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tuan-Anh Vu, Hai Nguyen-Truong, Ziqiang Zheng, Binh-Son Hua, Qing Guo</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为TransCues的新方法，通过结合边界特征增强与反射特征增强模块，利用金字塔视觉变换器（Pyramid Vision Transformer）实现对透明物体的语义分割。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 透明物体（如玻璃）的分割是计算机视觉中的难点，因其透明性和反射特性导致传统方法表现不佳。人类视觉依赖边界和反射特征识别透明物体，而现有方法未能充分捕捉这两种特性。</p>
<p><strong>Result:</strong> 在多个基准数据集（如Trans10K-v2、MSD、RGBD-Mirror等）上显著超越现有方法，最大提升达13.1% mIoU。</p>
<p><strong>Insight:</strong> 透明物体的分割需同时利用边界和反射特征；变换器架构在多任务协同优化中表现出强大潜力。</p>
<p><strong>Abstract:</strong> Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.</p>
  </div>
</details>

<hr>
<h3 id="94-DAUNet-A-Lightweight-UNet-Variant-with-Deformable-Convolutions-and-Parameter-Free-Attention-for-Medical-Image-Segmentation-cs-CV-cs-AI-cs-LGPDF"><a href="#94-DAUNet-A-Lightweight-UNet-Variant-with-Deformable-Convolutions-and-Parameter-Free-Attention-for-Medical-Image-Segmentation-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[94] DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation cs.CV | cs.AI | cs.LGPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07051">DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07051" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Adnan Munir, Shujaat Khan</span></p>
<p><strong>TL;DR:</strong> DAUNet是一种轻量级的UNet变体，结合了可变形卷积和无参数注意力机制（SimAM），用于医学图像分割，提升空间适应性和上下文感知能力，同时保持低复杂度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学图像分割在自动化诊断和治疗计划中至关重要，但现有模型难以同时满足高精度和低复杂度的需求。</p>
<p><strong>Result:</strong> 在FH-PS-AoP和FUMPE数据集上，DAUNet在Dice分数、HD95和ASD指标上优于SOTA模型，且参数效率更高。</p>
<p><strong>Insight:</strong> 1. 可变形卷积和SimAM的联合使用显著提升了模型对几何变化和低对比区域的鲁棒性；2. 轻量级设计适合实时和资源受限的临床环境。</p>
<p><strong>Abstract:</strong> Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet’s bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet’s robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.</p>
  </div>
</details>

<hr>
<h3 id="95-mathrm-D-mathrm-3-Predictor-Noise-Free-Deterministic-Diffusion-for-Dense-Prediction-cs-CV-cs-AIPDF"><a href="#95-mathrm-D-mathrm-3-Predictor-Noise-Free-Deterministic-Diffusion-for-Dense-Prediction-cs-CV-cs-AIPDF" class="headerlink" title="[95] $\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction cs.CV | cs.AIPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07062">$\mathrm{D}^{\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07062" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changliang Xia, Chengyou Jia, Minnan Luo, Zhuohang Dang, Xin Shen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种确定性扩散框架$	exttt{D}^{	exttt{3}}$-Predictor，解决了传统扩散模型中噪声破坏空间几何结构的问题，通过整合时间步依赖的视觉专家先验，实现高效的单步推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统扩散模型的核心随机噪声破坏了密集预测任务所需的确定性映射，导致几何结构信息丢失。作者希望通过消除噪声，保留扩散模型的视觉先验优势。</p>
<p><strong>Result:</strong> 在多种密集预测任务中达到竞争性或SOTA性能，训练数据需求减半，支持单步高效推理。</p>
<p><strong>Insight:</strong> 扩散模型的视觉先验可通过确定性重构保留，无需依赖噪声输入，显著提升密集预测任务的几何结构建模能力。</p>
<p><strong>Abstract:</strong> Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^{\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^{\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^{\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at <a target="_blank" rel="noopener" href="https://x-gengroup.github.io/HomePage_D3-Predictor/">https://x-gengroup.github.io/HomePage_D3-Predictor/</a>.</p>
  </div>
</details>

<hr>
<h3 id="96-Context-measure-Contextualizing-Metric-for-Camouflage-cs-CVPDF"><a href="#96-Context-measure-Contextualizing-Metric-for-Camouflage-cs-CVPDF" class="headerlink" title="[96] Context-measure: Contextualizing Metric for Camouflage cs.CVPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07076">Context-measure: Contextualizing Metric for Camouflage</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07076" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chen-Yang Wang, Gepeng Ji, Song Shao, Ming-Ming Cheng, Deng-Ping Fan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新的上下文感知评价指标Context-measure，用于评估伪装场景下的目标分割性能。与现有忽略空间依赖性的指标不同，该方法通过概率像素感知框架量化伪装效果，更符合人类感知。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的评价指标在伪装场景中表现不佳，因为它们最初是为通用或显著目标设计的，忽略了空间上下文的依赖性。</p>
<p><strong>Result:</strong> 在三个挑战性的伪装目标分割数据集上的实验表明，Context-measure比现有指标更可靠。</p>
<p><strong>Insight:</strong> 上下文感知的评价指标对伪装场景的性能评估至关重要，尤其在农业、工业和医疗等应用中。</p>
<p><strong>Abstract:</strong> Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at <a target="_blank" rel="noopener" href="https://github.com/pursuitxi/Context-measure">https://github.com/pursuitxi/Context-measure</a>.</p>
  </div>
</details>

<hr>
<h3 id="97-COREA-Coarse-to-Fine-3D-Representation-Alignment-Between-Relightable-3D-Gaussians-and-SDF-via-Bidirectional-3D-to-3D-Supervision-cs-CVPDF"><a href="#97-COREA-Coarse-to-Fine-3D-Representation-Alignment-Between-Relightable-3D-Gaussians-and-SDF-via-Bidirectional-3D-to-3D-Supervision-cs-CVPDF" class="headerlink" title="[97] COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision cs.CVPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07107">COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07107" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jaeyoon Lee, Hojoon Jung, Sungtae Hwang, Jihyong Oh, Jongwon Choi</span></p>
<p><strong>TL;DR:</strong> COREA提出了一种统一框架，联合学习可重新照明的3D高斯和SDF，通过双向3D到3D监督实现几何重建和光照分解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有3D高斯喷洒方法主要基于2D渲染学习几何，导致表面粗糙且光照分解不可靠，需要直接在3D空间中学习几何信号。</p>
<p><strong>Result:</strong> 在标准基准测试中，COREA在视角合成、网格重建和PBR方面表现优异。</p>
<p><strong>Insight:</strong> 直接在3D空间中学习几何信号能显著提升重建精度和光照分解稳定性。</p>
<p><strong>Abstract:</strong> We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.</p>
  </div>
</details>

<hr>
<h3 id="98-Training-free-Clothing-Region-of-Interest-Self-correction-for-Virtual-Try-On-cs-CVPDF"><a href="#98-Training-free-Clothing-Region-of-Interest-Self-correction-for-Virtual-Try-On-cs-CVPDF" class="headerlink" title="[98] Training-free Clothing Region of Interest Self-correction for Virtual Try-On cs.CVPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07126">Training-free Clothing Region of Interest Self-correction for Virtual Try-On</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07126" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shengjie Lu, Zhibin Wan, Jiejie Liu, Quan Zhang, Mingjie Sun</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种无需训练的虚拟试衣自校正方法，通过能量函数约束注意力图以生成更符合目标服装细节的结果，并设计了新评价指标VTID以全面评估性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有虚拟试衣方法生成的服装与目标存在图案、纹理和边界的差异，且评价指标仅关注图像真实感而忽略了与目标的对齐。</p>
<p><strong>Result:</strong> 在多项指标（LPIPS、FID、KID、VTID）上提升显著，并在下游任务（CC-Reid）中表现更优。</p>
<p><strong>Insight:</strong> 注意力校正和新评价指标的结合能有效提升虚拟试衣的生成质量和评估全面性。</p>
<p><strong>Abstract:</strong> VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at <a target="_blank" rel="noopener" href="https://github.com/MrWhiteSmall/CSC-VTON.git">https://github.com/MrWhiteSmall/CSC-VTON.git</a>.</p>
  </div>
</details>

<hr>
<h3 id="99-MulCLIP-A-Multi-level-Alignment-Framework-for-Enhancing-Fine-grained-Long-context-CLIP-cs-CVPDF"><a href="#99-MulCLIP-A-Multi-level-Alignment-Framework-for-Enhancing-Fine-grained-Long-context-CLIP-cs-CVPDF" class="headerlink" title="[99] MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP cs.CVPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07128">MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07128" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chau Truong, Hieu Ta Quang, Dung D. Le</span></p>
<p><strong>TL;DR:</strong> MulCLIP提出了一种多级对齐框架，增强CLIP模型在处理长文本和细粒度理解上的能力，通过全局对比对齐和两种新颖的细粒度对齐策略实现显著性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有CLIP模型在训练时使用简短标题，难以处理长文本描述的细粒度对齐，而基于区域提案的方法部署成本高。MulCLIP旨在通过多级对齐解决这一问题。</p>
<p><strong>Result:</strong> 在多个基准测试中，MulCLIP显著提升了下游性能，验证了其多尺度对齐的有效性，优于基于区域提案的方法。</p>
<p><strong>Insight:</strong> 多级对齐（全局+细粒度）是提升长文本细粒度理解的关键，避免了区域提案的高成本，适用于实际场景。</p>
<p><strong>Abstract:</strong> Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.</p>
  </div>
</details>

<hr>
<h3 id="100-TrajMoE-Scene-Adaptive-Trajectory-Planning-with-Mixture-of-Experts-and-Reinforcement-Learning-cs-CV-cs-AIPDF"><a href="#100-TrajMoE-Scene-Adaptive-Trajectory-Planning-with-Mixture-of-Experts-and-Reinforcement-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[100] TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning cs.CV | cs.AIPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07135">TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07135" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zebin Xing, Pengxuan Yang, Linbo Wang, Yichen Zhang, Yiming Hu</span></p>
<p><strong>TL;DR:</strong> TrajMoE提出了Mixture of Experts（MoE）和强化学习结合的轨迹规划方法，以适应不同场景并优化轨迹评分机制，最终在navsim ICCV基准测试中取得第三名。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有端到端自动驾驶系统在轨迹规划中常忽略场景多样性问题和轨迹评分机制的优化需求，限制了规划性能。</p>
<p><strong>Result:</strong> 在navsim ICCV基准测试中得分为51.08，排名第三。</p>
<p><strong>Insight:</strong> 场景自适应的轨迹规划和策略驱动的评分优化是提升自动驾驶规划性能的关键方向。</p>
<p><strong>Abstract:</strong> Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.</p>
  </div>
</details>

<hr>
<h3 id="101-A-Large-Scale-Multimodal-Dataset-and-Benchmarks-for-Human-Activity-Scene-Understanding-and-Reasoning-cs-CV-cs-AIPDF"><a href="#101-A-Large-Scale-Multimodal-Dataset-and-Benchmarks-for-Human-Activity-Scene-Understanding-and-Reasoning-cs-CV-cs-AIPDF" class="headerlink" title="[101] A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning cs.CV | cs.AIPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07136">A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07136" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siyang Jiang, Mu Yuan, Xiang Ji, Bufang Yang, Zeyu Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个大规模多模态数据集CUHK-X，用于支持人类活动场景理解与推理任务（HAU和HARn），并通过提示词场景生成方法提升了数据标注的逻辑一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的人类活动识别（HAR）数据集主要提供粗粒度的数据标签，难以支持需要细粒度动态特征的场景理解和推理任务。此外，大型语言模型（尤其是视觉语言模型）在处理非RGB模态（如深度、IMU和毫米波）时缺乏大规模的数据-文本资源。</p>
<p><strong>Result:</strong> 在CUHK-X上的实验结果展示了HAR、HAU和HARn任务的平均准确率分别为76.52%、40.76%和70.25%，验证了数据集的实用性和方法的有效性。</p>
<p><strong>Insight:</strong> 1. 多模态数据（如深度和毫米波）对提升活动理解和推理能力具有潜力。2. 提示词场景生成方法可以有效解决传统标注中逻辑一致性的问题，为其他领域的数据集构建提供了新思路。3. 细粒度的文本描述（而非单纯标签）是支持高级任务（如HAU和HARn）的关键。</p>
<p><strong>Abstract:</strong> Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: <a target="_blank" rel="noopener" href="https://openaiotlab.github.io/CUHK-X/">https://openaiotlab.github.io/CUHK-X/</a> and <a target="_blank" rel="noopener" href="https://github.com/openaiotlab/CUHK-X">https://github.com/openaiotlab/CUHK-X</a>.</p>
  </div>
</details>

<hr>
<h3 id="102-CHIMERA-Adaptive-Cache-Injection-and-Semantic-Anchor-Prompting-for-Zero-shot-Image-Morphing-with-Morphing-oriented-Metrics-cs-CVPDF"><a href="#102-CHIMERA-Adaptive-Cache-Injection-and-Semantic-Anchor-Prompting-for-Zero-shot-Image-Morphing-with-Morphing-oriented-Metrics-cs-CVPDF" class="headerlink" title="[102] CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics cs.CVPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07155">CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07155" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dahyeon Kye, Jeahun Sung, MinKyu Jeon, Jihyong Oh</span></p>
<p><strong>TL;DR:</strong> CHIMERA是一种基于扩散模型的零样本图像变形框架，通过自适应缓存注入和语义锚点提示实现平滑且语义一致的图像变形，并提出了面向变形的全局-局部一致性评分标准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在图像变形中常因缺乏自适应结构和语义对齐而导致突兀的过渡或过饱和的外观，CHIMERA旨在解决这一问题。</p>
<p><strong>Result:</strong> 实验和用户研究表明，CHIMERA实现了更平滑且语义对齐的变形效果，达到新的性能水平。</p>
<p><strong>Insight:</strong> 自适应特征注入和语义锚点的设计在扩散模型中的应用显著提升了图像变形的质量和一致性。</p>
<p><strong>Abstract:</strong> Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.</p>
  </div>
</details>

<hr>
<h3 id="103-MuSASplat-Efficient-Sparse-View-3D-Gaussian-Splats-via-Lightweight-Multi-Scale-Adaptation-cs-CVPDF"><a href="#103-MuSASplat-Efficient-Sparse-View-3D-Gaussian-Splats-via-Lightweight-Multi-Scale-Adaptation-cs-CVPDF" class="headerlink" title="[103] MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation cs.CVPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07165">MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07165" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muyu Xu, Fangneng Zhan, Xiaoqin Zhang, Ling Shao, Shijian Lu</span></p>
<p><strong>TL;DR:</strong> MuSASplat提出了一种轻量级多尺度适配器，显著降低了稀疏视图3D高斯点云训练的计算成本，同时保持高质量的渲染效果，并通过特征融合聚合器高效整合多视图特征。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 稀疏视图3D高斯点云渲染通常需要微调大型Vision Transformer（ViT）主干网络，导致高昂的GPU计算成本。MuSASplat旨在通过轻量化设计降低这些成本，同时保持渲染质量。</p>
<p><strong>Result:</strong> MuSASplat在渲染质量上达到SOTA，同时大幅减少了训练参数和资源需求。</p>
<p><strong>Insight:</strong> 通过轻量化适配器和高效特征融合，可以显著降低稀疏视图3D渲染的计算成本，为实际应用提供了更高效的解决方案。</p>
<p><strong>Abstract:</strong> Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.</p>
  </div>
</details>

<hr>
<h3 id="104-When-Privacy-Meets-Recovery-The-Overlooked-Half-of-Surrogate-Driven-Privacy-Preservation-for-MLLM-Editing-cs-CVPDF"><a href="#104-When-Privacy-Meets-Recovery-The-Overlooked-Half-of-Surrogate-Driven-Privacy-Preservation-for-MLLM-Editing-cs-CVPDF" class="headerlink" title="[104] When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing cs.CVPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07166">When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07166" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siyuan Xu, Yibing Liu, Peilin Chen, Yung-Hui Li, Shiqi Wang</span></p>
<p><strong>TL;DR:</strong> 该论文聚焦于多模态大语言模型（MLLM）隐私保护中恢复被编辑隐私数据的问题，提出了SPPE数据集和一个统一方法，实现了隐私恢复与模型编辑保真度的平衡。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究在保护MLLM中隐私信息时，常忽略隐私的真实性和恢复质量评估，导致隐私保护与模型实用性失衡。本文填补了这一研究空白。</p>
<p><strong>Result:</strong> 实验表明，该方法在SPPE和InstructPix2Pix数据集上泛化能力强，在隐私保护和MLLM实用性之间取得了良好平衡。</p>
<p><strong>Insight:</strong> 隐私恢复是多模态大语言模型隐私保护中不可忽视的环节，数据集的多样性和多模态信号的结合是关键。</p>
<p><strong>Abstract:</strong> Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.</p>
  </div>
</details>

<hr>
<h3 id="105-Towards-Unified-Semantic-and-Controllable-Image-Fusion-A-Diffusion-Transformer-Approach-cs-CV-cs-AIPDF"><a href="#105-Towards-Unified-Semantic-and-Controllable-Image-Fusion-A-Diffusion-Transformer-Approach-cs-CV-cs-AIPDF" class="headerlink" title="[105] Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach cs.CV | cs.AIPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07170">Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07170" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiayang Li, Chengjie Jiang, Junjun Jiang, Pengwei Liang, Jiayi Ma</span></p>
<p><strong>TL;DR:</strong> Diffusion Transformer (DiT)框架DiTFuse通过联合编码图像和自然语言指令，实现语义感知的多模态图像融合，具有层级控制和零样本泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有图像融合方法缺乏适应性、鲁棒性和用户控制性，且依赖特定任务的数据集。DiTFuse旨在统一语义理解和可控融合，解决复杂场景下的融合挑战。</p>
<p><strong>Result:</strong> 在红外-可见光、多焦点、多曝光融合任务中表现优异，支持零样本泛化和用户控制。</p>
<p><strong>Insight:</strong> Diffusion Transformer和指令驱动的联合编码方式为多模态融合提供了新思路，尤其在语义理解和可控性方面具有潜力。</p>
<p><strong>Abstract:</strong> Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.</p>
  </div>
</details>

<hr>
<h3 id="106-START-Spatial-and-Textual-Learning-for-Chart-Understanding-cs-CV-cs-AIPDF"><a href="#106-START-Spatial-and-Textual-Learning-for-Chart-Understanding-cs-CV-cs-AIPDF" class="headerlink" title="[106] START: Spatial and Textual Learning for Chart Understanding cs.CV | cs.AIPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07186">START: Spatial and Textual Learning for Chart Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07186" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhuoming Liu, Xiaofeng Gao, Feiyang Niu, Qiaozi Gao, Liu Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出了START方法，通过结合空间和文本学习来提升多模态大语言模型（MLLM）对图表的理解能力，包括图表元素定位和图表到代码的生成。利用创新的数据生成流程和基准测试（CS-Bench），START在多个模型尺寸和基准上表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 图表在科学论文和技术报告中广泛应用，但现有方法难以同时理解图表的视觉布局和底层数据表示，因此需要一种能够结合空间和文本学习的方法来提升模型能力。</p>
<p><strong>Result:</strong> START在多个基准测试上超越基线模型和现有最佳方法，表现显著提升。</p>
<p><strong>Insight:</strong> 结合空间和文本学习是提升图表理解的关键，而数据生成流程和基准测试能够有效支持模型的训练和评估。</p>
<p><strong>Abstract:</strong> Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) – grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM’s understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart’s visual structure, addressing challenges that existing methods cannot handle. To evaluate a model’s ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.</p>
  </div>
</details>

<hr>
<h3 id="107-Integrating-Multi-scale-and-Multi-filtration-Topological-Features-for-Medical-Image-Classification-cs-CVPDF"><a href="#107-Integrating-Multi-scale-and-Multi-filtration-Topological-Features-for-Medical-Image-Classification-cs-CVPDF" class="headerlink" title="[107] Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification cs.CVPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07190">Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07190" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pengfei Gu, Huimin Li, Haoteng Tang, Dongkuan, Xu</span></p>
<p><strong>TL;DR:</strong> 提出了一种结合多尺度与多过滤拓扑特征的医学图像分类框架，通过提取和整合拓扑特征增强模型对复杂解剖结构的识别能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有深度学习模型在医学图像分类中往往忽略拓扑不变性表征的基本解剖结构，或仅通过单参数持续性捕捉简单拓扑特征，限制了其性能。</p>
<p><strong>Result:</strong> 在三个公开数据集上验证，性能显著超越基线和前沿方法，证明了其鲁棒性和可解释性。</p>
<p><strong>Insight:</strong> 全面的拓扑视角可以显著提升医学图像分类性能，并增强模型的解释能力。</p>
<p><strong>Abstract:</strong> Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions&#x2F;scales. We then develop a &#96;&#96;vineyard’’ algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model’s capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.</p>
  </div>
</details>

<hr>
<h3 id="108-HVQ-CGIC-Enabling-Hyperprior-Entropy-Modeling-for-VQ-Based-Controllable-Generative-Image-Compression-cs-CVPDF"><a href="#108-HVQ-CGIC-Enabling-Hyperprior-Entropy-Modeling-for-VQ-Based-Controllable-Generative-Image-Compression-cs-CVPDF" class="headerlink" title="[108] HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression cs.CVPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07192">HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07192" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Niu Yi, Xu Tianyi, Ma Mingming, Wang Xinkun</span></p>
<p><strong>TL;DR:</strong> HVQ-CGIC提出了一种基于VQ超先验的框架，通过动态建模VQ索引的熵，显著提升了生成图像压缩的率失真性能和可控性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的基于VQ的生成图像压缩方法使用静态全局概率分布估计VQ索引的熵，无法适应不同图像内容，导致比特率利用不足且难以实现灵活的码率控制。</p>
<p><strong>Result:</strong> 在Kodak数据集上，HVQ-CGIC以61.3%更少的比特率实现了与Control-GIC、CDC和HiFiC相当的LPIPS指标。</p>
<p><strong>Insight:</strong> HVQ-CGIC展示了超先验在VQ生成压缩中的潜力，有望成为VQGAN图像压缩的基础组件，类似于超先验框架在神经图像压缩中的核心作用。</p>
<p><strong>Abstract:</strong> Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.</p>
  </div>
</details>

<hr>
<h3 id="109-Understanding-Diffusion-Models-via-Code-Execution-cs-CV-cs-LGPDF"><a href="#109-Understanding-Diffusion-Models-via-Code-Execution-cs-CV-cs-LGPDF" class="headerlink" title="[109] Understanding Diffusion Models via Code Execution cs.CV | cs.LGPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07201">Understanding Diffusion Models via Code Execution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07201" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cheng Yu</span></p>
<p><strong>TL;DR:</strong> 该论文通过简洁的代码实现（约300行）解释扩散模型，从代码执行的角度展示其工作原理，帮助研究人员理解理论与实践的对应关系。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散模型在生成建模中表现优异，但其理论复杂，且论文中的数学公式与实际开源实现之间存在较大差距，现有教程多聚焦于方程推导，缺乏对代码如何运行的指导。</p>
<p><strong>Result:</strong> 提供了一个易于理解的扩散模型实现，帮助研究者更好地理解扩散模型的实际运行机制，代码和预训练模型已开源。</p>
<p><strong>Insight:</strong> 理论与实践的对应关系是理解扩散模型的关键，通过代码实现可以直观地展示模型的核心思想和工作流程。</p>
<p><strong>Abstract:</strong> Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components – including forward diffusion, reverse sampling, the noise-prediction network, and the training loop – while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: <a target="_blank" rel="noopener" href="https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree">https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree</a>.</p>
  </div>
</details>

<hr>
<h3 id="110-MMRPT-MultiModal-Reinforcement-Pre-Training-via-Masked-Vision-Dependent-Reasoning-cs-CVPDF"><a href="#110-MMRPT-MultiModal-Reinforcement-Pre-Training-via-Masked-Vision-Dependent-Reasoning-cs-CVPDF" class="headerlink" title="[110] MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning cs.CVPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07203">MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07203" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuhui Zheng, Kang An, Ziliang Wang, Yuhang Wang, Faqiang Qian</span></p>
<p><strong>TL;DR:</strong> MMRPT提出了一种基于强化学习的多模态预训练框架，通过掩码视觉依赖推理提升模型的视觉理解能力，避免了对语言描述的依赖。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态预训练方法过于依赖图像-文本对的描述性偏差，导致模型更关注表面语言线索而非实际的视觉理解。</p>
<p><strong>Result:</strong> 实验显示了在零样本任务中的一致性能提升，以及有监督微调下的显著鲁棒性改进。</p>
<p><strong>Insight:</strong> 强化驱动的掩码推理为多模态模型提供了更可靠和泛化的预训练目标。</p>
<p><strong>Abstract:</strong> Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.</p>
  </div>
</details>

<hr>
<h3 id="111-Object-Pose-Distribution-Estimation-for-Determining-Revolution-and-Reflection-Uncertainty-in-Point-Clouds-cs-CVPDF"><a href="#111-Object-Pose-Distribution-Estimation-for-Determining-Revolution-and-Reflection-Uncertainty-in-Point-Clouds-cs-CVPDF" class="headerlink" title="[111] Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds cs.CVPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07211">Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07211" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Frederik Hagelskjær, Dimitrios Arapis, Steffen Madsen, Thorbjørn Mosekjær Iversen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于神经网络的新方法，仅使用3D无色数据估计物体姿态不确定性，解决了传统方法在工业场景中依赖颜色信息的限制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有姿态分布估计方法严重依赖颜色信息，而在工业场景中颜色数据往往不可用，因此需要一种仅依赖3D数据的姿态不确定性估计方法。</p>
<p><strong>Result:</strong> 在真实世界的箱体拣选场景中验证了方法的有效性，支持对几何模糊物体的姿态不确定性估计。</p>
<p><strong>Insight:</strong> 该方法为解决工业环境中无色数据下的姿态估计问题提供了新思路，并可通过扩展框架支持完整的SE(3)姿态分布估计。</p>
<p><strong>Abstract:</strong> Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.   We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io</p>
  </div>
</details>

<hr>
<h3 id="112-VFM-VLM-Vision-Foundation-Model-and-Vision-Language-Model-based-Visual-Comparison-for-3D-Pose-Estimation-cs-CV-cs-AIPDF"><a href="#112-VFM-VLM-Vision-Foundation-Model-and-Vision-Language-Model-based-Visual-Comparison-for-3D-Pose-Estimation-cs-CV-cs-AIPDF" class="headerlink" title="[112] VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation cs.CV | cs.AIPDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07215">VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07215" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Selim Sarowar, Sungho Kim</span></p>
<p><strong>TL;DR:</strong> 论文比较了基于CLIP和DINOv2的视觉基础模型（VFM）和视觉语言模型（VLM）在6D物体姿态估计任务中的表现，揭示了它们在语义和几何特征上的互补优势。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机在于利用VFM和VLM提供的丰富语义和几何表示，提升3D姿态估计的性能，尤其是在机器人抓取和操作等实际应用中。</p>
<p><strong>Result:</strong> 实验结果表明，CLIP在语义一致性上表现更好，而DINOv2在几何精度上具有竞争力。两者在功能上互补。</p>
<p><strong>Insight:</strong> 论文揭示了在实际应用中应根据任务需求（语义或几何特征）选择合适的视觉模型，尤其是在机器人抓取等场景中。</p>
<p><strong>Abstract:</strong> Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.</p>
  </div>
</details>

<hr>
<h3 id="113-Towards-Robust-Protective-Perturbation-against-DeepFake-Face-Swapping-cs-CV-cs-AI-cs-CR-cs-LGPDF"><a href="#113-Towards-Robust-Protective-Perturbation-against-DeepFake-Face-Swapping-cs-CV-cs-AI-cs-CR-cs-LGPDF" class="headerlink" title="[113] Towards Robust Protective Perturbation against DeepFake Face Swapping cs.CV | cs.AI | cs.CR | cs.LGPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07228">Towards Robust Protective Perturbation against DeepFake Face Swapping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CR | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07228" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hengyang Yao, Lin Li, Ke Sun, Jianing Qiu, Huiping Chen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为EOLT的新框架，通过强化学习自动学习关键变换的分布，以提高对抗DeepFake人脸替换的保护扰动的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的防御方法通过嵌入不可见的扰动来对抗DeepFake人脸替换，但这些扰动容易被压缩或调整大小等基本变换破坏。论文首先分析了30种变换的鲁棒性敏感性，发现传统均匀采样的EOT方法效果不佳，从而提出了改进的需求。</p>
<p><strong>Result:</strong> 实验结果表明，EOLT在平均鲁棒性上比现有方法提高了26%，在具有挑战性的变换类别上提升了30%。</p>
<p><strong>Insight:</strong> 论文强调变换选择的重要性，表明通过学习变换分布而非固定设计可以显著提高对抗扰动的鲁棒性，为防御DeepFake提供了新思路。</p>
<p><strong>Abstract:</strong> DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.</p>
  </div>
</details>

<hr>
<h3 id="114-STRinGS-Selective-Text-Refinement-in-Gaussian-Splatting-cs-CVPDF"><a href="#114-STRinGS-Selective-Text-Refinement-in-Gaussian-Splatting-cs-CVPDF" class="headerlink" title="[114] STRinGS: Selective Text Refinement in Gaussian Splatting cs.CVPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07230">STRinGS: Selective Text Refinement in Gaussian Splatting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07230" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abhinav Raundhal, Gaurav Behera, P J Narayanan, Ravi Kiran Sarvadevabhatla, Makarand Tapaswi</span></p>
<p><strong>TL;DR:</strong> STRinGS是一个用于3D高斯泼溅（3DGS）中选择性优化文本区域的框架，通过分开处理文本和非文本区域，显著提升了文本的可读性和重建质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实场景中的文本（如标志或标签）提供了重要的上下文信息，但现有3D表示方法（如3DGS）在保留文本细节时表现不佳，易导致语义信息丢失。</p>
<p><strong>Result:</strong> 在7K次迭代后，STRinGS相比3DGS将文本区域的CER相对降低了63.6%。</p>
<p><strong>Insight:</strong> 通过区域划分和选择性优化，3D重建可以有效保留细节丰富的文本信息，推动文本感知的3D场景理解。</p>
<p><strong>Abstract:</strong> Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.</p>
  </div>
</details>

<hr>
<h3 id="115-Dropout-Prompt-Learning-Towards-Robust-and-Adaptive-Vision-Language-Models-cs-CV-cs-AIPDF"><a href="#115-Dropout-Prompt-Learning-Towards-Robust-and-Adaptive-Vision-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[115] Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models cs.CV | cs.AIPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07234">Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07234" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Biao Chen, Lin Zuo, Mengmeng Jing, Kunbin He, Yuchen Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为’Dropout Prompt Learning’的方法，通过在多模态模型的文本和视觉分支上应用动态dropout，结合残差熵正则化，提升了模型在低样本学习、长尾分类和分布外泛化等挑战性任务中的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统dropout在提升模型泛化能力方面表现良好，但缺乏对多模态任务中模态间对齐和上下文信息的动态适应能力，因此需要一种更灵活的dropout机制。</p>
<p><strong>Result:</strong> 在base-to-novel泛化任务中，性能超过KgCoOp 5.10%和PromptSRC 2.13%，在低样本学习和长尾分类等任务中也表现优异。</p>
<p><strong>Insight:</strong> 动态dropout和残差熵正则化的结合能够有效提升多模态模型的适应性，尤其是在复杂场景下表现出色。</p>
<p><strong>Abstract:</strong> Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method’s effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.</p>
  </div>
</details>

<hr>
<h3 id="116-Unified-Camera-Positional-Encoding-for-Controlled-Video-Generation-cs-CVPDF"><a href="#116-Unified-Camera-Positional-Encoding-for-Controlled-Video-Generation-cs-CVPDF" class="headerlink" title="[116] Unified Camera Positional Encoding for Controlled Video Generation cs.CVPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07237">Unified Camera Positional Encoding for Controlled Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07237" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cheng Zhang, Boying Li, Meng Wei, Yan-Pei Cao, Camilo Cruz Gambardella</span></p>
<p><strong>TL;DR:</strong> 该论文提出了统一相机位置编码（UCPE），通过相对光线编码和绝对朝向编码实现了对相机位置的全面控制，提升了视频生成的视觉保真度和可控性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有相机编码方法通常基于简化的小孔假设，难以泛化到真实世界相机复杂的内部参数和镜头畸变。为了解决这一问题，本文提出了统一的相机表示方法。</p>
<p><strong>Result:</strong> 实验表明，UCPE在相机可控视频生成任务中实现了最优的视觉保真度和可控性，同时展示了其作为Transformer通用相机表示的潜力。</p>
<p><strong>Insight:</strong> 统一的相机编码方法可以显著提升Transformer在多视角、视频和3D任务中的性能，尤其是在处理复杂相机参数时表现出强大的泛化能力。</p>
<p><strong>Abstract:</strong> Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/chengzhag/UCPE">https://github.com/chengzhag/UCPE</a>.</p>
  </div>
</details>

<hr>
<h3 id="117-Zero-Shot-Textual-Explanations-via-Translating-Decision-Critical-Features-cs-CVPDF"><a href="#117-Zero-Shot-Textual-Explanations-via-Translating-Decision-Critical-Features-cs-CVPDF" class="headerlink" title="[117] Zero-Shot Textual Explanations via Translating Decision-Critical Features cs.CVPDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07245">Zero-Shot Textual Explanations via Translating Decision-Critical Features</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07245" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Toshinori Yamauchi, Hiroshi Kera, Kazuhiko Kawamoto</span></p>
<p><strong>TL;DR:</strong> TEXTER 是一种零样本文本解释方法，通过分离决策关键特征并映射到 CLIP 特征空间，生成更忠实和可解释的图像分类器预测原因描述。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的零样本解释方法通常生成描述可见内容的文本，而非驱动模型预测的关键特征，无法体现分类器的决策依据。</p>
<p><strong>Result:</strong> 实验证明 TEXTER 生成的文本解释比现有方法更忠实和可解释。</p>
<p><strong>Insight:</strong> 决策关键特征的分离和语言对齐是生成高质量解释的关键；稀疏自编码器可增强复杂架构的可解释性。</p>
<p><strong>Abstract:</strong> Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons – i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model’s reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.</p>
  </div>
</details>

<hr>
<h3 id="118-See-More-Change-Less-Anatomy-Aware-Diffusion-for-Contrast-Enhancement-cs-CVPDF"><a href="#118-See-More-Change-Less-Anatomy-Aware-Diffusion-for-Contrast-Enhancement-cs-CVPDF" class="headerlink" title="[118] See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement cs.CVPDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07251">See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07251" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junqi Liu, Zejun Wu, Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li</span></p>
<p><strong>TL;DR:</strong> SMILE是一种基于解剖学感知的扩散模型，用于图像增强，特别关注医学成像中的临床准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有图像增强方法在医学影像中常因过度编辑导致解剖结构失真或遗漏重要病变，SMILE旨在解决这一问题。</p>
<p><strong>Result:</strong> 在六个数据集上，SMILE在图像质量和临床实用性上显著优于现有方法，同时提升了癌症检测的准确性。</p>
<p><strong>Insight:</strong> 解剖学知识的引入是医学图像增强的关键，而扩散模型能够有效平衡增强效果与解剖保真度。</p>
<p><strong>Abstract:</strong> Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.</p>
  </div>
</details>

<hr>
<h3 id="119-DGGAN-Degradation-Guided-Generative-Adversarial-Network-for-Real-time-Endoscopic-Video-Enhancement-cs-CV-cs-AIPDF"><a href="#119-DGGAN-Degradation-Guided-Generative-Adversarial-Network-for-Real-time-Endoscopic-Video-Enhancement-cs-CV-cs-AIPDF" class="headerlink" title="[119] DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement cs.CV | cs.AIPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07253">DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07253" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Handing Xu, Zhenguo Nie, Tairan Peng, Huimin Pan, Xin-Jun Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于退化引导的生成对抗网络（DGGAN），用于实时内窥镜视频增强，通过提取和传播退化表示实现高效、高质量的图像增强。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 内窥镜手术依赖术中视频，图像质量直接影响手术安全和效果。现有深度学习方法计算量大，难以满足实时需求。因此，作者提出了一种轻量级且高效的解决方案。</p>
<p><strong>Result:</strong> 实验表明，该方法在性能和效率上优于其他先进方法，适用于实时内窥镜视频增强。</p>
<p><strong>Insight:</strong> 通过隐式学习和传播退化表示，可以高效提升内窥镜视频质量，为临床应用提供了可行方案。</p>
<p><strong>Abstract:</strong> Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.</p>
  </div>
</details>

<hr>
<h3 id="120-RVLF-A-Reinforcing-Vision-Language-Framework-for-Gloss-Free-Sign-Language-Translation-cs-CVPDF"><a href="#120-RVLF-A-Reinforcing-Vision-Language-Framework-for-Gloss-Free-Sign-Language-Translation-cs-CVPDF" class="headerlink" title="[120] RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation cs.CVPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07273">RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07273" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhi Rao, Yucheng Zhou, Benjia Zhou, Yiqing Huang, Sergio Escalera</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种强化视觉语言框架（RVLF），通过结合大型视觉语言模型（LVLM）和强化学习（RL），解决了无注释手语翻译中的视觉表示不足和语义对齐问题，显著提升了翻译质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前无注释手语翻译（SLT）面临两个主要挑战：一是手语视觉表示不足，无法捕捉细微的动作线索；二是现有LLM方法在句子级语义对齐方面的不足。这些问题限制了翻译质量的提升。</p>
<p><strong>Result:</strong> 在多个数据集上（CSL-Daily、PHOENIX-2014T等），RVLF显著提升了BLEU-4得分（最高提升+5.1）。实验验证了GRPO优化在翻译质量和语义一致性上的有效性。</p>
<p><strong>Insight:</strong> 1）结合骨架和视觉特征可以有效提升手语表示；2）GRPO优化策略在多任务奖励设计上有潜力；3）无需依赖外部大规模手语数据集，也能实现高性能。</p>
<p><strong>Abstract:</strong> Gloss-free sign language translation (SLT) is hindered by two key challenges: <strong>inadequate sign representation</strong> that fails to capture nuanced visual cues, and <strong>sentence-level semantic misalignment</strong> in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage <strong>r</strong>einforcing <strong>v</strong>ision-<strong>l</strong>anguage <strong>f</strong>ramework (<strong>RVLF</strong>). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.</p>
  </div>
</details>

<hr>
<h3 id="121-Geo3DVQA-Evaluating-Vision-Language-Models-for-3D-Geospatial-Reasoning-from-Aerial-Imagery-cs-CVPDF"><a href="#121-Geo3DVQA-Evaluating-Vision-Language-Models-for-3D-Geospatial-Reasoning-from-Aerial-Imagery-cs-CVPDF" class="headerlink" title="[121] Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery cs.CVPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07276">Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07276" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mai Tsujimoto, Junjue Wang, Weihao Xuan, Naoto Yokoya</span></p>
<p><strong>TL;DR:</strong> Geo3DVQA是一个用于评估视觉语言模型（VLMs）在RGB遥感图像中进行3D地理空间推理的综合性基准，涵盖了16种任务类别和11万个问题-答案对。结果表明，当前VLMs在这一任务上表现不佳，但领域自适应能显著提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前3D地理空间分析依赖昂贵且专业的传感器（如LiDAR），限制了全球范围内的可访问性。现有方法在多特征整合和多样化查询处理上表现不足，需要一种更高效、可解释的方法。</p>
<p><strong>Result:</strong> GPT-4o和Gemini-2.5-Flash在基准上分别取得28.6%和33.0%的准确率，而领域微调的Qwen2.5-VL-7B达到49.6%（+24.8分），显示领域自适应的重要性。</p>
<p><strong>Insight:</strong> 当前VLMs在RGB-to-3D推理任务上表现有限，但领域微调可显著提升性能，为未来研究提供了新的挑战方向。</p>
<p><strong>Abstract:</strong> Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at <a target="_blank" rel="noopener" href="https://github.com/mm1129/Geo3DVQA">https://github.com/mm1129/Geo3DVQA</a>.</p>
  </div>
</details>

<hr>
<h3 id="122-Towards-Accurate-UAV-Image-Perception-Guiding-Vision-Language-Models-with-Stronger-Task-Prompts-cs-CV-cs-AIPDF"><a href="#122-Towards-Accurate-UAV-Image-Perception-Guiding-Vision-Language-Models-with-Stronger-Task-Prompts-cs-CV-cs-AIPDF" class="headerlink" title="[122] Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts cs.CV | cs.AIPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07302">Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07302" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mingning Guo, Mengwei Wu, Shaoxian Li, Haifeng Li, Chao Tao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了AerialVP框架，通过增强任务提示（task prompts）来优化视觉语言模型（VLMs）在无人机图像感知中的表现，解决了目标混淆、尺度变化和复杂背景等问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于VLMs的图像感知方法在处理无人机图像时表现不佳，主要因为任务提示与复杂的图像内容语义对齐困难。</p>
<p><strong>Result:</strong> 实验表明AerialVP显著提升了开源和专有VLMs的性能。</p>
<p><strong>Insight:</strong> 任务提示的质量对VLMs的性能至关重要，尤其是面对复杂图像内容时。</p>
<p><strong>Abstract:</strong> Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs’ understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model’s ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at <a target="_blank" rel="noopener" href="https://github.com/lostwolves/AerialVP">https://github.com/lostwolves/AerialVP</a>.</p>
  </div>
</details>

<hr>
<h3 id="123-Reevaluating-Automated-Wildlife-Species-Detection-A-Reproducibility-Study-on-a-Custom-Image-Dataset-cs-CVPDF"><a href="#123-Reevaluating-Automated-Wildlife-Species-Detection-A-Reproducibility-Study-on-a-Custom-Image-Dataset-cs-CVPDF" class="headerlink" title="[123] Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset cs.CVPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07305">Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07305" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tobias Abraham Haider</span></p>
<p><strong>TL;DR:</strong> 该论文重新评估了Carl等人的研究成果，利用开放资源和不同数据集（900张图像覆盖90个物种）复现实验，结果显示62%的分类准确率与原文71%相近，但泛化能力有限，强调了物种特异性适应或迁移学习的必要性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 验证Carl等人的方法的可重复性和泛化性，为野生动物物种自动检测提供更可靠的基线评估。</p>
<p><strong>Result:</strong> 总体分类准确率为62%，与原文71%接近，但宏F1分数仅为0.28，显示泛化能力不足。</p>
<p><strong>Insight:</strong> 预训练卷积神经网络可作为基线方法，但需物种特异性优化或迁移学习以提高预测质量。</p>
<p><strong>Abstract:</strong> This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.</p>
  </div>
</details>

<hr>
<h3 id="124-ContextAnyone-Context-Aware-Diffusion-for-Character-Consistent-Text-to-Video-Generation-cs-CV-cs-AIPDF"><a href="#124-ContextAnyone-Context-Aware-Diffusion-for-Character-Consistent-Text-to-Video-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[124] ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation cs.CV | cs.AIPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07328">ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07328" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziyang Mai, Yu-Wing Tai</span></p>
<p><strong>TL;DR:</strong> ContextAnyone提出了一种上下文感知的扩散框架，用于从文本和单张参考图像生成角色一致性的视频，通过Emphasize-Attention模块和双引导损失，有效解决了现有方法在保持角色一致性上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的文本到视频（T2V）生成方法在保持角色身份一致性（如发型、服装和体型）方面存在不足，导致视觉连贯性差。</p>
<p><strong>Result:</strong> ContextAnyone在角色一致性和视觉质量上优于现有方法，能够生成多样动作和场景下的连贯视频。</p>
<p><strong>Insight:</strong> 通过上下文感知机制选择性强化参考特征，结合重建目标和扩散模型，显著提升了角色一致性视频的生成质量。</p>
<p><strong>Abstract:</strong> Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \href{<a target="_blank" rel="noopener" href="https://github.com/ziyang1106/ContextAnyone%7D%7Bhttps://github.com/ziyang1106/ContextAnyone%7D">https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}</a>.</p>
  </div>
</details>

<hr>
<h3 id="125-The-Inductive-Bottleneck-Data-Driven-Emergence-of-Representational-Sparsity-in-Vision-Transformers-cs-CVPDF"><a href="#125-The-Inductive-Bottleneck-Data-Driven-Emergence-of-Representational-Sparsity-in-Vision-Transformers-cs-CVPDF" class="headerlink" title="[125] The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers cs.CVPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07331">The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07331" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kanishk Awadhiya</span></p>
<p><strong>TL;DR:</strong> 该研究表明，视觉变换器（ViTs）在数据驱动下会自发形成“U形”熵分布，这种“归纳瓶颈”与任务语义抽象需求相关，而非架构特性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索ViTs在不同数据集上的层间信息压缩行为，揭示其与任务语义复杂性的关系，挑战传统CNN的层次偏置假设。</p>
<p><strong>Result:</strong> 实验显示，纹理密集型数据集保持高秩表示，而面向对象的数据集驱动网络在中间层抑制高频信息，形成语义特征隔离的瓶颈。</p>
<p><strong>Insight:</strong> ViTs的信息压缩行为反映了其对任务语义的自适应能力，为设计更高效的模型提供了新视角。</p>
<p><strong>Abstract:</strong> Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a “U-shaped” entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this “Inductive Bottleneck” is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively “learning” a bottleneck to isolate semantic features.</p>
  </div>
</details>

<hr>
<h3 id="126-Generalized-Referring-Expression-Segmentation-on-Aerial-Photos-cs-CVPDF"><a href="#126-Generalized-Referring-Expression-Segmentation-on-Aerial-Photos-cs-CVPDF" class="headerlink" title="[126] Generalized Referring Expression Segmentation on Aerial Photos cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07338">Generalized Referring Expression Segmentation on Aerial Photos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07338" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Luís Marnoto, Alexandre Bernardino, Bruno Martins</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个新的大规模遥感图像参照表达分割数据集Aerial-D，并通过结合规则生成和LLM增强的方法构建数据集，同时在RSRefSeg架构上进行训练，实现了对现代和历史航拍图像的语义分割。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 航拍图像（如无人机采集的现代照片、历史航拍档案、高分辨率卫星图像等）在参照表达分割任务中面临独特挑战，如分辨率变化大、目标像素少、场景密集和遮挡问题。需要构建一个大规模数据集并开发适应这些挑战的方法。</p>
<p><strong>Result:</strong> 模型在当代基准测试中表现优异，同时在单色、褪色和噪点等历史图像条件下保持高精度。</p>
<p><strong>Insight:</strong> 结合LLM增强的数据生成方法可以提升数据多样性，同时通过模拟历史条件，模型能够更好地适应复杂场景下的分割任务。</p>
<p><strong>Abstract:</strong> Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at <a target="_blank" rel="noopener" href="https://luispl77.github.io/aerial-d">https://luispl77.github.io/aerial-d</a> .</p>
  </div>
</details>

<hr>
<h3 id="127-MICo-150K-A-Comprehensive-Dataset-Advancing-Multi-Image-Composition-cs-CVPDF"><a href="#127-MICo-150K-A-Comprehensive-Dataset-Advancing-Multi-Image-Composition-cs-CVPDF" class="headerlink" title="[127] MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition cs.CVPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07348">MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07348" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinyu Wei, Kangrui Cen, Hongyang Wei, Zhen Guo, Bairui Li</span></p>
<p><strong>TL;DR:</strong> MICo-150K是一个大规模多图像合成数据集，包含15万张高质量合成图像，支持7种代表性任务和研究。数据集通过人工筛选和分解-重组方法构建，并引入了新的评估指标。基准测试表明，基于MICo-150K微调的模型在多图像合成任务上表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前可控图像生成研究中，多图像合成任务因缺乏高质量训练数据而受限。本文旨在填补这一空白，提供系统化的数据集和基准。</p>
<p><strong>Result:</strong> 实验表明，MICo-150K显著提升了模型的多图像合成能力。基线模型Qwen-MICo在3图像合成任务中表现优异，并支持更灵活的多图像输入。</p>
<p><strong>Insight:</strong> 高质量数据集是解决多图像合成任务的关键；分解-重组方法为研究提供了更多可能性；专用评估指标能更准确衡量任务复杂度。</p>
<p><strong>Abstract:</strong> In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&amp;Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&amp;Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter’s limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.</p>
  </div>
</details>

<hr>
<h3 id="128-DeepAgent-A-Dual-Stream-Multi-Agent-Fusion-for-Robust-Multimodal-Deepfake-Detection-cs-CV-cs-AI-cs-SDPDF"><a href="#128-DeepAgent-A-Dual-Stream-Multi-Agent-Fusion-for-Robust-Multimodal-Deepfake-Detection-cs-CV-cs-AI-cs-SDPDF" class="headerlink" title="[128] DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection cs.CV | cs.AI | cs.SDPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07351">DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.SD</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07351" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sayeem Been Zaman, Wasimul Karim, Arefin Ittesafun Abian, Reem E. Mohamed, Md Rafiqul Islam</span></p>
<p><strong>TL;DR:</strong> DeepAgent提出了一种基于双流多智能体融合的鲁棒多模态深度伪造检测框架，结合视觉和音频模态，通过随机森林元分类器提升性能，验证了其在多个数据集上的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有深度伪造检测方法多采用单一模型整合视觉和音频信息，易受模态不匹配、噪声和操纵的影响，因此需要更鲁棒的多模态融合方法。</p>
<p><strong>Result:</strong> Agent-1在Celeb-DF和FakeAVCeleb数据集上达到94.35%准确率；Agent-2和元分类器在FakeAVCeleb上分别为93.69%和81.56%；跨数据集DeepFakeTIMIT上元分类器达97.49%。</p>
<p><strong>Insight:</strong> 分层融合策略能有效缓解单模态弱点，多智能体协作可应对不同类型的深度伪造操纵，证明了多模态融合的重要性。</p>
<p><strong>Abstract:</strong> The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.</p>
  </div>
</details>

<hr>
<h3 id="129-Structure-Aware-Feature-Rectification-with-Region-Adjacency-Graphs-for-Training-Free-Open-Vocabulary-Semantic-Segmentation-cs-CV-cs-AIPDF"><a href="#129-Structure-Aware-Feature-Rectification-with-Region-Adjacency-Graphs-for-Training-Free-Open-Vocabulary-Semantic-Segmentation-cs-CV-cs-AIPDF" class="headerlink" title="[129] Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation cs.CV | cs.AIPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07360">Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07360" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiming Huang, Hao Ai, Jianbo Jiao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结构感知的特征校正方法，通过构建区域邻接图（RAG）来优化CLIP特征，从而提升开放词汇语义分割的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的开放词汇语义分割方法依赖于CLIP等视觉语言模型，但由于CLIP的预训练特性，其倾向于全局语义对齐，导致在局部区域的细粒度关联上表现不佳。</p>
<p><strong>Result:</strong> 实验表明，该方法能有效减少分割噪声，提高区域级一致性，并在多个开放词汇分割基准上取得了优秀表现。</p>
<p><strong>Insight:</strong> 研究表明，结合局部结构信息和视觉语言模型的全局特征，可以有效弥补CLIP在细粒度分割任务中的不足。</p>
<p><strong>Abstract:</strong> Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP’s pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="130-Tessellation-GS-Neural-Mesh-Gaussians-for-Robust-Monocular-Reconstruction-of-Dynamic-Objects-cs-CVPDF"><a href="#130-Tessellation-GS-Neural-Mesh-Gaussians-for-Robust-Monocular-Reconstruction-of-Dynamic-Objects-cs-CVPDF" class="headerlink" title="[130] Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects cs.CVPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07381">Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07381" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuohan Tao, Boyao Zhou, Hanzhang Tu, Yuwang Wang, Yebin Liu</span></p>
<p><strong>TL;DR:</strong> Tessellation GS提出了一种基于网格面的结构化2D GS方法，通过自适应细分策略和分层神经特征，解决了3D高斯喷溅在动态场景重建中的泛化问题，显著提升了重建质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 3D高斯喷溅（GS）在多视角图像重建中表现优异，但在稀疏视角和动态场景中因各向异性导致泛化能力差，难以支持单视角动态物体的鲁棒重建。</p>
<p><strong>Result:</strong> 在动态场景重建任务中，LPIPS降低29.1%，Chamfer距离减少49.2%，显著优于现有方法。</p>
<p><strong>Insight:</strong> 通过网格面的结构化约束和先验引导，可以有效提升高斯喷溅在动态场景重建中的鲁棒性和泛化能力。</p>
<p><strong>Abstract:</strong> 3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.</p>
  </div>
</details>

<hr>
<h3 id="131-How-Far-are-Modern-Trackers-from-UAV-Anti-UAV-A-Million-Scale-Benchmark-and-New-Baseline-cs-CVPDF"><a href="#131-How-Far-are-Modern-Trackers-from-UAV-Anti-UAV-A-Million-Scale-Benchmark-and-New-Baseline-cs-CVPDF" class="headerlink" title="[131] How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline cs.CVPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07385">How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07385" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chunhui Zhang, Li Liu, Zhipeng Zhang, Yong Wang, Hao Wen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个新的多模态视觉跟踪任务UAV-Anti-UAV，并构建了一个百万规模的数据集。作者提出了基于Mamba的基线方法MambaSTS，通过集成时空语义学习来解决动态干扰问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 无人机（UAV）的广泛应用带来了安全和隐私风险，但现有抗无人机技术主要关注固定地面摄像头捕捉的视频，忽略了从移动无人机平台跟踪目标无人机的挑战。</p>
<p><strong>Result:</strong> 实验验证了MambaSTS的有效性，并对50种现代深度跟踪算法进行了评估，结果表明UAV-Anti-UAV领域仍有很大改进空间。</p>
<p><strong>Insight:</strong> 动态干扰（双动态扰动）是UAV-Anti-UAV任务的核心挑战，Mamba模型的引入为长序列建模提供了新思路。</p>
<p><strong>Abstract:</strong> Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model’s strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\color{magenta}<a target="_blank" rel="noopener" href="https://github.com/983632847/Awesome-Multimodal-Object-Tracking%7D">https://github.com/983632847/Awesome-Multimodal-Object-Tracking}</a>.</p>
  </div>
</details>

<hr>
<h3 id="132-GlimmerNet-A-Lightweight-Grouped-Dilated-Depthwise-Convolutions-for-UAV-Based-Emergency-Monitoring-cs-CVPDF"><a href="#132-GlimmerNet-A-Lightweight-Grouped-Dilated-Depthwise-Convolutions-for-UAV-Based-Emergency-Monitoring-cs-CVPDF" class="headerlink" title="[132] GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring cs.CVPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07391">GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07391" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Đorđe Nedeljković</span></p>
<p><strong>TL;DR:</strong> GlimmerNet是一种基于分组扩张深度卷积的轻量级CNN，用于无人机应急监测，通过多尺度特征提取和高效特征融合，实现了高性能和低计算开销。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管基于自注意力的Vision Transformers在CNN中引入全局上下文理解能力，但其计算开销较大。本文旨在设计一种轻量级CNN，在不增加计算负担的情况下保留全局感知能力。</p>
<p><strong>Result:</strong> 仅需31K参数，计算量比基线减少29%，在AIDERv2数据集上F1-score达到0.966。</p>
<p><strong>Insight:</strong> 通过分离感受野多样性和特征重组，可以在不增加计算负担的情况下提升全局感知能力，适用于资源受限的无人机平台。</p>
<p><strong>Abstract:</strong> Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at <a target="_blank" rel="noopener" href="https://github.com/djordjened92/gdd-cnn">https://github.com/djordjened92/gdd-cnn</a>.</p>
  </div>
</details>

<hr>
<h3 id="133-Reconstructing-Objects-along-Hand-Interaction-Timelines-in-Egocentric-Video-cs-CVPDF"><a href="#133-Reconstructing-Objects-along-Hand-Interaction-Timelines-in-Egocentric-Video-cs-CVPDF" class="headerlink" title="[133] Reconstructing Objects along Hand Interaction Timelines in Egocentric Video cs.CVPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07394">Reconstructing Objects along Hand Interaction Timelines in Egocentric Video</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07394" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhifan Zhu, Siddhant Bansal, Shashank Tripathi, Dima Damen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了ROHIT任务，通过定义手交互时间线（HIT）并利用约束优化与传播（COP）框架改进物体姿态重建，特别关注稳定抓取阶段。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究目标是通过分析手交互时间线（HIT）中的物体姿态变化，改进第一人称视频中物体的三维重建，尤其是在稳定抓取阶段的姿态传播。</p>
<p><strong>Result:</strong> COP显著提升了稳定抓取阶段的重建效果（6.2-11.3%），并在HIT重建上实现了高达24.5%的改进。</p>
<p><strong>Insight:</strong> 通过约束姿态传播和稳定抓取阶段的分析，可以在缺乏三维真实标注的情况下，显著提升物体重建的质量。</p>
<p><strong>Abstract:</strong> We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object’s perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object’s pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.</p>
  </div>
</details>

<hr>
<h3 id="134-Data-driven-Exploration-of-Mobility-Interaction-Patterns-cs-CV-cs-AIPDF"><a href="#134-Data-driven-Exploration-of-Mobility-Interaction-Patterns-cs-CV-cs-AIPDF" class="headerlink" title="[134] Data-driven Exploration of Mobility Interaction Patterns cs.CV | cs.AIPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07415">Data-driven Exploration of Mobility Interaction Patterns</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07415" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gabriele Galatolo, Mirco Nanni</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种数据驱动的方法，用于探索个体之间的移动交互模式，旨在通过数据挖掘发现复杂且持续的交互模式，从而改进现有的模拟模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 理解个体移动行为及其对外界的反应是模拟人类动态的关键，尤其是在人群模拟和应急管理中。现有方法通常基于预设的行为模型，而本文希望通过数据直接探索交互模式。</p>
<p><strong>Result:</strong> 在两个真实案例（汽车和行人）上进行了实验，结果显示该方法在性能、参数敏感性和结果解释性方面表现良好。</p>
<p><strong>Insight:</strong> 通过数据驱动的方法，可以更准确地捕捉个体间的移动交互模式，为改进模拟模型提供新见解。</p>
<p><strong>Abstract:</strong> Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.</p>
  </div>
</details>

<hr>
<h3 id="135-Unified-Video-Editing-with-Temporal-Reasoner-cs-CVPDF"><a href="#135-Unified-Video-Editing-with-Temporal-Reasoner-cs-CVPDF" class="headerlink" title="[135] Unified Video Editing with Temporal Reasoner cs.CVPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07469">Unified Video Editing with Temporal Reasoner</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07469" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangpeng Yang, Ji Xie, Yiyuan Yang, Yan Huang, Min Xu</span></p>
<p><strong>TL;DR:</strong> 论文提出VideoCoF，通过Chain-of-Frames方法解决视频编辑中任务特定先验与统一模型的冲突，支持无需掩码的精确编辑和运动对齐。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频编辑方法在任务特定先验（如掩码）与统一模型之间存在权衡，前者精准但依赖任务先验，后者无需掩码但缺乏空间线索，导致编辑不精准。</p>
<p><strong>Result:</strong> 仅需50k视频对训练，VideoCoF在VideoCoF-Bench上达到SOTA性能，验证了方法的效率和有效性。</p>
<p><strong>Insight:</strong> 显式推理步骤和RoPE对齐策略为视频编辑提供了无需掩码的高精度解决方案，同时支持长视频编辑和运动一致性。</p>
<p><strong>Abstract:</strong> Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a &#96;&#96;see, reason, then edit” procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at <a target="_blank" rel="noopener" href="https://github.com/knightyxp/VideoCoF">https://github.com/knightyxp/VideoCoF</a>.</p>
  </div>
</details>

<hr>
<h3 id="136-Single-step-Diffusion-based-Video-Coding-with-Semantic-Temporal-Guidance-cs-CVPDF"><a href="#136-Single-step-Diffusion-based-Video-Coding-with-Semantic-Temporal-Guidance-cs-CVPDF" class="headerlink" title="[136] Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance cs.CVPDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07480">Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07480" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Naifu Xue, Zhaoyang Jia, Jiahao Li, Bin Li, Zihan Zheng</span></p>
<p><strong>TL;DR:</strong> S2VC提出了一种基于单步扩散的视频编码方法，结合语义-时间引导，显著提升了低码率下的感知质量，同时降低了采样复杂度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统和基于神经网络的视频编码（NVC）在码率-失真性能上表现优异，但在低码率下的感知质量提升仍有挑战。现有方法或依赖复杂采样，或生成能力有限导致伪影。</p>
<p><strong>Result:</strong> S2VC在感知质量上达到SOTA，平均节省52.73%的码率，显著优于现有感知方法。</p>
<p><strong>Insight:</strong> 单步扩散模型在视频压缩中具有高效性和高质量生成的潜力，语义和时间引导是关键优化方向。</p>
<p><strong>Abstract:</strong> While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.</p>
  </div>
</details>

<hr>
<h3 id="137-Towards-Robust-DeepFake-Detection-under-Unstable-Face-Sequences-Adaptive-Sparse-Graph-Embedding-with-Order-Free-Representation-and-Explicit-Laplacian-Spectral-Prior-cs-CVPDF"><a href="#137-Towards-Robust-DeepFake-Detection-under-Unstable-Face-Sequences-Adaptive-Sparse-Graph-Embedding-with-Order-Free-Representation-and-Explicit-Laplacian-Spectral-Prior-cs-CVPDF" class="headerlink" title="[137] Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior cs.CVPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07498">Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07498" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chih-Chung Hsu, Shao-Ning Chen, Chia-Ming Lee, Yi-Fang Wang, Yi-Shiuan Chou</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种拉普拉斯正则化图卷积网络（LR-GCN），用于在嘈杂或无序的人脸序列中鲁棒地检测DeepFakes。通过自适应的稀疏图嵌入和显式的拉普拉斯谱先验，该方法能够有效处理输入中的缺失、遮挡或对抗性干扰。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的DeepFake检测方法通常假设人脸序列在时间上是连续且干净的，但在实际场景中，压缩伪影、遮挡和对抗性攻击会导致人脸检测不稳定或失效。因此，需要一种能够应对这些噪声和干扰的鲁棒检测方法。</p>
<p><strong>Result:</strong> 在FF++、Celeb-DFv2和DFDC数据集上的实验表明，LR-GCN在缺失人脸、遮挡和对抗性干扰等严重扰动下表现出最先进的性能和更强的鲁棒性。</p>
<p><strong>Insight:</strong> 图卷积网络中引入谱域先验可以增强对伪造痕迹的检测能力；双级稀疏机制能够自适应忽略无效输入，提升模型的鲁棒性。</p>
<p><strong>Abstract:</strong> Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.</p>
  </div>
</details>

<hr>
<h3 id="138-MultiMotion-Multi-Subject-Video-Motion-Transfer-via-Video-Diffusion-Transformer-cs-CVPDF"><a href="#138-MultiMotion-Multi-Subject-Video-Motion-Transfer-via-Video-Diffusion-Transformer-cs-CVPDF" class="headerlink" title="[138] MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer cs.CVPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07500">MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07500" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Penghui Liu, Jiangshan Wang, Yutong Shen, Shanhui Mo, Chenyang Qi</span></p>
<p><strong>TL;DR:</strong> MultiMotion通过Mask-aware Attention Motion Flow（AMF）和RectPC采样器，解决了多对象视频运动转移中的运动纠缠和对象级控制问题，并构建了首个基于DiT的数据集。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多对象视频运动转移在DiT架构中面临运动纠缠和缺乏对象级控制的挑战，需要一种统一的解决方案。</p>
<p><strong>Result:</strong> 实现了精确、语义对齐且时间一致的多对象运动转移，保持了DiT的高质量和可扩展性。</p>
<p><strong>Insight:</strong> 通过显式对象级控制和高效采样，DiT能够有效处理复杂多对象场景，扩展了其适用性。</p>
<p><strong>Abstract:</strong> Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT’s high quality and scalability. The code is in the supp.</p>
  </div>
</details>

<hr>
<h3 id="139-ControlVP-Interactive-Geometric-Refinement-of-AI-Generated-Images-with-Consistent-Vanishing-Points-cs-CVPDF"><a href="#139-ControlVP-Interactive-Geometric-Refinement-of-AI-Generated-Images-with-Consistent-Vanishing-Points-cs-CVPDF" class="headerlink" title="[139] ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points cs.CVPDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07504">ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07504" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ryota Okumura, Kaede Shiohara, Toshihiko Yamasaki</span></p>
<p><strong>TL;DR:</strong> ControlVP提出了一种用户引导的方法，用于修正AI生成图像中的消失点不一致问题，通过引入结构引导和几何约束，增强图像的全局几何一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的文本到图像生成模型（如Stable Diffusion）在生成图像时，常常出现几何不一致问题，尤其是消失点不一致，导致场景结构不真实。这在需要精确空间结构的应用中（如图像到3D重建）尤为重要。</p>
<p><strong>Result:</strong> ControlVP显著提升了生成图像的几何一致性，尤其在建筑场景中表现突出，同时保持了与基线相当的视觉质量。</p>
<p><strong>Insight:</strong> 用户引导和几何约束的结合可以有效解决AI生成图像中的几何不一致问题，尤其是在需要精确结构的应用中。</p>
<p><strong>Abstract:</strong> Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at <a target="_blank" rel="noopener" href="https://github.com/RyotaOkumura/ControlVP">https://github.com/RyotaOkumura/ControlVP</a> .</p>
  </div>
</details>

<hr>
<h3 id="140-From-Orbit-to-Ground-Generative-City-Photogrammetry-from-Extreme-Off-Nadir-Satellite-Images-cs-CV-cs-GRPDF"><a href="#140-From-Orbit-to-Ground-Generative-City-Photogrammetry-from-Extreme-Off-Nadir-Satellite-Images-cs-CV-cs-GRPDF" class="headerlink" title="[140] From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images cs.CV | cs.GRPDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07527">From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07527" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fei Yu, Yu Liu, Luyang Tang, Mingchao Sun, Zengye Ge</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种从极端倾斜卫星图像生成城市3D重建的方法，通过使用2.5D高度图和生成式纹理恢复网络，实现了从稀疏卫星图像合成高质量地面视图的目标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 从稀疏的卫星图像中重建城市规模的3D模型面临极端视角外推的挑战，现有方法如NeRF和3DGS难以处理严重缩短的立面和模糊纹理。本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在实验中对大规模城市区域进行了重建，例如从少量卫星图像中成功重建了4平方千米的真实世界区域，生成了逼真的地面视图，性能优于现有方法。</p>
<p><strong>Insight:</strong> 该方法不仅解决了从稀疏倾斜卫星图像中重建城市的难题，还为城市规划等下游任务提供了高保真、可直接应用的模型。</p>
<p><strong>Abstract:</strong> City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.   To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.   Our method’s scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4,\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.</p>
  </div>
</details>

<hr>
<h3 id="141-Dual-Stream-Cross-Modal-Representation-Learning-via-Residual-Semantic-Decorrelation-cs-CV-cs-AI-eess-IVPDF"><a href="#141-Dual-Stream-Cross-Modal-Representation-Learning-via-Residual-Semantic-Decorrelation-cs-CV-cs-AI-eess-IVPDF" class="headerlink" title="[141] Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation cs.CV | cs.AI | eess.IVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07568">Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07568" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuecheng Li, Weikuan Jia, Alisher Kurbonaliev, Qurbonaliev Alisher, Khudzhamkulov Rustam</span></p>
<p><strong>TL;DR:</strong> 论文提出DSRSD-Net框架，通过残差分解和语义解相关约束，分离模态特定和共享信息，解决跨模态学习中的模态主导、冗余耦合和虚假相关性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 跨模态学习中，高方差模态容易掩盖语义重要信号，融合策略导致模态共享和特定信息纠缠，影响泛化能力和可解释性。</p>
<p><strong>Result:</strong> 在两个教育基准上，DSRSD-Net在下一步和最终结果预测中优于单模态和融合基线。</p>
<p><strong>Insight:</strong> 显式解相关和正交性设计有助于提升跨模态表示的可解释性和鲁棒性。</p>
<p><strong>Abstract:</strong> Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while naïve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.</p>
  </div>
</details>

<hr>
<h3 id="142-All-You-Need-Are-Random-Visual-Tokens-Demystifying-Token-Pruning-in-VLLMs-cs-CVPDF"><a href="#142-All-You-Need-Are-Random-Visual-Tokens-Demystifying-Token-Pruning-in-VLLMs-cs-CVPDF" class="headerlink" title="[142] All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs cs.CVPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07580">All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07580" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yahong Wang, Juncheng Wu, Zhangkai Ni, Longzhen Yang, Yihang Liu</span></p>
<p><strong>TL;DR:</strong> 论文揭示了视觉大模型（VLLMs）中视觉令牌的信息在深层网络中逐渐消失的现象，并提出了一种随机剪枝方法，显著提升了性能和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉大模型因依赖大量视觉令牌导致计算成本高昂，现有剪枝方法在深层网络效果不佳，研究者试图通过分析令牌信息消失的原因来改进剪枝策略。</p>
<p><strong>Result:</strong> 随机剪枝在深层网络中表现优异，DivPrune结合随机剪枝达到先进水平，剪枝50%令牌仍保持96.9%的模型性能。</p>
<p><strong>Insight:</strong> 深层网络中视觉令牌的信息冗余是现有剪枝方法失效的原因，随机剪枝因其简单性反而更高效。模型能力和任务类型决定了信息边界的深度。</p>
<p><strong>Abstract:</strong> Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by “vanishing token information”, where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token’s information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as “information horizon”, beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/YahongWang1/Information-Horizon">https://github.com/YahongWang1/Information-Horizon</a>.</p>
  </div>
</details>

<hr>
<h3 id="143-Robust-Variational-Model-Based-Tailored-UNet-Leveraging-Edge-Detector-and-Mean-Curvature-for-Improved-Image-Segmentation-cs-CVPDF"><a href="#143-Robust-Variational-Model-Based-Tailored-UNet-Leveraging-Edge-Detector-and-Mean-Curvature-for-Improved-Image-Segmentation-cs-CVPDF" class="headerlink" title="[143] Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation cs.CVPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07590">Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07590" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaili Qi, Zhongyi Huang, Wenli Yang</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合变分方法和深度学习的混合框架VM_TUNet，通过引入物理先验、边缘检测器和平均曲率项，改进了噪声图像的语义分割效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对噪声图像边界模糊或断裂的分割难题，传统CNN方法和纯变分方法各有不足，需要一种兼具变分方法边界平滑能力和深度学习强表征能力的解决方案。</p>
<p><strong>Result:</strong> 在三个基准数据集上验证表明，VM_TUNet在性能和计算效率之间取得了平衡，优于纯CNN方法，且接近Transformer方法的性能。</p>
<p><strong>Insight:</strong> 通过结合变分方法的物理可解释性和深度学习的表征能力，可以实现对小样本和噪声数据的稳健分割，为混合模型设计提供了新思路。</p>
<p><strong>Abstract:</strong> To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.</p>
  </div>
</details>

<hr>
<h3 id="144-More-than-Segmentation-Benchmarking-SAM-3-for-Segmentation-3D-Perception-and-Reconstruction-in-Robotic-Surgery-cs-CV-cs-ROPDF"><a href="#144-More-than-Segmentation-Benchmarking-SAM-3-for-Segmentation-3D-Perception-and-Reconstruction-in-Robotic-Surgery-cs-CV-cs-ROPDF" class="headerlink" title="[144] More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery cs.CV | cs.ROPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07596">More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07596" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenzhen Dong, Jieming Yu, Yiming Huang, Hongqiu Wang, Lei Zhu</span></p>
<p><strong>TL;DR:</strong> SAM 3在机器人手术中的应用得到了全面评测，展示了其在零样本分割、动态视频跟踪和3D重建方面的改进，但语言提示的表现仍需领域特定训练。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 评估SAM 3在机器人手术中的性能，特别是在分割、3D感知和重建方面的能力，以验证其在实际场景中的应用潜力。</p>
<p><strong>Result:</strong> SAM 3在空间提示下的分割性能优于SAM和SAM 2，语言提示表现不佳；3D重建在单目深度估计和器械重建方面表现出色，但在动态复杂场景中仍有局限。</p>
<p><strong>Insight:</strong> 尽管SAM 3在多模态提示和3D感知能力上有显著提升，但在手术领域的语言提示和复杂场景处理上仍需优化。</p>
<p><strong>Abstract:</strong> The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3’s 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.</p>
  </div>
</details>

<hr>
<h3 id="145-Online-Segment-Any-3D-Thing-as-Instance-Tracking-cs-CVPDF"><a href="#145-Online-Segment-Any-3D-Thing-as-Instance-Tracking-cs-CVPDF" class="headerlink" title="[145] Online Segment Any 3D Thing as Instance Tracking cs.CVPDF"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07599">Online Segment Any 3D Thing as Instance Tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07599" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hanshi Wang, Zijian Cai, Jin Gao, Yiwei Zhang, Weiming Hu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为AutoSeg3D的新方法，将在线3D分割重新定义为实例跟踪问题，通过长期和短期的实例关联与更新提升时空感知能力。该方法在多个数据集上表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法依赖于预定义的对象查询从视觉基础模型（VFMs）中聚合语义信息，但忽视了时间维度的动态感知。为提升智能体对环境的时空理解能力，本研究提出将在线3D分割重构为实例跟踪问题。</p>
<p><strong>Result:</strong> 在ScanNet200上超过ESAM 2.8 AP，并在ScanNet、SceneNN和3RScan数据集上表现一致提升。</p>
<p><strong>Insight:</strong> 时间信息传播和空间一致性学习能够显著提升3D分割性能，同时避免密集时间点云交互的计算负担。</p>
<p><strong>Abstract:</strong> Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.</p>
  </div>
</details>

<hr>
<h3 id="146-MoCA-Mixture-of-Components-Attention-for-Scalable-Compositional-3D-Generation-cs-CVPDF"><a href="#146-MoCA-Mixture-of-Components-Attention-for-Scalable-Compositional-3D-Generation-cs-CVPDF" class="headerlink" title="[146] MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation cs.CVPDF"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07628">MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07628" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiqi Li, Wenhuan Li, Tengfei Wang, Zhenwei Wang, Junta Wu</span></p>
<p><strong>TL;DR:</strong> MoCA是一种高效、细粒度的3D组合生成模型，通过重要性路由和无关组件压缩技术，解决了传统方法因全局注意力计算成本高而难以扩展的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于部分的3D生成方法因全局注意力计算成本随组件数量呈二次增长而扩展性差，限制了其在复杂场景中的应用。</p>
<p><strong>Result:</strong> 实验表明，MoCA在组合对象和场景生成任务上均优于基线方法。</p>
<p><strong>Insight:</strong> 稀疏化和上下文压缩是提高3D组合生成可扩展性的有效途径。</p>
<p><strong>Abstract:</strong> Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: <a target="_blank" rel="noopener" href="https://lizhiqi49.github.io/MoCA">https://lizhiqi49.github.io/MoCA</a></p>
  </div>
</details>

<hr>
<h3 id="147-An-AI-Powered-Autonomous-Underwater-System-for-Sea-Exploration-and-Scientific-Research-cs-CV-cs-AIPDF"><a href="#147-An-AI-Powered-Autonomous-Underwater-System-for-Sea-Exploration-and-Scientific-Research-cs-CV-cs-AIPDF" class="headerlink" title="[147] An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research cs.CV | cs.AIPDF"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07652">An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07652" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hamad Almazrouei, Mariam Al Nasseri, Maha Alzaabi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于人工智能的自主水下系统，结合了目标检测、特征提取、降维聚类和大语言模型，实现了高效的海底探索和科学数据分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统海底探索因极端条件、低可见性和高成本而受限，需自动化解决方案以提升效率和安全性。</p>
<p><strong>Result:</strong> <a href="mailto:&#x6d;&#65;&#80;&#x40;&#x30;&#x2e;&#x35;">mAP@0.5</a>达0.512，PCA保留98%方差，聚类和语言模型有效生成数据分析和报告。</p>
<p><strong>Insight:</strong> AI技术集成显著提升了海底探索的效率和数据分析深度，减少了对人工的依赖。</p>
<p><strong>Abstract:</strong> Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system’s capability to detect marine objects with a <a href="mailto:&#x6d;&#x41;&#80;&#64;&#x30;&#46;&#x35;">mAP@0.5</a> of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.</p>
  </div>
</details>

<hr>
<h3 id="148-EgoCampus-Egocentric-Pedestrian-Eye-Gaze-Model-and-Dataset-cs-CVPDF"><a href="#148-EgoCampus-Egocentric-Pedestrian-Eye-Gaze-Model-and-Dataset-cs-CVPDF" class="headerlink" title="[148] EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset cs.CVPDF"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07668">EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07668" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ronan John, Aditya Kesari, Vincenzo DiMatteo, Kristin Dana</span></p>
<p><strong>TL;DR:</strong> 论文介绍了EgoCampus数据集和EgoCampusNet方法，用于研究行人在户外校园环境中导航时的视觉注意力。数据集包含80多名行人的眼动追踪和传感器数据，方法是预测行人眼动注视点。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大多数自我中心数据集集中在室内任务或缺乏眼动追踪信息，而户外导航中的视觉注意力研究是一个未被充分探索的领域。</p>
<p><strong>Result:</strong> EgoCampus数据集为研究户外导航中的视觉注意力提供了新资源，EgoCampusNet展示了预测行人眼动的潜力。</p>
<p><strong>Insight:</strong> 户外环境中的行人视觉注意力研究需要多模态数据的支持，眼动追踪数据在导航任务中尤为重要。</p>
<p><strong>Abstract:</strong> We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta’s Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at <a target="_blank" rel="noopener" href="https://github.com/ComputerVisionRutgers/EgoCampus">https://github.com/ComputerVisionRutgers/EgoCampus</a> .</p>
  </div>
</details>

<hr>
<h3 id="149-sim2art-Accurate-Articulated-Object-Modeling-from-a-Single-Video-using-Synthetic-Training-Data-Only-cs-CV-cs-ROPDF"><a href="#149-sim2art-Accurate-Articulated-Object-Modeling-from-a-Single-Video-using-Synthetic-Training-Data-Only-cs-CV-cs-ROPDF" class="headerlink" title="[149] sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only cs.CV | cs.ROPDF"></a>[149] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07698">sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07698" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arslan Artykov, Corentin Sautier, Vincent Lepetit</span></p>
<p><strong>TL;DR:</strong> 提出了sim2art，首个仅用合成数据训练的数据驱动方法，从单目视频中联合预测部分分割和关节参数，实现了对真实世界的强泛化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究目的是解决关节物体建模的挑战，特别是从单目视频中恢复部分分割和关节参数。此前方法依赖多视角系统或静态相机，而sim2art专注于自由移动相机捕获的视频。</p>
<p><strong>Result:</strong> 实验展示了该方法对真实世界物体的强泛化能力，证明了其在实际应用中的有效性。</p>
<p><strong>Insight:</strong> 通过合成数据训练的方法可以成功地泛化到真实世界，为关节物体建模提供了可扩展且实用的解决方案。</p>
<p><strong>Abstract:</strong> Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: <a target="_blank" rel="noopener" href="https://aartykov.github.io/sim2art/">https://aartykov.github.io/sim2art/</a></p>
  </div>
</details>

<hr>
<h3 id="150-PVeRA-Probabilistic-Vector-Based-Random-Matrix-Adaptation-cs-CV-cs-LGPDF"><a href="#150-PVeRA-Probabilistic-Vector-Based-Random-Matrix-Adaptation-cs-CV-cs-LGPDF" class="headerlink" title="[150] PVeRA: Probabilistic Vector-Based Random Matrix Adaptation cs.CV | cs.LGPDF"></a>[150] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07703">PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07703" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Leo Fillioux, Enzo Ferrante, Paul-Henry Cournède, Maria Vakalopoulou, Stergios Christodoulidis</span></p>
<p><strong>TL;DR:</strong> PVeRA是一种基于概率的VeRA适配器改进版本，通过概率化处理低秩矩阵，提升了参数高效的模型适应能力，并在VTAB-1k基准测试中表现优于其他适配器。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模基础模型的训练和微调需要大量数据和计算资源，现有适配器方法（如VeRA）虽能高效适应新任务，但仍有改进空间。PVeRA通过引入概率性改进，增强了处理输入模糊性和不同采样配置的能力。</p>
<p><strong>Result:</strong> 在VTAB-1k基准测试中，PVeRA优于VeRA及其他七种适配器，展现了其高效性和优越性能。</p>
<p><strong>Insight:</strong> 概率化方法为参数高效的模型适应提供了新思路，能够更好地处理输入模糊性，同时支持灵活的采样配置，适用于多样化的任务场景。</p>
<p><strong>Abstract:</strong> Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available <a target="_blank" rel="noopener" href="https://github.com/leofillioux/pvera">https://github.com/leofillioux/pvera</a>.</p>
  </div>
</details>

<hr>
<h3 id="151-UnCageNet-Tracking-and-Pose-Estimation-of-Caged-Animal-cs-CVPDF"><a href="#151-UnCageNet-Tracking-and-Pose-Estimation-of-Caged-Animal-cs-CVPDF" class="headerlink" title="[151] UnCageNet: Tracking and Pose Estimation of Caged Animal cs.CVPDF"></a>[151] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07712">UnCageNet: Tracking and Pose Estimation of Caged Animal</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07712" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sayak Dutta, Harish Katti, Shashikant Verma, Shanmuganathan Raman</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为UnCageNet的三阶段预处理流程，旨在解决笼子结构和系统性遮挡对动物追踪和姿态估计系统的影响。通过笼子分割、笼子修复和去遮挡后的姿态估计与追踪，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有动物追踪和姿态估计系统（如STEP和ViTPose）在处理带有笼子结构和系统性遮挡的图像和视频时性能大幅下降，亟需一种有效的预处理方法解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提升了关键点检测精度和轨迹一致性，使得带遮挡场景的性能接近无遮挡场景。</p>
<p><strong>Insight:</strong> 通过多阶段预处理（分割+修复）可以有效解决遮挡问题，Gabor滤波器在捕捉方向性结构（如笼子）方面具有独特优势。</p>
<p><strong>Abstract:</strong> Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.</p>
  </div>
</details>

<hr>
<h3 id="152-ViSA-3D-Aware-Video-Shading-for-Real-Time-Upper-Body-Avatar-Creation-cs-CVPDF"><a href="#152-ViSA-3D-Aware-Video-Shading-for-Real-Time-Upper-Body-Avatar-Creation-cs-CVPDF" class="headerlink" title="[152] ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation cs.CVPDF"></a>[152] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07720">ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07720" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fan Yang, Heyuan Li, Peihao Li, Weihao Yuan, Lingteng Qiu</span></p>
<p><strong>TL;DR:</strong> ViSA 结合 3D 重建模型和视频生成模型的优势，提出了一种实时生成高质量上半身 3D 头像的方法，解决了现有方法中纹理模糊、动作僵硬和结构不稳定等问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前 3D 头像生成方法在稳定性和动态表现上存在矛盾：3D 重建方法快速且结构稳定，但纹理模糊且动作僵硬；视频生成模型能合成逼真动态效果，但结构易出错且身份漂移。ViSA 旨在结合两者的优势。</p>
<p><strong>Result:</strong> 实验表明，ViSA 显著减少了纹理模糊和动作僵硬，视觉效果优于现有方法，适用于游戏和虚拟现实等实时应用。</p>
<p><strong>Insight:</strong> 结合 3D 几何稳定性和视频生成灵活性是实现高质量动态 3D 头像的有效途径，为实时应用提供了新思路。</p>
<p><strong>Abstract:</strong> Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: <a target="_blank" rel="noopener" href="https://lhyfst.github.io/visa">https://lhyfst.github.io/visa</a></p>
  </div>
</details>

<hr>
<h3 id="153-SAVE-Sparse-Autoencoder-Driven-Visual-Information-Enhancement-for-Mitigating-Object-Hallucination-cs-CV-cs-AIPDF"><a href="#153-SAVE-Sparse-Autoencoder-Driven-Visual-Information-Enhancement-for-Mitigating-Object-Hallucination-cs-CV-cs-AIPDF" class="headerlink" title="[153] SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination cs.CV | cs.AIPDF"></a>[153] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07730">SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07730" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sangha Park, Seungryong Yoo, Jisoo Mok, Sungroh Yoon</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为SAVE的框架，通过稀疏自编码器（SAE）潜在特征增强视觉信息，减少多模态大语言模型（MLLMs）中的物体幻觉问题。该方法通过二进制物体存在问答探测确定关键的视觉理解特征，并基于这些特征调整模型，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型（MLLMs）在处理视觉信息时容易受到语言先验和视觉信息丢失的影响，导致物体幻觉问题。为了解决这一问题，作者提出了SAVE框架。</p>
<p><strong>Result:</strong> SAVE在CHAIR_S上实现了10%的提升，并在POPE和MMHal-Bench上表现一致。实验表明，该方法能有效抑制不确定物体标记的生成，并增强对图像标记的关注。</p>
<p><strong>Insight:</strong> 稀疏自编码器的潜在特征是解决物体幻觉问题的有效工具，通过增强视觉理解特征可以显著提升模型的性能。</p>
<p><strong>Abstract:</strong> Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model’s visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10%p improvement in CHAIR_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at <a target="_blank" rel="noopener" href="https://github.com/wiarae/SAVE">https://github.com/wiarae/SAVE</a>.</p>
  </div>
</details>

<hr>
<h3 id="154-SpatialDreamer-Incentivizing-Spatial-Reasoning-via-Active-Mental-Imagery-cs-CVPDF"><a href="#154-SpatialDreamer-Incentivizing-Spatial-Reasoning-via-Active-Mental-Imagery-cs-CVPDF" class="headerlink" title="[154] SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery cs.CVPDF"></a>[154] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07733">SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07733" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meng Cao, Xingyu Li, Xue Liu, Ian Reid, Xiaodan Liang</span></p>
<p><strong>TL;DR:</strong> SpatialDreamer提出了一种通过强化学习框架促进MLLMs空间推理的新方法，结合主动探索、视觉想象和基于证据的推理，显著提升了复杂空间任务的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的多模态大语言模型（MLLMs）在需要心理模拟的复杂空间推理任务上表现有限，主要依赖于被动观察空间数据，缺乏主动的心理意象过程。</p>
<p><strong>Result:</strong> 在多个具有挑战性的基准测试中，SpatialDreamer表现优异，显著提升了MLLMs在复杂空间任务上的能力。</p>
<p><strong>Insight:</strong> 通过主动心理模拟和几何一致性约束的结合，可以显著提升MLLMs的空间推理能力，为类人的主动空间心理模拟提供了新思路。</p>
<p><strong>Abstract:</strong> Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.</p>
  </div>
</details>

<hr>
<h3 id="155-HLTCOE-Evaluation-Team-at-TREC-2025-VQA-Track-cs-CVPDF"><a href="#155-HLTCOE-Evaluation-Team-at-TREC-2025-VQA-Track-cs-CVPDF" class="headerlink" title="[155] HLTCOE Evaluation Team at TREC 2025: VQA Track cs.CVPDF"></a>[155] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07738">HLTCOE Evaluation Team at TREC 2025: VQA Track</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07738" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dengjia Zhang, Charles Weng, Katherine Guerrerio, Yi Lu, Kenton Murray</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了HLTCOE团队在TREC 2025 VQA赛道中提出的列表学习框架，通过结合生成和判别式排名方法，提升了答案生成的语义精度和排名一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视频问答（VQA）任务中，现有方法在答案生成和排序上的表现不够理想，尤其是在需要时序推理和语义消歧的场景下。</p>
<p><strong>Result:</strong> 实验显示，该方法在准确性和排名稳定性上均有显著提升，特别是在需要时序推理和语义消歧的问题上。</p>
<p><strong>Insight:</strong> 通过生成与判别式排名的结合，能够生成更连贯且细粒度的答案列表，为VQA任务的答案生成提供了新思路。</p>
<p><strong>Abstract:</strong> The HLTCOE Evaluation team participated in TREC VQA’s Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.</p>
  </div>
</details>

<hr>
<h3 id="156-DiffusionDriveV2-Reinforcement-Learning-Constrained-Truncated-Diffusion-Modeling-in-End-to-End-Autonomous-Driving-cs-CVPDF"><a href="#156-DiffusionDriveV2-Reinforcement-Learning-Constrained-Truncated-Diffusion-Modeling-in-End-to-End-Autonomous-Driving-cs-CVPDF" class="headerlink" title="[156] DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving cs.CVPDF"></a>[156] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07745">DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07745" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jialv Zou, Shaoyu Chen, Bencheng Liao, Zhiyu Zheng, Yuehao Song</span></p>
<p><strong>TL;DR:</strong> DiffusionDriveV2 通过强化学习约束扩散模型，解决了端到端自动驾驶中多样性（多样性）与高质量行为之间的权衡问题，提出了新颖的噪声设计与优势估计方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了解决扩散模型在端到端自动驾驶中容易产生保守和同质化行为（模式坍塌）的问题，同时平衡多样性与高质量行为。</p>
<p><strong>Result:</strong> 在NAVSIM v1和v2数据集上分别达到91.2 PDMS和85.5 EPDMS，突破了多样性-高质量的权衡问题。</p>
<p><strong>Insight:</strong> 强化学习可以有效约束扩散模型的低质量模式，同时保留多模态特性；锚点设计是解决意图多样性问题的关键。</p>
<p><strong>Abstract:</strong> Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at <a target="_blank" rel="noopener" href="https://github.com/hustvl/DiffusionDriveV2">https://github.com/hustvl/DiffusionDriveV2</a></p>
  </div>
</details>

<hr>
<h3 id="157-Unison-A-Fully-Automatic-Task-Universal-and-Low-Cost-Framework-for-Unified-Understanding-and-Generation-cs-CVPDF"><a href="#157-Unison-A-Fully-Automatic-Task-Universal-and-Low-Cost-Framework-for-Unified-Understanding-and-Generation-cs-CVPDF" class="headerlink" title="[157] Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation cs.CVPDF"></a>[157] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07747">Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07747" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shihao Zhao, Yitong Chen, Zeyinzi Jiang, Bojia Zi, Shaozhe Hao</span></p>
<p><strong>TL;DR:</strong> Unison是一个低成本、全自动、任务通用的框架，用于统一的多模态理解和生成，通过两阶段方法覆盖广泛任务，并自动解析用户意图和任务参数。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有统一多模态方法或成本高昂，或任务覆盖有限且需手动配置参数，缺乏自动化能力，无法处理输入元信息。</p>
<p><strong>Result:</strong> 仅用50 GPU小时和500k样本即可实现多种任务的高性能表现，自动任务识别和参数提取准确。</p>
<p><strong>Insight:</strong> 统一多模态任务可通过低成本两阶段方法实现自动化，任务识别和元信息提取是关键。</p>
<p><strong>Abstract:</strong> Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.</p>
  </div>
</details>

<hr>
<h3 id="158-Modality-Aware-Bias-Mitigation-and-Invariance-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification-cs-CVPDF"><a href="#158-Modality-Aware-Bias-Mitigation-and-Invariance-Learning-for-Unsupervised-Visible-Infrared-Person-Re-Identification-cs-CVPDF" class="headerlink" title="[158] Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification cs.CVPDF"></a>[158] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07760">Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07760" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Menglin Wang, Xiaojin Gong, Jiachen Li, Genlin Ji</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种无监督可见光-红外行人重识别方法，通过模态感知的Jaccard距离和全局原型对齐策略，解决了跨模态偏差和特征学习问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于可见光和红外模态之间的显著差异，无监督跨模态行人重识别的核心挑战在于可靠的跨模态关联估计。现有方法通常基于局部聚类，容易传播误差并忽视了全局实例关系。</p>
<p><strong>Result:</strong> 在基准数据集上实现了最先进的性能，显著优于现有方法。</p>
<p><strong>Insight:</strong> 全局关联和模态不变特征学习是提升无监督跨模态行人重识别性能的关键。</p>
<p><strong>Abstract:</strong> Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a &#96;split-and-contrast’ strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.</p>
  </div>
</details>

<hr>
<h3 id="159-GorillaWatch-An-Automated-System-for-In-the-Wild-Gorilla-Re-Identification-and-Population-Monitoring-cs-CVPDF"><a href="#159-GorillaWatch-An-Automated-System-for-In-the-Wild-Gorilla-Re-Identification-and-Population-Monitoring-cs-CVPDF" class="headerlink" title="[159] GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring cs.CVPDF"></a>[159] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07776">GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07776" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maximilian Schall, Felix Leonard Knöfel, Noah Elias König, Jan Jonas Kubeler, Maximilian von Klinski</span></p>
<p><strong>TL;DR:</strong> GorillaWatch 是一个自动化系统，用于野外观测中的大猩猩重识别和种群监测，通过引入大规模数据集和端到端流程，结合自监督预训练和注意力机制验证，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前濒危西部低地大猩猩的监测依赖大量人工处理摄像机陷阱视频，自动化面临的主要挑战是缺乏适合训练深度学习模型的大规模野外数据集。</p>
<p><strong>Result:</strong> 大规模图像主干特征聚合优于专用视频架构，自监督和验证方法显著提升性能，所有代码和数据集已公开。</p>
<p><strong>Insight:</strong> 1) 野外数据集对模型泛化至关重要；2) 自监督预训练可有效利用无标注数据；3) 注意力机制验证增强模型科学可信度。</p>
<p><strong>Abstract:</strong> Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, “in-the-wild” video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species</p>
  </div>
</details>

<hr>
<h3 id="160-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory-cs-CVPDF"><a href="#160-OneStory-Coherent-Multi-Shot-Video-Generation-with-Adaptive-Memory-cs-CVPDF" class="headerlink" title="[160] OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory cs.CVPDF"></a>[160] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07802">OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07802" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhaochong An, Menglin Jia, Haonan Qiu, Zijian Zhou, Xiaoke Huang</span></p>
<p><strong>TL;DR:</strong> OneStory提出了一种用于多镜头视频生成（MSV）的新方法，通过自适应内存和全局上下文建模，实现了连贯且可控的长叙事视频生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的MSV方法在建模长跨镜头上下文时表现不足，往往依赖有限的时序窗口或单关键帧条件，导致复杂叙事下的性能下降。</p>
<p><strong>Result:</strong> 在60K数据集上微调后，OneStory在文本和图像条件下的叙事一致性方面达到了SOTA。</p>
<p><strong>Insight:</strong> 全局上下文建模和紧凑条件是实现长视频连贯性的关键，自适应内存机制提高了跨镜头语义关联的建模能力。</p>
<p><strong>Abstract:</strong> Storytelling in real-world videos often unfolds through multiple shots – discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.</p>
  </div>
</details>

<hr>
<h3 id="161-Lang3D-XL-Language-Embedded-3D-Gaussians-for-Large-scale-Scenes-cs-CV-cs-GRPDF"><a href="#161-Lang3D-XL-Language-Embedded-3D-Gaussians-for-Large-scale-Scenes-cs-CV-cs-GRPDF" class="headerlink" title="[161] Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes cs.CV | cs.GRPDF"></a>[161] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07807">Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07807" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shai Krakovsky, Gal Fiebelman, Sagie Benaim, Hadar Averbuch-Elor</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新型方法Lang3D-XL，通过将语言嵌入3D高斯表示中，解决了大规模场景下语义特征对齐和效率问题，提升了语义理解和交互能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在大规模场景中由于语义特征对齐不佳和效率问题，难以从海量互联网数据中学习。为此，论文提出了一种更高效且语义对齐的方法来改进3D场景的语言嵌入。</p>
<p><strong>Result:</strong> 在HolyScenes数据集上的实验表明，该方法在性能和效率上均优于现有方法。</p>
<p><strong>Insight:</strong> 极低维语义特征和多分辨率编码的结合为大规模场景的语义嵌入提供了高效解决方案，同时强调了语义对齐的重要性。</p>
<p><strong>Abstract:</strong> Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.</p>
  </div>
</details>

<hr>
<h3 id="162-WorldReel-4D-Video-Generation-with-Consistent-Geometry-and-Motion-Modeling-cs-CV-cs-AIPDF"><a href="#162-WorldReel-4D-Video-Generation-with-Consistent-Geometry-and-Motion-Modeling-cs-CV-cs-AIPDF" class="headerlink" title="[162] WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling cs.CV | cs.AIPDF"></a>[162] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07821">WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07821" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaoheng Fang, Hanwen Jiang, Yunpeng Bai, Niloy J. Mitra, Qixing Huang</span></p>
<p><strong>TL;DR:</strong> WorldReel是一种4D视频生成器，通过联合生成RGB帧和4D场景表示（包括点图、相机轨迹和密集流映射），实现了时空一致的几何和外观建模。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频生成器虽然在视觉上逼真，但在3D一致性上存在不足，WorldReel旨在解决这一问题，推动视频生成向4D一致性建模发展。</p>
<p><strong>Result:</strong> 实验表明，WorldReel在几何一致性、运动连贯性和减少视觉伪影方面优于现有方法。</p>
<p><strong>Insight:</strong> WorldReel的成功展示了4D一致性建模的重要性，为未来视频生成和场景理解提供了新方向。</p>
<p><strong>Abstract:</strong> Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.</p>
  </div>
</details>

<hr>
<h3 id="163-OpenVE-3M-A-Large-Scale-High-Quality-Dataset-for-Instruction-Guided-Video-Editing-cs-CVPDF"><a href="#163-OpenVE-3M-A-Large-Scale-High-Quality-Dataset-for-Instruction-Guided-Video-Editing-cs-CVPDF" class="headerlink" title="[163] OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing cs.CVPDF"></a>[163] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07826">OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07826" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoyang He, Jie Wang, Jiangning Zhang, Zhucun Xue, Xingyuan Bu</span></p>
<p><strong>TL;DR:</strong> OpenVE-3M 是一个开源、大规模、高质量的指令引导视频编辑数据集，填补了该领域数据集的空白。它包含空间对齐和非空间对齐的编辑类型，并通过严格的质量筛选生成。此外，还提出了基准评测 OpenVE-Bench 和高效模型 OpenVE-Edit，性能优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前指令引导的图像编辑数据集质量和多样性不断提升，但视频编辑领域缺乏大规模高质量数据集，OpenVE-3M 旨在解决这一问题。</p>
<p><strong>Result:</strong> OpenVE-Edit 在 OpenVE-Bench 上表现优异，优于现有开源模型（包括一个 14B 基线模型），成为新的 SOTA。</p>
<p><strong>Insight:</strong> 高质量数据集和统一评测基准是推动视频编辑技术进步的关键。OpenVE-3M 为指令引导的视频编辑提供了重要资源。</p>
<p><strong>Abstract:</strong> The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at <a target="_blank" rel="noopener" href="https://github.com/lewandofskee/OpenVE">https://github.com/lewandofskee/OpenVE</a>.</p>
  </div>
</details>

<hr>
<h3 id="164-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation-cs-CVPDF"><a href="#164-UnityVideo-Unified-Multi-Modal-Multi-Task-Learning-for-Enhancing-World-Aware-Video-Generation-cs-CVPDF" class="headerlink" title="[164] UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation cs.CVPDF"></a>[164] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07831">UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07831" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiehui Huang, Yuechen Zhang, Xu He, Yuan Gao, Zhi Cen</span></p>
<p><strong>TL;DR:</strong> UnityVideo是一个统一的多模态多任务学习框架，通过联合学习多种模态（如分割掩码、人体骨骼等）和训练范式，显著提升了视频生成的世界感知能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频生成模型受限于单模态条件，缺乏跨模态交互和模态多样性，无法全面理解世界信息。</p>
<p><strong>Result:</strong> UnityVideo在视频质量、一致性和物理世界对齐方面表现优异，并加速了收敛。</p>
<p><strong>Insight:</strong> 多模态联合训练能够显著提升视频生成的世界感知能力和泛化性能。</p>
<p><strong>Abstract:</strong> Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/UnityVideo">https://github.com/dvlab-research/UnityVideo</a></p>
  </div>
</details>

<hr>
<h3 id="165-Relational-Visual-Similarity-cs-CV-cs-AI-cs-LGPDF"><a href="#165-Relational-Visual-Similarity-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[165] Relational Visual Similarity cs.CV | cs.AI | cs.LGPDF"></a>[165] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07833">Relational Visual Similarity</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07833" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thao Nguyen, Sicheng Mo, Krishna Kumar Singh, Yilin Wang, Jing Shi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了关系视觉相似性的概念，指出现有视觉相似性指标（如LPIPS、CLIP、DINO）仅关注感知属性相似性，而忽视了人类能够感知的丰富关系相似性。作者通过构建一个匿名描述关系逻辑的数据集，并微调视觉-语言模型，首次实现了对图像关系相似性的测量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类的视觉感知不仅基于属性相似性（如颜色、形状），还包括关系相似性（如结构与功能的对应关系）。现有视觉相似性指标无法捕捉这种关系逻辑，而论文旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验表明，现有视觉相似性模型无法捕捉关系相似性，而论文提出的模型首次能够量化图像之间的这种关系逻辑。</p>
<p><strong>Insight:</strong> 关系相似性是视觉计算中被忽视但关键的一环，未来研究方向可以进一步探索其在视觉推理、类比学习等领域的应用。</p>
<p><strong>Abstract:</strong> Humans do not just see attribute similarity – we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach’s skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized – describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it – revealing a critical gap in visual computing.</p>
  </div>
</details>

<hr>
<h3 id="166-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering-cs-CVPDF"><a href="#166-Voxify3D-Pixel-Art-Meets-Volumetric-Rendering-cs-CVPDF" class="headerlink" title="[166] Voxify3D: Pixel Art Meets Volumetric Rendering cs.CVPDF"></a>[166] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07834">Voxify3D: Pixel Art Meets Volumetric Rendering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07834" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi-Chuan Huang, Jiewen Chan, Hao-Jen Chien, Yu-Lun Liu</span></p>
<p><strong>TL;DR:</strong> Voxify3D是一个两阶段可微分框架，将3D网格优化与2D像素艺术监督结合，解决了体素艺术生成中的几何抽象、语义保留和颜色一致性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前方法在从3D网格生成体素艺术时，要么过度简化几何形状，要么无法满足像素级精确和调色板约束的美学要求。</p>
<p><strong>Result:</strong> 实验显示优于其他方法（CLIP-IQA 37.12，用户偏好77.90%），支持多样角色和可控抽象。</p>
<p><strong>Insight:</strong> 通过正交投影和CLIP对齐，可在极强离散化下保留语义，同时满足像素艺术的美学要求。</p>
<p><strong>Abstract:</strong> Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: <a target="_blank" rel="noopener" href="https://yichuanh.github.io/Voxify-3D/">https://yichuanh.github.io/Voxify-3D/</a></p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="167-Empathy-by-Design-Aligning-Large-Language-Models-for-Healthcare-Dialogue-cs-CL-cs-AIPDF"><a href="#167-Empathy-by-Design-Aligning-Large-Language-Models-for-Healthcare-Dialogue-cs-CL-cs-AIPDF" class="headerlink" title="[167] Empathy by Design: Aligning Large Language Models for Healthcare Dialogue cs.CL | cs.AIPDF"></a>[167] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06097">Empathy by Design: Aligning Large Language Models for Healthcare Dialogue</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06097" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Emre Umucu, Guillermina Solis, Leon Garza, Emilia Rivas, Beatrice Lee</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于直接偏好优化（DPO）的方法，用于调整大型语言模型（LLM）在医疗保健对话中的表现，以提升事实准确性、语义连贯性和人文关怀特性（如同理心）。通过实验验证，该方法在语义对齐和人类评价得分上优于基线和商业替代方案。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 通用大型语言模型在医疗保健场景中存在事实不可靠和缺乏同理心的问题，限制了其在敏感上下文中的应用潜力。</p>
<p><strong>Result:</strong> 实验表明，经过DPO调整的模型在语义对齐、事实准确性和人类评价得分上优于基线和商业系统（如谷歌医疗对话系统）。</p>
<p><strong>Insight:</strong> 偏好驱动的对齐方法比传统的基于强化学习的对齐更高效，尤其适用于需要高度敏感性和准确性的医疗保健场景。</p>
<p><strong>Abstract:</strong> General-purpose large language models (LLMs) have demonstrated remarkable generative and reasoning capabilities but remain limited in healthcare and caregiving applications due to two key deficiencies: factual unreliability and a lack of empathetic communication. These shortcomings pose significant risks in sensitive contexts where users, particularly non-professionals and caregivers, seek medically relevant guidance or emotional reassurance. To address these challenges, we introduce a Direct Preference Optimization (DPO)-based alignment framework designed to improve factual correctness, semantic coherence, and human-centric qualities such as empathy, politeness, and simplicity in caregiver-patient dialogues. Our approach fine-tunes domain-adapted LLMs using pairwise preference data, where preferred responses reflect supportive and accessible communication styles while rejected ones represent prescriptive or overly technical tones. This direct optimization method aligns model outputs with human preferences more efficiently than traditional reinforcement-learning-based alignment. Empirical evaluations across multiple open and proprietary LLMs show that our DPO-tuned models achieve higher semantic alignment, improved factual accuracy, and stronger human-centric evaluation scores compared to baseline and commercial alternatives such as Google medical dialogue systems. These improvements demonstrate that preference-based alignment offers a scalable and transparent pathway toward developing trustworthy, empathetic, and clinically informed AI assistants for caregiver and healthcare communication. Our open-source code is available at: <a target="_blank" rel="noopener" href="https://github.com/LeonG19/Empathy-by-Design">https://github.com/LeonG19/Empathy-by-Design</a></p>
  </div>
</details>

<hr>
<h3 id="168-Do-You-Feel-Comfortable-Detecting-Hidden-Conversational-Escalation-in-AI-Chatbots-cs-CL-cs-AIPDF"><a href="#168-Do-You-Feel-Comfortable-Detecting-Hidden-Conversational-Escalation-in-AI-Chatbots-cs-CL-cs-AIPDF" class="headerlink" title="[168] Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots cs.CL | cs.AIPDF"></a>[168] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06193">Do You Feel Comfortable? Detecting Hidden Conversational Escalation in AI Chatbots</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06193" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jihyung Park, Saleh Afroogh, Junfeng Jiao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种轻量级框架GAUGE，用于实时检测AI聊天机器人中隐藏的对话升级（情感强化或情感漂移），弥补了传统毒性过滤器的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLMs在日常互动中的广泛应用，隐藏的情感强化或情感漂移可能导致用户逐渐陷入痛苦，而现有的毒性过滤器无法检测这种隐性伤害。</p>
<p><strong>Result:</strong> GAUGE能够有效检测传统方法难以捕捉的隐性情感伤害。</p>
<p><strong>Insight:</strong> 情感状态的动态监控对于AI聊天机器人的安全性至关重要，GAUGE为实时情感干预提供了新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLM) are increasingly integrated into everyday interactions, serving not only as information assistants but also as emotional companions. Even in the absence of explicit toxicity, repeated emotional reinforcement or affective drift can gradually escalate distress in a form of \textit{implicit harm} that traditional toxicity filters fail to detect. Existing guardrail mechanisms often rely on external classifiers or clinical rubrics that may lag behind the nuanced, real-time dynamics of a developing conversation. To address this gap, we propose GAUGE (Guarding Affective Utterance Generation Escalation), a lightweight, logit-based framework for the real-time detection of hidden conversational escalation. GAUGE measures how an LLM’s output probabilistically shifts the affective state of a dialogue.</p>
  </div>
</details>

<hr>
<h3 id="169-Nanbeige4-3B-Technical-Report-Exploring-the-Frontier-of-Small-Language-Models-cs-CLPDF"><a href="#169-Nanbeige4-3B-Technical-Report-Exploring-the-Frontier-of-Small-Language-Models-cs-CLPDF" class="headerlink" title="[169] Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models cs.CLPDF"></a>[169] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06266">Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06266" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng</span></p>
<p><strong>TL;DR:</strong> 本文介绍了Nanbeige4-3B，一种高性能的小规模语言模型，通过创新的预训练调度器、联合后训练机制和双偏好蒸馏方法，显著提升了模型性能，并在多类基准测试中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索小规模语言模型的性能极限，通过改进训练和数据优化方法，使其在性能和效率上与大模型竞争。</p>
<p><strong>Result:</strong> Nanbeige4-3B在多项基准测试中优于同类小模型，并能与更大的模型竞争。</p>
<p><strong>Insight:</strong> 通过精细的训练调度、高质量数据优化和多阶段强化学习，小模型也能在复杂任务中表现优异。</p>
<p><strong>Abstract:</strong> We present Nanbeige4-3B, a family of small-scale but high-performing language models. Pretrained on 23T high-quality tokens and finetuned on over 30 million diverse instructions, we extend the boundary of the scaling law for small language models. In pre-training, we design a Fine-Grained Warmup-Stable-Decay (FG-WSD) training scheduler, which progressively refines data mixtures across stages to boost model performance. In post-training, to improve the quality of the SFT data, we design a joint mechanism that integrates deliberative generation refinement and chain-of-thought reconstruction, yielding substantial gains on complex tasks. Following SFT, we employ our flagship reasoning model to distill Nanbeige4-3B through our proposed Dual Preference Distillation (DPD) method, which leads to further performance gains. Finally, a multi-stage reinforcement learning phase was applied, leveraging verifiable rewards and preference modeling to strengthen abilities on both reasoning and human alignment. Extensive evaluations show that Nanbeige4-3B not only significantly outperforms models of comparable parameter scale but also rivals much larger models across a wide range of benchmarks. The model checkpoints are available at <a target="_blank" rel="noopener" href="https://huggingface.co/Nanbeige">https://huggingface.co/Nanbeige</a>.</p>
  </div>
</details>

<hr>
<h3 id="170-Modeling-Contextual-Passage-Utility-for-Multihop-Question-Answering-cs-CLPDF"><a href="#170-Modeling-Contextual-Passage-Utility-for-Multihop-Question-Answering-cs-CLPDF" class="headerlink" title="[170] Modeling Contextual Passage Utility for Multihop Question Answering cs.CLPDF"></a>[170] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06464">Modeling Contextual Passage Utility for Multihop Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06464" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Akriti Jain, Aparna Garimella</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种轻量级方法，通过建模上下文段落效用（考虑段落间依赖关系）来提升多跳问答性能，利用了高级推理模型的推理轨迹生成训练数据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多跳问答需要从多个段落中综合信息，但现有方法独立建模段落效用，忽略了上下文依赖关系。冗余段落会增加噪声和答案错误。</p>
<p><strong>Result:</strong> 相比基于相关性的重排序方法，该方法在重排序和下游问答任务中表现更优。</p>
<p><strong>Insight:</strong> 上下文依赖关系在多跳问答中是关键；轻量级模型结合高级推理模型的轨迹可以提升性能。</p>
<p><strong>Abstract:</strong> Multihop Question Answering (QA) requires systems to identify and synthesize information from multiple text passages. While most prior retrieval methods assist in identifying relevant passages for QA, further assessing the utility of the passages can help in removing redundant ones, which may otherwise add to noise and inaccuracies in the generated answers. Existing utility prediction approaches model passage utility independently, overlooking a critical aspect of multihop reasoning: the utility of a passage can be context-dependent, influenced by its relation to other passages - whether it provides complementary information or forms a crucial link in conjunction with others. In this paper, we propose a lightweight approach to model contextual passage utility, accounting for inter-passage dependencies. We fine-tune a small transformer-based model to predict passage utility scores for multihop QA. We leverage the reasoning traces from an advanced reasoning model to capture the order in which passages are used to answer a question and obtain synthetic training data. Through comprehensive experiments, we demonstrate that our utility-based scoring of retrieved passages leads to improved reranking and downstream QA performance compared to relevance-based reranking methods.</p>
  </div>
</details>

<hr>
<h3 id="171-Knowing-What’s-Missing-Assessing-Information-Sufficiency-in-Question-Answering-cs-CLPDF"><a href="#171-Knowing-What’s-Missing-Assessing-Information-Sufficiency-in-Question-Answering-cs-CLPDF" class="headerlink" title="[171] Knowing What’s Missing: Assessing Information Sufficiency in Question Answering cs.CLPDF"></a>[171] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06476">Knowing What’s Missing: Assessing Information Sufficiency in Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06476" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Akriti Jain, Aparna Garimella</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种结构化框架（Identify-then-Verify），通过让模型首先推断缺失的信息并验证其是否存在，从而更准确地评估问答系统中上下文信息是否充分。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的问答系统在事实性问题上表现较好，但在需要推理的问题上经常失败，因此需要一种更可靠的方法来评估上下文中信息的充分性。</p>
<p><strong>Result:</strong> 在多个多跳和事实性问答数据集上的实验表明，该方法能够更准确地判断信息充分性，并清晰表达信息缺口。</p>
<p><strong>Insight:</strong> 通过明确推理缺失信息并验证其存在性，模型能够更可靠地评估上下文的充分性，这对复杂问答任务的可靠性提升具有重要意义。</p>
<p><strong>Abstract:</strong> Determining whether a provided context contains sufficient information to answer a question is a critical challenge for building reliable question-answering systems. While simple prompting strategies have shown success on factual questions, they frequently fail on inferential ones that require reasoning beyond direct text extraction. We hypothesize that asking a model to first reason about what specific information is missing provides a more reliable, implicit signal for assessing overall sufficiency. To this end, we propose a structured Identify-then-Verify framework for robust sufficiency modeling. Our method first generates multiple hypotheses about missing information and establishes a semantic consensus. It then performs a critical verification step, forcing the model to re-examine the source text to confirm whether this information is truly absent. We evaluate our method against established baselines across diverse multi-hop and factual QA datasets. The results demonstrate that by guiding the model to justify its claims about missing information, our framework produces more accurate sufficiency judgments while clearly articulating any information gaps.</p>
  </div>
</details>

<hr>
<h3 id="172-Classifying-German-Language-Proficiency-Levels-Using-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#172-Classifying-German-Language-Proficiency-Levels-Using-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[172] Classifying German Language Proficiency Levels Using Large Language Models cs.CL | cs.AIPDF"></a>[172] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06483">Classifying German Language Proficiency Levels Using Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06483" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Elias-Leander Ahlers, Witold Brunsmann, Malte Schilling</span></p>
<p><strong>TL;DR:</strong> TL;DR: 论文提出利用大语言模型（LLMs）对德语文本进行CEFR语言能力等级分类，结合合成数据构建数据集，并评估了提示工程、微调和探测方法，性能优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机：语言能力等级的自动化评估对教育至关重要，现有方法效果有限，LLMs在文本分类任务中的潜力尚未充分探索。</p>
<p><strong>Result:</strong> 结果：方法在所有实验中均优于现有技术，证明了LLMs在CEFR分类任务中的可靠性和扩展性。</p>
<p><strong>Insight:</strong> 洞察：LLMs的微调和内部状态探测方法在语言能力分类任务中具有显著优势，未来可探索其他语言的类似应用。</p>
<p><strong>Abstract:</strong> Assessing language proficiency is essential for education, as it enables instruction tailored to learners needs. This paper investigates the use of Large Language Models (LLMs) for automatically classifying German texts according to the Common European Framework of Reference for Languages (CEFR) into different proficiency levels. To support robust training and evaluation, we construct a diverse dataset by combining multiple existing CEFR-annotated corpora with synthetic data. We then evaluate prompt-engineering strategies, fine-tuning of a LLaMA-3-8B-Instruct model and a probing-based approach that utilizes the internal neural state of the LLM for classification. Our results show a consistent performance improvement over prior methods, highlighting the potential of LLMs for reliable and scalable CEFR classification.</p>
  </div>
</details>

<hr>
<h3 id="173-PersonaMem-v2-Towards-Personalized-Intelligence-via-Learning-Implicit-User-Personas-and-Agentic-Memory-cs-CLPDF"><a href="#173-PersonaMem-v2-Towards-Personalized-Intelligence-via-Learning-Implicit-User-Personas-and-Agentic-Memory-cs-CLPDF" class="headerlink" title="[173] PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory cs.CLPDF"></a>[173] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06688">PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06688" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bowen Jiang, Yuan Yuan, Maohao Shen, Zhuoqun Hao, Zhangchen Xu</span></p>
<p><strong>TL;DR:</strong> PersonaMem-v2是一个用于LLM个性化的先进数据集，模拟了1000次真实用户与聊天机器人的交互，研究了强化微调如何提升模型的长上下文推理能力，并开发了一个代理记忆框架，实现了55%的隐式个性化准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 个性化是AI能力与对齐的下一个重要里程碑。现有的LLM在隐式个性化任务上表现不佳，长上下文推理成为瓶颈。</p>
<p><strong>Result:</strong> Qwen3-4B经过强化微调后，隐式个性化准确率达到53%，代理记忆框架进一步提升至55%，同时减少了16倍的输入令牌。</p>
<p><strong>Insight:</strong> 长上下文推理是隐式个性化的瓶颈；代理记忆可以高效地维护用户偏好，是未来个性化AI的可扩展路径。</p>
<p><strong>Abstract:</strong> Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time.   In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.</p>
  </div>
</details>

<hr>
<h3 id="174-Think-While-Generating-On-the-Fly-Reasoning-for-Personalized-Long-Form-Generation-cs-CLPDF"><a href="#174-Think-While-Generating-On-the-Fly-Reasoning-for-Personalized-Long-Form-Generation-cs-CLPDF" class="headerlink" title="[174] Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation cs.CLPDF"></a>[174] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06690">Think-While-Generating: On-the-Fly Reasoning for Personalized Long-Form Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06690" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chengbing Wang, Yang Zhang, Wenjie Wang, Xiaoyan Zhao, Fuli Feng</span></p>
<p><strong>TL;DR:</strong> 论文提出了FlyThinker框架，实现个性化长文本生成的动态推理，通过并行生成潜在推理与内容，提升了生成质量和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在个性化长文本生成中难以动态推理，无法捕捉隐式偏好或适应内容演化。</p>
<p><strong>Result:</strong> 实验表明FlyThinker在个性化生成任务中效果更优，同时保持高效训练和推理。</p>
<p><strong>Insight:</strong> 动态推理是长文本个性化的关键，脱离传统‘先推理后生成’的静态模式可显著提升生成质量和适应性。</p>
<p><strong>Abstract:</strong> Preference alignment has enabled large language models (LLMs) to better reflect human expectations, but current methods mostly optimize for population-level preferences, overlooking individual users. Personalization is essential, yet early approaches-such as prompt customization or fine-tuning-struggle to reason over implicit preferences, limiting real-world effectiveness. Recent “think-then-generate” methods address this by reasoning before response generation. However, they face challenges in long-form generation: their static one-shot reasoning must capture all relevant information for the full response generation, making learning difficult and limiting adaptability to evolving content. To address this issue, we propose FlyThinker, an efficient “think-while-generating” framework for personalized long-form generation. FlyThinker employs a separate reasoning model that generates latent token-level reasoning in parallel, which is fused into the generation model to dynamically guide response generation. This design enables reasoning and generation to run concurrently, ensuring inference efficiency. In addition, the reasoning model is designed to depend only on previous responses rather than its own prior outputs, which preserves training parallelism across different positions-allowing all reasoning tokens for training data to be produced in a single forward pass like standard LLM training, ensuring training efficiency. Extensive experiments on real-world benchmarks demonstrate that FlyThinker achieves better personalized generation while keeping training and inference efficiency.</p>
  </div>
</details>

<hr>
<h3 id="175-TopiCLEAR-Topic-extraction-by-CLustering-Embeddings-with-Adaptive-dimensional-Reduction-cs-CLPDF"><a href="#175-TopiCLEAR-Topic-extraction-by-CLustering-Embeddings-with-Adaptive-dimensional-Reduction-cs-CLPDF" class="headerlink" title="[175] TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction cs.CLPDF"></a>[175] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06694">TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06694" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aoi Fujita, Taichi Yamamoto, Yuri Nakayama, Ryota Kobayashi</span></p>
<p><strong>TL;DR:</strong> TopiCLEAR是一种基于嵌入聚类和自适应降维的主题提取方法，专为处理社交媒体短文本设计，通过SBERT嵌入和GMM聚类迭代优化主题，显著优于现有基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统主题模型在处理社交媒体短文本时效果不佳，因数据稀疏和语义碎片化。TopiCLEAR旨在解决这一问题。</p>
<p><strong>Result:</strong> 在四个数据集上优于七种基线方法，主题相似度和可解释性显著提升。</p>
<p><strong>Insight:</strong> 直接处理原始文本且无需预处理的设计简化了流程，自适应降维和迭代优化提升了主题提取的准确性。</p>
<p><strong>Abstract:</strong> Rapid expansion of social media platforms such as X (formerly Twitter), Facebook, and Reddit has enabled large-scale analysis of public perceptions on diverse topics, including social issues, politics, natural disasters, and consumer sentiment. Topic modeling is a widely used approach for uncovering latent themes in text data, typically framed as an unsupervised classification task. However, traditional models, originally designed for longer and more formal documents, struggle with short social media posts due to limited co-occurrence statistics, fragmented semantics, inconsistent spelling, and informal language. To address these challenges, we propose a new method, TopiCLEAR: Topic extraction by CLustering Embeddings with Adaptive dimensional Reduction. Specifically, each text is embedded using Sentence-BERT (SBERT) and provisionally clustered using Gaussian Mixture Models (GMM). The clusters are then refined iteratively using a supervised projection based on linear discriminant analysis, followed by GMM-based clustering until convergence. Notably, our method operates directly on raw text, eliminating the need for preprocessing steps such as stop word removal. We evaluate our approach on four diverse datasets, 20News, AgNewsTitle, Reddit, and TweetTopic, each containing human-labeled topic information. Compared with seven baseline methods, including a recent SBERT-based method and a zero-shot generative AI method, our approach achieves the highest similarity to human-annotated topics, with significant improvements for both social media posts and online news articles. Additionally, qualitative analysis shows that our method produces more interpretable topics, highlighting its potential for applications in social media data and web content analytics.</p>
  </div>
</details>

<hr>
<h3 id="176-Becoming-Experienced-Judges-Selective-Test-Time-Learning-for-Evaluators-cs-CL-cs-AI-cs-LGPDF"><a href="#176-Becoming-Experienced-Judges-Selective-Test-Time-Learning-for-Evaluators-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[176] Becoming Experienced Judges: Selective Test-Time Learning for Evaluators cs.CL | cs.AI | cs.LGPDF"></a>[176] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06751">Becoming Experienced Judges: Selective Test-Time Learning for Evaluators</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06751" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Seungyeon Jwa, Daechul Ahn, Reokyoung Kim, Dongyeop Kang, Jonghyun Choi</span></p>
<p><strong>TL;DR:</strong> 论文提出了Learning While Evaluating (LWE)框架，通过选择性测试时学习改进评估模型的表现，特别关注自我不一致的情况。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型评估方法通常独立处理每个样本且使用固定提示，忽略了累积经验的机会和样本特定的评估需求。</p>
<p><strong>Result:</strong> 在成对比较任务中，Selective LWE超越基线方法，证明了选择性更新能显著提升评估性能。</p>
<p><strong>Insight:</strong> 评估器可以通过测试时学习逐步改进，且选择性更新能在保证性能的同时显著降低计算成本。</p>
<p><strong>Abstract:</strong> Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.</p>
  </div>
</details>

<hr>
<h3 id="177-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs-cs-CL-cs-AIPDF"><a href="#177-From-Next-Token-to-Next-Block-A-Principled-Adaptation-Path-for-Diffusion-LLMs-cs-CL-cs-AIPDF" class="headerlink" title="[177] From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs cs.CL | cs.AIPDF"></a>[177] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06776">From Next-Token to Next-Block: A Principled Adaptation Path for Diffusion LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06776" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuchuan Tian, Yuchen Liang, Jiacheng Sun, Shuo Zhang, Guangwen Yang</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种从自回归（AR）语言模型到块扩散（Block-Diffusion）语言模型的原则性适配路径，解决了传统方法中因AR因果性与块双向性不匹配的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自回归解码（AR）因其顺序性成为生成吞吐量的瓶颈，而扩散语言模型（DLMs）支持并行生成和块内双向推理，但从头训练DLMs成本高昂且浪费现有AR模型的知识。</p>
<p><strong>Result:</strong> NBDiff-7B继承了长上下文建模和推理能力，在7B级DLMs中表现最优，尤其在常识、数学和代码任务上显著优于基线。</p>
<p><strong>Insight:</strong> AR到块扩散的原则性适配是一种高效且计算友好的替代方案，无需从头训练DLMs，同时保留了预训练模型的知识和能力。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) excel at generation but dominant autoregressive (AR) decoding is inherently sequential, creating a throughput bottleneck. Diffusion Language Models (DLMs)–especially block-wise variants–enable parallel generation and intra-block bidirectional reasoning, yet training large DLMs from scratch is costly and wastes the knowledge in mature AR checkpoints. Prior “adaptation” attempts either modify logits or randomly grow attention masks to full-sequence diffusion, or simply transplant AR weights into a block-diffusion recipe, leaving a fundamental mismatch between AR causality and block-wise bidirectionality unaddressed. We reframe adaptation as a intra-paradigm path from AR to Block-Diffusion by viewing AR as Block-Diffusion with blocksize&#x3D;1. Concretely, we design the pathway of adaptation as follows: we use a context-causal attention mask (causal in context, bidirectional only within the active block), an efficient parallel adaptation procedure, an auxiliary AR loss to maximize data utilization and retain pretrained knowledge, and gradual increment of the generation block size. The recipe integrates cleanly with masked block-diffusion and maintains train-inference consistency. Built on these components, NBDiff-7B (Base and Instruct) could inherit the long-context modeling and reasoning capabilities, and achieve state-of-the-art performance among the 7B-class DLMs, delivering strong gains on general-knowledge, math, and code benchmarks over strong baselines. These results demonstrate that principled AR-to-block-diffusion adaptation is an effective and compute-efficient alternative to training DLMs from scratch. Codes: <a target="_blank" rel="noopener" href="https://github.com/YuchuanTian/NBDiff">https://github.com/YuchuanTian/NBDiff</a>.</p>
  </div>
</details>

<hr>
<h3 id="178-CAuSE-Decoding-Multimodal-Classifiers-using-Faithful-Natural-Language-Explanation-cs-CL-cs-AIPDF"><a href="#178-CAuSE-Decoding-Multimodal-Classifiers-using-Faithful-Natural-Language-Explanation-cs-CL-cs-AIPDF" class="headerlink" title="[178] CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation cs.CL | cs.AIPDF"></a>[178] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06814">CAuSE: Decoding Multimodal Classifiers using Faithful Natural Language Explanation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06814" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dibyanayan Bandyopadhyay, Soham Bhattacharjee, Mohammed Hasanuzzaman, Asif Ekbal</span></p>
<p><strong>TL;DR:</strong> CAuSE是一个新颖的框架，用于为预训练的多模态分类器生成可信的自然语言解释（NLEs），通过因果抽象和模拟解释提高解释的忠实性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法中，自然语言解释（NLEs）虽然直观，但难以忠实捕捉多模态分类器的内部决策行为。CAuSE旨在解决这一忠实性问题。</p>
<p><strong>Result:</strong> CAuSE在忠实性度量上优于其他方法，并通过定性分析验证了其优势。</p>
<p><strong>Insight:</strong> CAuSE的可扩展性和忠实性使其在实际应用中具有潜力，但也揭示了其可能的失败案例。</p>
<p><strong>Abstract:</strong> Multimodal classifiers function as opaque black box models. While several techniques exist to interpret their predictions, very few of them are as intuitive and accessible as natural language explanations (NLEs). To build trust, such explanations must faithfully capture the classifier’s internal decision making behavior, a property known as faithfulness. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanations), a novel framework to generate faithful NLEs for any pretrained multimodal classifier. We demonstrate that CAuSE generalizes across datasets and models through extensive empirical evaluations. Theoretically, we show that CAuSE, trained via interchange intervention, forms a causal abstraction of the underlying classifier. We further validate this through a redesigned metric for measuring causal faithfulness in multimodal settings. CAuSE surpasses other methods on this metric, with qualitative analysis reinforcing its advantages. We perform detailed error analysis to pinpoint the failure cases of CAuSE. For replicability, we make the codes available at <a target="_blank" rel="noopener" href="https://github.com/newcodevelop/CAuSE">https://github.com/newcodevelop/CAuSE</a></p>
  </div>
</details>

<hr>
<h3 id="179-AquaFusionNet-Lightweight-VisionSensor-Fusion-Framework-for-Real-Time-Pathogen-Detection-and-Water-Quality-Anomaly-Prediction-on-Edge-Devices-cs-CL-cs-CVPDF"><a href="#179-AquaFusionNet-Lightweight-VisionSensor-Fusion-Framework-for-Real-Time-Pathogen-Detection-and-Water-Quality-Anomaly-Prediction-on-Edge-Devices-cs-CL-cs-CVPDF" class="headerlink" title="[179] AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices cs.CL | cs.CVPDF"></a>[179] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06848">AquaFusionNet: Lightweight VisionSensor Fusion Framework for Real-Time Pathogen Detection and Water Quality Anomaly Prediction on Edge Devices</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06848" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sepyan Purnama Kristanto, Lutfi Hakim, Hermansyah</span></p>
<p><strong>TL;DR:</strong> AquaFusionNet提出了一种轻量级的跨模态融合框架，用于边缘设备上实时检测病原体并预测水质异常，将微生物图像与传感器数据结合，提升了决策可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的水质监测工具无法实时捕获微生物污染与化学变化的动态关系，导致决策困难。AquaFusionNet旨在通过统一多模态信息解决这一问题。</p>
<p><strong>Result:</strong> 在实际部署中，污染物检测达到94.8% <a href="mailto:&#109;&#65;&#80;&#x40;&#48;&#46;&#53;">mAP@0.5</a>，异常预测准确率96.3%，功耗仅4.8W，优于其他轻量级检测器。</p>
<p><strong>Insight:</strong> 跨模态耦合可以显著减少单模态检测器在污浊、浑浊和光照不一致情况下的常见错误，提升了鲁棒性。</p>
<p><strong>Abstract:</strong> Evidence from many low and middle income regions shows that microbial contamination in small scale drinking water systems often fluctuates rapidly, yet existing monitoring tools capture only fragments of this behaviour. Microscopic imaging provides organism level visibility, whereas physicochemical sensors reveal shortterm changes in water chemistry; in practice, operators must interpret these streams separately, making realtime decision-making unreliable. This study introduces AquaFusionNet, a lightweight cross-modal framework that unifies both information sources inside a single edge deployable model. Unlike prior work that treats microscopic detection and water quality prediction as independent tasks, AquaFusionNet learns the statistical dependencies between microbial appearance and concurrent sensor dynamics through a gated crossattention mechanism designed specifically for lowpower hardware. The framework is trained on AquaMicro12K, a new dataset comprising 12,846 annotated 1000 micrographs curated for drinking water contexts, an area where publicly accessible microscopic datasets are scarce. Deployed for six months across seven facilities in East Java, Indonesia, the system processed 1.84 million frames and consistently detected contamination events with 94.8% <a href="mailto:&#x6d;&#65;&#80;&#64;&#48;&#46;&#x35;">mAP@0.5</a> and 96.3% anomaly prediction accuracy, while operating at 4.8 W on a Jetson Nano. Comparative experiments against representative lightweight detectors show that AquaFusionNet provides higher accuracy at comparable or lower power, and field results indicate that cross-modal coupling reduces common failure modes of unimodal detectors, particularly under fouling, turbidity spikes, and inconsistent illumination. All models, data, and hardware designs are released openly to facilitate replication and adaptation in decentralized water safety infrastructures.</p>
  </div>
</details>

<hr>
<h3 id="180-Rhea-Role-aware-Heuristic-Episodic-Attention-for-Conversational-LLMs-cs-CLPDF"><a href="#180-Rhea-Role-aware-Heuristic-Episodic-Attention-for-Conversational-LLMs-cs-CLPDF" class="headerlink" title="[180] Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs cs.CLPDF"></a>[180] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06869">Rhea: Role-aware Heuristic Episodic Attention for Conversational LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06869" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wanyang Hong, Zhaoning Zhang, Yi Chen, Libo Zhang, Baihui Liu</span></p>
<p><strong>TL;DR:</strong> Rhea提出了一种新颖的角色感知启发式情景注意力框架，通过分离对话历史为两个独立的内存模块，解决了大语言模型在多轮对话中上下文衰减的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大语言模型在单轮任务中表现优异，但在多轮对话中效果下降。这种累积上下文衰减现象削弱了对话的连贯性和一致性。</p>
<p><strong>Result:</strong> Rhea在多个多轮对话基准测试中表现优异，提升了16%的相对准确性，同时保持了高指令一致性。</p>
<p><strong>Insight:</strong> 分离和管理对话历史的模块化方法可以有效缓解多轮对话中的注意力污染和上下文衰减问题。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have achieved remarkable performance on single-turn tasks, yet their effectiveness deteriorates in multi-turn conversations. We define this phenomenon as cumulative contextual decay - a progressive degradation of contextual integrity caused by attention pollution, dilution, and drift. To address this challenge, we propose Rhea (Role-aware Heuristic Episodic Attention), a novel framework that decouples conversation history into two functionally independent memory modules: (1) an Instructional Memory (IM) that persistently stores high-fidelity global constraints via a structural priority mechanism, and (2) an Episodic Memory (EM) that dynamically manages user-model interactions via asymmetric noise control and heuristic context retrieval. During inference, Rhea constructs a high signal-to-noise context by applying its priority attention: selectively integrating relevant episodic information while always prioritizing global instructions. To validate this approach, experiments on multiple multi-turn conversation benchmarks - including MT-Eval and Long-MT-Bench+ - show that Rhea mitigates performance decay and improves overall accuracy by 1.04 points on a 10-point scale (a 16% relative gain over strong baselines). Moreover, Rhea maintains near-perfect instruction fidelity (IAR &gt; 8.1) across long-horizon interactions. These results demonstrate that Rhea provides a principled and effective framework for building more precise, instruction-consistent conversational LLMs.</p>
  </div>
</details>

<hr>
<h3 id="181-An-Analysis-of-Large-Language-Models-for-Simulating-User-Responses-in-Surveys-cs-CLPDF"><a href="#181-An-Analysis-of-Large-Language-Models-for-Simulating-User-Responses-in-Surveys-cs-CLPDF" class="headerlink" title="[181] An Analysis of Large Language Models for Simulating User Responses in Surveys cs.CLPDF"></a>[181] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06874">An Analysis of Large Language Models for Simulating User Responses in Surveys</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06874" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziyun Yu, Yiru Zhou, Chen Zhao, Hongyi Wen</span></p>
<p><strong>TL;DR:</strong> 本文探讨了大型语言模型（LLM）在模拟用户调查响应时的能力，发现其因RLHF训练存在偏向主流观点的偏见，提出CLAIMSIM方法增强多样性，但LLM仍难以准确模拟多元用户。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLM在模拟用户观点中的应用增加，其因RLHF训练导致的偏见问题引发关注，作者研究LLM是否能准确模拟跨领域调查问题中的多元用户响应。</p>
<p><strong>Result:</strong> CLAIMSIM能生成更多样化的响应，但LLM仍难以准确模拟多元用户，实验揭示了其观点固化与推理能力不足的问题。</p>
<p><strong>Insight:</strong> LLM在模拟用户调查响应时存在固有偏见和推理局限，未来需改进其对多元用户特征的适应性。</p>
<p><strong>Abstract:</strong> Using Large Language Models (LLMs) to simulate user opinions has received growing attention. Yet LLMs, especially trained with reinforcement learning from human feedback (RLHF), are known to exhibit biases toward dominant viewpoints, raising concerns about their ability to represent users from diverse demographic and cultural backgrounds. In this work, we examine the extent to which LLMs can simulate human responses to cross-domain survey questions through direct prompting and chain-of-thought prompting. We further propose a claim diversification method CLAIMSIM, which elicits viewpoints from LLM parametric knowledge as contextual input. Experiments on the survey question answering task indicate that, while CLAIMSIM produces more diverse responses, both approaches struggle to accurately simulate users. Further analysis reveals two key limitations: (1) LLMs tend to maintain fixed viewpoints across varying demographic features, and generate single-perspective claims; and (2) when presented with conflicting claims, LLMs struggle to reason over nuanced differences among demographic features, limiting their ability to adapt responses to specific user profiles.</p>
  </div>
</details>

<hr>
<h3 id="182-Automated-PRO-CTCAE-Symptom-Selection-based-on-Prior-Adverse-Event-Profiles-cs-CLPDF"><a href="#182-Automated-PRO-CTCAE-Symptom-Selection-based-on-Prior-Adverse-Event-Profiles-cs-CLPDF" class="headerlink" title="[182] Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles cs.CLPDF"></a>[182] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06919">Automated PRO-CTCAE Symptom Selection based on Prior Adverse Event Profiles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06919" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Francois Vandenhende, Anna Georgiou, Michalis Georgiou, Theodoros Psaras, Ellie Karekla</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于历史安全数据的自动化方法，用于选择最小但全面的PRO-CTCAE症状子集，以平衡信号覆盖与患者负担。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> PRO-CTCAE系统中症状项目过多会增加患者负担，过少可能遗漏重要安全信号。需要一种自动化方法优化选择过程。</p>
<p><strong>Result:</strong> 通过模拟和肿瘤学案例研究验证了方法的性能，实现了信号覆盖与患者负担的平衡。</p>
<p><strong>Insight:</strong> 利用MedDRA语义和历史数据可以优化PRO-CTCAE设计，提供客观、可重复的症状选择方法。</p>
<p><strong>Abstract:</strong> The PRO-CTCAE is an NCI-developed patient-reported outcome system for capturing symptomatic adverse events in oncology trials. It comprises a large library drawn from the CTCAE vocabulary, and item selection for a given trial is typically guided by expected toxicity profiles from prior data. Selecting too many PRO-CTCAE items can burden patients and reduce compliance, while too few may miss important safety signals. We present an automated method to select a minimal yet comprehensive PRO-CTCAE subset based on historical safety data. Each candidate PRO-CTCAE symptom term is first mapped to its corresponding MedDRA Preferred Terms (PTs), which are then encoded into Safeterm, a high-dimensional semantic space capturing clinical and contextual diversity in MedDRA terminology. We score each candidate PRO item for relevance to the historical list of adverse event PTs and combine relevance and incidence into a utility function. Spectral analysis is then applied to the combined utility and diversity matrix to identify an orthogonal set of medical concepts that balances relevance and diversity. Symptoms are rank-ordered by importance, and a cut-off is suggested based on the explained information. The tool is implemented as part of the Safeterm trial-safety app. We evaluate its performance using simulations and oncology case studies in which PRO-CTCAE was employed. This automated approach can streamline PRO-CTCAE design by leveraging MedDRA semantics and historical data, providing an objective and reproducible method to balance signal coverage against patient burden.</p>
  </div>
</details>

<hr>
<h3 id="183-Large-Language-Models-and-Forensic-Linguistics-Navigating-Opportunities-and-Threats-in-the-Age-of-Generative-AI-cs-CL-cs-CYPDF"><a href="#183-Large-Language-Models-and-Forensic-Linguistics-Navigating-Opportunities-and-Threats-in-the-Age-of-Generative-AI-cs-CL-cs-CYPDF" class="headerlink" title="[183] Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI cs.CL | cs.CYPDF"></a>[183] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06922">Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06922" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">George Mikros</span></p>
<p><strong>TL;DR:</strong> 大语言模型（LLMs）为法庭语言学带来双重挑战：既是强大的分析工具，又威胁到语言独特性的基本假设。文章探讨了LLMs的模拟风格能力与检测技术的局限性，并提出了方法论调整的建议。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大语言模型的出现改变了法庭语言学的传统方法，既提供了新的分析工具，又引入了风格模拟和文本合成的挑战，亟需方法论上的调整以保持科学和法律的可信度。</p>
<p><strong>Result:</strong> 结果表明，LLMs在表面风格特征上可以接近人类，但仍存在可检测的差异；现有检测技术则存在高误报率和对抗攻击的脆弱性。</p>
<p><strong>Insight:</strong> 核心洞见是‘语言揭示生产者信息’的原则仍然有效，但需适应人类与机器共同创作的复杂链条。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline’s core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.</p>
  </div>
</details>

<hr>
<h3 id="184-XAM-Interactive-Explainability-for-Authorship-Attribution-Models-cs-CLPDF"><a href="#184-XAM-Interactive-Explainability-for-Authorship-Attribution-Models-cs-CLPDF" class="headerlink" title="[184] XAM: Interactive Explainability for Authorship Attribution Models cs.CLPDF"></a>[184] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06924">XAM: Interactive Explainability for Authorship Attribution Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06924" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Milad Alshomary, Anisha Bhatnagar, Peter Zeng, Smaranda Muresan, Owen Rambow</span></p>
<p><strong>TL;DR:</strong> IXAM是一个交互式的解释性框架，用于辅助用户理解基于嵌入的作者归属模型的预测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的作者归属模型缺乏交互式的解释工具，限制了用户对模型预测的理解和信任。</p>
<p><strong>Result:</strong> 用户评估表明，IXAM比预定义的风格解释更有效。</p>
<p><strong>Insight:</strong> 交互式解释工具能提升用户对模型预测的理解和信任。</p>
<p><strong>Abstract:</strong> We present IXAM, an Interactive eXplainability framework for Authorship Attribution Models. Given an authorship attribution (AA) task and an embedding-based AA model, our tool enables users to interactively explore the model’s embedding space and construct an explanation of the model’s prediction as a set of writing style features at different levels of granularity. Through a user evaluation, we demonstrate the value of our framework compared to predefined stylistic explanations.</p>
  </div>
</details>

<hr>
<h3 id="185-Replicating-TEMPEST-at-Scale-Multi-Turn-Adversarial-Attacks-Against-Trillion-Parameter-Frontier-Models-cs-CLPDF"><a href="#185-Replicating-TEMPEST-at-Scale-Multi-Turn-Adversarial-Attacks-Against-Trillion-Parameter-Frontier-Models-cs-CLPDF" class="headerlink" title="[185] Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models cs.CLPDF"></a>[185] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07059">Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07059" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Richard Young</span></p>
<p><strong>TL;DR:</strong> 该论文研究了多轮对抗攻击对大语言模型的安全性影响，发现当前的安全对齐技术仍存在显著漏洞，模型规模与对抗鲁棒性无关，而思维模式（如扩展推理）可显著提升防御能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机在于评估大型语言模型在多轮对抗攻击（如TEMPEST框架）下的脆弱性，以及模型规模或推理模式对其鲁棒性的影响。</p>
<p><strong>Result:</strong> 结果显示，部分模型攻击成功率高达96%-100%，而部分模型展现出显著抵抗力（ASR为42%-78%）；扩展推理模式可将ASR从97%降至42%。</p>
<p><strong>Insight:</strong> 研究揭示了当前安全对齐技术的局限性，同时扩展推理可能是提升模型安全性的可行方向。</p>
<p><strong>Abstract:</strong> Despite substantial investment in safety alignment, the vulnerability of large language models to sophisticated multi-turn adversarial attacks remains poorly characterized, and whether model scale or inference mode affects robustness is unknown. This study employed the TEMPEST multi-turn attack framework to evaluate ten frontier models from eight vendors across 1,000 harmful behaviors, generating over 97,000 API queries across adversarial conversations with automated evaluation by independent safety classifiers. Results demonstrated a spectrum of vulnerability: six models achieved 96% to 100% attack success rate (ASR), while four showed meaningful resistance, with ASR ranging from 42% to 78%; enabling extended reasoning on identical architecture reduced ASR from 97% to 42%. These findings indicate that safety alignment quality varies substantially across vendors, that model scale does not predict adversarial robustness, and that thinking mode provides a deployable safety enhancement. Collectively, this work establishes that current alignment techniques remain fundamentally vulnerable to adaptive multi-turn attacks regardless of model scale, while identifying deliberative inference as a promising defense direction.</p>
  </div>
</details>

<hr>
<h3 id="186-SETUP-Sentence-level-English-To-Uniform-Meaning-Representation-Parser-cs-CLPDF"><a href="#186-SETUP-Sentence-level-English-To-Uniform-Meaning-Representation-Parser-cs-CLPDF" class="headerlink" title="[186] SETUP: Sentence-level English-To-Uniform Meaning Representation Parser cs.CLPDF"></a>[186] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07068">SETUP: Sentence-level English-To-Uniform Meaning Representation Parser</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07068" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Emma Markle, Javier Gutierrez Bach, Shira Wein</span></p>
<p><strong>TL;DR:</strong> SETUP是一种基于句子的英语到统一意义表示（UMR）的解析器，通过改进现有方法（如抽象意义表示和通用依赖转换器），实现了自动生成高精度UMR图的目标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> UMR作为一种图基的语义表示方法，具有多语言适应性，但目前缺乏高效的自动解析工具。SETUP旨在填补这一空白，支持语言文档化和低资源语言技术的发展。</p>
<p><strong>Result:</strong> SETUP模型在评测中表现出色，AnCast和SMATCH++得分分别为84和91，显著提升了UMR的自动解析能力。</p>
<p><strong>Insight:</strong> UMR解析器的开发为多语言语义表示提供了实用工具，尤其对低资源语言技术具有重要推动作用。</p>
<p><strong>Abstract:</strong> Uniform Meaning Representation (UMR) is a novel graph-based semantic representation which captures the core meaning of a text, with flexibility incorporated into the annotation schema such that the breadth of the world’s languages can be annotated (including low-resource languages). While UMR shows promise in enabling language documentation, improving low-resource language technologies, and adding interpretability, the downstream applications of UMR can only be fully explored when text-to-UMR parsers enable the automatic large-scale production of accurate UMR graphs at test time. Prior work on text-to-UMR parsing is limited to date. In this paper, we introduce two methods for English text-to-UMR parsing, one of which fine-tunes existing parsers for Abstract Meaning Representation and the other, which leverages a converter from Universal Dependencies, using prior work as a baseline. Our best-performing model, which we call SETUP, achieves an AnCast score of 84 and a SMATCH++ score of 91, indicating substantial gains towards automatic UMR parsing.</p>
  </div>
</details>

<hr>
<h3 id="187-Do-Large-Language-Models-Truly-Understand-Cross-cultural-Differences-cs-CLPDF"><a href="#187-Do-Large-Language-Models-Truly-Understand-Cross-cultural-Differences-cs-CLPDF" class="headerlink" title="[187] Do Large Language Models Truly Understand Cross-cultural Differences? cs.CLPDF"></a>[187] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07075">Do Large Language Models Truly Understand Cross-cultural Differences?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07075" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shiwei Guo, Sihang Jiang, Qianxi He, Yanghua Xiao, Jiaqing Liang</span></p>
<p><strong>TL;DR:</strong> 本文提出了SAGE基准测试，通过跨文化核心概念对齐和生成式任务设计，评估大语言模型（LLMs）的跨文化理解与推理能力，揭示了LLMs在该领域的系统性局限。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的评估LLMs跨文化理解能力的基准测试存在三大局限性：缺乏上下文场景、跨文化概念映射不足以及深层文化推理能力有限。</p>
<p><strong>Result:</strong> 实验表明SAGE具有可迁移性，但在跨文化推理任务中LLMs表现不佳，显示其在深层文化理解上的局限。</p>
<p><strong>Insight:</strong> 尽管LLMs在多语言任务中表现优异，但在跨文化理解与推理方面仍有明显不足，未来需进一步优化模型能力。</p>
<p><strong>Abstract:</strong> In recent years, large language models (LLMs) have demonstrated strong performance on multilingual tasks. Given its wide range of applications, cross-cultural understanding capability is a crucial competency. However, existing benchmarks for evaluating whether LLMs genuinely possess this capability suffer from three key limitations: a lack of contextual scenarios, insufficient cross-cultural concept mapping, and limited deep cultural reasoning capabilities. To address these gaps, we propose SAGE, a scenario-based benchmark built via cross-cultural core concept alignment and generative task design, to evaluate LLMs’ cross-cultural understanding and reasoning. Grounded in cultural theory, we categorize cross-cultural capabilities into nine dimensions. Using this framework, we curated 210 core concepts and constructed 4530 test items across 15 specific real-world scenarios, organized under four broader categories of cross-cultural situations, following established item design principles. The SAGE dataset supports continuous expansion, and experiments confirm its transferability to other languages. It reveals model weaknesses across both dimensions and scenarios, exposing systematic limitations in cross-cultural reasoning. While progress has been made, LLMs are still some distance away from reaching a truly nuanced cross-cultural understanding. In compliance with the anonymity policy, we include data and code in the supplement materials. In future versions, we will make them publicly available online.</p>
  </div>
</details>

<hr>
<h3 id="188-Leveraging-KV-Similarity-for-Online-Structured-Pruning-in-LLMs-cs-CL-cs-AIPDF"><a href="#188-Leveraging-KV-Similarity-for-Online-Structured-Pruning-in-LLMs-cs-CL-cs-AIPDF" class="headerlink" title="[188] Leveraging KV Similarity for Online Structured Pruning in LLMs cs.CL | cs.AIPDF"></a>[188] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07090">Leveraging KV Similarity for Online Structured Pruning in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07090" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jungmin Lee, Gwangeun Byeon, Yulhwa Kim, Seokin Hong</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种轻量级的在线结构化剪枝方法——Token Filtering，通过直接测量token的冗余性并结合键值相似性跳过冗余注意力计算，从而降低推理成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的剪枝方法依赖于离线校准数据，可能导致输入间的泛化性不足和不稳定性。本文旨在设计一种无需校准数据的在线剪枝技术。</p>
<p><strong>Result:</strong> 在LLaMA和Mistral模型上验证了有效性，50%剪枝率下仍能在MMLU等任务中保持性能。</p>
<p><strong>Insight:</strong> 键值相似性是衡量token冗余性的有效指标，结合自适应策略可以显著提升剪枝稳定性。</p>
<p><strong>Abstract:</strong> Pruning has emerged as a promising direction for accelerating large language model (LLM) inference, yet existing approaches often suffer from instability because they rely on offline calibration data that may not generalize across inputs. In this work, we introduce Token Filtering, a lightweight online structured pruning technique that makes pruning decisions directly during inference without any calibration data. The key idea is to measure token redundancy via joint key-value similarity and skip redundant attention computations, thereby reducing inference cost while preserving critical information. To further enhance stability, we design a variance-aware fusion strategy that adaptively weights key and value similarity across heads, ensuring that informative tokens are retained even under high pruning ratios. This design introduces no additional memory overhead and provides a more reliable criterion for token importance. Extensive experiments on LLaMA-2 (7B&#x2F;13B), LLaMA-3 (8B), and Mistral (7B) demonstrate that Token Filtering consistently outperforms prior structured pruning methods, preserving accuracy on commonsense reasoning benchmarks and maintaining strong performance on challenging tasks such as MMLU, even with 50% pruning.</p>
  </div>
</details>

<hr>
<h3 id="189-DART-Leveraging-Multi-Agent-Disagreement-for-Tool-Recruitment-in-Multimodal-Reasoning-cs-CL-cs-AI-cs-CVPDF"><a href="#189-DART-Leveraging-Multi-Agent-Disagreement-for-Tool-Recruitment-in-Multimodal-Reasoning-cs-CL-cs-AI-cs-CVPDF" class="headerlink" title="[189] DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning cs.CL | cs.AI | cs.CVPDF"></a>[189] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07132">DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07132" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nithin Sivakumaran, Justin Chih-Yao Chen, David Wan, Yue Zhang, Jaehong Yoon</span></p>
<p><strong>TL;DR:</strong> DART 提出了一种多智能体框架，通过视觉智能体之间的分歧来识别有用的视觉工具，从而增强多模态推理能力。该方法在多个基准测试中表现优越，显著优于现有基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在视觉语言模型中，如何有效地选择和调用专有视觉工具以解决多模态推理问题是一个挑战。现有的单智能体或多智能体方法在工具调用和分歧解决上存在不足。</p>
<p><strong>Result:</strong> 在 A-OKVQA 和 MMMU 基准测试中，DART 分别比最强基线（带评委模型的多智能体辩论）高出 3.4% 和 2.4%，在 M3D 医学数据集上也有 1.3% 的提升。</p>
<p><strong>Insight:</strong> DART 展示了多智能体分歧可以有效地引导工具调用，而工具的多样性和讨论的丰富性是提升性能的关键。</p>
<p><strong>Abstract:</strong> Specialized visual tools can augment large language models or vision language models with expert knowledge (e.g., grounding, spatial reasoning, medical knowledge, etc.), but knowing which tools to call (and when to call them) can be challenging. We introduce DART, a multi-agent framework that uses disagreements between multiple debating visual agents to identify useful visual tools (e.g., object detection, OCR, spatial reasoning, etc.) that can resolve inter-agent disagreement. These tools allow for fruitful multi-agent discussion by introducing new information, and by providing tool-aligned agreement scores that highlight agents in agreement with expert tools, thereby facilitating discussion. We utilize an aggregator agent to select the best answer by providing the agent outputs and tool information. We test DART on four diverse benchmarks and show that our approach improves over multi-agent debate as well as over single agent tool-calling frameworks, beating the next-strongest baseline (multi-agent debate with a judge model) by 3.4% and 2.4% on A-OKVQA and MMMU respectively. We also find that DART adapts well to new tools in applied domains, with a 1.3% improvement on the M3D medical dataset over other strong tool-calling, single agent, and multi-agent baselines. Additionally, we measure text overlap across rounds to highlight the rich discussion in DART compared to existing multi-agent methods. Finally, we study the tool call distribution, finding that diverse tools are reliably used to help resolve disagreement.</p>
  </div>
</details>

<hr>
<h3 id="190-NeSTR-A-Neuro-Symbolic-Abductive-Framework-for-Temporal-Reasoning-in-Large-Language-Models-cs-CL-cs-AI-cs-LGPDF"><a href="#190-NeSTR-A-Neuro-Symbolic-Abductive-Framework-for-Temporal-Reasoning-in-Large-Language-Models-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[190] NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models cs.CL | cs.AI | cs.LGPDF"></a>[190] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07218">NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07218" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Feng Liang, Weixin Zeng, Runhao Zhao, Xiang Zhao</span></p>
<p><strong>TL;DR:</strong> NeSTR结合神经与符号方法，通过符号编码和反事实推理提升LLM的时间推理能力，无需微调即可在零样本任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLM在时间推理任务中表现不佳，符号方法未能充分利用LLM能力，而纯神经方法又缺乏结构化时间表示。</p>
<p><strong>Result:</strong> 在多样化的时间问答基准测试中，NeSTR实现了优异的零样本性能，且无需微调。</p>
<p><strong>Insight:</strong> 神经与符号的结合能有效弥补纯神经方法在结构化推理中的不足，特别是在时间敏感的复杂任务中。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.</p>
  </div>
</details>

<hr>
<h3 id="191-Training-Language-Models-to-Use-Prolog-as-a-Tool-cs-CLPDF"><a href="#191-Training-Language-Models-to-Use-Prolog-as-a-Tool-cs-CLPDF" class="headerlink" title="[191] Training Language Models to Use Prolog as a Tool cs.CLPDF"></a>[191] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07407">Training Language Models to Use Prolog as a Tool</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07407" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Niklas Mellgren, Peter Schneider-Kamp, Lukas Galke Poech</span></p>
<p><strong>TL;DR:</strong> 这篇论文研究了如何通过微调语言模型使用Prolog作为外部工具，以提高AI系统的可靠性和可验证性，使用了GRPO方法在GSM8K-Prolog-Prover数据集上进行实验，展示了其在语法、逻辑和零样本泛化方面的优势。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了提高AI系统的可靠性和安全性，减少语言模型中不可信的推理和错误解，论文探索了将Prolog作为外部工具用于验证计算的可能性。</p>
<p><strong>Result:</strong> 实验结果表明：1）联合调优提示、奖励和推理可以改善语法和逻辑；2）使用外部Prolog验证的best-of-N方法在GSM8K上表现最佳；3）内部修复的代理推理在MMLU任务上显示出更强的零样本泛化能力。</p>
<p><strong>Insight:</strong> 研究揭示了将模型推理与形式化验证系统结合可以显著提高可靠性和可审计性，尤其是在安全关键应用中。</p>
<p><strong>Abstract:</strong> Ensuring reliable tool use is critical for safe agentic AI systems. Language models frequently produce unreliable reasoning with plausible but incorrect solutions that are difficult to verify. To address this, we investigate fine-tuning models to use Prolog as an external tool for verifiable computation. Using Group Relative Policy Optimization (GRPO), we fine-tune Qwen2.5-3B-Instruct on a cleaned GSM8K-Prolog-Prover dataset while varying (i) prompt structure, (ii) reward composition (execution, syntax, semantics, structure), and (iii) inference protocol: single-shot, best-of-N, and two agentic modes where Prolog is invoked internally or independently. Our reinforcement learning approach outperforms supervised fine-tuning, with our 3B model achieving zero-shot MMLU performance comparable to 7B few-shot results. Our findings reveal that: 1) joint tuning of prompt, reward, and inference shapes program syntax and logic; 2) best-of-N with external Prolog verification maximizes accuracy on GSM8K; 3) agentic inference with internal repair yields superior zero-shot generalization on MMLU-Stem and MMLU-Pro. These results demonstrate that grounding model reasoning in formal verification systems substantially improves reliability and auditability for safety-critical applications. The source code for reproducing our experiments is available under <a target="_blank" rel="noopener" href="https://github.com/niklasmellgren/grpo-prolog-inference">https://github.com/niklasmellgren/grpo-prolog-inference</a></p>
  </div>
</details>

<hr>
<h3 id="192-Persian-Phi-Efficient-Cross-Lingual-Adaptation-of-Compact-LLMs-via-Curriculum-Learning-cs-CL-cs-AIPDF"><a href="#192-Persian-Phi-Efficient-Cross-Lingual-Adaptation-of-Compact-LLMs-via-Curriculum-Learning-cs-CL-cs-AIPDF" class="headerlink" title="[192] Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning cs.CL | cs.AIPDF"></a>[192] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07454">Persian-Phi: Efficient Cross-Lingual Adaptation of Compact LLMs via Curriculum Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07454" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amir Mohammad Akhlaghi, Amirhossein Shabani, Mostafa Abdolmaleki, Saeed Reza Kheradpisheh</span></p>
<p><strong>TL;DR:</strong> 论文Persian-Phi提出了一种通过课程学习实现高效跨语言迁移的紧凑型大语言模型（LLM）方法，将原本单语（英语）的Phi-3 Mini模型适配到波斯语，并在资源有限的情况下实现了竞争性表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前AI民主化的主要障碍在于训练低资源语言大语言模型的高计算成本。本文旨在证明紧凑型模型也能实现稳健的多语言能力，无需庞大的模型或多语言基线。</p>
<p><strong>Result:</strong> Persian-Phi在Open Persian LLM Leaderboard上表现优异，证明紧凑型模型也能在低资源语言任务中具有竞争力。</p>
<p><strong>Insight:</strong> 通过分阶段训练和参数高效方法，可以在有限硬件资源下高效扩展LLM到低资源语言，为AI民主化提供了可行路径。</p>
<p><strong>Abstract:</strong> The democratization of AI is currently hindered by the immense computational costs required to train Large Language Models (LLMs) for low-resource languages. This paper presents Persian-Phi, a 3.8B parameter model that challenges the assumption that robust multilingual capabilities require massive model sizes or multilingual baselines. We demonstrate how Microsoft Phi-3 Mini – originally a monolingual English model – can be effectively adapted to Persian through a novel, resource-efficient curriculum learning pipeline. Our approach employs a unique “warm-up” stage using bilingual narratives (Tiny Stories) to align embeddings prior to heavy training, followed by continual pretraining and instruction tuning via Parameter-Efficient Fine-Tuning (PEFT). Despite its compact size, Persian-Phi achieves competitive results on Open Persian LLM Leaderboard in HuggingFace. Our findings provide a validated, scalable framework for extending the reach of state-of-the-art LLMs to underrepresented languages with minimal hardware resources. The Persian-Phi model is publicly available at <a target="_blank" rel="noopener" href="https://huggingface.co/amirakhlaghiqqq/PersianPhi">https://huggingface.co/amirakhlaghiqqq/PersianPhi</a>.</p>
  </div>
</details>

<hr>
<h3 id="193-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning-cs-CLPDF"><a href="#193-Native-Parallel-Reasoner-Reasoning-in-Parallelism-via-Self-Distilled-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[193] Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning cs.CLPDF"></a>[193] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07461">Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07461" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tong Wu, Yang Liu, Jun Bai, Zixia Jia, Shuyi Zhang</span></p>
<p><strong>TL;DR:</strong> NPR（Native Parallel Reasoner）是一种无教师框架，通过自蒸馏强化学习让大语言模型（LLMs）具备真正的并行推理能力，提高了性能和推理速度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLMs通常通过顺序模拟进行推理，效率较低。NPR旨在通过并行化和自蒸馏技术，提升模型的推理能力和效率。</p>
<p><strong>Result:</strong> 在八个推理基准测试中，NPR性能提升高达24.5%，推理速度提升4.6倍，实现了100%真正的并行执行。</p>
<p><strong>Insight:</strong> NPR展示了自蒸馏和并行化技术的潜力，为高效、可扩展的智能推理设定了新标准。</p>
<p><strong>Abstract:</strong> We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from &#96;&#96;cold-start’’ format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.</p>
  </div>
</details>

<hr>
<h3 id="194-Enhancing-Agentic-RL-with-Progressive-Reward-Shaping-and-Value-based-Sampling-Policy-Optimization-cs-CLPDF"><a href="#194-Enhancing-Agentic-RL-with-Progressive-Reward-Shaping-and-Value-based-Sampling-Policy-Optimization-cs-CLPDF" class="headerlink" title="[194] Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization cs.CLPDF"></a>[194] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07478">Enhancing Agentic RL with Progressive Reward Shaping and Value-based Sampling Policy Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07478" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhuoran Zhuang, Ye Chen, Jianghao Su, Chao Luo, Luhui Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了两种互补技术——渐进奖励塑造（PRS）和基于价值的采样策略优化（VSPO）——以解决Agentic强化学习中稀疏奖励和梯度退化的问题，并在多个问答任务上验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统Agentic强化学习在工具调用任务中存在稀疏奖励和梯度退化问题，限制了模型的收敛速度和效率，因此需要新的方法改进训练过程。</p>
<p><strong>Result:</strong> PRS和VSPO在短形式和长形式问答任务中表现优于传统方法，实现了更快的收敛速度和更高的最终性能。</p>
<p><strong>Insight:</strong> 分阶段奖励设计和基于价值的采样策略能显著提升强化学习在复杂任务中的训练效果，为LLM集成工具任务提供了新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) empowered with Tool-Integrated Reasoning (TIR) can iteratively plan, call external tools, and integrate returned information to solve complex, long-horizon reasoning tasks. Agentic Reinforcement Learning (Agentic RL) optimizes such models over full tool-interaction trajectories, but two key challenges hinder effectiveness: (1) Sparse, non-instructive rewards, such as binary 0-1 verifiable signals, provide limited guidance for intermediate steps and slow convergence; (2) Gradient degradation in Group Relative Policy Optimization (GRPO), where identical rewards within a rollout group yield zero advantage, reducing sample efficiency and destabilizing training. To address these challenges, we propose two complementary techniques: Progressive Reward Shaping (PRS) and Value-based Sampling Policy Optimization (VSPO). PRS is a curriculum-inspired reward design that introduces dense, stage-wise feedback - encouraging models to first master parseable and properly formatted tool calls, then optimize for factual correctness and answer quality. We instantiate PRS for short-form QA (with a length-aware BLEU to fairly score concise answers) and long-form QA (with LLM-as-a-Judge scoring to prevent reward hacking). VSPO is an enhanced GRPO variant that replaces low-value samples with prompts selected by a task-value metric balancing difficulty and uncertainty, and applies value-smoothing clipping to stabilize gradient updates. Experiments on multiple short-form and long-form QA benchmarks show that PRS consistently outperforms traditional binary rewards, and VSPO achieves superior stability, faster convergence, and higher final performance compared to PPO, GRPO, CISPO, and SFT-only baselines. Together, PRS and VSPO yield LLM-based TIR agents that generalize better across domains.</p>
  </div>
</details>

<hr>
<h3 id="195-LIME-Making-LLM-Data-More-Efficient-with-Linguistic-Metadata-Embeddings-cs-CL-cs-AIPDF"><a href="#195-LIME-Making-LLM-Data-More-Efficient-with-Linguistic-Metadata-Embeddings-cs-CL-cs-AIPDF" class="headerlink" title="[195] LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings cs.CL | cs.AIPDF"></a>[195] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07522">LIME: Making LLM Data More Efficient with Linguistic Metadata Embeddings</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07522" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sebastian Sztwiertnia, Felix Friedrich, Kristian Kersting, Patrick Schramowski, Björn Deiseroth</span></p>
<p><strong>TL;DR:</strong> 论文提出了LIME（语言元数据嵌入），通过在token嵌入中融入语法、语义和上下文属性的元数据，显著提升了预训练效率，并改善了语言建模能力和生成任务表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于解码器的语言模型依赖大量高质量数据，但数据资源接近上限。元数据通常仅用于数据集的创建和管理，其直接作为训练信号的潜力未被充分挖掘。</p>
<p><strong>Result:</strong> 1. 预训练效率提升56%，参数仅增加0.01%；2. 语言建模和生成能力显著增强；3. LIME+1在推理和算术任务上分别提升38%和35%的准确率。</p>
<p><strong>Insight:</strong> 元数据不仅是数据集管理的工具，还可作为直接的训练信号，显著提升模型效率和性能。</p>
<p><strong>Abstract:</strong> Pre-training decoder-only language models relies on vast amounts of high-quality data, yet the availability of such data is increasingly reaching its limits. While metadata is commonly used to create and curate these datasets, its potential as a direct training signal remains under-explored. We challenge this status quo and propose LIME (Linguistic Metadata Embeddings), a method that enriches token embeddings with metadata capturing syntax, semantics, and contextual properties. LIME substantially improves pre-training efficiency. Specifically, it adapts up to 56% faster to the training data distribution, while introducing only 0.01% additional parameters at negligible compute overhead. Beyond efficiency, LIME improves tokenization, leading to remarkably stronger language modeling capabilities and generative task performance. These benefits persist across model scales (500M to 2B). In addition, we develop a variant with shifted metadata, LIME+1, that can guide token generation. Given prior metadata for the next token, LIME+1 improves reasoning performance by up to 38% and arithmetic accuracy by up to 35%.</p>
  </div>
</details>

<hr>
<h3 id="196-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs-cs-CLPDF"><a href="#196-Beyond-Real-Imaginary-Extension-of-Rotary-Position-Embeddings-for-Long-Context-LLMs-cs-CLPDF" class="headerlink" title="[196] Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs cs.CLPDF"></a>[196] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07525">Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07525" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaoran Liu, Yuerong Song, Zhigeng Liu, Zengfeng Huang, Qipeng Guo</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种扩展RoPE（Rotary Position Embeddings）的方法，通过重新引入被丢弃的虚部信息，利用完整的复数值表示来增强长上下文依赖建模。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准的RoPE实现仅使用复数值点积的实部计算注意力分数，忽略了虚部包含的重要相位信息，可能导致长上下文依赖建模中的潜在信息丢失。</p>
<p><strong>Result:</strong> 在长上下文语言建模基准测试中，该方法相较于标准RoPE表现出更好的性能提升，且随着上下文长度的增加，效果更加显著。</p>
<p><strong>Insight:</strong> 复数值表示中的虚部信息对长上下文依赖建模至关重要，通过充分利用复数值的点积特性，可以显著提升模型对位置信息的捕捉能力。</p>
<p><strong>Abstract:</strong> Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at <a target="_blank" rel="noopener" href="https://github.com/OpenMOSS/rope_pp">https://github.com/OpenMOSS/rope_pp</a>.</p>
  </div>
</details>

<hr>
<h3 id="197-SwissGov-RSD-A-Human-annotated-Cross-lingual-Benchmark-for-Token-level-Recognition-of-Semantic-Differences-Between-Related-Documents-cs-CLPDF"><a href="#197-SwissGov-RSD-A-Human-annotated-Cross-lingual-Benchmark-for-Token-level-Recognition-of-Semantic-Differences-Between-Related-Documents-cs-CLPDF" class="headerlink" title="[197] SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents cs.CLPDF"></a>[197] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07538">SwissGov-RSD: A Human-annotated, Cross-lingual Benchmark for Token-level Recognition of Semantic Differences Between Related Documents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07538" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Michelle Wastl, Jannis Vamvas, Rico Sennrich</span></p>
<p><strong>TL;DR:</strong> 本论文介绍了SwissGov-RSD，这是首个自然、文档级别、跨语言的语义差异识别数据集，包含224份多语言平行文档，并在token级别进行了人工标注。作者评估了多种开源和闭源大语言模型及编码器模型在不同微调设置下的表现，发现当前自动方法在此任务上表现不佳，尤其在跨语言和文档级别任务中存在显著差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 识别跨文档语义差异（尤其是不同语言间）对于文本生成评估和多语言内容对齐至关重要，但目前这一独立任务的研究较少。因此，作者旨在填补这一空白。</p>
<p><strong>Result:</strong> 实验结果显示，当前自动方法在多语言和文档级别的语义差异识别任务中表现较差，与单语言、句子级别和合成数据集上的表现形成鲜明对比。</p>
<p><strong>Insight:</strong> 跨语言和文档级别的语义差异识别是一个具有挑战性的任务，现有方法的性能仍有很大提升空间。</p>
<p><strong>Abstract:</strong> Recognizing semantic differences across documents, especially in different languages, is crucial for text generation evaluation and multilingual content alignment. However, as a standalone task it has received little attention. We address this by introducing SwissGov-RSD, the first naturalistic, document-level, cross-lingual dataset for semantic difference recognition. It encompasses a total of 224 multi-parallel documents in English-German, English-French, and English-Italian with token-level difference annotations by human annotators. We evaluate a variety of open-source and closed source large language models as well as encoder models across different fine-tuning settings on this new benchmark. Our results show that current automatic approaches perform poorly compared to their performance on monolingual, sentence-level, and synthetic benchmarks, revealing a considerable gap for both LLMs and encoder models. We make our code and datasets publicly available.</p>
  </div>
</details>

<hr>
<h3 id="198-A-Simple-Method-to-Enhance-Pre-trained-Language-Models-with-Speech-Tokens-for-Classification-cs-CL-cs-MMPDF"><a href="#198-A-Simple-Method-to-Enhance-Pre-trained-Language-Models-with-Speech-Tokens-for-Classification-cs-CL-cs-MMPDF" class="headerlink" title="[198] A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification cs.CL | cs.MMPDF"></a>[198] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07571">A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07571" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nicolas Calbucura, Valentin Barriere</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种简单的方法，通过LASSO特征选择和多模态词袋表示，将语音信息融入预训练语言模型中，用于分类任务，提升了性能并达到SOTA结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 如何低成本地将长序列语音信息融入文本预训练模型中，以提升分类任务的性能，是一个关键挑战。</p>
<p><strong>Result:</strong> 在两个论证谬误检测任务上，该方法超越了单模态模型和其他多模态融合方法，达到SOTA性能。</p>
<p><strong>Insight:</strong> 研究发现，即使是随机的音频标记选择也能提升单模态模型的性能，揭示了语音信息在分类任务中的潜在价值。</p>
<p><strong>Abstract:</strong> This paper presents a simple method that allows to easily enhance textual pre-trained large language models with speech information, when fine-tuned for a specific classification task. A classical issue with the fusion of many embeddings from audio with text is the large length of the audio sequence compared to the text one. Our method benefits from an existing speech tokenizer trained for Audio Speech Recognition that output long sequences of tokens from a large vocabulary, making it difficult to integrate it at low cost in a large language model. By applying a simple lasso-based feature selection on multimodal Bag-of-Words representation, we retain only the most important audio tokens for the task, and adapt the language model to them with a self-supervised language modeling objective, before fine-tuning it on the downstream task. We show this helps to improve the performances compared to an unimodal model, to a bigger SpeechLM or to integrating audio via a learned representation. We show the effectiveness of our method on two recent Argumentative Fallacy Detection and Classification tasks where the use of audio was believed counterproductive, reaching state-of-the-art results. We also provide an in-depth analysis of the method, showing that even a random audio token selection helps enhancing the unimodal model. Our code is available <a target="_blank" rel="noopener" href="https://github.com/salocinc/EACL26SpeechTokFallacy/">online</a>.</p>
  </div>
</details>

<hr>
<h3 id="199-Complementary-Learning-Approach-for-Text-Classification-using-Large-Language-Models-cs-CL-cs-AIPDF"><a href="#199-Complementary-Learning-Approach-for-Text-Classification-using-Large-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[199] Complementary Learning Approach for Text Classification using Large Language Models cs.CL | cs.AIPDF"></a>[199] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07583">Complementary Learning Approach for Text Classification using Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07583" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Navid Asgari, Benjamin M. Cole</span></p>
<p><strong>TL;DR:</strong> 提出了一种结合大语言模型（LLMs）和人类协作的低成本高效方法，用于文本分类任务，通过思维链和少样本学习提示技术，弥补LLMs的缺陷。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 利用人类和大语言模型的互补性，解决LLMs在文本分类中的不足，同时降低成本。</p>
<p><strong>Result:</strong> 成功应用于1,934份制药联盟新闻稿的分类任务，展示了人类与机器协作的有效性。</p>
<p><strong>Insight:</strong> 通过低成本方法，人类可以更好地监督和补充LLMs的能力，提升分类任务的准确性。</p>
<p><strong>Abstract:</strong> In this study, we propose a structured methodology that utilizes large language models (LLMs) in a cost-efficient and parsimonious manner, integrating the strengths of scholars and machines while offsetting their respective weaknesses. Our methodology, facilitated through a chain of thought and few-shot learning prompting from computer science, extends best practices for co-author teams in qualitative research to human-machine teams in quantitative research. This allows humans to utilize abductive reasoning and natural language to interrogate not just what the machine has done but also what the human has done. Our method highlights how scholars can manage inherent weaknesses OF LLMs using careful, low-cost techniques. We demonstrate how to use the methodology to interrogate human-machine rating discrepancies for a sample of 1,934 press releases announcing pharmaceutical alliances (1990-2017).</p>
  </div>
</details>

<hr>
<h3 id="200-Metric-Fair-Prompting-Treating-Similar-Samples-Similarly-cs-CL-cs-AIPDF"><a href="#200-Metric-Fair-Prompting-Treating-Similar-Samples-Similarly-cs-CL-cs-AIPDF" class="headerlink" title="[200] Metric-Fair Prompting: Treating Similar Samples Similarly cs.CL | cs.AIPDF"></a>[200] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07608">Metric-Fair Prompting: Treating Similar Samples Similarly</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07608" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jing Wang, Jie Shen, Xing Niu, Tong Zhang, Jeremy Weiss</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种公平性提示框架（Metric-Fair Prompting），通过度量公平性约束指导大语言模型（LLMs）在医疗多选问题中的应用，确保相似样本得到相似处理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLMs在处理高风险的临床多选问题时，存在对相似问题处理不一致的情况，可能影响公平性和准确性。论文旨在通过引入度量公平性约束，提升模型的公平性和性能。</p>
<p><strong>Result:</strong> 在MedQA（US）基准测试中，Metric-Fair Prompting优于标准的单问题提示方法，证明了公平性引导的置信度定向推理能提升LLMs在临床多选问题中的准确性。</p>
<p><strong>Insight:</strong> 公平性约束不仅有助于提升模型的公平性，还能通过一致性推理提升模型的整体性能。这表明在高风险任务中，公平性和准确性可以相辅相成。</p>
<p><strong>Abstract:</strong> We introduce \emph{Metric-Fair Prompting}, a fairness-aware prompting framework that guides large language models (LLMs) to make decisions under metric-fairness constraints. In the application of multiple-choice medical question answering, each {(question, option)} pair is treated as a binary instance with label $+1$ (correct) or $-1$ (incorrect). To promote {individual fairness}<del>–</del>treating similar instances similarly<del>–</del>we compute question similarity using NLP embeddings and solve items in \emph{joint pairs of similar questions} rather than in isolation. The prompt enforces a global decision protocol: extract decisive clinical features, map each ((\text{question}, \text{option})) to a score $f(x)$ that acts as confidence, and impose a Lipschitz-style constraint so that similar inputs receive similar scores and, hence, consistent outputs. Evaluated on the {MedQA (US)} benchmark, Metric-Fair Prompting is shown to improve performance over standard single-item prompting, demonstrating that fairness-guided, confidence-oriented reasoning can enhance LLM accuracy on high-stakes clinical multiple-choice questions.</p>
  </div>
</details>

<hr>
<h3 id="201-HalluShift-Bridging-Language-and-Vision-through-Internal-Representation-Shifts-for-Hierarchical-Hallucinations-in-MLLMs-cs-CL-cs-CVPDF"><a href="#201-HalluShift-Bridging-Language-and-Vision-through-Internal-Representation-Shifts-for-Hierarchical-Hallucinations-in-MLLMs-cs-CL-cs-CVPDF" class="headerlink" title="[201] HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs cs.CL | cs.CVPDF"></a>[201] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07687">HalluShift++: Bridging Language and Vision through Internal Representation Shifts for Hierarchical Hallucinations in MLLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07687" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sujoy Nath, Arkaprabha Basu, Sharanya Dasgupta, Swagatam Das</span></p>
<p><strong>TL;DR:</strong> HalluShift++ 提出了一种新方法，通过分析 MLLMs 内部层的动态变化来检测幻觉现象，避免了对外部 LLM 评估器的依赖。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> MLLMs 在多模态理解任务中表现优异，但常常产生与视觉内容不符的幻觉描述，这可能带来负面影响。当前方法依赖外部 LLM 评估器，但这些评估器本身也存在幻觉问题且难以适应不同领域。</p>
<p><strong>Result:</strong> HalluShift++ 在检测幻觉方面表现优于依赖外部评估器的方法，特别适用于多模态场景。</p>
<p><strong>Insight:</strong> 幻觉不仅是语言模型的分布偏移问题，还可能与模型内部层的动态变化有关，提供了新的评估视角。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding tasks. While these models often produce linguistically coherent output, they often suffer from hallucinations, generating descriptions that are factually inconsistent with the visual content, potentially leading to adverse consequences. Therefore, the assessment of hallucinations in MLLM has become increasingly crucial in the model development process. Contemporary methodologies predominantly depend on external LLM evaluators, which are themselves susceptible to hallucinations and may present challenges in terms of domain adaptation. In this study, we propose the hypothesis that hallucination manifests as measurable irregularities within the internal layer dynamics of MLLMs, not merely due to distributional shifts but also in the context of layer-wise analysis of specific assumptions. By incorporating such modifications, \textsc{\textsc{HalluShift++}} broadens the efficacy of hallucination detection from text-based large language models (LLMs) to encompass multimodal scenarios. Our codebase is available at <a target="_blank" rel="noopener" href="https://github.com/C0mRD/HalluShift_Plus">https://github.com/C0mRD/HalluShift_Plus</a>.</p>
  </div>
</details>

<hr>
<h3 id="202-Automated-Generation-of-Custom-MedDRA-Queries-Using-SafeTerm-Medical-Map-cs-CLPDF"><a href="#202-Automated-Generation-of-Custom-MedDRA-Queries-Using-SafeTerm-Medical-Map-cs-CLPDF" class="headerlink" title="[202] Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map cs.CLPDF"></a>[202] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07694">Automated Generation of Custom MedDRA Queries Using SafeTerm Medical Map</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07694" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Francois Vandenhende, Anna Georgiou, Michalis Georgiou, Theodoros Psaras, Ellie Karekla</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于人工智能的SafeTerm系统，用于自动生成MedDRA查询，通过嵌入医学术语和MedDRA术语在多维向量空间中，结合余弦相似性和极值聚类方法，实现了高召回率和高精度的查询匹配。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在药物安全审查中，将相关不良事件术语分类为标准化MedDRA查询或FDA自定义医学查询（OCMQs）对信号检测至关重要。传统方法耗时且依赖人工，因此需要一种自动化、高效的系统来解决这一问题。</p>
<p><strong>Result:</strong> 在FDA OCMQ v3.0数据集上验证，系统在高召回率（&gt;95%）的同时，通过调整阈值实现了高精度（最高86%）。最优阈值（0.70-0.75）下召回率为50%，精度为33%。</p>
<p><strong>Insight:</strong> SafeTerm系统可作为自动生成MedDRA查询的有效补充工具。建议初始使用相似性阈值0.60，并在后续优化中逐步提高阈值以提高精度。</p>
<p><strong>Abstract:</strong> In pre-market drug safety review, grouping related adverse event terms into standardised MedDRA queries or the FDA Office of New Drugs Custom Medical Queries (OCMQs) is critical for signal detection. We present a novel quantitative artificial intelligence system that understands and processes medical terminology and automatically retrieves relevant MedDRA Preferred Terms (PTs) for a given input query, ranking them by a relevance score using multi-criteria statistical methods. The system (SafeTerm) embeds medical query terms and MedDRA PTs in a multidimensional vector space, then applies cosine similarity and extreme-value clustering to generate a ranked list of PTs. Validation was conducted against the FDA OCMQ v3.0 (104 queries), restricted to valid MedDRA PTs. Precision, recall and F1 were computed across similarity-thresholds. High recall (&gt;95%) is achieved at moderate thresholds. Higher thresholds improve precision (up to 86%). The optimal threshold (~0.70 - 0.75) yielded recall ~50% and precision ~33%. Narrow-term PT subsets performed similarly but required slightly higher similarity thresholds. The SafeTerm AI-driven system provides a viable supplementary method for automated MedDRA query generation. A similarity threshold of ~0.60 is recommended initially, with increased thresholds for refined term selection.</p>
  </div>
</details>

<hr>
<h3 id="203-Mary-the-Cheeseburger-Eating-Vegetarian-Do-LLMs-Recognize-Incoherence-in-Narratives-cs-CLPDF"><a href="#203-Mary-the-Cheeseburger-Eating-Vegetarian-Do-LLMs-Recognize-Incoherence-in-Narratives-cs-CLPDF" class="headerlink" title="[203] Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives? cs.CLPDF"></a>[203] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07777">Mary, the Cheeseburger-Eating Vegetarian: Do LLMs Recognize Incoherence in Narratives?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07777" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Karin de Langis, Püren Öncel, Ryan Peters, Andrew Elfenbein, Laura Kristen Allen</span></p>
<p><strong>TL;DR:</strong> 论文研究发现，尽管大型语言模型（LLMs）的内部表征能够识别不连贯叙述，但生成的回答在区分连贯与不连贯故事方面表现不佳，表明LLMs在叙事连贯性理解上存在缺陷。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索LLMs在识别和区分连贯与不连贯叙事方面的能力，揭示其在叙事理解上的局限性。</p>
<p><strong>Result:</strong> LLMs的生成回答未能有效区分连贯与不连贯叙事，且推理能力无法完全弥补这一缺陷。模型对背景冲突的敏感性高于角色行为矛盾。</p>
<p><strong>Insight:</strong> LLMs可能更依赖原型世界知识而非基于意义的叙事连贯性构建，表明其在完整叙事理解上仍有不足。</p>
<p><strong>Abstract:</strong> Leveraging a dataset of paired narratives, we investigate the extent to which large language models (LLMs) can reliably separate incoherent and coherent stories. A probing study finds that LLMs’ internal representations can reliably identify incoherent narratives. However, LLMs generate responses to rating questions that fail to satisfactorily separate the coherent and incoherent narratives across several prompt variations, hinting at a gap in LLM’s understanding of storytelling. The reasoning LLMs tested do not eliminate these deficits, indicating that thought strings may not be able to fully address the discrepancy between model internal state and behavior. Additionally, we find that LLMs appear to be more sensitive to incoherence resulting from an event that violates the setting (e.g., a rainy day in the desert) than to incoherence arising from a character violating an established trait (e.g., Mary, a vegetarian, later orders a cheeseburger), suggesting that LLMs may rely more on prototypical world knowledge than building meaning-based narrative coherence. The consistent asymmetry found in our results suggests that LLMs do not have a complete grasp on narrative coherence.</p>
  </div>
</details>

<hr>
<h3 id="204-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models-cs-CLPDF"><a href="#204-On-the-Interplay-of-Pre-Training-Mid-Training-and-RL-on-Reasoning-Language-Models-cs-CLPDF" class="headerlink" title="[204] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models cs.CLPDF"></a>[204] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07783">On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07783" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Charlie Zhang, Graham Neubig, Xiang Yue</span></p>
<p><strong>TL;DR:</strong> 该论文通过一个完全受控的实验框架，研究了预训练、中期训练和RL后训练对语言模型推理能力的因果贡献，揭示了RL在特定条件下才能真正提升能力，中期训练的重要性，以及过程级奖励的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前RL技术在语言模型推理能力上的效果尚不明确，缺乏对训练管道中各阶段（预训练、中期训练、RL后训练）的因果分离和系统控制。</p>
<p><strong>Result:</strong> RL仅在预训练留有空间且目标任务是模型能力边界时有效；中期训练在固定计算下优于RL；过程级奖励减少奖励作弊。</p>
<p><strong>Insight:</strong> 训练策略的优化需综合考虑预训练、中期训练和RL的相互作用；过程级奖励提升推理的准确性。</p>
<p><strong>Abstract:</strong> Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model’s reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL’s effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model’s edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.</p>
  </div>
</details>

<hr>
<h3 id="205-Collaborative-Causal-Sensemaking-Closing-the-Complementarity-Gap-in-Human-AI-Decision-Support-cs-CL-cs-AI-cs-HC-cs-LGPDF"><a href="#205-Collaborative-Causal-Sensemaking-Closing-the-Complementarity-Gap-in-Human-AI-Decision-Support-cs-CL-cs-AI-cs-HC-cs-LGPDF" class="headerlink" title="[205] Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support cs.CL | cs.AI | cs.HC | cs.LGPDF"></a>[205] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07801">Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.HC | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07801" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Raunak Jain, Mudita Khurana</span></p>
<p><strong>TL;DR:</strong> 论文提出了 Collaborative Causal Sensemaking (CCS) 框架，旨在解决人类- AI 团队在复杂决策中互补性不足的问题，强调协作式的认知过程和共同构建的心理模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的 AI 辅助决策系统在高风险复杂场景中表现不佳，人类- AI 团队未能实现预期的互补性。专家需要在认知过程中与 AI 协作，但目前的设计未能支持这种动态交互。</p>
<p><strong>Result:</strong> 框架有望提升人类- AI 团队的决策效能，实现真正的互补性，同时促进 AI 与人类专家的共同进步。</p>
<p><strong>Insight:</strong> AI 辅助决策应从单纯的准确性提升转向协作式的认知支持，重点关注人类- AI 之间的动态交互和学习过程。</p>
<p><strong>Abstract:</strong> LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.</p>
  </div>
</details>

<hr>
<h3 id="206-Do-Generalisation-Results-Generalise-cs-CL-cs-LGPDF"><a href="#206-Do-Generalisation-Results-Generalise-cs-CL-cs-LGPDF" class="headerlink" title="[206] Do Generalisation Results Generalise? cs.CL | cs.LGPDF"></a>[206] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07832">Do Generalisation Results Generalise?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07832" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matteo Boglioni, Andrea Sgobbi, Gabriel Tavernini, Francesco Rita, Marius Mosbach</span></p>
<p><strong>TL;DR:</strong> 本论文探讨了大型语言模型（LLM）在分布外（OOD）数据上的泛化能力是否具有普适性，并通过多数据集评估和偏相关性分析发现泛化结果因模型而异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究通常仅通过一个OOD数据集评估LLM的泛化能力，忽略了实际部署中更为多样的数据分布变化，因此需要更全面的评估方法。</p>
<p><strong>Result:</strong> 分析了OLMo2和OPT模型，发现泛化结果无统一趋势，任何两个OOD测试集之间的相关性取决于具体模型。</p>
<p><strong>Insight:</strong> 泛化能力的评估需考虑多数据集，且模型设计对泛化性能的影响显著，单一OOD测试不足以全面反映模型能力。</p>
<p><strong>Abstract:</strong> A large language model’s (LLM’s) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs’ generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model’s performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="207-Semantic-Temporal-Single-photon-LiDAR-eess-IV-cs-CV-quant-phPDF"><a href="#207-Semantic-Temporal-Single-photon-LiDAR-eess-IV-cs-CV-quant-phPDF" class="headerlink" title="[207] Semantic Temporal Single-photon LiDAR eess.IV | cs.CV | quant-phPDF"></a>[207] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06008">Semantic Temporal Single-photon LiDAR</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | quant-ph</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06008" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fang Li, Tonglin Mu, Shuling Li, Junran Guo, Keyuan Li</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于语义知识库（SKB）的语义TSP-LiDAR系统，能够在开放集场景和低信噪比、短采集时间条件下实现自适应且鲁棒的目标识别。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有TSP-LiDAR方法在开放集场景和低SNR条件下表现不佳，需要一种能够动态适应新目标且无需重训练的方法。</p>
<p><strong>Result:</strong> 实验表明，该系统在未知目标识别中达到89%准确率（传统方法为66%），且在低SNR和短采集时间条件下性能优越。</p>
<p><strong>Insight:</strong> 自更新的SKB机制为实现复杂动态环境中的自适应目标识别提供了新思路。</p>
<p><strong>Abstract:</strong> Temporal single-photon (TSP-) LiDAR presents a promising solution for imaging-free target recognition over long distances with reduced size, cost, and power consumption. However, existing TSP-LiDAR approaches are ineffective in handling open-set scenarios where unknown targets emerge, and they suffer significant performance degradation under low signal-to-noise ratio (SNR) and short acquisition times (fewer photons). Here, inspired by semantic communication, we propose a semantic TSP-LiDAR based on a self-updating semantic knowledge base (SKB), in which the target recognition processing of TSP-LiDAR is formulated as a semantic communication. The results, both simulation and experiment, demonstrate that our approach surpasses conventional methods, particularly under challenging conditions of low SNR and limited acquisition time. More importantly, our self-updating SKB mechanism can dynamically update the semantic features of newly encountered targets in the SKB, enabling continuous adaptation without the need for extensive retraining of the neural network. In fact, a recognition accuracy of 89% is achieved on nine types of unknown targets in real-world experiments, compared to 66% without the updating mechanism. These findings highlight the potential of our framework for adaptive and robust target recognition in complex and dynamic environments.</p>
  </div>
</details>

<hr>
<h3 id="208-Clinical-Interpretability-of-Deep-Learning-Segmentation-Through-Shapley-Derived-Agreement-and-Uncertainty-Metrics-eess-IV-cs-CV-cs-LGPDF"><a href="#208-Clinical-Interpretability-of-Deep-Learning-Segmentation-Through-Shapley-Derived-Agreement-and-Uncertainty-Metrics-eess-IV-cs-CV-cs-LGPDF" class="headerlink" title="[208] Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics eess.IV | cs.CV | cs.LGPDF"></a>[208] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07224">Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07224" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianyi Ren, Daniel Low, Pittra Jaengprajak, Juampablo Heras Rivera, Jacob Ruzevick</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种通过Shapley值评估深度分割模型临床可解释性的方法，利用对比级别的Shapley值量化特征重要性，并通过与临床排名的符合度及Shapley排名的方差评估模型可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管深度学习在医学图像分割中表现出色，但其缺乏解释性限制了临床应用。为此，论文旨在通过Shapley值提供一种临床可解释的评估方法。</p>
<p><strong>Result:</strong> 高表现模型（Dice&gt;0.6）与临床排名符合度显著更高；Shapley排名方差与模型性能负相关（如U-Net中r&#x3D;-0.581）。</p>
<p><strong>Insight:</strong> Shapley值不仅能量化特征重要性，还能为模型可靠性提供临床可解释的代理指标，帮助医生理解模型决策。</p>
<p><strong>Abstract:</strong> Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and &#96;&#96;clinician” imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r&#x3D;-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.</p>
  </div>
</details>

<hr>
<h3 id="209-R2MF-Net-A-Recurrent-Residual-Multi-Path-Fusion-Network-for-Robust-Multi-directional-Spine-X-ray-Segmentation-eess-IV-cs-AI-cs-CVPDF"><a href="#209-R2MF-Net-A-Recurrent-Residual-Multi-Path-Fusion-Network-for-Robust-Multi-directional-Spine-X-ray-Segmentation-eess-IV-cs-AI-cs-CVPDF" class="headerlink" title="[209] R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation eess.IV | cs.AI | cs.CVPDF"></a>[209] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07576">R2MF-Net: A Recurrent Residual Multi-Path Fusion Network for Robust Multi-directional Spine X-ray Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07576" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuecheng Li, Weikuan Jia, Komildzhon Sharipov, Sharipov Hotam Beknazarovich, Farzona S. Ataeva</span></p>
<p><strong>TL;DR:</strong> 论文提出了R2MF-Net，一种用于多方向脊柱X射线图像自动分割的循环残差多路径融合网络，解决了手动分割的低效和图像质量差的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 脊柱X射线图像的分割对于定量评估脊柱侧弯至关重要，但现有方法依赖手动分割，效率低且难以复现，尤其是在图像对比度低或存在干扰结构的情况下。</p>
<p><strong>Result:</strong> 在包含228组多视角脊柱X射线图像的临床数据集上进行了评估，表现优于基线方法。</p>
<p><strong>Insight:</strong> 多方向图像的分割可通过级联网络和特征复用机制实现稳定性和鲁棒性的提升，轻量化模块能有效聚焦目标结构。</p>
<p><strong>Abstract:</strong> Accurate segmentation of spinal structures in X-ray images is a prerequisite for quantitative scoliosis assessment, including Cobb angle measurement, vertebral translation estimation and curvature classification. In routine practice, clinicians acquire coronal, left-bending and right-bending radiographs to jointly evaluate deformity severity and spinal flexibility. However, the segmentation step remains heavily manual, time-consuming and non-reproducible, particularly in low-contrast images and in the presence of rib shadows or overlapping tissues. To address these limitations, this paper proposes R2MF-Net, a recurrent residual multi-path encoder–decoder network tailored for automatic segmentation of multi-directional spine X-ray images. The overall design consists of a coarse segmentation network and a fine segmentation network connected in cascade. Both stages adopt an improved Inception-style multi-branch feature extractor, while a recurrent residual jump connection (R2-Jump) module is inserted into skip paths to gradually align encoder and decoder semantics. A multi-scale cross-stage skip (MC-Skip) mechanism allows the fine network to reuse hierarchical representations from multiple decoder levels of the coarse network, thereby strengthening the stability of segmentation across imaging directions and contrast conditions. Furthermore, a lightweight spatial-channel squeeze-and-excitation block (SCSE-Lite) is employed at the bottleneck to emphasize spine-related activations and suppress irrelevant structures and background noise. We evaluate R2MF-Net on a clinical multi-view radiograph dataset comprising 228 sets of coronal, left-bending and right-bending spine X-ray images with expert annotations.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="210-When-Distance-Distracts-Representation-Distance-Bias-in-BT-Loss-for-Reward-Models-cs-LG-cs-AI-cs-CLPDF"><a href="#210-When-Distance-Distracts-Representation-Distance-Bias-in-BT-Loss-for-Reward-Models-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[210] When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models cs.LG | cs.AI | cs.CLPDF"></a>[210] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06343">When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06343" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tong Xie, Andrew Bai, Yuanhao Ban, Yunqi Hong, Haoyu Li</span></p>
<p><strong>TL;DR:</strong> 论文分析了BT损失函数在奖励模型中的局限性，发现其梯度大小受表示距离和预测误差的双重影响，导致学习信号失衡。作者提出NormBT方法，通过自适应归一化优化学习效果，显著提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准BT损失函数在奖励模型中的应用存在梯度失衡问题，可能导致模型在小距离差异对的学习信号不足，影响性能。论文旨在揭示并解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，NormBT在多LLM骨干和数据集上性能一致提升，在Reasoning类别上增益超5%。</p>
<p><strong>Insight:</strong> BT损失的梯度设计可能隐式引入偏差，NormBT的简单高效展示了通过调整损失函数可以显著优化模型学习效果。</p>
<p><strong>Abstract:</strong> Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.</p>
  </div>
</details>

<hr>
<h3 id="211-LLM-Upgraded-Graph-Reinforcement-Learning-for-Carbon-Aware-Job-Scheduling-in-Smart-Manufacturing-cs-LG-cs-CLPDF"><a href="#211-LLM-Upgraded-Graph-Reinforcement-Learning-for-Carbon-Aware-Job-Scheduling-in-Smart-Manufacturing-cs-LG-cs-CLPDF" class="headerlink" title="[211] LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing cs.LG | cs.CLPDF"></a>[211] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06351">LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06351" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiying Yang, Fang Liu, Wei Zhang, Xin Lou, Malcolm Yoke Hean Low</span></p>
<p><strong>TL;DR:</strong> 论文提出了Luca框架，结合大型语言模型（LLM）和图强化学习，用于智能制造中碳感知的柔性作业车间调度。该方法通过融合图嵌入和LLM语义，实现了高效的动态调度优化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 智能制造系统需要动态且可持续的作业调度，传统方法难以同时优化时间（makespan）和碳排放目标。Luca框架旨在解决这一挑战。</p>
<p><strong>Result:</strong> 在合成数据集上，Luca平均降低makespan 4.1%（最高12.2%），同时保持碳排放水平；在公开数据集上表现出进一步优化效果。</p>
<p><strong>Insight:</strong> LLM能够增强传统GNN的语义表达能力，结合强化学习可显著提升复杂调度问题的优化效果。</p>
<p><strong>Abstract:</strong> This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1% and up to 12.2% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.</p>
  </div>
</details>

<hr>
<h3 id="212-A-Fast-and-Effective-Solution-to-the-Problem-of-Look-ahead-Bias-in-LLMs-cs-LG-cs-CLPDF"><a href="#212-A-Fast-and-Effective-Solution-to-the-Problem-of-Look-ahead-Bias-in-LLMs-cs-LG-cs-CLPDF" class="headerlink" title="[212] A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs cs.LG | cs.CLPDF"></a>[212] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06607">A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06607" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Humzah Merchant, Bradford Levy</span></p>
<p><strong>TL;DR:</strong> 为解决LLMs在金融预测任务中的前瞻性偏差问题，本文提出了一种快速、低成本的方法，通过调整大型基础模型的logits，结合两个较小、专用模型，有效去除知识和纠正偏差。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> LLMs在金融预测任务中因训练数据的时间跨度问题存在前瞻性偏差，导致无法进行常规的回测，而重新训练模型的成本又过高。</p>
<p><strong>Result:</strong> 该方法能有效去除知识和偏差，性能优于现有方法。</p>
<p><strong>Insight:</strong> 通过高效的专用模型组合调整logits，是一种低成本解决前瞻性偏差的有效途径。</p>
<p><strong>Abstract:</strong> Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models – one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.</p>
  </div>
</details>

<hr>
<h3 id="213-Block-Sparse-Flash-Attention-cs-LG-cs-CL-cs-PFPDF"><a href="#213-Block-Sparse-Flash-Attention-cs-LG-cs-CL-cs-PFPDF" class="headerlink" title="[213] Block Sparse Flash Attention cs.LG | cs.CL | cs.PFPDF"></a>[213] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07011">Block Sparse Flash Attention</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL | cs.PF</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07011" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Daniel Ohayon, Itay Lamprecht, Itay Hubara, Israel Cohen, Daniel Soudry</span></p>
<p><strong>TL;DR:</strong> 论文提出了Block-Sparse FlashAttention（BSFA），一种无需训练的注意力加速方法，通过计算精确的查询-键相似性来选择最重要的值块，从而减少约50%的计算量和内存传输。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于注意力机制的二次复杂度限制了大规模语言模型的长上下文推理能力，需要一种高效的替代方案以减少计算负担。</p>
<p><strong>Result:</strong> 在Llama-3.1-8B上，BSFA实现了最高1.24倍的加速，同时保持99%以上的基准准确率，甚至在某些情况下提升了准确性。</p>
<p><strong>Insight:</strong> BSFA通过聚焦最相关内容，不仅能加速推理，还能提升模型性能，优于现有的稀疏注意力方法。</p>
<p><strong>Abstract:</strong> Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention’s quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Danielohayon/Block-Sparse-Flash-Attention">https://github.com/Danielohayon/Block-Sparse-Flash-Attention</a></p>
  </div>
</details>

<hr>
<h3 id="214-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models-cs-LG-cs-CLPDF"><a href="#214-Pay-Less-Attention-to-Function-Words-for-Free-Robustness-of-Vision-Language-Models-cs-LG-cs-CLPDF" class="headerlink" title="[214] Pay Less Attention to Function Words for Free Robustness of Vision-Language Models cs.LG | cs.CLPDF"></a>[214] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07222">Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07222" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiwei Tian, Chenhao Lin, Zhengyu Zhao, Chao Shen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为Function-word De-Attention (FDA)的方法，通过减少对功能词的注意力来增强视觉语言模型的对抗鲁棒性，同时保持性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉语言模型（VLMs）在跨模态对抗攻击中容易因功能词而表现出脆弱性，如何在不牺牲性能的情况下提升其鲁棒性成为研究动机。</p>
<p><strong>Result:</strong> 在3种测试模型上，FDA平均降低了18&#x2F;13&#x2F;53%的攻击成功率（ASR），而性能仅下降0.2&#x2F;0.3&#x2F;0.6%。在视觉定位任务中，ASR降幅达90%且性能提升0.3%。</p>
<p><strong>Insight:</strong> 功能词是VLMs的潜在弱点，通过抑制其影响可以有效提升模型鲁棒性，且对性能影响极小。</p>
<p><strong>Abstract:</strong> To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18&#x2F;13&#x2F;53% ASR drop with only 0.2&#x2F;0.3&#x2F;0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at <a target="_blank" rel="noopener" href="https://github.com/michaeltian108/FDA">https://github.com/michaeltian108/FDA</a>.</p>
  </div>
</details>

<hr>
<h3 id="215-Group-Representational-Position-Encoding-cs-LG-cs-AI-cs-CLPDF"><a href="#215-Group-Representational-Position-Encoding-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[215] Group Representational Position Encoding cs.LG | cs.AI | cs.CLPDF"></a>[215] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07805">Group Representational Position Encoding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07805" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifan Zhang, Zixiang Chen, Yifeng Liu, Zhen Qin, Huizhuo Yuan</span></p>
<p><strong>TL;DR:</strong> GRAPE提出了一个基于群作用的统一位置编码框架，分为乘法旋转（Multiplicative GRAPE）和加法对数偏置（Additive GRAPE），扩展了RoPE和ALiBi的能力，支持更复杂的位置几何建模。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统位置编码方法（如RoPE和ALiBi）各自解决不同问题，但缺乏统一的框架。GRAPE旨在通过群作用理论提供一个通用的位置编码设计空间。</p>
<p><strong>Result:</strong> GRAPE严格包含RoPE和ALiBi作为特例，并能以更低成本（O(d)或O(r d)）支持更复杂的几何关系，适用于长上下文建模。</p>
<p><strong>Insight:</strong> 群作用理论为位置编码提供了一个自然的设计空间，既能统一现有方法，又能灵活扩展支持新功能。</p>
<p><strong>Abstract:</strong> We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)&#x3D;\exp(n,ω,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d&#x2F;2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: <a target="_blank" rel="noopener" href="https://github.com/model-architectures/GRAPE">https://github.com/model-architectures/GRAPE</a>.</p>
  </div>
</details>

<hr>
<h3 id="216-Estimating-Black-Carbon-Concentration-from-Urban-Traffic-Using-Vision-Based-Machine-Learning-cs-LG-cs-CV-cs-CY-cs-ETPDF"><a href="#216-Estimating-Black-Carbon-Concentration-from-Urban-Traffic-Using-Vision-Based-Machine-Learning-cs-LG-cs-CV-cs-CY-cs-ETPDF" class="headerlink" title="[216] Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning cs.LG | cs.CV | cs.CY | cs.ETPDF"></a>[216] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06649">Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV | cs.CY | cs.ET</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06649" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Camellia Zakaria, Aryan Sadeghi, Weaam Jaafar, Junshi Xu, Alex Mariakakis</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于视觉的机器学习系统，利用交通视频和天气数据估计城市道路的黑碳浓度，填补了交通监控与环境影响之间的数据空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 城市地区的黑碳（BC）排放主要由交通驱动，但目前缺乏低成本的大规模监测手段。现有的BC监测设备昂贵且稀少，导致缺乏本地化的BC数据以支持政策干预。</p>
<p><strong>Result:</strong> 模型的预测性能较好，R²为0.72，RMSE为129.42 ng&#x2F;m³，验证了方法的有效性。</p>
<p><strong>Insight:</strong> 该研究利用城市已有的交通监控基础设施，低成本地生成了与污染相关的数据，为本地化污染治理、城市规划及环境正义提供了支持。</p>
<p><strong>Abstract:</strong> Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng&#x2F;m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.</p>
  </div>
</details>

<hr>
<h3 id="217-Enhancing-Interpretability-of-AR-SSVEP-Based-Motor-Intention-Recognition-via-CNN-BiLSTM-and-SHAP-Analysis-on-EEG-Data-cs-LG-cs-CVPDF"><a href="#217-Enhancing-Interpretability-of-AR-SSVEP-Based-Motor-Intention-Recognition-via-CNN-BiLSTM-and-SHAP-Analysis-on-EEG-Data-cs-LG-cs-CVPDF" class="headerlink" title="[217] Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data cs.LG | cs.CVPDF"></a>[217] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06730">Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06730" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lin Yang, Xiang Li, Xin Ma, Xinxin Zhao</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于AR-SSVEP的脑机接口系统，结合CNN-BiLSTM和多头注意力机制（MACNN-BiLSTM）来识别运动意图，并通过SHAP分析增强模型可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统SSVEP-BCI系统依赖外部视觉刺激设备，限制了实际应用。患者缺乏主观参与，治疗师工作负担高，因此需要一种更实用的解决方案。</p>
<p><strong>Result:</strong> 系统提升了运动意图识别的实时性，支持运动功能障碍患者的康复训练。</p>
<p><strong>Insight:</strong> AR技术与EEG结合可提升患者参与度；SHAP方法能为神经网络决策提供透明解释。</p>
<p><strong>Abstract:</strong> Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network’s decision-making process, enhancing the model’s interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.</p>
  </div>
</details>

<hr>
<h3 id="218-Transformation-of-Biological-Networks-into-Images-via-Semantic-Cartography-for-Visual-Interpretation-and-Scalable-Deep-Analysis-cs-LG-cs-CVPDF"><a href="#218-Transformation-of-Biological-Networks-into-Images-via-Semantic-Cartography-for-Visual-Interpretation-and-Scalable-Deep-Analysis-cs-LG-cs-CVPDF" class="headerlink" title="[218] Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis cs.LG | cs.CVPDF"></a>[218] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07040">Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07040" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sakib Mostafa, Lei Xing, Md. Tauhidul Islam</span></p>
<p><strong>TL;DR:</strong> Graph2Image将复杂的生物网络转化为二维图像，利用CNN实现高效、可解释的网络分析，显著提升了分类性能和可扩展性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生物网络的规模和复杂性限制了现有分析方法的效果，尤其是可扩展性和解释性。Graph2Image旨在解决这些问题。</p>
<p><strong>Result:</strong> 在多个大型生物网络数据集上，Graph2Image显著优于现有方法，且能在个人计算机上处理十亿级节点的网络。</p>
<p><strong>Insight:</strong> 图像化网络突破了传统方法的限制，为多模态数据整合和可解释分析开辟了新途径。</p>
<p><strong>Abstract:</strong> Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes &gt; 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.</p>
  </div>
</details>

<hr>
<h3 id="219-Winning-the-Lottery-by-Preserving-Network-Training-Dynamics-with-Concrete-Ticket-Search-cs-LG-cs-AI-cs-CV-cs-NEPDF"><a href="#219-Winning-the-Lottery-by-Preserving-Network-Training-Dynamics-with-Concrete-Ticket-Search-cs-LG-cs-AI-cs-CV-cs-NEPDF" class="headerlink" title="[219] Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search cs.LG | cs.AI | cs.CV | cs.NEPDF"></a>[219] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07142">Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | cs.NE</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07142" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tanay Arora, Christof Teuscher</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Concrete Ticket Search (CTS)的算法，通过组合优化和梯度平衡机制，高效地识别高性能稀疏子网络，解决了现有方法在稀疏性-准确性权衡上的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的Pruning-at-Initialization (PaI)方法依赖一阶显著性指标，忽略了权重间的依赖关系，导致在稀疏性任务中性能下降。论文通过CTS算法解决这一问题，同时提升计算效率和准确性。</p>
<p><strong>Result:</strong> 在CIFAR10上，CTS在ResNet-20上达到99.3%稀疏性和74.0%准确率，仅需7.9分钟，显著优于LTR方法（68.3%准确率，95.2分钟）。</p>
<p><strong>Insight:</strong> CTS在高度稀疏任务中表现出色，其优势在于全局优化权重依赖关系，而不仅仅是局部显著性。</p>
<p><strong>Abstract:</strong> The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks (‘winning tickets’) within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI’s reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS’s subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.</p>
  </div>
</details>

<hr>
<h3 id="220-Revolutionizing-Mixed-Precision-Quantization-Towards-Training-free-Automatic-Proxy-Discovery-via-Large-Language-Models-cs-LG-cs-CVPDF"><a href="#220-Revolutionizing-Mixed-Precision-Quantization-Towards-Training-free-Automatic-Proxy-Discovery-via-Large-Language-Models-cs-LG-cs-CVPDF" class="headerlink" title="[220] Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models cs.LG | cs.CVPDF"></a>[220] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07419">Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07419" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haidong Kang, Jun Du, Lihong Lin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于大型语言模型（LLMs）的无训练自动代理发现框架TAP，用于混合精度量化（MPQ），通过强化学习优化提示，实现了高性能的量化代理自动生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统MPQ方法依赖于昂贵的可微分优化或人工设计的代理，效率低且依赖专家知识。因此，作者提出利用LLMs自动发现代理，无需训练或专家介入。</p>
<p><strong>Result:</strong> 在主流基准测试中，TAP实现了state-of-the-art性能，验证了其有效性。</p>
<p><strong>Insight:</strong> LLMs可以用于自动化设计任务（如MPQ），结合强化学习的提示优化方法能显著提升LLMs的任务表现。</p>
<p><strong>Abstract:</strong> Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs’ reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.</p>
  </div>
</details>

<hr>
<h3 id="221-KAN-Dreamer-Benchmarking-Kolmogorov-Arnold-Networks-as-Function-Approximators-in-World-Models-cs-LG-cs-AI-cs-CV-cs-NE-cs-ROPDF"><a href="#221-KAN-Dreamer-Benchmarking-Kolmogorov-Arnold-Networks-as-Function-Approximators-in-World-Models-cs-LG-cs-AI-cs-CV-cs-NE-cs-ROPDF" class="headerlink" title="[221] KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models cs.LG | cs.AI | cs.CV | cs.NE | cs.ROPDF"></a>[221] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07437">KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | cs.NE | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07437" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenwei Shi, Xueyu Luan</span></p>
<p><strong>TL;DR:</strong> 论文探讨了将Kolmogorov-Arnold Networks (KANs) 和其高效变体FastKAN集成到DreamerV3框架中，提出了KAN-Dreamer，旨在提升模型的参数效率和可解释性，同时保持性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> DreamerV3是MBRL领域的先进算法，但MLP结构在参数效率和可解释性上有局限。KANs作为替代方案，具有优势，但其计算开销较大，需要优化以适应实际应用。</p>
<p><strong>Result:</strong> 实验显示FastKAN作为Reward和Continue预测器的替代方案，性能与MLP相当，同时在样本效率和训练速度上保持一致。</p>
<p><strong>Insight:</strong> KANs的参数效率和可解释性优势在MBRL中有潜力，FastKAN的计算优化是关键。未来研究可进一步探讨KAN在其他模块的应用。</p>
<p><strong>Abstract:</strong> DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs’ computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.</p>
  </div>
</details>

<hr>
<h3 id="222-Exploring-possible-vector-systems-for-faster-training-of-neural-networks-with-preconfigured-latent-spaces-cs-LG-cs-AI-cs-CVPDF"><a href="#222-Exploring-possible-vector-systems-for-faster-training-of-neural-networks-with-preconfigured-latent-spaces-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[222] Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces cs.LG | cs.AI | cs.CVPDF"></a>[222] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07509">Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07509" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nikita Gabdullin</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了预定义向量系统在神经网络的潜在空间配置中的作用，通过使用特定向量系统（如An根系向量），可以显著加速训练并减少潜在空间维度，从而优化大类别数据集上的分类任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 神经网络的性能与其潜在空间的嵌入分布特性密切相关。传统的分类层在大类别数据集上效率低下，而预定义的潜在空间配置可以绕过这一问题，同时优化训练速度和模型表现。</p>
<p><strong>Result:</strong> 实验表明，预定义的潜在空间配置显著加快了训练速度，尤其是在大类别数据集上。同时，最小的潜在空间维度进一步提升了收敛效率。</p>
<p><strong>Insight:</strong> 预定义的向量系统不仅简化了分类任务的结构，还能优化训练效率和存储成本。这一方法为处理超大规模分类问题提供了新的可能性。</p>
<p><strong>Abstract:</strong> The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.</p>
  </div>
</details>

<hr>
<h3 id="223-ReLaX-Reasoning-with-Latent-Exploration-for-Large-Reasoning-Models-cs-LG-cs-CVPDF"><a href="#223-ReLaX-Reasoning-with-Latent-Exploration-for-Large-Reasoning-Models-cs-LG-cs-CVPDF" class="headerlink" title="[223] ReLaX: Reasoning with Latent Exploration for Large Reasoning Models cs.LG | cs.CVPDF"></a>[223] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07558">ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07558" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shimin Zhang, Xianwei Chen, Yufan Shen, Ziyuan Ye, Jibin Wu</span></p>
<p><strong>TL;DR:</strong> 论文提出ReLaX方法，通过量化模型潜在动态的异质性（DSD指标），结合Koopman算子理论，优化大型推理模型的探索与利用平衡，显著改善了性能饱和问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> RLVR方法在推理任务中表现优异，但容易导致熵崩塌和性能饱和的问题。因此，作者希望通过分析模型的潜在动态，设计更有效的探索与利用策略。</p>
<p><strong>Result:</strong> 在多模态和纯文本推理任务中，ReLaX显著缓解了熵崩塌问题，性能达到SOTA。</p>
<p><strong>Insight:</strong> 潜在动态比token级熵更能反映模型的探索能力，通过显式量化潜在动态的多样性可以更有效地优化推理模型的性能。</p>
<p><strong>Abstract:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model’s latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="224-An-Index-based-Approach-for-Efficient-and-Effective-Web-Content-Extraction-cs-IR-cs-CLPDF"><a href="#224-An-Index-based-Approach-for-Efficient-and-Effective-Web-Content-Extraction-cs-IR-cs-CLPDF" class="headerlink" title="[224] An Index-based Approach for Efficient and Effective Web Content Extraction cs.IR | cs.CLPDF"></a>[224] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06641">An Index-based Approach for Efficient and Effective Web Content Extraction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06641" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yihan Chen, Benfeng Xu, Xiaorui Wang, Zhendong Mao</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于索引的方法，用于高效且有效地提取网页内容，解决了现有方法的延迟高、适应性差和对网页结构无视的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着网络代理（如Deep Research）需要处理大量网页信息，如何在大量低信号密度的内容中高效提取相关信息成为一个重要但技术挑战性高的问题。现有方法如生成式提取模型延迟高，基于规则的方法缺乏适应性，而分块重排序方法忽略网页结构。</p>
<p><strong>Result:</strong> 实验表明，该方法在RAG QA系统中提高了问答准确性，并在两种场景（主内容提取和查询相关提取）中显著优于现有方法，同时速度快。</p>
<p><strong>Insight:</strong> 该方法通过结构感知的索引预测，高效地解决了LLM在处理大量网页内容时的延迟和准确性问题，为RAG流程提供了重要改进。</p>
<p><strong>Abstract:</strong> As web agents (e.g., Deep Research) routinely consume massive volumes of web pages to gather and analyze information, LLM context management – under large token budgets and low signal density – emerges as a foundational, high-importance, and technically challenging problem for agentic and RAG pipelines. Existing solutions for extracting relevant content are inadequate: generative extraction models suffer from high latency, rule-based heuristics lack adaptability, and chunk-and-rerank methods are blind to webpage structure. To overcome these issues, we introduce Index-based Web Content Extraction to reframe the extraction process from slow, token-by-token generation into a highly efficient, discriminative task of index prediction, achieving both effectiveness and efficiency. We partition HTML into structure-aware, addressable segments, and extract only the positional indices of content relevant to a given query. This method decouples extraction latency from content length, enabling rapid, query-relevant extraction. We first evaluate our method as a post-retrieval processing component within an RAG QA system and find that it improves QA accuracy. Then we directly measure its match rate with the target content in two scenarios: main content extraction (ME) and query-relevant extraction (QE). Experimental results show that our method outperforms existing works in both accuracy and speed, effectively bridging the gap between LLMs and the vast webpages.</p>
  </div>
</details>

<hr>
<div id='cs.MA'></div>

<h1 id="cs-MA-Back"><a href="#cs-MA-Back" class="headerlink" title="cs.MA [Back]"></a>cs.MA <a href="#toc">[Back]</a></h1><h3 id="225-AI-Generated-Compromises-for-Coalition-Formation-Modeling-Simulation-and-a-Textual-Case-Study-cs-MA-cs-CL-cs-GTPDF"><a href="#225-AI-Generated-Compromises-for-Coalition-Formation-Modeling-Simulation-and-a-Textual-Case-Study-cs-MA-cs-CL-cs-GTPDF" class="headerlink" title="[225] AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study cs.MA | cs.CL | cs.GTPDF"></a>[225] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.05983">AI-Generated Compromises for Coalition Formation: Modeling, Simulation, and a Textual Case Study</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MA | cs.CL | cs.GT</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.05983" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eyal Briman, Ehud Shapiro, Nimrod Talmon</span></p>
<p><strong>TL;DR:</strong> 提出了一个全面的模型，结合AI技术生成妥协提案，支持民主文本编辑。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在联盟形成过程中，如何找到多数支持的妥协提案是一个开放性问题，传统工具在此领域存在局限。</p>
<p><strong>Result:</strong> 展示了AI在支持大规模民主文本编辑（如宪法起草）中的潜力，优于传统工具。</p>
<p><strong>Insight:</strong> AI技术可以有效地支持复杂的社会协作任务，尤其是在需要多代理妥协的场景中。</p>
<p><strong>Abstract:</strong> The challenge of finding compromises between agent proposals is fundamental to AI sub-fields such as argumentation, mediation, and negotiation. Building on this tradition, Elkind et al. (2021) introduced a process for coalition formation that seeks majority-supported proposals preferable to the status quo, using a metric space where each agent has an ideal point. The crucial step in this iterative process involves identifying compromise proposals around which agent coalitions can unite. How to effectively find such compromise proposals, however, remains an open question. We address this gap by formalizing a holistic model that encompasses agent bounded rationality and uncertainty and developing AI models to generate such compromise proposals. We focus on the domain of collaboratively writing text documents – e.g., to enable the democratic creation of a community constitution. We apply NLP (Natural Language Processing) techniques and utilize LLMs (Large Language Models) to create a semantic metric space for text and develop algorithms to suggest suitable compromise points. To evaluate the effectiveness of our algorithms, we simulate various coalition formation processes and demonstrate the potential of AI to facilitate large-scale democratic text editing, such as collaboratively drafting a constitution, an area where traditional tools are limited.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="226-ARCANE-A-Multi-Agent-Framework-for-Interpretable-and-Configurable-Alignment-cs-AI-cs-CLPDF"><a href="#226-ARCANE-A-Multi-Agent-Framework-for-Interpretable-and-Configurable-Alignment-cs-AI-cs-CLPDF" class="headerlink" title="[226] ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment cs.AI | cs.CLPDF"></a>[226] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06196">ARCANE: A Multi-Agent Framework for Interpretable and Configurable Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06196" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Charlie Masters, Marta Grześkiewicz, Stefano V. Albrecht</span></p>
<p><strong>TL;DR:</strong> ARCANE是一个多智能体框架，通过动态生成自然语言准则（rubrics）来实现可解释和可配置的AI对齐。它结合了多智能体协作和效用理论，提出了一种正则化的GSPO方法，能够在交互时调整偏好而无需重新训练。实验证明了其在高复杂度任务中的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着基于大型语言模型的智能体被广泛部署于长期任务，如何保持其与利益相关者的偏好对齐变得至关重要，需要一个可解释且能在交互时调整的奖励模型。</p>
<p><strong>Result:</strong> ARCANE生成的准则具有紧凑性和可读性，支持无需重新训练的偏好调整（如正确性与简洁性的权衡），在高复杂度任务中表现优越。</p>
<p><strong>Insight:</strong> 自然语言准则为AI对齐提供了一种可解释且灵活的解决方案，尤其适用于长期复杂任务，为未来自适应对齐研究提供了新方向。</p>
<p><strong>Abstract:</strong> As agents based on large language models are increasingly deployed to long-horizon tasks, maintaining their alignment with stakeholder preferences becomes critical. Effective alignment in such settings requires reward models that are interpretable so that stakeholders can understand and audit model objectives. Moreover, reward models must be capable of steering agents at interaction time, allowing preference shifts to be incorporated without retraining. We introduce ARCANE, a framework that frames alignment as a multi-agent collaboration problem that dynamically represents stakeholder preferences as natural-language rubrics: weighted sets of verifiable criteria that can be generated on-the-fly from task context. Inspired by utility theory, we formulate rubric learning as a reconstruction problem and apply a regularized Group-Sequence Policy Optimization (GSPO) procedure that balances interpretability, faithfulness, and computational efficiency. Using a corpus of 219 labeled rubrics derived from the GDPVal benchmark, we evaluate ARCANE on challenging tasks requiring multi-step reasoning and tool use. The learned rubrics produce compact, legible evaluations and enable configurable trade-offs (e.g., correctness vs. conciseness) without retraining. Our results show that rubric-based reward models offer a promising path toward interpretable, test-time adaptive alignment for complex, long-horizon AI systems.</p>
  </div>
</details>

<hr>
<h3 id="227-Less-Is-More-for-Multi-Step-Logical-Reasoning-of-LLM-Generalisation-Under-Rule-Removal-Paraphrasing-and-Compression-cs-AI-cs-CL-cs-LG-cs-LOPDF"><a href="#227-Less-Is-More-for-Multi-Step-Logical-Reasoning-of-LLM-Generalisation-Under-Rule-Removal-Paraphrasing-and-Compression-cs-AI-cs-CL-cs-LG-cs-LOPDF" class="headerlink" title="[227] Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression cs.AI | cs.CL | cs.LG | cs.LOPDF"></a>[227] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06393">Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG | cs.LO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06393" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiming Bao, Xiaoxuan Fu</span></p>
<p><strong>TL;DR:</strong> LLMs在逻辑推理中表现稳定，但对缺失或矛盾信息脆弱。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探究LLMs在逻辑结构扰动下的泛化能力，揭示其在推理中的局限性。</p>
<p><strong>Result:</strong> LLMs在语义保留的逻辑变换中表现稳定，但对缺失规则或矛盾信息时准确率骤降。</p>
<p><strong>Insight:</strong> LLMs的逻辑泛化能力仍有显著缺陷，尤其在关键规则缺失或存在冲突时失效。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.   Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.</p>
  </div>
</details>

<hr>
<h3 id="228-Cognitive-Control-Architecture-CCA-A-Lifecycle-Supervision-Framework-for-Robustly-Aligned-AI-Agents-cs-AI-cs-CL-cs-CRPDF"><a href="#228-Cognitive-Control-Architecture-CCA-A-Lifecycle-Supervision-Framework-for-Robustly-Aligned-AI-Agents-cs-AI-cs-CL-cs-CRPDF" class="headerlink" title="[228] Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents cs.AI | cs.CL | cs.CRPDF"></a>[228] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06716">Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.CR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06716" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhibo Liang, Tianze Hu, Zaiye Chen, Mingjie Tang</span></p>
<p><strong>TL;DR:</strong> 论文提出了Cognitive Control Architecture（CCA），一个全生命周期监督框架，通过意图图和分层裁决器双重防线，有效抵御间接提示注入攻击，同时实现安全、功能和效率的平衡。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大型语言模型代理在面对间接提示注入攻击时表现出系统性脆弱性，现有防御机制存在安全和功能之间的权衡不足。因此，需要一种能够全面保护任务执行流程完整性且不牺牲效率的框架。</p>
<p><strong>Result:</strong> 在AgentDojo基准测试上，CCA不仅能抵御复杂攻击，还能在不妥协安全性的情况下保持高效和鲁棒性。</p>
<p><strong>Insight:</strong> 任何间接提示注入攻击的恶意目标都会表现为行为轨迹的可检测偏离，因此可以通过全生命周期监督实现防御。</p>
<p><strong>Abstract:</strong> Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated “Intent Graph”; and (ii) an innovative “Tiered Adjudicator” that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.</p>
  </div>
</details>

<hr>
<h3 id="229-ProAgent-Harnessing-On-Demand-Sensory-Contexts-for-Proactive-LLM-Agent-Systems-cs-AI-cs-CL-cs-HCPDF"><a href="#229-ProAgent-Harnessing-On-Demand-Sensory-Contexts-for-Proactive-LLM-Agent-Systems-cs-AI-cs-CL-cs-HCPDF" class="headerlink" title="[229] ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems cs.AI | cs.CL | cs.HCPDF"></a>[229] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06721">ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06721" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bufang Yang, Lilin Xu, Liekang Zeng, Yunqi Guo, Siyang Jiang</span></p>
<p><strong>TL;DR:</strong> ProAgent是一种基于感知上下文和LLM推理的主动式代理系统，首次实现了终端到终端的主动辅助。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLM代理多为反应式系统，依赖用户显式指令启动服务，增加了用户认知负担，亟需一种主动感知和响应的解决方案。</p>
<p><strong>Result:</strong> 在真实测试中，ProAgent的主动预测准确率提高33.4%，工具调用F1分数提高16.8%，用户满意度显著提升。</p>
<p><strong>Insight:</strong> ProAgent通过融合多模态感知和LLM推理，展现了主动式代理系统的潜力，为未来智能助手的发展奠定了基础。</p>
<p><strong>Abstract:</strong> Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at <a target="_blank" rel="noopener" href="https://youtu.be/pRXZuzvrcVs">https://youtu.be/pRXZuzvrcVs</a>.</p>
  </div>
</details>

<hr>
<h3 id="230-A-Neural-Affinity-Framework-for-Abstract-Reasoning-Diagnosing-the-Compositional-Gap-in-Transformer-Architectures-via-Procedural-Task-Taxonomy-cs-AI-cs-CL-cs-LGPDF"><a href="#230-A-Neural-Affinity-Framework-for-Abstract-Reasoning-Diagnosing-the-Compositional-Gap-in-Transformer-Architectures-via-Procedural-Task-Taxonomy-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[230] A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy cs.AI | cs.CL | cs.LGPDF"></a>[230] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07109">A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07109" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miguel Ingram, Arthur Joseph Merritt</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个9类别的任务分类法，用于诊断Transformer架构在抽象推理任务中的组成性差距，揭示了神经亲和力对任务表现的限制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 回应Hodel等人的需求，定义一个任务相关性的正式框架，用以研究Transformer架构在抽象推理任务中的局限性。</p>
<p><strong>Result:</strong> 1）69.5%的任务表现出局部模式高准确率但全局合成低准确率（组成性差距）；2）神经亲和力显著影响任务表现；3）低亲和力任务的表现上限明显低于高亲和力任务。</p>
<p><strong>Insight:</strong> 进步需要混合架构，其模块应与任务亲和力对齐，而非单纯增加数据或训练规模。</p>
<p><strong>Abstract:</strong> Responding to Hodel et al.’s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy’s visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers–a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve &gt;80% cell accuracy (local patterns) but &lt;10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.’s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p&lt;0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,</p>
  </div>
</details>

<hr>
<h3 id="231-ReasonBENCH-Benchmarking-the-In-Stability-of-LLM-Reasoning-cs-AI-cs-CL-cs-LGPDF"><a href="#231-ReasonBENCH-Benchmarking-the-In-Stability-of-LLM-Reasoning-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[231] ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning cs.AI | cs.CL | cs.LGPDF"></a>[231] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07795">ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07795" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nearchos Potamitis, Lars Klein, Akhil Arora</span></p>
<p><strong>TL;DR:</strong> ReasonBENCH是首个专注于评估大语言模型（LLM）推理稳定性的基准测试工具，通过多轮运行协议和统计度量揭示了当前推理方法的高不稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LLM评估主要依赖单次运行准确率，忽略了随机解码带来的不确定性，导致性能报告的可靠性和可重复性存疑。</p>
<p><strong>Result:</strong> 多数推理策略和模型表现出高不稳定性，即使平均性能相近的方法也可能存在四倍差异的置信区间，且高性能方法成本更不稳定。</p>
<p><strong>Insight:</strong> 推理稳定性是LLM可靠性的关键维度，提示设计和模型规模显著影响解率与稳定性之间的权衡。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method’s reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at <a target="_blank" rel="noopener" href="https://github.com/au-clan/ReasonBench">https://github.com/au-clan/ReasonBench</a> .</p>
  </div>
</details>

<hr>
<h3 id="232-Utilizing-Multi-Agent-Reinforcement-Learning-with-Encoder-Decoder-Architecture-Agents-to-Identify-Optimal-Resection-Location-in-Glioblastoma-Multiforme-Patients-cs-AI-cs-CV-eess-IVPDF"><a href="#232-Utilizing-Multi-Agent-Reinforcement-Learning-with-Encoder-Decoder-Architecture-Agents-to-Identify-Optimal-Resection-Location-in-Glioblastoma-Multiforme-Patients-cs-AI-cs-CV-eess-IVPDF" class="headerlink" title="[232] Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients cs.AI | cs.CV | eess.IVPDF"></a>[232] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06990">Utilizing Multi-Agent Reinforcement Learning with Encoder-Decoder Architecture Agents to Identify Optimal Resection Location in Glioblastoma Multiforme Patients</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06990" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Krishna Arun, Moinak Bhattachrya, Paras Goel</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种利用多智能体强化学习和编码器-解码器架构的AI系统，用于诊断和治疗多形性胶质母细胞瘤（GBM）。系统通过渐进分类和生成模型实现诊断和治疗规划，显著降低了计算成本并提高了生存率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> GBM是最致命的人类癌症，五年生存率仅为5.1%，目前缺乏AI工具支持医生的诊断和治疗规划。论文旨在填补这一空白，提供端到端的解决方案。</p>
<p><strong>Result:</strong> 1. 计算成本降低22.28倍。2. 肿瘤进展推断时间减少113小时。3. Dice分数提高2.9%，预计生存率提高0.9%。</p>
<p><strong>Insight:</strong> 1. 渐进式分类模型能有效降低计算开销。2. Transformer在时间序列数据（如MRI）中表现出强大的回归能力。3. 真实数据增强显著提升了模型的泛化性能。</p>
<p><strong>Abstract:</strong> Currently, there is a noticeable lack of AI in the medical field to support doctors in treating heterogenous brain tumors such as Glioblastoma Multiforme (GBM), the deadliest human cancer in the world with a five-year survival rate of just 5.1%. This project develops an AI system offering the only end-to-end solution by aiding doctors with both diagnosis and treatment planning. In the diagnosis phase, a sequential decision-making framework consisting of 4 classification models (Convolutional Neural Networks and Support Vector Machine) are used. Each model progressively classifies the patient’s brain into increasingly specific categories, with the final step being named diagnosis. For treatment planning, an RL system consisting of 3 generative models is used. First, the resection model (diffusion model) analyzes the diagnosed GBM MRI and predicts a possible resection outcome. Second, the radiotherapy model (Spatio-Temporal Vision Transformer) generates an MRI of the brain’s progression after a user-defined number of weeks. Third, the chemotherapy model (Diffusion Model) produces the post-treatment MRI. A survival rate calculator (Convolutional Neural Network) then checks if the generated post treatment MRI has a survival rate within 15% of the user defined target. If not, a feedback loop using proximal policy optimization iterates over this system until an optimal resection location is identified. When compared to existing solutions, this project found 3 key findings: (1) Using a sequential decision-making framework consisting of 4 small diagnostic models reduced computing costs by 22.28x, (2) Transformers regression capabilities decreased tumor progression inference time by 113 hours, and (3) Applying Augmentations resembling Real-life situations improved overall DICE scores by 2.9%. These results project to increase survival rates by 0.9%, potentially saving approximately 2,250 lives.</p>
  </div>
</details>

<hr>
<h3 id="233-A-Geometric-Unification-of-Concept-Learning-with-Concept-Cones-cs-AI-cs-CV-cs-LGPDF"><a href="#233-A-Geometric-Unification-of-Concept-Learning-with-Concept-Cones-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[233] A Geometric Unification of Concept Learning with Concept Cones cs.AI | cs.CV | cs.LGPDF"></a>[233] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07355">A Geometric Unification of Concept Learning with Concept Cones</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07355" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alexandre Rocchi–Henry, Thomas Fel, Gianni Franchi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个几何框架，统一了监督和非监督概念学习的范式，展示了概念瓶颈模型（CBM）和稀疏自编码器（SAE）在激活空间中学习的概念锥的共性，并提出了一种量化方法评估SAE发现的概念与人类定义概念的匹配度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长期以来，解释性研究中的监督（CBM）和非监督（SAE）方法各自发展但缺乏交流。本文旨在通过几何统一揭示它们的共同结构，并建立桥梁。</p>
<p><strong>Result:</strong> 发现了稀疏度和扩展因子的最佳组合，使得SAE学习的概念锥与CBM的定义概念在几何和语义上高度对齐。</p>
<p><strong>Insight:</strong> 监督和非监督概念学习方法本质上共享相同的几何结构，其差异仅在于概念锥的选择方式。这一框架为评估和指导SAE的改进提供了原则性工具。</p>
<p><strong>Abstract:</strong> Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases – such as SAE type, sparsity, or expansion ratio – to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction – selected or annotated by humans – though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a &#96;&#96;sweet spot’’ in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.</p>
  </div>
</details>

<hr>
<div id='cs.CY'></div>

<h1 id="cs-CY-Back"><a href="#cs-CY-Back" class="headerlink" title="cs.CY [Back]"></a>cs.CY <a href="#toc">[Back]</a></h1><h3 id="234-Why-They-Disagree-Decoding-Differences-in-Opinions-about-AI-Risk-on-the-Lex-Fridman-Podcast-cs-CY-cs-AI-cs-CLPDF"><a href="#234-Why-They-Disagree-Decoding-Differences-in-Opinions-about-AI-Risk-on-the-Lex-Fridman-Podcast-cs-CY-cs-AI-cs-CLPDF" class="headerlink" title="[234] Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast cs.CY | cs.AI | cs.CLPDF"></a>[234] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06350">Why They Disagree: Decoding Differences in Opinions about AI Risk on the Lex Fridman Podcast</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06350" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nghi Truong, Phanish Puranam, Özgecan Koçak</span></p>
<p><strong>TL;DR:</strong> 本文通过分析’末日论者’和’乐观论者’对AI风险的观点差异，发现争议核心在于复杂系统中的设计vs.涌现因果假设以及历史理论的适用性，并提出了一种基于LLM的文本分析方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> AI技术的快速发展引发了社会对风险的激烈争论，两派观点虽有共同目标（造福人类），但分歧显著。研究旨在解码这些差异的底层逻辑。</p>
<p><strong>Result:</strong> 两派在道德价值观上无显著分歧，争议集中在人类理性的有限性假设上，X-risk和E-risk的分歧源于不同因果模型。</p>
<p><strong>Insight:</strong> AI风险争议的底层逻辑揭示了技术和社会认知的复杂性，LLM为大规模分析提供了新工具。</p>
<p><strong>Abstract:</strong> The emergence of transformative technologies often surfaces deep societal divisions, nowhere more evident than in contemporary debates about artificial intelligence (AI). A striking feature of these divisions is that they persist despite shared interests in ensuring that AI benefits humanity and avoiding catastrophic outcomes. This paper analyzes contemporary debates about AI risk, parsing the differences between the “doomer” and “boomer” perspectives into definitional, factual, causal, and moral premises to identify key points of contention. We find that differences in perspectives about existential risk (“X-risk”) arise fundamentally from differences in causal premises about design vs. emergence in complex systems, while differences in perspectives about employment risks (“E-risks”) pertain to different causal premises about the applicability of past theories (evolution) vs their inapplicability (revolution). Disagreements about these two forms of AI risk appear to share two properties: neither involves significant disagreements on moral values and both can be described in terms of differing views on the extent of boundedness of human rationality. Our approach to analyzing reasoning chains at scale, using an ensemble of LLMs to parse textual data, can be applied to identify key points of contention in debates about risk to the public in any arena.</p>
  </div>
</details>

<hr>
<div id='cs.CE'></div>

<h1 id="cs-CE-Back"><a href="#cs-CE-Back" class="headerlink" title="cs.CE [Back]"></a>cs.CE <a href="#toc">[Back]</a></h1><h3 id="235-MATEX-A-Multi-Agent-Framework-for-Explaining-Ethereum-Transactions-cs-CE-cs-CL-cs-HCPDF"><a href="#235-MATEX-A-Multi-Agent-Framework-for-Explaining-Ethereum-Transactions-cs-CE-cs-CL-cs-HCPDF" class="headerlink" title="[235] MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions cs.CE | cs.CL | cs.HCPDF"></a>[235] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06933">MATEX: A Multi-Agent Framework for Explaining Ethereum Transactions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CE | cs.CL | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06933" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zifan Peng</span></p>
<p><strong>TL;DR:</strong> MATEX是一个多智能体框架，旨在为复杂的以太坊交易提供逐步解释，结合链上证据和实际协议语义，通过协作调查生成可信的解释。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 复杂的以太坊交易（如多跳代币流、嵌套合约调用和不透明的执行路径）导致用户难以理解，需要一种基于链上证据和协议语义的解释方法。</p>
<p><strong>Result:</strong> MATEX能够生成用户友好的交易解释，帮助用户理解复杂的交易逻辑。</p>
<p><strong>Insight:</strong> 多智能体协作可以有效结合链上和离链信息，提升区块链交易的透明度和理解性。</p>
<p><strong>Abstract:</strong> Understanding a complicated Ethereum transaction remains challenging: multi-hop token flows, nested contract calls, and opaque execution paths routinely lead users to blind signing. Based on interviews with everyday users, developers, and auditors, we identify the need for faithful, step-wise explanations grounded in both on-chain evidence and real-world protocol semantics. To meet this need, we introduce (matex, a cognitive multi-agent framework that models transaction understanding as a collaborative investigation-combining rapid hypothesis generation, dynamic off-chain knowledge retrieval, evidence-aware synthesis, and adversarial validation to produce faithful explanations.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="236-GuideNav-User-Informed-Development-of-a-Vision-Only-Robotic-Navigation-Assistant-For-Blind-Travelers-cs-RO-cs-CV-cs-HCPDF"><a href="#236-GuideNav-User-Informed-Development-of-a-Vision-Only-Robotic-Navigation-Assistant-For-Blind-Travelers-cs-RO-cs-CV-cs-HCPDF" class="headerlink" title="[236] GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers cs.RO | cs.CV | cs.HCPDF"></a>[236] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06147">GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06147" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hochul Hwang, Soowan Yang, Jahir Sadik Monon, Nicholas A Giudice, Sunghoon Ivan Lee</span></p>
<p><strong>TL;DR:</strong> GuideNav是一个专为盲人或低视力人群设计的视觉导航助手系统，通过用户研究和开源数据集开发，无需依赖昂贵的传感器，实现机器人自主导航。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有针对盲人或低视力人群的移动辅助系统研究较少直接涉及机器人导航设计，因此需要通过用户研究和实际观察填补这一空白。</p>
<p><strong>Result:</strong> 在野外测试中，GuideNav能够在5种室外环境中稳定实现公里级路径跟随，即使在场景变化显著的情况下仍保持可靠性。用户研究也验证了其可行性。</p>
<p><strong>Insight:</strong> 通过模仿导盲犬的导航方式，视觉导航系统可以在无需复杂传感器的情况下为盲人或低视力人群提供实用的导航辅助。</p>
<p><strong>Abstract:</strong> While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O&amp;M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system’s feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.</p>
  </div>
</details>

<hr>
<h3 id="237-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment-cs-RO-cs-CVPDF"><a href="#237-MIND-V-Hierarchical-Video-Generation-for-Long-Horizon-Robotic-Manipulation-with-RL-based-Physical-Alignment-cs-RO-cs-CVPDF" class="headerlink" title="[237] MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment cs.RO | cs.CVPDF"></a>[237] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06628">MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06628" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruicheng Zhang, Mingyang Zhang, Jun Zhou, Zhangrui Guo, Xiaofan Liu</span></p>
<p><strong>TL;DR:</strong> MIND-V是一种分层视频生成框架，通过结合语义推理、行为语义桥和运动视频生成器，生成长时程机器人操作的物理合理视频，并通过强化学习后训练优化物理一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频生成模型在长时程机器人操作数据生成上表现有限，依赖手动定义轨迹且难以保证物理一致性，MIND-V旨在解决这一问题。</p>
<p><strong>Result:</strong> MIND-V在长时程机器人操作视频生成中达到SOTA性能，展示了可扩展和可控的数据合成范式。</p>
<p><strong>Insight:</strong> 通过分层设计和物理对齐奖励，MIND-V为长时程机器人操作视频生成提供了一种新颖且高效的解决方案。</p>
<p><strong>Abstract:</strong> Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.</p>
  </div>
</details>

<hr>
<h3 id="238-Dynamic-Visual-SLAM-using-a-General-3D-Prior-cs-RO-cs-CVPDF"><a href="#238-Dynamic-Visual-SLAM-using-a-General-3D-Prior-cs-RO-cs-CVPDF" class="headerlink" title="[238] Dynamic Visual SLAM using a General 3D Prior cs.RO | cs.CVPDF"></a>[238] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06868">Dynamic Visual SLAM using a General 3D Prior</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06868" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xingguang Zhong, Liren Jin, Marija Popović, Jens Behley, Cyrill Stachniss</span></p>
<p><strong>TL;DR:</strong> 本文提出一种基于通用3D先验的动态视觉SLAM系统，通过结合几何补丁的在线束调整和前馈重建模型，有效过滤动态区域并提升相机位姿估计的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动态自然场景中的SLAM任务极具挑战性，动态物体会严重影响相机位姿估计的准确性。本文旨在解决这一问题，结合几何方法和数据驱动方法提升鲁棒性。</p>
<p><strong>Result:</strong> 实验表明，该系统在动态场景中能够鲁棒地估计相机位姿和3D重建，优于传统方法。</p>
<p><strong>Insight:</strong> 结合几何与数据驱动方法是处理动态SLAM的有效途径，深度对齐是关键步骤之一。</p>
<p><strong>Abstract:</strong> Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.</p>
  </div>
</details>

<hr>
<h3 id="239-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge-cs-RO-cs-AI-cs-CV-cs-LGPDF"><a href="#239-Task-adaptation-of-Vision-Language-Action-model-1st-Place-Solution-for-the-2025-BEHAVIOR-Challenge-cs-RO-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[239] Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge cs.RO | cs.AI | cs.CV | cs.LGPDF"></a>[239] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06951">Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06951" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ilia Larchenko, Gleb Zarin, Akash Karnatak</span></p>
<p><strong>TL;DR:</strong> 论文介绍了2025 BEHAVIOR挑战赛冠军解决方案，提出了一种视觉-语言-动作策略，通过创新方法如相关噪声流匹配和可学习混合层注意力，显著提升了任务的适应性和性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> BEHAVIOR挑战赛提出了大规模、多样化的家庭任务场景，需要解决双手机器人操作、导航和上下文感知决策等问题，现有方法难以高效应对这些复杂任务。</p>
<p><strong>Result:</strong> 在50项任务中，该方法实现了26%的q-score，在公开和私有榜单上均排名第一。</p>
<p><strong>Insight:</strong> 相关噪声流匹配和混合注意力机制的结合，为解决复杂的长视野任务提供了新的技术路径。</p>
<p><strong>Abstract:</strong> We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.   Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.   Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.</p>
  </div>
</details>

<hr>
<h3 id="240-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators-cs-RO-cs-AI-cs-CVPDF"><a href="#240-VideoVLA-Video-Generators-Can-Be-Generalizable-Robot-Manipulators-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[240] VideoVLA: Video Generators Can Be Generalizable Robot Manipulators cs.RO | cs.AI | cs.CVPDF"></a>[240] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06963">VideoVLA: Video Generators Can Be Generalizable Robot Manipulators</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06963" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yichao Shen, Fangyun Wei, Zhiying Du, Yaobo Liang, Yan Lu</span></p>
<p><strong>TL;DR:</strong> 论文提出VideoVLA方法，将大型视频生成模型转化为机器人视觉-语言-动作（VLA）操纵器，能够预测动作序列和未来视觉结果，展示了强泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的视觉-语言-动作（VLA）模型在新任务、新物体和新环境中的泛化能力有限，需要探索新的方法提升机器人操纵的通用性。</p>
<p><strong>Result:</strong> 实验表明，高质量的想象未来与可靠的动作预测和任务成功相关，VideoVLA在模仿其他机器人技能和处理新物体方面表现优异。</p>
<p><strong>Insight:</strong> 通过同时预测动作及其视觉结果的双重预测策略，VideoVLA展示了视觉想象在机器人操纵中的重要性，并为泛化能力提供了新思路。</p>
<p><strong>Abstract:</strong> Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments’ skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="241-Living-the-Novel-A-System-for-Generating-Self-Training-Timeline-Aware-Conversational-Agents-from-Novels-cs-HC-cs-CLPDF"><a href="#241-Living-the-Novel-A-System-for-Generating-Self-Training-Timeline-Aware-Conversational-Agents-from-Novels-cs-HC-cs-CLPDF" class="headerlink" title="[241] Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels cs.HC | cs.CLPDF"></a>[241] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.07474">Living the Novel: A System for Generating Self-Training Timeline-Aware Conversational Agents from Novels</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.07474" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifei Huang, Tianyu Yan, Sitong Gong, Xiwei Gao, Caixin Kang</span></p>
<p><strong>TL;DR:</strong> Living Novel是一个端到端系统，将文学作品转化为沉浸式多角色对话体验，通过两阶段训练解决LLM驱动的角色对话中的角色漂移和叙事连贯性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决通用LLM在角色对话中表现的角色漂移（persona drift）和叙事逻辑不一致（如剧透或框架破坏）的问题。</p>
<p><strong>Result:</strong> 系统在角色特化指标上超越GPT-4o，在连贯性和鲁棒性指标上接近完美。用户研究表明，角色优先的自训练和时间约束是关键设计原则。</p>
<p><strong>Insight:</strong> 角色优先的自训练是实现逼真对话的基础，而显式的时间约束在多平台交互中至关重要。</p>
<p><strong>Abstract:</strong> We present the Living Novel, an end-to-end system that transforms any literary work into an immersive, multi-character conversational experience. This system is designed to solve two fundamental challenges for LLM-driven characters. Firstly, generic LLMs suffer from persona drift, often failing to stay in character. Secondly, agents often exhibit abilities that extend beyond the constraints of the story’s world and logic, leading to both narrative incoherence (spoiler leakage) and robustness failures (frame-breaking). To address these challenges, we introduce a novel two-stage training pipeline. Our Deep Persona Alignment (DPA) stage uses data-free reinforcement finetuning to instill deep character fidelity. Our Coherence and Robustness Enhancing (CRE) stage then employs a story-time-aware knowledge graph and a second retrieval-grounded training pass to architecturally enforce these narrative constraints. We validate our system through a multi-phase evaluation using Jules Verne’s Twenty Thousand Leagues Under the Sea. A lab study with a detailed ablation of system components is followed by a 5-day in-the-wild diary study. Our DPA pipeline helps our specialized model outperform GPT-4o on persona-specific metrics, and our CRE stage achieves near-perfect performance in coherence and robustness measures. Our study surfaces practical design guidelines for AI-driven narrative systems: we find that character-first self-training is foundational for believability, while explicit story-time constraints are crucial for sustaining coherent, interruption-resilient mobile-web experiences.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="242-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation-cs-CR-cs-CVPDF"><a href="#242-OmniSafeBench-MM-A-Unified-Benchmark-and-Toolbox-for-Multimodal-Jailbreak-Attack-Defense-Evaluation-cs-CR-cs-CVPDF" class="headerlink" title="[242] OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation cs.CR | cs.CVPDF"></a>[242] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.06589">OmniSafeBench-MM: A Unified Benchmark and Toolbox for Multimodal Jailbreak Attack-Defense Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2512.06589" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaojun Jia, Jie Liao, Qi Guo, Teng Ma, Simeng Qin</span></p>
<p><strong>TL;DR:</strong> OmniSafeBench-MM是一个多模态逃逸攻击-防御评估的统一基准和工具箱，填补了现有基准的不足，提供了全面的攻击方法、防御策略和多维评估协议，揭示了多模态大语言模型的漏洞。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型（MLLMs）在感知-推理能力上的进步使其面临逃逸攻击的风险，但现有基准在攻击场景、防御评估和工具统一性方面存在局限性，OmniSafeBench-MM旨在解决这些问题。</p>
<p><strong>Result:</strong> 实验评估了10个开源和8个闭源MLLM，展示了它们对多模态逃逸攻击的脆弱性。</p>
<p><strong>Insight:</strong> 统一的数据、方法和评估工具有助于标准化研究，揭示了MLLMs在多模态场景下的安全挑战。</p>
<p><strong>Abstract:</strong> Recent advances in multi-modal large language models (MLLMs) have enabled unified perception-reasoning capabilities, yet these systems remain highly vulnerable to jailbreak attacks that bypass safety alignment and induce harmful behaviors. Existing benchmarks such as JailBreakV-28K, MM-SafetyBench, and HADES provide valuable insights into multi-modal vulnerabilities, but they typically focus on limited attack scenarios, lack standardized defense evaluation, and offer no unified, reproducible toolbox. To address these gaps, we introduce OmniSafeBench-MM, which is a comprehensive toolbox for multi-modal jailbreak attack-defense evaluation. OmniSafeBench-MM integrates 13 representative attack methods, 15 defense strategies, and a diverse dataset spanning 9 major risk domains and 50 fine-grained categories, structured across consultative, imperative, and declarative inquiry types to reflect realistic user intentions. Beyond data coverage, it establishes a three-dimensional evaluation protocol measuring (1) harmfulness, distinguished by a granular, multi-level scale ranging from low-impact individual harm to catastrophic societal threats, (2) intent alignment between responses and queries, and (3) response detail level, enabling nuanced safety-utility analysis. We conduct extensive experiments on 10 open-source and 8 closed-source MLLMs to reveal their vulnerability to multi-modal jailbreak. By unifying data, methodology, and evaluation into an open-source, reproducible platform, OmniSafeBench-MM provides a standardized foundation for future research. The code is released at <a target="_blank" rel="noopener" href="https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM">https://github.com/jiaxiaojunQAQ/OmniSafeBench-MM</a>.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-12-11/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-12-09/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
