<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-11-05 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-11-05
            
            
        </div>
        <span class="post-date">
            Nov 5, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 142]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 49]</li>
<li><a href="#cs.CY">cs.CY</a> [Total: 2]</li>
<li><a href="#cs.SE">cs.SE</a> [Total: 2]</li>
<li><a href="#math.NA">math.NA</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 7]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 11]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 3]</li>
<li><a href="#econ.GN">econ.GN</a> [Total: 1]</li>
<li><a href="#stat.ML">stat.ML</a> [Total: 1]</li>
<li><a href="#cs.MM">cs.MM</a> [Total: 1]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 2]</li>
<li><a href="#cs.DB">cs.DB</a> [Total: 1]</li>
<li><a href="#cs.PL">cs.PL</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 6]</li>
<li><a href="#eess.AS">eess.AS</a> [Total: 1]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Deep-Learning-Models-for-Coral-Bleaching-Classification-in-Multi-Condition-Underwater-Image-Datasets-cs-CV-cs-AIPDF"><a href="#1-Deep-Learning-Models-for-Coral-Bleaching-Classification-in-Multi-Condition-Underwater-Image-Datasets-cs-CV-cs-AIPDF" class="headerlink" title="[1] Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets cs.CV | cs.AIPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00021">Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00021" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Julio Jerison E. Macrohon, Gordon Hung</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于深度学习的珊瑚白化分类系统，使用全球多样化的数据集评估了ResNet、ViT和CNN三种模型，CNN表现最佳，准确率达88%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 珊瑚礁对海洋生态系统至关重要，但面临污染、海洋酸化和海水温度异常等威胁，亟需高效的监测与保护方法。</p>
<p><strong>Result:</strong> CNN模型表现最佳，准确率达到88%，超越了现有基准。</p>
<p><strong>Insight:</strong> 研究表明CNN在多环境条件下的珊瑚白化分类任务中具有显著优势，为自动化珊瑚监测提供了重要参考。</p>
<p><strong>Abstract:</strong> Coral reefs support numerous marine organisms and are an important source of coastal protection from storms and floods, representing a major part of marine ecosystems. However coral reefs face increasing threats from pollution, ocean acidification, and sea temperature anomalies, making efficient protection and monitoring heavily urgent. Therefore, this study presents a novel machine-learning-based coral bleaching classification system based on a diverse global dataset with samples of healthy and bleached corals under varying environmental conditions, including deep seas, marshes, and coastal zones. We benchmarked and compared three state-of-the-art models: Residual Neural Network (ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN). After comprehensive hyperparameter tuning, the CNN model achieved the highest accuracy of 88%, outperforming existing benchmarks. Our findings offer important insights into autonomous coral monitoring and present a comprehensive analysis of the most widely used computer vision models.</p>
  </div>
</details>

<hr>
<h3 id="2-Automating-Coral-Reef-Fish-Family-Identification-on-Video-Transects-Using-a-YOLOv8-Based-Deep-Learning-Pipeline-cs-CVPDF"><a href="#2-Automating-Coral-Reef-Fish-Family-Identification-on-Video-Transects-Using-a-YOLOv8-Based-Deep-Learning-Pipeline-cs-CVPDF" class="headerlink" title="[2] Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline cs.CVPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00022">Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00022" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jules Gerard, Leandro Di Bella, Filip Huyghe, Marc Kochzius</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于YOLOv8的深度学习流水线，用于自动化识别肯尼亚和坦桑尼亚珊瑚礁视频样带中的鱼类家族，为西印度洋地区提供了一个区域特定的鱼类监测基准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 珊瑚礁监测在西印度洋地区因人工水下视觉普查的劳动需求而受限，亟需自动化解决方案来提高效率和可扩展性。</p>
<p><strong>Result:</strong> 最佳模型在<a href="mailto:&#x6d;&#x41;&#x50;&#x40;&#48;&#46;&#53;">mAP@0.5</a>指标上达到了0.52，对常见鱼类家族的识别准确率高，但对稀有或复杂类群的检测效果较弱。</p>
<p><strong>Insight:</strong> 研究表明深度学习可以作为传统监测方法的可扩展补充，尤其是在处理常见鱼类家族时表现良好，但对稀有类群的识别仍需改进。</p>
<p><strong>Abstract:</strong> Coral reef monitoring in the Western Indian Ocean is limited by the labor demands of underwater visual censuses. This work evaluates a YOLOv8-based deep learning pipeline for automating family-level fish identification from video transects collected in Kenya and Tanzania. A curated dataset of 24 families was tested under different configurations, providing the first region-specific benchmark for automated reef fish monitoring in the Western Indian Ocean. The best model achieved <a href="mailto:&#109;&#65;&#x50;&#64;&#x30;&#x2e;&#53;">mAP@0.5</a> of 0.52, with high accuracy for abundant families but weaker detection of rare or complex taxa. Results demonstrate the potential of deep learning as a scalable complement to traditional monitoring methods.</p>
  </div>
</details>

<hr>
<h3 id="3-Mutual-Information-guided-Visual-Contrastive-Learning-cs-CV-cs-AIPDF"><a href="#3-Mutual-Information-guided-Visual-Contrastive-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[3] Mutual Information guided Visual Contrastive Learning cs.CV | cs.AIPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00028">Mutual Information guided Visual Contrastive Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00028" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hanyang Chen, Yanchao Yang</span></p>
<p><strong>TL;DR:</strong> 提出了一种基于互信息的视觉对比学习方法，通过选择具有高互信息的真实世界数据作为正样本，提升了特征学习的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的对比学习方法在数据选择和增强上依赖人工假设或工程，可能不是最优解。本文探索基于互信息的数据选择方法，以提升在开放环境中的泛化性能。</p>
<p><strong>Result:</strong> 在多个基准测试和框架中验证了方法的有效性，表现优于传统工程化的数据增强方法。</p>
<p><strong>Insight:</strong> 互信息可以作为一种自然的数据选择指标，帮助模型学习更具泛化性的特征，减少对人工设计的依赖。</p>
<p><strong>Abstract:</strong> Representation learning methods utilizing the InfoNCE loss have demonstrated considerable capacity in reducing human annotation effort by training invariant neural feature extractors. Although different variants of the training objective adhere to the information maximization principle between the data and learned features, data selection and augmentation still rely on human hypotheses or engineering, which may be suboptimal. For instance, data augmentation in contrastive learning primarily focuses on color jittering, aiming to emulate real-world illumination changes. In this work, we investigate the potential of selecting training data based on their mutual information computed from real-world distributions, which, in principle, should endow the learned features with better generalization when applied in open environments. Specifically, we consider patches attached to scenes that exhibit high mutual information under natural perturbations, such as color changes and motion, as positive samples for learning with contrastive loss. We evaluate the proposed mutual-information-informed data augmentation method on several benchmarks across multiple state-of-the-art representation learning frameworks, demonstrating its effectiveness and establishing it as a promising direction for future research.</p>
  </div>
</details>

<hr>
<h3 id="4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI-cs-CV-cs-AI-cs-LG-cs-ROPDF"><a href="#4-World-Simulation-with-Video-Foundation-Models-for-Physical-AI-cs-CV-cs-AI-cs-LG-cs-ROPDF" class="headerlink" title="[4] World Simulation with Video Foundation Models for Physical AI cs.CV | cs.AI | cs.LG | cs.ROPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00062">World Simulation with Video Foundation Models for Physical AI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00062" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">NVIDIA, :, Arslan Ali, Junjie Bai, Maciej Bala</span></p>
<p><strong>TL;DR:</strong> 论文介绍了新一代的Cosmos World Foundation Models（Cosmos-Predict2.5）及其配套框架（Cosmos-Transfer2.5），通过统一的流式架构实现文本、图像和视频到世界的生成，并结合物理AI视觉语言模型（Cosmos-Reason1）提供更丰富的文本基础和精细控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在通过更高质量的视频生成和指令对齐，为机器人及自主系统提供可靠的合成数据生成、策略评估和闭环仿真工具，推动物理AI的发展。</p>
<p><strong>Result:</strong> Cosmos-Predict2.5在视频质量和指令对齐上显著优于前代模型，Cosmos-Transfer2.5在更小的模型规模下实现了更高保真度和长时程视频生成。</p>
<p><strong>Insight:</strong> 统一的生成模型和高效的转换框架可以显著提升物理AI的应用潜力，开源资源有望推动更多创新。</p>
<p><strong>Abstract:</strong> We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World Foundation Models for Physical AI. Built on a flow-based architecture, [Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language model, to provide richer text grounding and finer control of world simulation. Trained on 200M curated video clips and refined with reinforcement learning-based post-training, [Cosmos-Predict2.5] achieves substantial improvements over [Cosmos-Predict1] in video quality and instruction alignment, with models released at 2B and 14B scales. These capabilities enable more reliable synthetic data generation, policy evaluation, and closed-loop simulation for robotics and autonomous systems. We further extend the family with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and Real2Real world translation. Despite being 3.5$\times$ smaller than [Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video generation. Together, these advances establish [Cosmos-Predict2.5] and [Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To accelerate research and deployment in Physical AI, we release source code, pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model License at <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-predict2.5">https://github.com/nvidia-cosmos/cosmos-predict2.5</a> and <a target="_blank" rel="noopener" href="https://github.com/nvidia-cosmos/cosmos-transfer2.5">https://github.com/nvidia-cosmos/cosmos-transfer2.5</a>. We hope these open resources lower the barrier to adoption and foster innovation in building the next generation of embodied intelligence.</p>
  </div>
</details>

<hr>
<h3 id="5-Habitat-and-Land-Cover-Change-Detection-in-Alpine-Protected-Areas-A-Comparison-of-AI-Architectures-cs-CVPDF"><a href="#5-Habitat-and-Land-Cover-Change-Detection-in-Alpine-Protected-Areas-A-Comparison-of-AI-Architectures-cs-CVPDF" class="headerlink" title="[5] Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures cs.CVPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00073">Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00073" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Harald Kristen, Daniel Kulmer, Manuela Hirschmugl</span></p>
<p><strong>TL;DR:</strong> 本文探讨了在阿尔卑斯保护区内利用不同AI架构进行生境和土地覆盖变化检测的方法，比较了后分类变化检测和直接变化检测范式，结果表明GeoFM（如Clay v1.0）在多类生境变化检测中表现优于传统U-Net，而直接变化检测在二元变化检测中更优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 高山生态系统的快速气候变化和干扰需要频繁监测，但手动制图成本高昂。本文旨在利用深度学习填补地理空间基础模型（GFMs）在复杂自然环境中应用的空白。</p>
<p><strong>Result:</strong> 1. Clay v1.0在多类生境变化检测中总体准确率为51%，优于U-Net的41%；2. 直接变化检测在二元检测中IoU更高（0.53 vs 0.35）；3. LiDAR提升语义分割准确率至50%。</p>
<p><strong>Insight:</strong> 1. 在复杂生境中，GeoFM表现优于传统方法；2. 直接变化检测适合二元任务，但在多类任务中表现较差；3. 未来可通过对象后处理和物理约束进一步提升性能。</p>
<p><strong>Abstract:</strong> Rapid climate change and other disturbances in alpine ecosystems demand frequent habitat monitoring, yet manual mapping remains prohibitively expensive for the required temporal resolution. We employ deep learning for change detection using long-term alpine habitat data from Gesaeuse National Park, Austria, addressing a major gap in applying geospatial foundation models (GFMs) to complex natural environments with fuzzy class boundaries and highly imbalanced classes. We compare two paradigms: post-classification change detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the transformer ChangeViT against U-Net baselines. Using high-resolution multimodal data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus U-Net’s 41% for multi-class habitat change, while both reach 67% for binary change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net’s 23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy. Although overall accuracies are lower than in more homogeneous landscapes, they reflect realistic performance for complex alpine habitats. Future work will integrate object-based post-processing and physical constraints to enhance applicability.</p>
  </div>
</details>

<hr>
<h3 id="6-LeMiCa-Lexicographic-Minimax-Path-Caching-for-Efficient-Diffusion-Based-Video-Generation-cs-CV-cs-AIPDF"><a href="#6-LeMiCa-Lexicographic-Minimax-Path-Caching-for-Efficient-Diffusion-Based-Video-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[6] LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation cs.CV | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00090">LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00090" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu</span></p>
<p><strong>TL;DR:</strong> LeMiCa提出了一种无需训练的高效加速框架，用于基于扩散的视频生成，通过词典序最小最大路径优化显著提升全局内容一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有缓存策略主要关注减少局部启发式误差，而忽略了全局误差的累积，导致加速视频与原始视频之间存在明显的质量下降。</p>
<p><strong>Result:</strong> 在多个文本到视频基准测试中，LeMiCa实现了推理速度和生成质量的双重提升，例如Latte模型上2.9倍加速，Open-Sora上LPIPS得分0.05。</p>
<p><strong>Insight:</strong> 显式约束全局误差可以有效提升视频生成的一致性和质量，为未来高效可靠的视频合成研究提供了重要基础。</p>
<p><strong>Abstract:</strong> We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :<a target="_blank" rel="noopener" href="https://github.com/UnicomAI/LeMiCa">https://github.com/UnicomAI/LeMiCa</a></p>
  </div>
</details>

<hr>
<h3 id="7-Self-Improving-Vision-Language-Action-Models-with-Data-Generation-via-Residual-RL-cs-CV-cs-ROPDF"><a href="#7-Self-Improving-Vision-Language-Action-Models-with-Data-Generation-via-Residual-RL-cs-CV-cs-ROPDF" class="headerlink" title="[7] Self-Improving Vision-Language-Action Models with Data Generation via Residual RL cs.CV | cs.ROPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00091">Self-Improving Vision-Language-Action Models with Data Generation via Residual RL</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00091" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenli Xiao, Haotian Lin, Andy Peng, Haoru Xue, Tairan He</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为PLD的三阶段框架，通过残差强化学习和分布感知数据生成，提升视觉-语言-动作模型的性能，实现了任务成功率的显著提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的监督微调方法依赖昂贵的人类演示数据，限制了模型的扩展性和泛化能力。PLD旨在通过自动化的数据生成和自我提升机制解决这一问题。</p>
<p><strong>Result:</strong> 在LIBERO任务中达到99%的成功率，SimperEnv提升50%以上，真实机器人任务（Franka和YAM手臂）实现100%成功率。</p>
<p><strong>Insight:</strong> 残差探测和分布感知数据收集是实现自我提升VLA模型的关键，为泛化和扩展提供了可行路径。</p>
<p><strong>Abstract:</strong> Supervised fine-tuning (SFT) has become the de facto post-training strategy for large vision-language-action (VLA) models, but its reliance on costly human demonstrations limits scalability and generalization. We propose Probe, Learn, Distill (PLD), a three-stage plug-and-play framework that improves VLAs through residual reinforcement learning (RL) and distribution-aware data collection. In Stage 1, we train lightweight residual actors to probe failure regions of the VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns collected trajectories with the generalist’s deployment distribution while capturing recovery behaviors. In Stage 3, we distill the curated trajectories back into the generalist with standard SFT. PLD achieves near-saturated 99% task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on real-world Franka and YAM arm manipulation tasks. Ablations show that residual probing and distribution-aware replay are key to collecting deployment-aligned data that improves both seen and unseen tasks, offering a scalable path toward self-improving VLA models.</p>
  </div>
</details>

<hr>
<h3 id="8-SpinalSAM-R1-A-Vision-Language-Multimodal-Interactive-System-for-Spine-CT-Segmentation-cs-CV-cs-AI-92C55-I-2-10PDF"><a href="#8-SpinalSAM-R1-A-Vision-Language-Multimodal-Interactive-System-for-Spine-CT-Segmentation-cs-CV-cs-AI-92C55-I-2-10PDF" class="headerlink" title="[8] SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation cs.CV | cs.AI | 92C55 | I.2.10PDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00095">SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | 92C55 | I.2.10</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00095" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaming Liu, Dingwei Fan, Junyong Zhao, Chunlin Li, Haipeng Si</span></p>
<p><strong>TL;DR:</strong> 论文提出SpinalSAM-R1，一种基于视觉-语言多模态交互的脊柱CT分割系统，通过结合微调的SAM和DeepSeek-R1，解决了传统方法在脊柱CT图像分割中的低对比度和复杂边界的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 脊柱CT图像分割在疾病诊断和治疗中至关重要，但现有方法（如SAM）因高标注需求和领域适应性差而表现不佳。</p>
<p><strong>Result:</strong> 系统在脊柱CT分割任务中表现优异，支持11种临床操作，解析准确率达94.3%，响应时间低于800毫秒。</p>
<p><strong>Insight:</strong> 1) 多模态交互（视觉+语言）能有效提升医学图像分割的精度和效率；2) LoRA微调是一种高效的领域适配方法。</p>
<p><strong>Abstract:</strong> The anatomical structure segmentation of the spine and adjacent structures from computed tomography (CT) images is a key step for spinal disease diagnosis and treatment. However, the segmentation of CT images is impeded by low contrast and complex vertebral boundaries. Although advanced models such as the Segment Anything Model (SAM) have shown promise in various segmentation tasks, their performance in spinal CT imaging is limited by high annotation requirements and poor domain adaptability. To address these limitations, we propose SpinalSAM-R1, a multimodal vision-language interactive system that integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation. Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism to improve spine segmentation performance, and a semantics-driven interaction protocol powered by DeepSeek-R1, enabling natural language-guided refinement. The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with CT images. Experimental results suggest that our method achieves superior segmentation performance. Meanwhile, we develop a PyQt5-based interactive software, which supports point, box, and text-based prompts. The system supports 11 clinical operations with 94.3% parsing accuracy and sub-800 ms response times. The software is released on <a target="_blank" rel="noopener" href="https://github.com/6jm233333/spinalsam-r1">https://github.com/6jm233333/spinalsam-r1</a>.</p>
  </div>
</details>

<hr>
<h3 id="9-A-filtering-scheme-for-confocal-laser-endomicroscopy-CLE-video-sequences-for-self-supervised-learning-cs-CV-cs-AI-cs-LGPDF"><a href="#9-A-filtering-scheme-for-confocal-laser-endomicroscopy-CLE-video-sequences-for-self-supervised-learning-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[9] A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning cs.CV | cs.AI | cs.LGPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00098">A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00098" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nils Porsche, Flurin Müller-Diesing, Sweta Banerjee, Miguel Goncalves, Marc Aubreville</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种针对共聚焦激光内窥镜（CLE）视频序列的过滤方案，旨在提升自监督学习（SSL）的训练效率和收敛性。通过减少数据冗余，该方法显著提高了SSL模型的性能和在两项下游任务中的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> CLE成像在诊断中由于图像难以解读且缺乏标注数据，导致机器学习模型容易过拟合。自监督学习（SSL）可以利用大量无标签数据，但由于CLE视频帧间相关性高，数据分布不平衡，影响了SSL的效果。</p>
<p><strong>Result:</strong> 在鼻窦肿瘤和皮肤鳞状细胞癌数据集上，过滤后的SSL预训练模型分别取得了67.48%和73.52%的最高测试准确率，显著优于非SSL基线。</p>
<p><strong>Insight:</strong> 过滤冗余数据可以提高SSL在CLE视频数据上的训练效率和模型性能，为医疗影像的小样本学习提供了新思路。</p>
<p><strong>Abstract:</strong> Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.</p>
  </div>
</details>

<hr>
<h3 id="10-FreeSliders-Training-Free-Modality-Agnostic-Concept-Sliders-for-Fine-Grained-Diffusion-Control-in-Images-Audio-and-Video-cs-CV-cs-AIPDF"><a href="#10-FreeSliders-Training-Free-Modality-Agnostic-Concept-Sliders-for-Fine-Grained-Diffusion-Control-in-Images-Audio-and-Video-cs-CV-cs-AIPDF" class="headerlink" title="[10] FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video cs.CV | cs.AIPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00103">FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00103" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rotem Ezra, Hedi Zisling, Nimrod Berman, Ilan Naiman, Alexey Gorkor</span></p>
<p><strong>TL;DR:</strong> FreeSliders提出了一种无需训练、模态无关的概念滑块方法，通过在推理过程中部分估计概念滑块的公式，实现对图像、音频和视频的细粒度生成控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散模型在多模态生成任务中表现优异，但在无需训练和模态无关的条件下实现细粒度控制仍然具有挑战性。</p>
<p><strong>Result:</strong> 实验表明，FreeSliders在多模态任务中实现了即插即用、无需训练的生成控制，性能优于现有基线。</p>
<p><strong>Insight:</strong> FreeSliders揭示了在扩散模型中实现跨模态细粒度控制的潜力，同时提供了新的评测工具和解决非线性问题的思路。</p>
<p><strong>Abstract:</strong> Diffusion models have become state-of-the-art generative models for images, audio, and video, yet enabling fine-grained controllable generation, i.e., continuously steering specific concepts without disturbing unrelated content, remains challenging. Concept Sliders (CS) offer a promising direction by discovering semantic directions through textual contrasts, but they require per-concept training and architecture-specific fine-tuning (e.g., LoRA), limiting scalability to new modalities. In this work we introduce FreeSliders, a simple yet effective approach that is fully training-free and modality-agnostic, achieved by partially estimating the CS formula during inference. To support modality-agnostic evaluation, we extend the CS benchmark to include both video and audio, establishing the first suite for fine-grained concept generation control with multiple modalities. We further propose three evaluation properties along with new metrics to improve evaluation quality. Finally, we identify an open problem of scale selection and non-linear traversals and introduce a two-stage procedure that automatically detects saturation points and reparameterizes traversal for perceptually uniform, semantically meaningful edits. Extensive experiments demonstrate that our method enables plug-and-play, training-free concept control across modalities, improves over existing baselines, and establishes new tools for principled controllable generation. An interactive presentation of our benchmark and method is available at: <a target="_blank" rel="noopener" href="https://azencot-group.github.io/FreeSliders/">https://azencot-group.github.io/FreeSliders/</a></p>
  </div>
</details>

<hr>
<h3 id="11-AI-Powered-High-Quality-Text-to-Video-Generation-with-Enhanced-Temporal-Consistency-cs-CV-cs-AI-cs-IRPDF"><a href="#11-AI-Powered-High-Quality-Text-to-Video-Generation-with-Enhanced-Temporal-Consistency-cs-CV-cs-AI-cs-IRPDF" class="headerlink" title="[11] AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency cs.CV | cs.AI | cs.IRPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00107">AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00107" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Piyushkumar Patel</span></p>
<p><strong>TL;DR:</strong> MOVAI是一种新颖的分层框架，通过集成组合场景理解和时序感知扩散模型，实现了高质量文本到视频的生成。它通过三个关键创新（CSP、TSAM和PVR）提升了时序一致性和视频质量，并在多项指标上显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的文本到视频生成方法在时序一致性、组合理解和细粒度控制方面存在不足，MOVAI旨在解决这些问题，实现更真实和可控的视频合成。</p>
<p><strong>Result:</strong> 在标准基准测试中，MOVAI在LPIPS、FVD和用户偏好研究上分别提升了15.3%、12.7%和18.9%，特别是在复杂多物体场景中表现优异。</p>
<p><strong>Insight:</strong> MOVAI的创新在于将组合场景理解与时序建模紧密结合，通过分层和迭代优化显著提升了文本到视频生成的真实性和可控性。</p>
<p><strong>Abstract:</strong> Text to video generation has emerged as a critical frontier in generative artificial intelligence, yet existing approaches struggle with maintaining temporal consistency, compositional understanding, and fine grained control over visual narratives. We present MOVAI (Multimodal Original Video AI), a novel hierarchical framework that integrates compositional scene understanding with temporal aware diffusion models for high fidelity text to video synthesis. Our approach introduces three key innovations: (1) a Compositional Scene Parser (CSP) that decomposes textual descriptions into hierarchical scene graphs with temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that ensures coherent motion dynamics across frames while preserving spatial details, and (3) a Progressive Video Refinement (PVR) module that iteratively enhances video quality through multi-scale temporal reasoning. Extensive experiments on standard benchmarks demonstrate that MOVAI achieves state-of-the-art performance, improving video quality metrics by 15.3% in LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing methods. Our framework shows particular strength in generating complex multi-object scenes with realistic temporal dynamics and fine-grained semantic control.</p>
  </div>
</details>

<hr>
<h3 id="12-Chain-of-Time-In-Context-Physical-Simulation-with-Image-Generation-Models-cs-CV-cs-AIPDF"><a href="#12-Chain-of-Time-In-Context-Physical-Simulation-with-Image-Generation-Models-cs-CV-cs-AIPDF" class="headerlink" title="[12] Chain of Time: In-Context Physical Simulation with Image Generation Models cs.CV | cs.AIPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00110">Chain of Time: In-Context Physical Simulation with Image Generation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00110" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">YingQiao Wang, Eric Bigelow, Boyi Li, Tomer Ullman</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为”Chain of Time”的认知启发方法，通过生成一系列中间图像改进和解释视觉语言模型中的物理模拟。该方法无需额外微调，显著提升了图像生成模型的性能，并揭示了传统评估中无法观察到的动态物理特性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 受机器学习中的上下文推理和人类心理模拟启发，作者旨在改进视觉语言模型对物理过程的模拟能力，并揭示其对动态物理特性的理解。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提升了图像生成模型的性能，并揭示了对动态物理特性（如速度、重力、碰撞）的模拟能力。</p>
<p><strong>Insight:</strong> 分析发现，模型能够模拟随时间展开的物理特性，但在从输入图像推断特定物理参数时可能存在困难，这为未来研究提供了方向。</p>
<p><strong>Abstract:</strong> We propose a novel cognitively-inspired method to improve and interpret physical simulation in vision-language models. Our &#96;&#96;Chain of Time” method involves generating a series of intermediate images during a simulation, and it is motivated by in-context reasoning in machine learning, as well as mental simulation in humans. Chain of Time is used at inference time, and requires no additional fine-tuning. We apply the Chain-of-Time method to synthetic and real-world domains, including 2-D graphics simulations and natural 3-D videos. These domains test a variety of particular physical properties, including velocity, acceleration, fluid dynamics, and conservation of momentum. We found that using Chain-of-Time simulation substantially improves the performance of a state-of-the-art image generation model. Beyond examining performance, we also analyzed the specific states of the world simulated by an image model at each time step, which sheds light on the dynamics underlying these simulations. This analysis reveals insights that are hidden from traditional evaluations of physical reasoning, including cases where an image generation model is able to simulate physical properties that unfold over time, such as velocity, gravity, and collisions. Our analysis also highlights particular cases where the image generation model struggles to infer particular physical parameters from input images, despite being capable of simulating relevant physical processes.</p>
  </div>
</details>

<hr>
<h3 id="13-End-to-End-Framework-Integrating-Generative-AI-and-Deep-Reinforcement-Learning-for-Autonomous-Ultrasound-Scanning-cs-CV-cs-AI-cs-LGPDF"><a href="#13-End-to-End-Framework-Integrating-Generative-AI-and-Deep-Reinforcement-Learning-for-Autonomous-Ultrasound-Scanning-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[13] End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning cs.CV | cs.AI | cs.LGPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00114">End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00114" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hanae Elmekki, Amanda Spilkin, Ehsan Zakeri, Antonela Mariel Zanuttini, Ahmed Alagha</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种集成了生成AI和深度强化学习（DRL）的端到端框架，用于实现自主的心脏超声扫描。该框架通过生成模拟器和DRL模块的结合，解决了现有方法的可重复性和数据依赖性问题，并通过公开数据集和实验验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 心脏超声诊断受到操作者依赖和专业人员短缺的限制，亟需自动化解决方案。现有DRL方法缺乏可重复性且依赖专有数据，因此研究团队提出了结合生成AI和DRL的框架，以填补这些空白。</p>
<p><strong>Result:</strong> 实验验证了VAE-GAN的性能优于现有GAN变体，DRL扫描系统在不同配置下表现出有效性。公开数据集确保了方法的可重复性。</p>
<p><strong>Insight:</strong> 生成AI与DRL的结合为医疗影像自动化提供了新思路，公开数据集促进了领域研究的可复现性。这种框架可扩展至其他器官的扫描任务。</p>
<p><strong>Abstract:</strong> Cardiac ultrasound (US) is among the most widely used diagnostic tools in cardiology for assessing heart health, but its effectiveness is limited by operator dependence, time constraints, and human error. The shortage of trained professionals, especially in remote areas, further restricts access. These issues underscore the need for automated solutions that can ensure consistent, and accessible cardiac imaging regardless of operator skill or location. Recent progress in artificial intelligence (AI), especially in deep reinforcement learning (DRL), has gained attention for enabling autonomous decision-making. However, existing DRL-based approaches to cardiac US scanning lack reproducibility, rely on proprietary data, and use simplified models. Motivated by these gaps, we present the first end-to-end framework that integrates generative AI and DRL to enable autonomous and reproducible cardiac US scanning. The framework comprises two components: (i) a conditional generative simulator combining Generative Adversarial Networks (GANs) with Variational Autoencoders (VAEs), that models the cardiac US environment producing realistic action-conditioned images; and (ii) a DRL module that leverages this simulator to learn autonomous, accurate scanning policies. The proposed framework delivers AI-driven guidance through expert-validated models that classify image type and assess quality, supports conditional generation of realistic US images, and establishes a reproducible foundation extendable to other organs. To ensure reproducibility, a publicly available dataset of real cardiac US scans is released. The solution is validated through several experiments. The VAE-GAN is benchmarked against existing GAN variants, with performance assessed using qualitative and quantitative approaches, while the DRL-based scanning system is evaluated under varying configurations to demonstrate effectiveness.</p>
  </div>
</details>

<hr>
<h3 id="14-VLM6D-VLM-based-6Dof-Pose-Estimation-based-on-RGB-D-Images-cs-CV-cs-AIPDF"><a href="#14-VLM6D-VLM-based-6Dof-Pose-Estimation-based-on-RGB-D-Images-cs-CV-cs-AIPDF" class="headerlink" title="[14] VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images cs.CV | cs.AIPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00120">VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00120" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Selim Sarowar, Sungho Kim</span></p>
<p><strong>TL;DR:</strong> VLM6D提出了一种基于RGB-D图像的双流架构，利用视觉和几何数据的优势，通过ViT和PointNet++编码器分别处理RGB和3D点云数据，实现了对6D物体姿态的鲁棒和精确估计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前6D姿态估计方法在真实场景中存在光照变化、无纹理物体和严重遮挡等问题，泛化能力不足。</p>
<p><strong>Result:</strong> 在Occluded-LineMOD数据集上实现了新的SOTA性能，验证了方法的鲁棒性和准确性。</p>
<p><strong>Insight:</strong> 通过结合视觉和几何特征的互补性，可以有效解决光照变化、无纹理和遮挡等挑战。</p>
<p><strong>Abstract:</strong> The primary challenge in computer vision is precisely calculating the pose of 6D objects, however many current approaches are still fragile and have trouble generalizing from synthetic data to real-world situations with fluctuating lighting, textureless objects, and significant occlusions. To address these limitations, VLM6D, a novel dual-stream architecture that leverages the distinct strengths of visual and geometric data from RGB-D input for robust and precise pose estimation. Our framework uniquely integrates two specialized encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the RGB modality, harnessing its rich, pre-trained understanding of visual grammar to achieve remarkable resilience against texture and lighting variations. Concurrently, a PointNet++ encoder processes the 3D point cloud derived from depth data, enabling robust geometric reasoning that excels even with the sparse, fragmented data typical of severe occlusion. These complementary feature streams are effectively fused to inform a multi task prediction head. We demonstrate through comprehensive experiments that VLM6D obtained new SOTA performance on the challenging Occluded-LineMOD, validating its superior robustness and accuracy.</p>
  </div>
</details>

<hr>
<h3 id="15-Integrating-ConvNeXt-and-Vision-Transformers-for-Enhancing-Facial-Age-Estimation-cs-CV-cs-LGPDF"><a href="#15-Integrating-ConvNeXt-and-Vision-Transformers-for-Enhancing-Facial-Age-Estimation-cs-CV-cs-LGPDF" class="headerlink" title="[15] Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation cs.CV | cs.LGPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00123">Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00123" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gaby Maroun, Salah Eddine Bekhouche, Fadi Dornaika</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合ConvNeXt和视觉变换器（ViT）的混合架构，用于提升面部年龄估计的性能，通过互补CNN的局部特征提取和ViT的全局注意力机制，显著降低了平均绝对误差（MAE）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的年龄估计方法主要依赖单一模型（CNN或ViT），未能充分利用两者的互补优势，导致性能受限。本研究的动机是通过整合两者的优势提升年龄估计的准确性。</p>
<p><strong>Result:</strong> 实验结果表明，ConvNeXt-ViT混合架构在MAE指标上优于传统方法，且通过消融研究验证了各组件的重要性。</p>
<p><strong>Insight:</strong> 混合架构是解决复杂视觉任务的有效方向，未来可通过进一步融合CNN和ViT的优势推动相关领域的发展。</p>
<p><strong>Abstract:</strong> Age estimation from facial images is a complex and multifaceted challenge in computer vision. In this study, we present a novel hybrid architecture that combines ConvNeXt, a state-of-the-art advancement of convolutional neural networks (CNNs), with Vision Transformers (ViT). While each model independently delivers excellent performance on a variety of tasks, their integration leverages the complementary strengths of the CNNs localized feature extraction capabilities and the Transformers global attention mechanisms. Our proposed ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior performance in terms of mean absolute error (MAE). To address computational constraints, we leverage pre-trained models and systematically explore different configurations, using linear layers and advanced regularization techniques to optimize the architecture. Comprehensive ablation studies highlight the critical role of individual components and training strategies, and in particular emphasize the importance of adapted attention mechanisms within the CNN framework to improve the model focus on age-relevant facial features. The results show that the ConvNeXt-ViT hybrid not only outperforms traditional methods, but also provides a robust foundation for future advances in age estimation and related visual tasks. This work underscores the transformative potential of hybrid architectures and represents a promising direction for the seamless integration of CNNs and transformers to address complex computer vision challenges.</p>
  </div>
</details>

<hr>
<h3 id="16-FLoC-Facility-Location-Based-Efficient-Visual-Token-Compression-for-Long-Video-Understanding-cs-CV-cs-AIPDF"><a href="#16-FLoC-Facility-Location-Based-Efficient-Visual-Token-Compression-for-Long-Video-Understanding-cs-CV-cs-AIPDF" class="headerlink" title="[16] FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding cs.CV | cs.AIPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00141">FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00141" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli</span></p>
<p><strong>TL;DR:</strong> FLoC是一种基于设施定位函数的高效视觉令牌压缩框架，用于长视频理解，通过选择紧凑且多样化的视觉令牌子集，显著减少令牌数量并保持性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长视频理解中的视觉令牌数量庞大，限制了大型多模态模型的扩展性，因此需要一种高效的令牌压缩方法。</p>
<p><strong>Result:</strong> 在大规模基准测试中，FLoC优于现有压缩技术，展现出高效性和鲁棒性。</p>
<p><strong>Insight:</strong> 设施定位函数的引入为令牌压缩提供了一种原则性方法，懒惰贪婪算法则实现了高效计算。</p>
<p><strong>Abstract:</strong> Recent studies in long video understanding have harnessed the advanced visual-language reasoning capabilities of Large Multimodal Models (LMMs), driving the evolution of video-LMMs specialized for processing extended video sequences. However, the scalability of these models is severely limited by the overwhelming volume of visual tokens generated from extended video sequences. To address this challenge, this paper proposes FLoC, an efficient visual token compression framework based on the facility location function, a principled approach that swiftly selects a compact yet highly representative and diverse subset of visual tokens within a predefined budget on the number of visual tokens. By integrating the lazy greedy algorithm, our method achieves remarkable efficiency gains by swiftly selecting a compact subset of tokens, drastically reducing the number of visual tokens while guaranteeing near-optimal performance. Notably, our approach is training-free, model-agnostic, and query-agnostic, providing a versatile solution that seamlessly integrates with diverse video-LLMs and existing workflows. Extensive evaluations on large-scale benchmarks, such as Video-MME, MLVU, and LongVideoBench, demonstrate that our framework consistently surpasses recent compression techniques, highlighting not only its effectiveness and robustness in addressing the critical challenges of long video understanding, but also its efficiency in processing speed.</p>
  </div>
</details>

<hr>
<h3 id="17-BlurGuard-A-Simple-Approach-for-Robustifying-Image-Protection-Against-AI-Powered-Editing-cs-CVPDF"><a href="#17-BlurGuard-A-Simple-Approach-for-Robustifying-Image-Protection-Against-AI-Powered-Editing-cs-CVPDF" class="headerlink" title="[17] BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing cs.CVPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00143">BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00143" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinsu Kim, Yunhun Nam, Minseon Kim, Sangpil Kim, Jongheon Jeong</span></p>
<p><strong>TL;DR:</strong> BlurGuard提出了一种简单方法，通过自适应高斯模糊增强对抗性噪声的不可逆性，提高图像保护方法的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着文本到图像模型的发展，恶意图像编辑的风险增加。现有对抗性噪声保护方法易被简单技术（如JPEG压缩）逆转，因此需要更鲁棒的不可逆保护。</p>
<p><strong>Result:</strong> 实验表明，BlurGuard显著提升了现有方法在多样编辑场景下的最坏情况保护性能，同时减少了噪声对图像质量的感知影响。</p>
<p><strong>Insight:</strong> 图像保护不仅需要对抗性噪声不可感知，还需不可逆。自适应模糊提供了一种简单但有效的鲁棒性增强手段。</p>
<p><strong>Abstract:</strong> Recent advances in text-to-image models have increased the exposure of powerful image editing techniques as a tool, raising concerns about their potential for malicious use. An emerging line of research to address such threats focuses on implanting “protective” adversarial noise into images before their public release, so future attempts to edit them using text-to-image models can be impeded. However, subsequent works have shown that these adversarial noises are often easily “reversed,” e.g., with techniques as simple as JPEG compression, casting doubt on the practicality of the approach. In this paper, we argue that adversarial noise for image protection should not only be imperceptible, as has been a primary focus of prior work, but also irreversible, viz., it should be difficult to detect as noise provided that the original image is hidden. We propose a surprisingly simple method to enhance the robustness of image protection methods against noise reversal techniques. Specifically, it applies an adaptive per-region Gaussian blur on the noise to adjust the overall frequency spectrum. Through extensive experiments, we show that our method consistently improves the per-sample worst-case protection performance of existing methods against a wide range of reversal techniques on diverse image editing scenarios, while also reducing quality degradation due to noise in terms of perceptual metrics. Code is available at <a target="_blank" rel="noopener" href="https://github.com/jsu-kim/BlurGuard">https://github.com/jsu-kim/BlurGuard</a>.</p>
  </div>
</details>

<hr>
<h3 id="18-CompAgent-An-Agentic-Framework-for-Visual-Compliance-Verification-cs-CVPDF"><a href="#18-CompAgent-An-Agentic-Framework-for-Visual-Compliance-Verification-cs-CVPDF" class="headerlink" title="[18] CompAgent: An Agentic Framework for Visual Compliance Verification cs.CVPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00171">CompAgent: An Agentic Framework for Visual Compliance Verification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00171" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rahul Ghosh, Baishali Chaudhury, Hari Prasanna Das, Meghana Ashok, Ryan Razkenari</span></p>
<p><strong>TL;DR:</strong> CompAgent 是一个基于代理的视觉合规验证框架，通过动态选择和整合多种视觉工具，结合多模态大语言模型（MLLMs），显著提升了合规检测的效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉合规验证在媒体、娱乐和广告等领域非常重要，但现有方法依赖特定任务的深度学习模型，泛化能力有限，而 MLLMs 又难以处理细粒度的视觉细节和结构化规则。</p>
<p><strong>Result:</strong> 在 UnsafeBench 数据集上，CompAgent 达到了 76% 的 F1 分数，比现有最佳方法提升了 10%。</p>
<p><strong>Insight:</strong> 代理规划和工具增强的推理能够显著提升复杂视觉任务的准确性和适应性。</p>
<p><strong>Abstract:</strong> Visual compliance verification is a critical yet underexplored problem in computer vision, especially in domains such as media, entertainment, and advertising where content must adhere to complex and evolving policy rules. Existing methods often rely on task-specific deep learning models trained on manually labeled datasets, which are costly to build and limited in generalizability. While recent multi-modal large language models (MLLMs) offer broad real-world knowledge and policy understanding, they struggle to reason over fine-grained visual details and apply structured compliance rules effectively on their own. In this paper, we propose CompAgent, the first agentic framework for visual compliance verification. CompAgent augments MLLMs with a suite of visual tools - such as object detectors, face analyzers, NSFW detectors, and captioning models - and introduces a planning agent that dynamically selects appropriate tools based on the compliance policy. A verification agent then integrates image, tool outputs, and policy context to perform multi-modal reasoning. Experiments on public benchmarks show that CompAgent outperforms specialized classifiers, direct MLLM prompting, and curated routing baselines, achieving up to 76% F1 score and a 10% improvement over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate the effectiveness of agentic planning and tool-augmented reasoning for scalable, accurate, and adaptable visual compliance verification.</p>
  </div>
</details>

<hr>
<h3 id="19-From-Evidence-to-Verdict-An-Agent-Based-Forensic-Framework-for-AI-Generated-Image-Detection-cs-CV-cs-CRPDF"><a href="#19-From-Evidence-to-Verdict-An-Agent-Based-Forensic-Framework-for-AI-Generated-Image-Detection-cs-CV-cs-CRPDF" class="headerlink" title="[19] From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection cs.CV | cs.CRPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00181">From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00181" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengfei Liang, Yiting Qu, Yukun Jiang, Michael Backes, Yang Zhang</span></p>
<p><strong>TL;DR:</strong> 论文提出了AIFo框架，通过多智能体协作模拟人类取证调查，无需训练即可实现AI生成图像的检测，显著提升了准确性和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> AI生成图像的快速发展对信息完整性和媒体真实性提出了挑战。现有方法在可解释性和泛化能力上存在不足，需要一种更鲁棒、灵活的解决方案。</p>
<p><strong>Result:</strong> 在6000张图像的测试中，AIFo达到了97.05%的准确率，明显优于传统分类器和先进的VLM方法。</p>
<p><strong>Insight:</strong> 基于智能体的程序化推理为AI生成图像检测提供了新范式，强调了跨源证据融合和多智能体协作的重要性。</p>
<p><strong>Abstract:</strong> The rapid evolution of AI-generated images poses unprecedented challenges to information integrity and media authenticity. Existing detection approaches suffer from fundamental limitations: traditional classifiers lack interpretability and fail to generalize across evolving generative models, while vision-language models (VLMs), despite their promise, remain constrained to single-shot analysis and pixel-level reasoning. To address these challenges, we introduce AIFo (Agent-based Image Forensics), a novel training-free framework that emulates human forensic investigation through multi-agent collaboration. Unlike conventional methods, our framework employs a set of forensic tools, including reverse image search, metadata extraction, pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based agents that collect, synthesize, and reason over cross-source evidence. When evidence is conflicting or insufficient, a structured multi-agent debate mechanism allows agents to exchange arguments and reach a reliable conclusion. Furthermore, we enhance the framework with a memory-augmented reasoning module that learns from historical cases to improve future detection accuracy. Our comprehensive evaluation spans 6,000 images across both controlled laboratory settings and challenging real-world scenarios, including images from modern generative platforms and diverse online sources. AIFo achieves 97.05% accuracy, substantially outperforming traditional classifiers and state-of-the-art VLMs. These results demonstrate that agent-based procedural reasoning offers a new paradigm for more robust, interpretable, and adaptable AI-generated image detection.</p>
  </div>
</details>

<hr>
<h3 id="20-A-Retrospect-to-Multi-prompt-Learning-across-Vision-and-Language-cs-CV-cs-AI-cs-LGPDF"><a href="#20-A-Retrospect-to-Multi-prompt-Learning-across-Vision-and-Language-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[20] A Retrospect to Multi-prompt Learning across Vision and Language cs.CV | cs.AI | cs.LGPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00191">A Retrospect to Multi-prompt Learning across Vision and Language</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00191" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziliang Chen, Xin Huang, Quanlong Guan, Liang Lin, Weiqi Luo</span></p>
<p><strong>TL;DR:</strong> 该论文回顾了视觉-语言多提示学习的技术潜力，提出了基于能量的多提示学习方法（EMPL），通过从能量分布中生成多提示嵌入，实现了高效的参数利用和泛化能力的平衡。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉-语言预训练模型（VLMs）的快速发展推动了视觉社区的进步，但现有研究主要集中在单提示范式，多提示学习的潜力尚未充分挖掘。</p>
<p><strong>Result:</strong> 综合实验验证了EMPL方法的优越性，证明了其在视觉-语言任务中的高效性和泛化能力。</p>
<p><strong>Insight:</strong> 多提示学习在视觉-语言任务中具有重要潜力，通过能量分布生成提示嵌入可以有效提升模型的泛化能力。</p>
<p><strong>Abstract:</strong> The vision community is undergoing the unprecedented progress with the emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays as the holy grail of accessing VLMs since it enables their fast adaptation to downstream tasks with limited resources. Whereas existing researches milling around single-prompt paradigms, rarely investigate the technical potential behind their multi-prompt learning counterparts. This paper aims to provide a principled retrospect for vision-language multi-prompt learning. We extend the recent constant modality gap phenomenon to learnable prompts and then, justify the superiority of vision-language transfer with multi-prompt augmentation, empirically and theoretically. In terms of this observation, we propose an Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt embeddings by drawing instances from an energy-based distribution, which is implicitly defined by VLMs. So our EMPL is not only parameter-efficient but also rigorously lead to the balance between in-domain and out-of-domain open-vocabulary generalization. Comprehensive experiments have been conducted to justify our claims and the excellence of EMPL.</p>
  </div>
</details>

<hr>
<h3 id="21-DM-QPMNET-Dual-modality-fusion-network-for-cell-segmentation-in-quantitative-phase-microscopy-cs-CV-cs-AIPDF"><a href="#21-DM-QPMNET-Dual-modality-fusion-network-for-cell-segmentation-in-quantitative-phase-microscopy-cs-CV-cs-AIPDF" class="headerlink" title="[21] DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy cs.CV | cs.AIPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00218">DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00218" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rajatsubhra Chakraborty, Ana Espinosa-Momox, Riley Haskin, Depeng Xu, Rosario Porras-Aguilar</span></p>
<p><strong>TL;DR:</strong> DM-QPMNet是一个双编码器网络，通过多头注意力机制融合偏振强度图像和相位图这两种模态的特征，改进了单次定量相位显微镜中的细胞分割效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的阈值方法对噪声和细胞密度敏感，而简单通道拼接的深度学习方法未能充分利用偏振强度图像和相位图的互补性。</p>
<p><strong>Result:</strong> 与单模态和简单拼接方法相比，DM-QPMNet在分割性能上有显著提升。</p>
<p><strong>Insight:</strong> 模态特异性编码与可学习的融合机制能更好地利用ssQPM中互补的照明和相位信息，从而提高分割鲁棒性。</p>
<p><strong>Abstract:</strong> Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces challenges from traditional thresholding methods that are sensitive to noise and cell density, while deep learning approaches using simple channel concatenation fail to exploit the complementary nature of polarized intensity images and phase maps. We introduce DM-QPMNet, a dual-encoder network that treats these as distinct modalities with separate encoding streams. Our architecture fuses modality-specific features at intermediate depth via multi-head attention, enabling polarized edge and texture representations to selectively integrate complementary phase information. This content-aware fusion preserves training stability while adding principled multi-modal integration through dual-source skip connections and per-modality normalization at minimal overhead. Our approach demonstrates substantial improvements over monolithic concatenation and single-modality baselines, showing that modality-specific encoding with learnable fusion effectively exploits ssQPM’s simultaneous capture of complementary illumination and phase cues for robust cell segmentation.</p>
  </div>
</details>

<hr>
<h3 id="22-Hyperbolic-Optimal-Transport-cs-CVPDF"><a href="#22-Hyperbolic-Optimal-Transport-cs-CVPDF" class="headerlink" title="[22] Hyperbolic Optimal Transport cs.CVPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00244">Hyperbolic Optimal Transport</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00244" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yan Bin Ng, Xianfeng Gu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种在双曲空间中计算最优传输映射的新算法，填补了当前最优传输问题在非欧空间中的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有最优传输方法多针对欧几里得空间或球面，而在处理分层数据、网络和多亏格黎曼曲面等问题时，双曲空间的自然适应性更强。</p>
<p><strong>Result:</strong> 在合成数据和多亏格曲面模型上的实验验证了方法的有效性。</p>
<p><strong>Insight:</strong> 双曲空间的最优传输问题在分层结构和复杂几何场景中具有天然优势，为未来的相关应用提供了理论基础和工具支持。</p>
<p><strong>Abstract:</strong> The optimal transport (OT) problem aims to find the most efficient mapping between two probability distributions under a given cost function, and has diverse applications in many fields such as machine learning, computer vision and computer graphics. However, existing methods for computing optimal transport maps are primarily developed for Euclidean spaces and the sphere. In this paper, we explore the problem of computing the optimal transport map in hyperbolic space, which naturally arises in contexts involving hierarchical data, networks, and multi-genus Riemann surfaces. We propose a novel and efficient algorithm for computing the optimal transport map in hyperbolic space using a geometric variational technique by extending methods for Euclidean and spherical geometry to the hyperbolic setting. We also perform experiments on synthetic data and multi-genus surface models to validate the efficacy of the proposed method.</p>
  </div>
</details>

<hr>
<h3 id="23-Object-Aware-4D-Human-Motion-Generation-cs-CV-cs-GRPDF"><a href="#23-Object-Aware-4D-Human-Motion-Generation-cs-CV-cs-GRPDF" class="headerlink" title="[23] Object-Aware 4D Human Motion Generation cs.CV | cs.GRPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00248">Object-Aware 4D Human Motion Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00248" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shurui Gui, Deep Anil Patel, Xiner Li, Martin Renqiang Min</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于3D高斯表示和运动扩散先验的对象感知4D人体运动生成框架，通过结合大语言模型（LLM）和运动扩散分数蒸馏采样（MSDS），解决了现有视频扩散模型生成运动中存在的非真实变形、语义违规和物理不一致问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频扩散模型生成的视频存在非真实变形、语义违规和物理不一致问题，主要原因是缺乏3D物理先验。为了解决这些问题，论文提出了一种基于3D高斯表示和运动扩散先验的对象感知4D人体运动生成方法。</p>
<p><strong>Result:</strong> 实验表明，该方法能够生成自然且物理合理的人体运动，同时尊重3D空间上下文。</p>
<p><strong>Insight:</strong> 通过结合3D高斯表示和大语言模型，可以在不重新训练的情况下生成高质量的对象感知4D人体运动，为解决现实感问题提供了可扩展的解决方案。</p>
<p><strong>Abstract:</strong> Recent advances in video diffusion models have enabled the generation of high-quality videos. However, these videos still suffer from unrealistic deformations, semantic violations, and physical inconsistencies that are largely rooted in the absence of 3D physical priors. To address these challenges, we propose an object-aware 4D human motion generation framework grounded in 3D Gaussian representations and motion diffusion priors. With pre-generated 3D humans and objects, our method, Motion Score Distilled Interaction (MSDI), employs the spatial and prompt semantic information in large language models (LLMs) and motion priors through the proposed Motion Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs enables our spatial-aware motion optimization, which distills score gradients from pre-trained motion diffusion models, to refine human motion while respecting object and semantic constraints. Unlike prior methods requiring joint training on limited interaction datasets, our zero-shot approach avoids retraining and generalizes to out-of-distribution object aware human motions. Experiments demonstrate that our framework produces natural and physically plausible human motions that respect 3D spatial context, offering a scalable solution for realistic 4D generation.</p>
  </div>
</details>

<hr>
<h3 id="24-BeetleFlow-An-Integrative-Deep-Learning-Pipeline-for-Beetle-Image-Processing-cs-CVPDF"><a href="#24-BeetleFlow-An-Integrative-Deep-Learning-Pipeline-for-Beetle-Image-Processing-cs-CVPDF" class="headerlink" title="[24] BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing cs.CVPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00255">BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00255" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fangxun Liu, S M Rayeed, Samuel Stevens, Alyson East, Cheng Hsuan Chiang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为BeetleFlow的三阶段深度学习管道，专门用于处理甲虫图像，包括检测、裁剪和形态分割，以提高大规模甲虫数据处理的效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在昆虫学和生态学研究中，生物学家需要处理大量甲虫图像。传统的手工处理方法效率低下，因此需要一个自动化的管道来加速数据处理和科学研究。</p>
<p><strong>Result:</strong> 管道能够高效地处理大规模甲虫图像，实现细粒度分割和自动化处理，显著提升生物学研究的效率。</p>
<p><strong>Insight:</strong> 论文展示了深度学习在特定生物学任务中的潜力，尤其是transformer模型在复杂任务（如甲虫分割）中的应用效果。</p>
<p><strong>Abstract:</strong> In entomology and ecology research, biologists often need to collect a large number of insects, among which beetles are the most common species. A common practice for biologists to organize beetles is to place them on trays and take a picture of each tray. Given the images of thousands of such trays, it is important to have an automated pipeline to process the large-scale data for further research. Therefore, we develop a 3-stage pipeline to detect all the beetles on each tray, sort and crop the image of each beetle, and do morphological segmentation on the cropped beetles. For detection, we design an iterative process utilizing a transformer-based open-vocabulary object detector and a vision-language model. For segmentation, we manually labeled 670 beetle images and fine-tuned two variants of a transformer-based segmentation model to achieve fine-grained segmentation of beetles with relatively high accuracy. The pipeline integrates multiple deep learning methods and is specialized for beetle image processing, which can greatly improve the efficiency to process large-scale beetle data and accelerate biological research.</p>
  </div>
</details>

<hr>
<h3 id="25-Spot-The-Ball-A-Benchmark-for-Visual-Social-Inference-cs-CV-cs-HCPDF"><a href="#25-Spot-The-Ball-A-Benchmark-for-Visual-Social-Inference-cs-CV-cs-HCPDF" class="headerlink" title="[25] Spot The Ball: A Benchmark for Visual Social Inference cs.CV | cs.HCPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00261">Spot The Ball: A Benchmark for Visual Social Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00261" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Neha Balamurugan, Sarah Wu, Adam Chun, Gabe Gaw, Cristobal Eyzaguirre</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为’Spot The Ball’的基准测试，用于评估视觉语言模型（VLMs）在视觉社交推理任务上的表现。任务是通过足球、篮球和排球图像中球员的凝视、姿态等线索定位被移除的球。实验表明，人类的表现优于当前最先进的VLMs，揭示了模型在视觉社交推理上的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类擅长通过细微的行为线索（如凝视、姿态）推断场景中的隐藏元素，这种能力对开发更类人的AI代理至关重要。然而，现有视觉语言模型在此类任务上的表现仍有不足，因此需要一个专门的基准测试来衡量和改进模型的视觉社交推理能力。</p>
<p><strong>Result:</strong> 实验结果显示，人类被试的准确率（20-34%）是模型（≤17%）的两到三倍。模型倾向于依赖表面的空间启发式（如图像中心或球员附近），而人类则更善于利用社交线索（如凝视和身体姿态）。</p>
<p><strong>Insight:</strong> 研究发现，当前VLMs在处理视觉社交推理任务时存在明显不足，缺乏对人类社交线索的理解能力。这强调了改进模型架构的必要性，使其能够显式编码结构化的行为线索，以实现更接近人类的推理能力。</p>
<p><strong>Abstract:</strong> Humans excel at visual social inference, the ability to infer hidden elements of a scene from subtle behavioral cues such as other people’s gaze, pose, and orientation. This ability drives everyday social reasoning in humans and is critical for developing more human-like AI agents. We introduce Spot The Ball, a challenging benchmark for evaluating visual social inference in vision-language models (VLMs) using sports as a test domain. The task is to localize a removed sports ball from soccer, basketball, and volleyball images. We present a curated evaluation set with human baselines and a scalable pipeline for generating additional test items. We evaluate four state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting strategies, finding that humans are consistently two to three times more accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show that models rely on superficial spatial heuristics–such as guessing near the image center or nearby players–while humans leverage social cues like gaze direction and body pose. These findings reveal a persistent human-model gap in visual social reasoning and underscore the need for architectures that explicitly encode structured behavioral cues to achieve robust, human-like inference.</p>
  </div>
</details>

<hr>
<h3 id="26-FedReplay-A-Feature-Replay-Assisted-Federated-Transfer-Learning-Framework-for-Efficient-and-Privacy-Preserving-Smart-Agriculture-cs-CV-cs-AIPDF"><a href="#26-FedReplay-A-Feature-Replay-Assisted-Federated-Transfer-Learning-Framework-for-Efficient-and-Privacy-Preserving-Smart-Agriculture-cs-CV-cs-AIPDF" class="headerlink" title="[26] FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture cs.CV | cs.AIPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00269">FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00269" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为FedReplay的联邦学习框架，结合了预训练的CLIP视觉Transformer和轻量级Transformer分类器，旨在解决智能农业中的隐私保护和异构数据问题，显著提升分类准确性并降低通信开销。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 智能农业中的精准分类（如作物监测、病虫害识别）需要大规模数据，但集中式训练引发隐私问题，而传统联邦学习在非独立同分布（non-IID）数据和高通信成本方面表现不佳。</p>
<p><strong>Result:</strong> 在农业分类任务中达到86.6%准确率，比基线联邦学习方法提升4倍以上。</p>
<p><strong>Insight:</strong> 通过预训练模型的特征提取能力和联邦学习的隐私保护特性，可以在低成本下实现高性能的智能农业应用。</p>
<p><strong>Abstract:</strong> Accurate classification plays a pivotal role in smart agriculture, enabling applications such as crop monitoring, fruit recognition, and pest detection. However, conventional centralized training often requires large-scale data collection, which raises privacy concerns, while standard federated learning struggles with non-independent and identically distributed (non-IID) data and incurs high communication costs. To address these challenges, we propose a federated learning framework that integrates a frozen Contrastive Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight transformer classifier. By leveraging the strong feature extraction capability of the pre-trained CLIP ViT, the framework avoids training large-scale models from scratch and restricts federated updates to a compact classifier, thereby reducing transmission overhead significantly. Furthermore, to mitigate performance degradation caused by non-IID data distribution, a small subset (1%) of CLIP-extracted feature representations from all classes is shared across clients. These shared features are non-reversible to raw images, ensuring privacy preservation while aligning class representation across participants. Experimental results on agricultural classification tasks show that the proposed method achieve 86.6% accuracy, which is more than 4 times higher compared to baseline federated learning approaches. This demonstrates the effectiveness and efficiency of combining vision-language model features with federated learning for privacy-preserving and scalable agricultural intelligence.</p>
  </div>
</details>

<hr>
<h3 id="27-Beyond-ImageNet-Understanding-Cross-Dataset-Robustness-of-Lightweight-Vision-Models-cs-CV-cs-LGPDF"><a href="#27-Beyond-ImageNet-Understanding-Cross-Dataset-Robustness-of-Lightweight-Vision-Models-cs-CV-cs-LGPDF" class="headerlink" title="[27] Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models cs.CV | cs.LGPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00335">Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00335" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weidong Zhang, Pak Lun Kevin Ding, Huan Liu</span></p>
<p><strong>TL;DR:</strong> 该研究系统评估了11种轻量级视觉模型在7个不同数据集上的表现，提出了跨数据集评分（xScore）来衡量模型的鲁棒性，揭示了ImageNet性能无法预测细粒度或医学数据集表现，并指出了促进泛化的关键架构设计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前轻量级视觉模型主要在ImageNet上评估，但其在其他领域的泛化能力未知，亟需系统量化跨数据集鲁棒性并识别关键设计原则。</p>
<p><strong>Result:</strong> 1. ImageNet性能无法预测其他领域表现；2. xScore可用四个数据集高效估计；3. 各向同性卷积和高分辨率空间注意力等设计更利于泛化，Transformer模块则未显著提升性能。</p>
<p><strong>Insight:</strong> 跨数据集评估至关重要，轻量模型设计应注重空间分辨率和通道注意力，而非盲目增加Transformer模块。</p>
<p><strong>Abstract:</strong> Lightweight vision classification models such as MobileNet, ShuffleNet, and EfficientNet are increasingly deployed in mobile and embedded systems, yet their performance has been predominantly benchmarked on ImageNet. This raises critical questions: Do models that excel on ImageNet also generalize across other domains? How can cross-dataset robustness be systematically quantified? And which architectural elements consistently drive generalization under tight resource constraints? Here, we present the first systematic evaluation of 11 lightweight vision models (2.5M parameters), trained under a fixed 100-epoch schedule across 7 diverse datasets. We introduce the Cross-Dataset Score (xScore), a unified metric that quantifies the consistency and robustness of model performance across diverse visual domains. Our results show that (1) ImageNet accuracy does not reliably predict performance on fine-grained or medical datasets, (2) xScore provides a scalable predictor of mobile model performance that can be estimated from just four datasets, and (3) certain architectural components–such as isotropic convolutions with higher spatial resolution and channel-wise attention–promote broader generalization, while Transformer-based blocks yield little additional benefit, despite incurring higher parameter overhead. This study provides a reproducible framework for evaluating lightweight vision models beyond ImageNet, highlights key design principles for mobile-friendly architectures, and guides the development of future models that generalize robustly across diverse application domains.</p>
  </div>
</details>

<hr>
<h3 id="28-Federated-Dialogue-Semantic-Diffusion-for-Emotion-Recognition-under-Incomplete-Modalities-cs-CVPDF"><a href="#28-Federated-Dialogue-Semantic-Diffusion-for-Emotion-Recognition-under-Incomplete-Modalities-cs-CVPDF" class="headerlink" title="[28] Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities cs.CVPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00344">Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00344" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xihang Qiu, Jiarong Cheng, Yuhao Fang, Wanpeng Zhang, Yao Lu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种联邦对话语义扩散框架（FedDISC），用于解决多模态情感识别中模态缺失的问题，通过联邦学习聚合客户端训练的扩散模型，并引入对话引导和语义一致的生成策略，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实场景中多模态数据的模态缺失问题严重影响了情感识别的性能，传统方法依赖完整数据且易受语义失真的影响，亟需一种能适应模态缺失的鲁棒方法。</p>
<p><strong>Result:</strong> 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上，FedDISC在多种模态缺失模式下均优于现有方法。</p>
<p><strong>Insight:</strong> 联邦学习的引入可以分散模态完整性的依赖，对话和语义一致性设计提升了生成模态的质量，交替冻结策略有效平衡了恢复与分类的训练目标。</p>
<p><strong>Abstract:</strong> Multimodal Emotion Recognition in Conversations (MERC) enhances emotional understanding through the fusion of multimodal signals. However, unpredictable modality absence in real-world scenarios significantly degrades the performance of existing methods. Conventional missing-modality recovery approaches, which depend on training with complete multimodal data, often suffer from semantic distortion under extreme data distributions, such as fixed-modality absence. To address this, we propose the Federated Dialogue-guided and Semantic-Consistent Diffusion (FedDISC) framework, pioneering the integration of federated learning into missing-modality recovery. By federated aggregation of modality-specific diffusion models trained on clients and broadcasting them to clients missing corresponding modalities, FedDISC overcomes single-client reliance on modality completeness. Additionally, the DISC-Diffusion module ensures consistency in context, speaker identity, and semantics between recovered and available modalities, using a Dialogue Graph Network to capture conversational dependencies and a Semantic Conditioning Network to enforce semantic alignment. We further introduce a novel Alternating Frozen Aggregation strategy, which cyclically freezes recovery and classifier modules to facilitate collaborative optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI datasets demonstrate that FedDISC achieves superior emotion classification performance across diverse missing modality patterns, outperforming existing approaches.</p>
  </div>
</details>

<hr>
<h3 id="29-OSMGen-Highly-Controllable-Satellite-Image-Synthesis-using-OpenStreetMap-Data-cs-CV-cs-LGPDF"><a href="#29-OSMGen-Highly-Controllable-Satellite-Image-Synthesis-using-OpenStreetMap-Data-cs-CV-cs-LGPDF" class="headerlink" title="[29] OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data cs.CV | cs.LGPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00345">OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00345" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amir Ziashahabi, Narges Ghasemi, Sajjad Shahabi, John Krumm, Salman Avestimehr</span></p>
<p><strong>TL;DR:</strong> OSMGen是一个生成框架，利用OpenStreetMap（OSM）数据合成逼真的卫星图像，支持通过JSON输入精确控制场景生成，特别适合模拟城市变化和生成训练数据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有城市监测数据稀缺且更新慢，难以支持自动化监测和规划需求。OSMGen希望通过直接生成卫星图像解决这一问题。</p>
<p><strong>Result:</strong> 能够生成逼真的卫星图像，并通过编辑OSM数据模拟变化场景，为城市规划和机器学习训练提供支持。</p>
<p><strong>Insight:</strong> OSMGen展示了如何利用结构化地图数据生成高质量视觉内容，为自动化城市监测和闭环更新系统铺平道路。</p>
<p><strong>Abstract:</strong> Accurate and up-to-date geospatial data are essential for urban planning, infrastructure monitoring, and environmental management. Yet, automating urban monitoring remains difficult because curated datasets of specific urban features and their changes are scarce. We introduce OSMGen, a generative framework that creates realistic satellite imagery directly from raw OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen uses the full richness of OSM JSON, including vector geometries, semantic tags, location, and time, giving fine-grained control over how scenes are generated. A central feature of the framework is the ability to produce consistent before-after image pairs: user edits to OSM inputs translate into targeted visual changes, while the rest of the scene is preserved. This makes it possible to generate training data that addresses scarcity and class imbalance, and to give planners a simple way to preview proposed interventions by editing map data. More broadly, OSMGen produces paired (JSON, image) data for both static and changed states, paving the way toward a closed-loop system where satellite imagery can automatically drive structured OSM updates. Source code is available at <a target="_blank" rel="noopener" href="https://github.com/amir-zsh/OSMGen">https://github.com/amir-zsh/OSMGen</a>.</p>
  </div>
</details>

<hr>
<h3 id="30-Oitijjo-3D-Generative-AI-Framework-for-Rapid-3D-Heritage-Reconstruction-from-Street-View-Imagery-cs-CV-cs-AI-cs-GRPDF"><a href="#30-Oitijjo-3D-Generative-AI-Framework-for-Rapid-3D-Heritage-Reconstruction-from-Street-View-Imagery-cs-CV-cs-AI-cs-GRPDF" class="headerlink" title="[30] Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery cs.CV | cs.AI | cs.GRPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00362">Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00362" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Momen Khandoker Ope, Akif Islam, Mohd Ruhul Ameen, Abu Saleh Musa Miah, Md Rashedul Islam</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Oitijjo-3D，一种基于生成式AI的框架，利用公开的街景图像快速重建文化遗产的3D模型，解决了发展中国家文化遗产保护中资源和技术限制的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 发展中国家文化遗产保护面临资源匮乏和技术专家稀缺的问题，传统3D数字化方法成本高且复杂。论文旨在通过AI技术降低门槛，实现低成本、高效的遗产重建。</p>
<p><strong>Result:</strong> 实验证明，Oitijjo-3D能够在几秒内生成逼真且结构一致的3D模型，速度显著优于传统方法。</p>
<p><strong>Insight:</strong> 通过AI技术，文化遗产保护可以成为社区驱动的活动，降低发展中国家的经济和技本门槛，促进文化的数字化延续。</p>
<p><strong>Abstract:</strong> Cultural heritage restoration in Bangladesh faces a dual challenge of limited resources and scarce technical expertise. Traditional 3D digitization methods, such as photogrammetry or LiDAR scanning, require expensive hardware, expert operators, and extensive on-site access, which are often infeasible in developing contexts. As a result, many of Bangladesh’s architectural treasures, from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a cost-free generative AI framework that democratizes 3D cultural preservation. By using publicly available Google Street View imagery, Oitijjo-3D reconstructs faithful 3D models of heritage structures through a two-stage pipeline - multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture synthesis, and neural image-to-3D generation through Hexagen for geometry recovery. The system produces photorealistic, metrically coherent reconstructions in seconds, achieving significant speedups compared to conventional Structure-from-Motion pipelines, without requiring any specialized hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil, Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both visual and structural fidelity while drastically lowering economic and technical barriers. By turning open imagery into digital heritage, this work reframes preservation as a community-driven, AI-assisted act of cultural continuity for resource-limited nations.</p>
  </div>
</details>

<hr>
<h3 id="31-Who-Can-We-Trust-Scope-Aware-Video-Moment-Retrieval-with-Multi-Agent-Conflict-cs-CV-cs-AIPDF"><a href="#31-Who-Can-We-Trust-Scope-Aware-Video-Moment-Retrieval-with-Multi-Agent-Conflict-cs-CV-cs-AIPDF" class="headerlink" title="[31] Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict cs.CV | cs.AIPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00370">Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00370" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chaochen Wu, Guan Luo, Meiyun Zuo, Zhitao Fan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于强化学习的视频片段检索模型，通过多智能体系统和证据学习解决模型间定位冲突，并能检测查询的无效性（超出范围）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频片段检索方法未考虑不同模型间的定位结果冲突，导致无法有效整合模型输出以提高检索效果。</p>
<p><strong>Result:</strong> 在基准数据集上表现出优于当前最优方法的性能，证明了多智能体冲突建模对强化学习的提升作用。</p>
<p><strong>Insight:</strong> 多智能体系统中的竞争与冲突建模是改进视频片段检索的有效手段，同时展示了证据学习在多智能体框架中的新作用。</p>
<p><strong>Abstract:</strong> Video moment retrieval uses a text query to locate a moment from a given untrimmed video reference. Locating corresponding video moments with text queries helps people interact with videos efficiently. Current solutions for this task have not considered conflict within location results from different models, so various models cannot integrate correctly to produce better results. This study introduces a reinforcement learning-based video moment retrieval model that can scan the whole video once to find the moment’s boundary while producing its locational evidence. Moreover, we proposed a multi-agent system framework that can use evidential learning to resolve conflicts between agents’ localization output. As a side product of observing and dealing with conflicts between agents, we can decide whether a query has no corresponding moment in a video (out-of-scope) without additional training, which is suitable for real-world applications. Extensive experiments on benchmark datasets show the effectiveness of our proposed methods compared with state-of-the-art approaches. Furthermore, the results of our study reveal that modeling competition and conflict of the multi-agent system is an effective way to improve RL performance in moment retrieval and show the new role of evidential learning in the multi-agent framework.</p>
  </div>
</details>

<hr>
<h3 id="32-VisionCAD-An-Integration-Free-Radiology-Copilot-Framework-cs-CV-cs-HCPDF"><a href="#32-VisionCAD-An-Integration-Free-Radiology-Copilot-Framework-cs-CV-cs-HCPDF" class="headerlink" title="[32] VisionCAD: An Integration-Free Radiology Copilot Framework cs.CV | cs.HCPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00381">VisionCAD: An Integration-Free Radiology Copilot Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00381" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaming Li, Junlei Wu, Sheng Wang, Honglin Xiong, Jiangdong Cai</span></p>
<p><strong>TL;DR:</strong> VisionCAD 是一个基于视觉的放射学辅助框架，通过摄像头直接捕获显示屏上的医学图像，绕过了传统 CAD 系统需要集成医院 IT 基础设施的障碍。该系统能够将捕获的图像恢复为诊断质量的图像，并利用先进的诊断模型进行分析和报告生成。实验表明，其诊断性能与传统 CAD 系统接近，且适用于多种临床环境。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的计算机辅助诊断（CAD）系统需要与医院 IT 基础设施集成，部署困难且成本高昂。VisionCAD 的提出旨在通过简单的摄像头设备实现无需集成的 AI 辅助诊断，降低医疗 AI 的门槛。</p>
<p><strong>Result:</strong> 在多个分类任务中，VisionCAD 的诊断性能（F1-score）仅比传统 CAD 系统低不到 2%。自动报告的生成指标也与原始图像生成的报告相差不超过 1%。</p>
<p><strong>Insight:</strong> VisionCAD 展示了通过简单的硬件（摄像头）和标准化计算资源，可以在无需修改现有基础设施的情况下实现高效的 AI 辅助诊断，为医疗 AI 的广泛部署提供了新思路。</p>
<p><strong>Abstract:</strong> Widespread clinical deployment of computer-aided diagnosis (CAD) systems is hindered by the challenge of integrating with existing hospital IT infrastructure. Here, we introduce VisionCAD, a vision-based radiological assistance framework that circumvents this barrier by capturing medical images directly from displays using a camera system. The framework operates through an automated pipeline that detects, restores, and analyzes on-screen medical images, transforming camera-captured visual data into diagnostic-quality images suitable for automated analysis and report generation. We validated VisionCAD across diverse medical imaging datasets, demonstrating that our modular architecture can flexibly utilize state-of-the-art diagnostic models for specific tasks. The system achieves diagnostic performance comparable to conventional CAD systems operating on original digital images, with an F1-score degradation typically less than 2% across classification tasks, while natural language generation metrics for automated reports remain within 1% of those derived from original images. By requiring only a camera device and standard computing resources, VisionCAD offers an accessible approach for AI-assisted diagnosis, enabling the deployment of diagnostic capabilities in diverse clinical settings without modifications to existing infrastructure.</p>
  </div>
</details>

<hr>
<h3 id="33-Rethinking-Facial-Expression-Recognition-in-the-Era-of-Multimodal-Large-Language-Models-Benchmark-Datasets-and-Beyond-cs-CVPDF"><a href="#33-Rethinking-Facial-Expression-Recognition-in-the-Era-of-Multimodal-Large-Language-Models-Benchmark-Datasets-and-Beyond-cs-CVPDF" class="headerlink" title="[33] Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond cs.CVPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00389">Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00389" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fan Zhang, Haoxuan Li, Shengju Qian, Xin Wang, Zheng Lian</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了多模态大语言模型（MLLMs）在面部表情识别（FER）任务中的应用，提出了FERBench基准测试，揭示了MLLMs在该任务中的性能局限，并通过构建高质量数据集和训练策略开发了统一且可解释的模型UniFER-7B。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究旨在探索MLLMs在FER任务中的潜力，并解决其推理和可解释性不足的问题。</p>
<p><strong>Result:</strong> UniFER-7B在FER任务中表现优异，超越了许多通用MLLMs。</p>
<p><strong>Insight:</strong> MLLMs在FER任务中具有潜力，但仍需改进推理和可解释性；后训练策略和高质量数据集是关键。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have revolutionized numerous research fields, including computer vision and affective computing. As a pivotal challenge in this interdisciplinary domain, facial expression recognition (FER) has evolved from separate, domain-specific models to more unified approaches. One promising avenue to unify FER tasks is converting conventional FER datasets into visual question-answering (VQA) formats, enabling the direct application of powerful generalist MLLMs for inference. However, despite the success of cutting-edge MLLMs in various tasks, their performance on FER tasks remains largely unexplored. To address this gap, we provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art MLLMs across four widely used FER datasets. Our results reveal that, while MLLMs exhibit good classification performance, they still face significant limitations in reasoning and interpretability. To this end, we introduce post-training strategies aimed at enhancing the facial expression reasoning capabilities of MLLMs. Specifically, we curate two high-quality and large-scale datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K for reinforcement learning with verifiable rewards (RLVR), respectively. Building upon them, we develop a unified and interpretable FER foundation model termed UniFER-7B, which outperforms many open-sourced and closed-source generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).</p>
  </div>
</details>

<hr>
<h3 id="34-VinciCoder-Unifying-Multimodal-Code-Generation-via-Coarse-to-fine-Visual-Reinforcement-Learning-cs-CVPDF"><a href="#34-VinciCoder-Unifying-Multimodal-Code-Generation-via-Coarse-to-fine-Visual-Reinforcement-Learning-cs-CVPDF" class="headerlink" title="[34] VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning cs.CVPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00391">VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00391" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuanle Zhao, Deyang Jiang, Zhixiong Zeng, Lei Chen, Haibo Qiu</span></p>
<p><strong>TL;DR:</strong> 本文提出了VinciCoder，一种通过粗到细视觉强化学习统一多模态代码生成的模型，解决了现有视觉-语言模型在单一任务训练上的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉-语言模型（VLMs）在多模态代码生成中依赖于单一任务训练，导致泛化性不足。本文旨在通过统一的训练框架提升模型的泛化能力。</p>
<p><strong>Result:</strong> 在多个多模态代码生成基准测试中，VinciCoder实现了最先进的性能。</p>
<p><strong>Insight:</strong> 粗到细的视觉强化学习策略能够有效提升多模态代码生成的视觉保真度和泛化能力。</p>
<p><strong>Abstract:</strong> Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like Chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on various multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, underscoring the effectiveness of our coarse-to-fine ViRL strategy. The code and model will be available at <a target="_blank" rel="noopener" href="https://github.com/DocTron-hub/VinciCoder">https://github.com/DocTron-hub/VinciCoder</a>.</p>
  </div>
</details>

<hr>
<h3 id="35-CoT-Saliency-Unified-Chain-of-Thought-Reasoning-for-Heterogeneous-Saliency-Tasks-cs-CVPDF"><a href="#35-CoT-Saliency-Unified-Chain-of-Thought-Reasoning-for-Heterogeneous-Saliency-Tasks-cs-CVPDF" class="headerlink" title="[35] CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks cs.CVPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00396">CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00396" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Long Li, Shuichen Ji, Ziyang Luo, Nian Liu, Dingwen Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了CoT-Saliency框架，通过链式思维（CoT）推理统一处理三种异质显著性任务（SOD、CoSOD、SIS）。方法包括两阶段训练（SFT和RL）和创新的CGPO算法，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的显著性任务（如SOD、CoSOD、SIS）通常独立处理，导致模型泛化能力受限。作者希望通过统一的链式思维推理框架，解决任务异质性问题并提升性能。</p>
<p><strong>Result:</strong> 模型在三个任务上均匹配或超越SOTA方法，尤其是在CoSOD任务中S-measure达到0.899，比之前最佳结果提升8.0个百分点。</p>
<p><strong>Insight:</strong> 统一的链式思维推理不仅能解决任务异质性，还能显著减少训练数据需求，为多任务学习提供了新思路。</p>
<p><strong>Abstract:</strong> We present the first unified framework that jointly handles three operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model (VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a lightweight single-sample algorithm that leverages the discrepancy between reward and model confidence as a per-sample advantage signal. This design naturally focuses updates on informative responses while eliminating group sampling, thereby addressing GRPO’s key limitations: confidence-agnostic learning, signal dilution, and prohibitive computational overhead. We also introduce an “output-to-reasoning” strategy to construct high-fidelity SFT data that ensures logical consistency with ground-truth masks. Experiments show our model matches or outperforms specialized SOTA methods and strong closed-source VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for CoSOD, surpassing the prior best by 8.0 percentage points, despite using far less training data.</p>
  </div>
</details>

<hr>
<h3 id="36-LGCA-Enhancing-Semantic-Representation-via-Progressive-Expansion-cs-CV-cs-AIPDF"><a href="#36-LGCA-Enhancing-Semantic-Representation-via-Progressive-Expansion-cs-CV-cs-AIPDF" class="headerlink" title="[36] LGCA: Enhancing Semantic Representation via Progressive Expansion cs.CV | cs.AIPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00419">LGCA: Enhancing Semantic Representation via Progressive Expansion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00419" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thanh Hieu Cao, Trung Khang Tran, Gia Thinh Pham, Tuong Nghiem Diep, Thanh Binh Nguyen</span></p>
<p><strong>TL;DR:</strong> LGCA是一种新的框架，通过渐进扩展图像区域来增强语义表示，避免随机裁剪引入的误导信息，显著提升零样本图像分类性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有CLIP模型在零样本图像分类中表现良好，但随机裁剪可能导致误导信息和小尺度特征的偏差，需要一种更有效的方法捕捉局部和全局特征。</p>
<p><strong>Result:</strong> 实验表明，LGCA在多个数据集上的零样本性能显著优于现有基线方法。</p>
<p><strong>Insight:</strong> 渐进扩展显著区域是一种有效的方法，既能减少误导信息，又能捕捉多尺度特征，而不会显著增加计算复杂度。</p>
<p><strong>Abstract:</strong> Recent advancements in large-scale pretraining in natural language processing have enabled pretrained vision-language models such as CLIP to effectively align images and text, significantly improving performance in zero-shot image classification tasks. Subsequent studies have further demonstrated that cropping images into smaller regions and using large language models to generate multiple descriptions for each caption can further enhance model performance. However, due to the inherent sensitivity of CLIP, random image crops can introduce misinformation and bias, as many images share similar features at small scales. To address this issue, we propose Localized-Globalized Cross-Alignment (LGCA), a framework that first captures the local features of an image and then repeatedly selects the most salient regions and expands them. The similarity score is designed to incorporate both the original and expanded images, enabling the model to capture both local and global features while minimizing misinformation. Additionally, we provide a theoretical analysis demonstrating that the time complexity of LGCA remains the same as that of the original model prior to the repeated expansion process, highlighting its efficiency and scalability. Extensive experiments demonstrate that our method substantially improves zero-shot performance across diverse datasets, outperforming state-of-the-art baselines.</p>
  </div>
</details>

<hr>
<h3 id="37-Leveraging-Hierarchical-Image-Text-Misalignment-for-Universal-Fake-Image-Detection-cs-CV-cs-AIPDF"><a href="#37-Leveraging-Hierarchical-Image-Text-Misalignment-for-Universal-Fake-Image-Detection-cs-CV-cs-AIPDF" class="headerlink" title="[37] Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection cs.CV | cs.AIPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00427">Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00427" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Daichi Zhang, Tong Zhang, Jianmin Bao, Shiming Ge, Sabine Süsstrunk</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种利用图像-文本不对齐的通用假图像检测方法（ITEM），通过多模态视角检测假图像的生成，展示了优异的泛化性和鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着生成模型的快速发展，检测生成的假图像以防恶意使用成为一个关键问题。现有方法局限于二分类任务，仅依赖视觉线索，易过拟合且泛化性差。</p>
<p><strong>Result:</strong> 实验表明，ITEM在多种生成模型上优于现有方法，具有显著的泛化性和鲁棒性。</p>
<p><strong>Insight:</strong> 多模态视角（图像-文本对齐）可作为检测假图像的有效线索，层次化设计能进一步提升检测性能。</p>
<p><strong>Abstract:</strong> With the rapid development of generative models, detecting generated fake images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a naive binary image classification task. However, such methods focus only on visual clues, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images cannot be properly aligned with corresponding captions compared to real images. Upon this observation, we propose a simple yet effective detector termed ITEM by leveraging the image-text misalignment in a joint visual-language space as discriminative clues. Specifically, we first measure the misalignment of the images and captions in pre-trained CLIP’s space, and then tune a MLP head to perform the usual detection task. Furthermore, we propose a hierarchical misalignment scheme that first focuses on the whole image and then each semantic object described in the caption, which can explore both global and fine-grained local semantic misalignment as clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models.</p>
  </div>
</details>

<hr>
<h3 id="38-ToxicTextCLIP-Text-Based-Poisoning-and-Backdoor-Attacks-on-CLIP-Pre-training-cs-CV-cs-CR-cs-LGPDF"><a href="#38-ToxicTextCLIP-Text-Based-Poisoning-and-Backdoor-Attacks-on-CLIP-Pre-training-cs-CV-cs-CR-cs-LGPDF" class="headerlink" title="[38] ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training cs.CV | cs.CR | cs.LGPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00446">ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CR | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00446" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Yao, Haiyang Zhao, Yimin Chen, Jiawei Guo, Kecheng Huang</span></p>
<p><strong>TL;DR:</strong> ToxicTextCLIP是一个针对CLIP预训练阶段的文本投毒和后门攻击框架，通过背景感知选择和背景驱动增强生成高质量对抗文本，实验证明其攻击效果显著并能绕过现有防御。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> CLIP模型依赖大规模网络数据进行训练，但未经过滤的数据可能存在投毒和后门攻击风险，尤其是文本模态的攻击研究较少。</p>
<p><strong>Result:</strong> 在分类和检索任务中，攻击成功率高达95.83%，后门攻击命中率达98.68%，且能绕过多种防御方法。</p>
<p><strong>Insight:</strong> 文本模态的攻击对CLIP等模型构成严重威胁，现有防御方法对此类攻击无效。</p>
<p><strong>Abstract:</strong> The Contrastive Language-Image Pretraining (CLIP) model has significantly advanced vision-language modeling by aligning image-text pairs from large-scale web data through self-supervised contrastive learning. Yet, its reliance on uncurated Internet-sourced data exposes it to data poisoning and backdoor risks. While existing studies primarily investigate image-based attacks, the text modality, which is equally central to CLIP’s training, remains underexplored. In this work, we introduce ToxicTextCLIP, a framework for generating high-quality adversarial texts that target CLIP during the pre-training phase. The framework addresses two key challenges: semantic misalignment caused by background inconsistency with the target class, and the scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively applies: 1) a background-aware selector that prioritizes texts with background content aligned to the target class, and 2) a background-driven augmenter that generates semantically coherent and diverse poisoned samples. Extensive experiments on classification and retrieval tasks show that ToxicTextCLIP achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be accessed via <a target="_blank" rel="noopener" href="https://github.com/xinyaocse/ToxicTextCLIP/">https://github.com/xinyaocse/ToxicTextCLIP/</a>.</p>
  </div>
</details>

<hr>
<h3 id="39-Weakly-Supervised-Pneumonia-Localization-from-Chest-X-Rays-Using-Deep-Neural-Network-and-Grad-CAM-Explanations-cs-CVPDF"><a href="#39-Weakly-Supervised-Pneumonia-Localization-from-Chest-X-Rays-Using-Deep-Neural-Network-and-Grad-CAM-Explanations-cs-CVPDF" class="headerlink" title="[39] Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations cs.CVPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00456">Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00456" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kiran Shahi, Anup Bagale</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种弱监督深度学习框架，通过Grad-CAM解释从胸部X光片中分类和定位肺炎，无需昂贵的像素级标注，仅使用图像级标签生成具有临床意义的热力图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的肺炎检测方法需要像素级标注，成本高昂且耗时。本文旨在通过弱监督学习减少标注需求，同时保持高准确性和可解释性。</p>
<p><strong>Result:</strong> 在Kermany CXR数据集上，ResNet-18和EfficientNet-B0达到最佳测试精度（98%）、ROC-AUC（0.997）和F1分数（0.987），而MobileNet-V2在精度与计算成本之间取得平衡。Grad-CAM可视化确认模型聚焦于临床相关肺部区域。</p>
<p><strong>Insight:</strong> 弱监督可解释模型能够在不依赖像素级标注的情况下实现高性能肺炎检测，增强了AI在医学影像中的透明度和临床信任。</p>
<p><strong>Abstract:</strong> This study proposes a weakly supervised deep learning framework for pneumonia classification and localization from chest X-rays, utilizing Grad-CAM explanations. Instead of costly pixel-level annotations, our approach utilizes image-level labels to generate clinically meaningful heatmaps that highlight regions affected by pneumonia. We evaluate seven ImageNet-pretrained architectures ResNet-18&#x2F;50, DenseNet-121, EfficientNet-B0, MobileNet-V2&#x2F;V3, and ViT-B16 under identical training conditions with focal loss and patient-wise splits to prevent data leakage. Experimental results on the Kermany CXR dataset demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test accuracy of 98%, ROC-AUC &#x3D; 0.997, and F1 &#x3D; 0.987, while MobileNet-V2 provides an optimal trade-off between accuracy and computational cost. Grad-CAM visualizations confirm that the proposed models focus on clinically relevant lung regions, supporting the use of interpretable AI for radiological diagnostics. This work highlights the potential of weakly supervised explainable models that enhance pneumonia screening transparency, and clinical trust in AI-assisted medical imaging.   <a target="_blank" rel="noopener" href="https://github.com/kiranshahi/pneumonia-analysis">https://github.com/kiranshahi/pneumonia-analysis</a></p>
  </div>
</details>

<hr>
<h3 id="40-HumanCrafter-Synergizing-Generalizable-Human-Reconstruction-and-Semantic-3D-Segmentation-cs-CVPDF"><a href="#40-HumanCrafter-Synergizing-Generalizable-Human-Reconstruction-and-Semantic-3D-Segmentation-cs-CVPDF" class="headerlink" title="[40] HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation cs.CVPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00468">HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00468" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Panwang Pan, Tingting Shen, Chenxin Li, Yunlong Lin, Kairun Wen</span></p>
<p><strong>TL;DR:</strong> HumanCrafter是一个统一的框架，能够从单张图像中联合建模外观和人体部位语义，通过整合几何先验和自监督语义先验，实现高质量的3D人体重建和分割。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的生成模型在3D人体重建上取得了高保真度，但在特定任务（如人体部位分割）中的实用性有限。作者希望通过联合建模外观和语义，解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，HumanCrafter在单图像3D人体分割和重建任务中超越了现有最优方法。</p>
<p><strong>Insight:</strong> 联合建模外观和语义可以提升任务的性能；自监督学习和交互式标注是解决数据稀缺的有效途径；跨任务协同和多任务优化能够增强模型的通用性。</p>
<p><strong>Abstract:</strong> Recent advances in generative models have achieved high-fidelity in 3D human reconstruction, yet their utility for specific tasks (e.g., human 3D segmentation) remains constrained. We propose HumanCrafter, a unified framework that enables the joint modeling of appearance and human-part semantics from a single image in a feed-forward manner. Specifically, we integrate human geometric priors in the reconstruction stage and self-supervised semantic priors in the segmentation stage. To address labeled 3D human datasets scarcity, we further develop an interactive annotation procedure for generating high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task synergy, while the multi-task objective simultaneously optimizes texture modeling fidelity and semantic consistency. Extensive experiments demonstrate that HumanCrafter surpasses existing state-of-the-art methods in both 3D human-part segmentation and 3D human reconstruction from a single image.</p>
  </div>
</details>

<hr>
<h3 id="41-Longitudinal-Vestibular-Schwannoma-Dataset-with-Consensus-based-Human-in-the-loop-Annotations-cs-CV-cs-AIPDF"><a href="#41-Longitudinal-Vestibular-Schwannoma-Dataset-with-Consensus-based-Human-in-the-loop-Annotations-cs-CV-cs-AIPDF" class="headerlink" title="[41] Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations cs.CV | cs.AIPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00472">Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00472" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Navodini Wijethilake, Marina Ivory, Oscar MacCormac, Siddhant Kumar, Aaron Kujawa</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于人类专家共识和深度学习的迭代分割框架，用于准确分割前庭神经鞘瘤（VS）MRI数据，显著提高了分割精度并提升了效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 前庭神经鞘瘤（VS）的准确分割对患者管理至关重要，但传统手动标注耗时且依赖专家。尽管深度学习（DL）已实现自动化分割，但其在多样数据集和复杂病例上的鲁棒性仍有挑战。</p>
<p><strong>Result:</strong> 目标数据集的分割DSC从0.9125提升至0.9670，外部数据集表现稳定，效率提升约37.4%。专家评估揭示了需要干预的复杂病例。</p>
<p><strong>Insight:</strong> 人类专家参与的迭代优化是提升医学图像分割鲁棒性和效率的有效策略，尤其在复杂临床场景中；公开数据集有助于推动领域研究。</p>
<p><strong>Abstract:</strong> Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance Imaging (MRI) is essential for patient management but often requires time-intensive manual annotations by experts. While recent advances in deep learning (DL) have facilitated automated segmentation, challenges remain in achieving robust performance across diverse datasets and complex clinical cases. We present an annotated dataset stemming from a bootstrapped DL-based framework for iterative segmentation and quality refinement of VS in MRI. We combine data from multiple centres and rely on expert consensus for trustworthiness of the annotations. We show that our approach enables effective and resource-efficient generalisation of automated segmentation models to a target data distribution. The framework achieved a significant improvement in segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from 0.9125 to 0.9670 on our target internal validation dataset, while maintaining stable performance on representative external datasets. Expert evaluation on 143 scans further highlighted areas for model refinement, revealing nuanced cases where segmentation required expert intervention. The proposed approach is estimated to enhance efficiency by approximately 37.4% compared to the conventional manual annotation process. Overall, our human-in-the-loop model training approach achieved high segmentation accuracy, highlighting its potential as a clinically adaptable and generalisable strategy for automated VS segmentation in diverse clinical settings. The dataset includes 190 patients, with tumour annotations available for 534 longitudinal contrast-enhanced T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans from 6 patients. This dataset is publicly accessible on The Cancer Imaging Archive (TCIA) (<a target="_blank" rel="noopener" href="https://doi.org/10.7937/bq0z-xa62">https://doi.org/10.7937/bq0z-xa62</a>).</p>
  </div>
</details>

<hr>
<h3 id="42-FedMGP-Personalized-Federated-Learning-with-Multi-Group-Text-Visual-Prompts-cs-CV-cs-LGPDF"><a href="#42-FedMGP-Personalized-Federated-Learning-with-Multi-Group-Text-Visual-Prompts-cs-CV-cs-LGPDF" class="headerlink" title="[42] FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts cs.CV | cs.LGPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00480">FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00480" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weihao Bo, Yanpeng Sun, Yu Wang, Xinyu Zhang, Zechao Li</span></p>
<p><strong>TL;DR:</strong> FedMGP提出了一种个性化的联邦提示学习方法，通过多组文本-视觉提示捕捉多样化的语义和实例级线索，并利用动态提示聚合策略平衡全局和客户端特征。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的联邦学习方法在视觉语言模型中难以同时实现高效的参数利用和个性化的特征学习，FedMGP旨在解决这一问题。</p>
<p><strong>Result:</strong> 在多个联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均优于现有方法。</p>
<p><strong>Insight:</strong> 通过分层提示设计和动态聚合策略，FedMGP能够在保护共同知识的同时有效学习客户端特异性特征。</p>
<p><strong>Abstract:</strong> In this paper, we introduce FedMGP, a new paradigm for personalized federated prompt learning in vision-language models. FedMGP equips each client with multiple groups of paired textual and visual prompts, enabling the model to capture diverse, fine-grained semantic and instance-level cues. A diversity loss is introduced to drive each prompt group to specialize in distinct and complementary semantic aspects, ensuring that the groups collectively cover a broader range of local characteristics. During communication, FedMGP employs a dynamic prompt aggregation strategy based on similarity-guided probabilistic sampling: each client computes the cosine similarity between its prompt groups and the global prompts from the previous round, then samples s groups via a softmax-weighted distribution. This soft selection mechanism preferentially aggregates semantically aligned knowledge while still enabling exploration of underrepresented patterns effectively balancing the preservation of common knowledge with client-specific features. Notably, FedMGP maintains parameter efficiency by redistributing a fixed prompt capacity across multiple groups, achieving state-of-the-art performance with the lowest communication parameters among all federated prompt learning methods. Theoretical analysis shows that our dynamic aggregation strategy promotes robust global representation learning by reinforcing shared semantics while suppressing client-specific noise. Extensive experiments demonstrate that FedMGP consistently outperforms prior approaches in both personalization and domain generalization across diverse federated vision-language benchmarks. The code will be released on <a target="_blank" rel="noopener" href="https://github.com/weihao-bo/FedMGP.git">https://github.com/weihao-bo/FedMGP.git</a>.</p>
  </div>
</details>

<hr>
<h3 id="43-Diff4Splat-Controllable-4D-Scene-Generation-with-Latent-Dynamic-Reconstruction-Models-cs-CVPDF"><a href="#43-Diff4Splat-Controllable-4D-Scene-Generation-with-Latent-Dynamic-Reconstruction-Models-cs-CVPDF" class="headerlink" title="[43] Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models cs.CVPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00503">Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00503" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Panwang Pan, Chenguo Lin, Jingjing Zhao, Chenxin Li, Yuchen Lin</span></p>
<p><strong>TL;DR:</strong> Diff4Splat是一种前馈方法，通过单张图像合成可控且显式的4D场景，结合视频扩散模型的生成先验与大规模4D数据集学习的几何和运动约束，实现高效动态场景合成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有动态场景合成方法通常依赖测试时优化或后处理，效率较低。Diff4Splat旨在通过前馈方式直接预测4D场景，避免耗时优化。</p>
<p><strong>Result:</strong> Diff4Splat在30秒内合成高质量4D场景，在视频生成、新视角合成和几何提取任务中优于或匹配基于优化的方法，同时效率显著提升。</p>
<p><strong>Insight:</strong> 通过将扩散模型与3D高斯场结合，Diff4Splat展示了前馈方法在动态场景合成中的潜力，为实时高效应用提供了新方向。</p>
<p><strong>Abstract:</strong> We introduce Diff4Splat, a feed-forward method that synthesizes controllable and explicit 4D scenes from a single image. Our approach unifies the generative priors of video diffusion models with geometry and motion constraints learned from large-scale 4D datasets. Given a single input image, a camera trajectory, and an optional text prompt, Diff4Splat directly predicts a deformable 3D Gaussian field that encodes appearance, geometry, and motion, all in a single forward pass, without test-time optimization or post-hoc refinement. At the core of our framework lies a video latent transformer, which augments video diffusion models to jointly capture spatio-temporal dependencies and predict time-varying 3D Gaussian primitives. Training is guided by objectives on appearance fidelity, geometric accuracy, and motion consistency, enabling Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate the effectiveness of Diff4Splatacross video generation, novel view synthesis, and geometry extraction, where it matches or surpasses optimization-based methods for dynamic scene synthesis while being significantly more efficient.</p>
  </div>
</details>

<hr>
<h3 id="44-VinDr-CXR-VQA-A-Visual-Question-Answering-Dataset-for-Explainable-Chest-X-Ray-Analysis-with-Multi-Task-Learning-cs-CVPDF"><a href="#44-VinDr-CXR-VQA-A-Visual-Question-Answering-Dataset-for-Explainable-Chest-X-Ray-Analysis-with-Multi-Task-Learning-cs-CVPDF" class="headerlink" title="[44] VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning cs.CVPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00504">VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00504" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hai-Dang Nguyen, Ha-Hieu Pham, Hao T. Nguyen, Huy-Hieu Pham</span></p>
<p><strong>TL;DR:</strong> 该论文提出了VinDr-CXR-VQA数据集，用于可解释的胸部X光视觉问答（Med-VQA），包含17,597个问答对和4,394张图像，标注了放射科医生验证的边界框和临床推理解释。基准测试显示性能提升了11.8%，并支持病变定位。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的医学视觉问答（Med-VQA）数据集缺乏可解释性和空间定位能力，限制了其在临床决策中的应用。本文旨在填补这一空白，提供一个具有多样临床意图和平衡样本分布的大规模数据集。</p>
<p><strong>Result:</strong> 基准测试表明，F1得分为0.624，比基线提升11.8%，并实现了病变的精确定位。</p>
<p><strong>Insight:</strong> 数据集的设计强调了临床可解释性和样本平衡性，为未来的Med-VQA研究提供了可重复且可靠的基准。</p>
<p><strong>Abstract:</strong> We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset contains 17,597 question-answer pairs across 4,394 images, each annotated with radiologist-verified bounding boxes and clinical reasoning explanations. Our question taxonomy spans six diagnostic types-Where, What, Is there, How many, Which, and Yes&#x2F;No-capturing diverse clinical intents. To improve reliability, we construct a balanced distribution of 41.7% positive and 58.3% negative samples, mitigating hallucinations in normal cases. Benchmarking with MedGemma-4B-it demonstrates improved performance (F1 &#x3D; 0.624, +11.8% over baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance reproducible and clinically grounded Med-VQA research. The dataset and evaluation tools are publicly available at huggingface.co&#x2F;datasets&#x2F;Dangindev&#x2F;VinDR-CXR-VQA.</p>
  </div>
</details>

<hr>
<h3 id="45-OmniTrack-Omnidirectional-Multi-Object-Tracking-by-Learning-Large-FoV-Trajectory-Feedback-cs-CV-cs-RO-eess-IVPDF"><a href="#45-OmniTrack-Omnidirectional-Multi-Object-Tracking-by-Learning-Large-FoV-Trajectory-Feedback-cs-CV-cs-RO-eess-IVPDF" class="headerlink" title="[45] OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback cs.CV | cs.RO | eess.IVPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00510">OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00510" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kai Luo, Hao Shi, Kunyu Peng, Fei Teng, Sheng Wu</span></p>
<p><strong>TL;DR:</strong> OmniTrack++提出了一种全景多目标跟踪方法，通过轨迹反馈驱动的框架解决全景图像中的特有挑战，包括360°视场、分辨率稀释和视角依赖性失真。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统多目标跟踪方法在窄视场的针孔相机中表现良好，但在全景图像中由于360°视场带来的失真、大搜索空间和身份模糊问题而表现不佳。</p>
<p><strong>Result:</strong> 在JRDB和EmboTrack数据集上实现了最先进性能，HOTA分别提升了25.5%和43.07%。</p>
<p><strong>Insight:</strong> 轨迹反馈和自适应设计在处理全景图像中的跟踪问题时至关重要。</p>
<p><strong>Abstract:</strong> This paper investigates Multi-Object Tracking (MOT) in panoramic imagery, which introduces unique challenges including a 360{\deg} Field of View (FoV), resolution dilution, and severe view-dependent distortions. Conventional MOT methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily under these conditions. To address panoramic distortion, large search space, and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a feedback-driven framework that progressively refines perception with trajectory cues. A DynamicSSM block first stabilizes panoramic features, implicitly alleviating geometric distortion. On top of normalized representations, FlexiTrack Instances use trajectory-informed feedback for flexible localization and reliable short-term association. To ensure long-term robustness, an ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts design, enabling recovery from fragmented tracks and reducing identity drift. Finally, a Tracklet Management module adaptively switches between end-to-end and tracking-by-detection modes according to scene dynamics, offering a balanced and scalable solution for panoramic MOT. To support rigorous evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for panoramic MOT that includes QuadTrack, captured with a quadruped robot, and BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets span wide-angle environments and diverse motion patterns, providing a challenging testbed for real-world panoramic perception. Extensive experiments on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art performance, yielding substantial HOTA improvements of +25.5% on JRDB and +43.07% on QuadTrack over the original OmniTrack. Datasets and code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/xifen523/OmniTrack">https://github.com/xifen523/OmniTrack</a>.</p>
  </div>
</details>

<hr>
<h3 id="46-ID-Composer-Multi-Subject-Video-Synthesis-with-Hierarchical-Identity-Preservation-cs-CVPDF"><a href="#46-ID-Composer-Multi-Subject-Video-Synthesis-with-Hierarchical-Identity-Preservation-cs-CVPDF" class="headerlink" title="[46] ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00511">ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00511" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Panwang Pan, Jingjing Zhao, Yuchen Lin, Chenguo Lin, Chenxin Li</span></p>
<p><strong>TL;DR:</strong> ID-Composer提出了一种新型框架，用于从文本提示和参考图像生成多主体视频，通过层级身份保留注意力机制、预训练视觉语言模型的语义理解和在线强化学习，显著提升了身份保留、时间一致性和视频质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频生成模型通常仅基于文本或单一图像，限制了可控性和适用性。ID-Composer旨在解决多主体视频生成的挑战，要求同时保留主体身份、跨主体和模态的语义整合以及时间一致性。</p>
<p><strong>Result:</strong> 实验表明，ID-Composer在身份保留、时间一致性和视频质量上优于现有方法。</p>
<p><strong>Insight:</strong> 通过结合注意力机制、VLM和强化学习，ID-Composer在多主体视频生成任务中实现了更高的可控性和生成质量。</p>
<p><strong>Abstract:</strong> Video generative models pretrained on large-scale datasets can produce high-quality videos, but are often conditioned on text or a single image, limiting controllability and applicability. We introduce ID-Composer, a novel framework that addresses this gap by tackling multi-subject video generation from a text prompt and reference images. This task is challenging as it requires preserving subject identities, integrating semantics across subjects and modalities, and maintaining temporal consistency. To faithfully preserve the subject consistency and textual information in synthesized videos, ID-Composer designs a \textbf{hierarchical identity-preserving attention mechanism}, which effectively aggregates features within and across subjects and modalities. To effectively allow for the semantic following of user intention, we introduce \textbf{semantic understanding via pretrained vision-language model (VLM)}, leveraging VLM’s superior semantic understanding to provide fine-grained guidance and capture complex interactions between multiple subjects. Considering that standard diffusion loss often fails in aligning the critical concepts like subject ID, we employ an \textbf{online reinforcement learning phase} to drive the overall training objective of ID-Composer into RLVR. Extensive experiments demonstrate that our model surpasses existing methods in identity preservation, temporal consistency, and video quality.</p>
  </div>
</details>

<hr>
<h3 id="47-SegDebias-Test-Time-Bias-Mitigation-for-ViT-Based-CLIP-via-Segmentation-cs-CVPDF"><a href="#47-SegDebias-Test-Time-Bias-Mitigation-for-ViT-Based-CLIP-via-Segmentation-cs-CVPDF" class="headerlink" title="[47] SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation cs.CVPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00523">SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00523" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fangyu Wu, Yujun Cai</span></p>
<p><strong>TL;DR:</strong> SegDebias是一种无需训练数据或显式偏置标注的测试时去偏方法，通过预训练的分割模型隔离目标视觉属性，调整非目标区域以减少偏置。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有去偏方法需要训练数据或显式偏置标注，限制了实用性。测试时方法虽避免这一限制，但仍依赖于对偏置的先验知识。</p>
<p><strong>Result:</strong> 在Waterbirds和CelebA数据集上，方法在组鲁棒性和Attention IoU方面优于现有测试时去偏方法。</p>
<p><strong>Insight:</strong> 分割引导的去偏方法能有效实现无标注的偏置缓解，展示了在开放集合任务中的泛化能力。</p>
<p><strong>Abstract:</strong> Vision language models such as CLIP have shown remarkable performance in zero shot classification, but remain susceptible to spurious correlations, where irrelevant visual features influence predictions. Existing debiasing methods often require access to training data and explicit group labels to perform fine-tuning or adjust embeddings, which limits their practicality in real-world settings. Test-time methods attempt to avoid this constraint, but many still depend on prior knowledge of dataset specific biases, limiting their generalizability in open set settings. In this work, we propose a test-time debiasing method for ViT based CLIP models that requires no additional training or assumptions of bias annotations. Our approach uses a pretrained segmentation model to isolate the target visual attribute, then adjusts the non target regions so that their embeddings are uniformly similar to all class specific text prompts. This procedure removes unintended bias signals from confounding visual regions while preserving the target attribute. Experiments on Waterbirds and CelebA show that our method outperforms existing test-time debiasing approaches in both group robustness metrics and Attention IoU. These results demonstrate the effectiveness of segmentation guided interventions for scalable and annotation free bias mitigation in vision language models.</p>
  </div>
</details>

<hr>
<h3 id="48-Text-guided-Fine-Grained-Video-Anomaly-Detection-cs-CVPDF"><a href="#48-Text-guided-Fine-Grained-Video-Anomaly-Detection-cs-CVPDF" class="headerlink" title="[48] Text-guided Fine-Grained Video Anomaly Detection cs.CVPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00524">Text-guided Fine-Grained Video Anomaly Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00524" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jihao Gu, Kun Li, He Wang, Kaan Akşit</span></p>
<p><strong>TL;DR:</strong> 提出了T-VAD框架，通过文本引导精细化视频异常检测，结合视觉-语言模型和异常热图解码器，实现了高精度异常定位和描述。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统视频异常检测方法输出有限且依赖人工评估，缺乏精细化和交互性。</p>
<p><strong>Result:</strong> 在UBnormal和ShanghaiTech数据集上取得SOTA性能，AUC达94.8%，并展示了高精度异常描述能力。</p>
<p><strong>Insight:</strong> 文本引导方法显著提升了异常检测的精细化和交互性，为大语言模型在多模态任务中的应用提供了新思路。</p>
<p><strong>Abstract:</strong> Video Anomaly Detection (VAD) aims to identify anomalous events within video segments. In scenarios such as surveillance or industrial process monitoring, anomaly detection is of critical importance. While existing approaches are semi-automated, requiring human assessment for anomaly detection, traditional VADs offer limited output as either normal or anomalous. We propose Text-guided Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD) that performs pixel-wise visual-textual feature alignment to generate fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly Encoder (RAE) that transforms the heatmaps into learnable textual embeddings, guiding the LVLM to accurately identify and localize anomalous events in videos. This significantly enhances both the granularity and interactivity of anomaly detection. The proposed method achieving SOTA performance by demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and 67.8%&#x2F;76.7% accuracy in anomaly heatmaps (RBDC&#x2F;TBDC) on the UBnormal dataset, and subjectively verified more preferable textual description on the ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories; Yes&#x2F;No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for targets, 78.10 for trajectories; Yes&#x2F;No accuracy: 89.73%).</p>
  </div>
</details>

<hr>
<h3 id="49-Real-IAD-Variety-Pushing-Industrial-Anomaly-Detection-Dataset-to-a-Modern-Era-cs-CVPDF"><a href="#49-Real-IAD-Variety-Pushing-Industrial-Anomaly-Detection-Dataset-to-a-Modern-Era-cs-CVPDF" class="headerlink" title="[49] Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era cs.CVPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00540">Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00540" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenbing Zhu, Chengjie Wang, Bin-Bin Gao, Jiangning Zhang, Guannan Jiang</span></p>
<p><strong>TL;DR:</strong> 论文提出了Real-IAD Variety数据集，是目前最大、最多样的工业异常检测（IAD）基准，旨在解决现有数据集的局限性问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有公共基准在类别多样性和规模上严重不足，导致算法性能饱和且难以迁移到真实场景。因此，需要构建一个更全面的基准。</p>
<p><strong>Result:</strong> 实验表明，当前最先进的多类别无监督异常检测方法在扩展到160个类别时性能显著下降，而视觉语言模型表现出更强的鲁棒性和泛化能力。</p>
<p><strong>Insight:</strong> 数据集的多样性和规模对异常检测模型的性能至关重要，视觉语言模型在泛化能力上具有显著优势，可作为未来研究的方向。</p>
<p><strong>Abstract:</strong> Industrial Anomaly Detection (IAD) is critical for enhancing operational safety, ensuring product quality, and optimizing manufacturing efficiency across global industries. However, the IAD algorithms are severely constrained by the limitations of existing public benchmarks. Current datasets exhibit restricted category diversity and insufficient scale, frequently resulting in metric saturation and limited model transferability to real-world scenarios. To address this gap, we introduce Real-IAD Variety, the largest and most diverse IAD benchmark, comprising 198,960 high-resolution images across 160 distinct object categories. Its diversity is ensured through comprehensive coverage of 28 industries, 24 material types, and 22 color variations. Our comprehensive experimental analysis validates the benchmark’s substantial challenge: state-of-the-art multi-class unsupervised anomaly detection methods experience significant performance degradation when scaled from 30 to 160 categories. Crucially, we demonstrate that vision-language models exhibit remarkable robustness to category scale-up, with minimal performance variation across different category counts, significantly enhancing generalization capabilities in diverse industrial contexts. The unprecedented scale and complexity of Real-IAD Variety position it as an essential resource for training and evaluating next-generation foundation models for anomaly detection. By providing this comprehensive benchmark with rigorous evaluation protocols across multi-class unsupervised, multi-view, and zero-&#x2F;few-shot settings, we aim to accelerate research beyond domain-specific constraints, enabling the development of scalable, general-purpose anomaly detection systems. Real-IAD Variety will be made publicly available to facilitate innovation in this critical field.</p>
  </div>
</details>

<hr>
<h3 id="50-TRACES-Temporal-Recall-with-Contextual-Embeddings-for-Real-Time-Video-Anomaly-Detection-cs-CV-cs-AI-68T07-68T45-68U10-I-2-10-I-5-4-I-4-8-C-3PDF"><a href="#50-TRACES-Temporal-Recall-with-Contextual-Embeddings-for-Real-Time-Video-Anomaly-Detection-cs-CV-cs-AI-68T07-68T45-68U10-I-2-10-I-5-4-I-4-8-C-3PDF" class="headerlink" title="[50] TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection cs.CV | cs.AI | 68T07, 68T45, 68U10 | I.2.10; I.5.4; I.4.8; C.3PDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00580">TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | 68T07, 68T45, 68U10 | I.2.10; I.5.4; I.4.8; C.3</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00580" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yousuf Ahmed Siddiqui, Sufiyaan Usmani, Umer Tariq, Jawwad Ahmed Shamsi, Muhammad Burhan Khan</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合时间信号与视觉嵌入的上下文感知零样本异常检测方法TRACES，通过跨注意力机制和内存增强管道实现实时检测，在UCF-Crime和XD-Violence数据集上达到SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的异常检测方法通常忽略了上下文信息的重要性，导致泛化能力不足。作者希望通过结合时间演化和上下文信息，实现更适应现实场景的零样本异常检测。</p>
<p><strong>Result:</strong> 在UCF-Crime上AUC达到90.4%，在XD-Violence上AP达到83.67%，均为零样本模型的SOTA性能，同时实现了实时推理和高精度。</p>
<p><strong>Insight:</strong> 上下文信息和时间演化对异常检测至关重要，尤其是在零样本场景下。内存增强和跨注意力机制的融合为实时应用提供了高效且可解释的解决方案。</p>
<p><strong>Abstract:</strong> Video anomalies often depend on contextual information available and temporal evolution. Non-anomalous action in one context can be anomalous in some other context. Most anomaly detectors, however, do not notice this type of context, which seriously limits their capability to generalize to new, real-life situations. Our work addresses the context-aware zero-shot anomaly detection challenge, in which systems need to learn adaptively to detect new events by correlating temporal and appearance features with textual traces of memory in real time. Our approach defines a memory-augmented pipeline, correlating temporal signals with visual embeddings using cross-attention, and real-time zero-shot anomaly classification by contextual similarity scoring. We achieve 90.4% AUC on UCF-Crime and 83.67% AP on XD-Violence, a new state-of-the-art among zero-shot models. Our model achieves real-time inference with high precision and explainability for deployment. We show that, by fusing cross-attention temporal fusion and contextual memory, we achieve high fidelity anomaly detection, a step towards the applicability of zero-shot models in real-world surveillance and infrastructure monitoring.</p>
  </div>
</details>

<hr>
<h3 id="51-CueBench-Advancing-Unified-Understanding-of-Context-Aware-Video-Anomalies-in-Real-World-cs-CVPDF"><a href="#51-CueBench-Advancing-Unified-Understanding-of-Context-Aware-Video-Anomalies-in-Real-World-cs-CVPDF" class="headerlink" title="[51] CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World cs.CVPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00613">CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00613" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yating Yu, Congqi Cao, Zhaoying Wang, Weihua Meng, Jie Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了首个专注于上下文感知视频异常的基准数据集CueBench，并通过统一的生成方法Cue-R1显著提升了现有模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频异常理解方法对真实世界中复杂的上下文和细微差异理解不足，因此需要一个新的基准数据集和模型来解决这一问题。</p>
<p><strong>Result:</strong> 在CueBench上，Cue-R1平均性能超过现有最先进模型24%。</p>
<p><strong>Insight:</strong> 现有视觉语言模型在真实世界异常理解上仍有不足，上下文和多任务统一优化是关键改进方向。</p>
<p><strong>Abstract:</strong> How far are deep models from real-world video anomaly understanding (VAU)? Current works typically emphasize on detecting unexpected occurrences deviated from normal patterns or comprehending anomalous events with interpretable descriptions. However, they exhibit only a superficial comprehension of real-world anomalies, with limited breadth in complex principles and subtle context that distinguish the anomalies from normalities, e.g., climbing cliffs with safety gear vs. without it. To this end, we introduce CueBench, the first of its kind Benchmark, devoted to Context-aware video anomalies within a Unified Evaluation framework. We comprehensively establish an event-centric hierarchical taxonomy that anchors two core event types: 14 conditional and 18 absolute anomaly events, defined by their refined semantics from diverse contexts across 174 scenes and 198 attributes. Based on this, we propose to unify and benchmark context-aware VAU with various challenging tasks across recognition, temporal grounding, detection, and anticipation. This also serves as a rigorous and fair probing evaluation suite for generative-discriminative as well as generalized-specialized vision-language models (VLMs). To address the challenges underlying CueBench, we further develop Cue-R1 based on R1-style reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined rewards in a unified generative manner. Extensive results on CueBench reveal that, existing VLMs are still far from satisfactory real-world anomaly understanding, while our Cue-R1 surpasses these state-of-the-art approaches by over 24% on average.</p>
  </div>
</details>

<hr>
<h3 id="52-Grounding-Surgical-Action-Triplets-with-Instrument-Instance-Segmentation-A-Dataset-and-Target-Aware-Fusion-Approach-cs-CVPDF"><a href="#52-Grounding-Surgical-Action-Triplets-with-Instrument-Instance-Segmentation-A-Dataset-and-Target-Aware-Fusion-Approach-cs-CVPDF" class="headerlink" title="[52] Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach cs.CVPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00643">Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00643" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Oluwatosin Alabi, Meng Wei, Charlie Budd, Tom Vercauteren, Miaojing Shi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新任务——通过器械实例分割实现手术动作三元组的空间定位（三元组分��），并发布了CholecTriplet-Seg数据集和TargetFusionNet模型，显著提升了手术动作理解的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有手术动作三元组识别方法仅限于帧级分类，难以可靠地将动作与特定器械实例关联。类激活图的空间定位方法缺乏精确性和鲁棒性。论文旨在填补这一空白。</p>
<p><strong>Result:</strong> TargetFusionNet在识别、检测和三元组分割指标上均优于基线模型，证明了强实例监督与弱目标先验的有效性。</p>
<p><strong>Insight:</strong> 三元组分割为手术动作提供了统一的空���定位框架，显著提升了手术场景理解的解释性和鲁棒性。</p>
<p><strong>Abstract:</strong> Understanding surgical instrument-tissue interactions requires not only identifying which instrument performs which action on which anatomical target, but also grounding these interactions spatially within the surgical scene. Existing surgical action triplet recognition methods are limited to learning from frame-level classification, failing to reliably link actions to specific instrument instances.Previous attempts at spatial grounding have primarily relied on class activation maps, which lack the precision and robustness required for detailed instrument-tissue interaction analysis.To address this gap, we propose grounding surgical action triplets with instrument instance segmentation, or triplet segmentation for short, a new unified task which produces spatially grounded &lt;instrument, verb, target&gt; outputs.We start by presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000 annotated frames, linking instrument instance masks with action verb and anatomical target annotations, and establishing the first benchmark for strongly supervised, instance-level triplet grounding and evaluation.To learn triplet segmentation, we propose TargetFusionNet, a novel architecture that extends Mask2Former with a target-aware fusion mechanism to address the challenge of accurate anatomical target prediction by fusing weak anatomy priors with instrument instance queries.Evaluated across recognition, detection, and triplet segmentation metrics, TargetFusionNet consistently improves performance over existing baselines, demonstrating that strong instance supervision combined with weak target priors significantly enhances the accuracy and robustness of surgical action understanding.Triplet segmentation establishes a unified framework for spatially grounding surgical action triplets. The proposed benchmark and architecture pave the way for more interpretable, surgical scene understanding.</p>
  </div>
</details>

<hr>
<h3 id="53-Benchmarking-individual-tree-segmentation-using-multispectral-airborne-laser-scanning-data-the-FGI-EMIT-dataset-cs-CVPDF"><a href="#53-Benchmarking-individual-tree-segmentation-using-multispectral-airborne-laser-scanning-data-the-FGI-EMIT-dataset-cs-CVPDF" class="headerlink" title="[53] Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset cs.CVPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00653">Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00653" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lassi Ruoppa, Tarmo Hietala, Verneri Seppänen, Josef Taher, Teemu Hakala</span></p>
<p><strong>TL;DR:</strong> 本文介绍了FGI-EMIT数据集，这是首个用于单木分割（ITS）的大规模多光谱激光雷达基准数据集，并对比了传统无监督算法和深度学习方法的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 单木分割在林业应用中至关重要，但缺乏大规模多光谱LiDAR数据集和方法对比研究。</p>
<p><strong>Result:</strong> 深度学习表现最优（F1-score 73.3%），尤其在底层树木分割中领先传统方法25.9%。多光谱数据利用效果有限。</p>
<p><strong>Insight:</strong> 深度学习方法在不同点密度下均优于无监督算法，但当前方法未充分利用多光谱信息。</p>
<p><strong>Abstract:</strong> Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for applications such as forest inventory, carbon monitoring and biodiversity assessment. Traditionally, ITS has been achieved with unsupervised geometry-based algorithms, while more recent advances have shifted toward supervised deep learning (DL). In the past, progress in method development was hindered by the lack of large-scale benchmark datasets, and the availability of novel data formats, particularly multispectral (MS) LiDAR, remains limited to this day, despite evidence that MS reflectance can improve the accuracy of ITS. This study introduces FGI-EMIT, the first large-scale MS airborne laser scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550 nm, the dataset consists of 1,561 manually annotated trees, with a particular focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked four conventional unsupervised algorithms and four supervised DL approaches. Hyperparameters of unsupervised methods were optimized using a Bayesian approach, while DL models were trained from scratch. Among the unsupervised methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL approaches performed significantly better overall, with the best model, ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9 percentage points. An ablation study demonstrated that current DL-based approaches generally fail to leverage MS reflectance information when it is provided as additional input features, although single channel reflectance can improve accuracy marginally, especially for understory trees. A performance analysis across point densities further showed that DL methods consistently remain superior to unsupervised algorithms, even at densities as low as 10 points&#x2F;m$^2$.</p>
  </div>
</details>

<hr>
<h3 id="54-Metadata-Aligned-3D-MRI-Representations-for-Contrast-Understanding-and-Quality-Control-cs-CV-cs-AI-cs-LGPDF"><a href="#54-Metadata-Aligned-3D-MRI-Representations-for-Contrast-Understanding-and-Quality-Control-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[54] Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control cs.CV | cs.AI | cs.LGPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00681">Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00681" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mehmet Yigit Avci, Pedro Borges, Virginia Fernandez, Paul Wright, Mehmet Yigitsoy</span></p>
<p><strong>TL;DR:</strong> 该论文提出了MR-CLIP框架，利用DICOM元数据对齐三维MRI图像，通过学习对比表示，解决了MRI数据异质性和缺乏标准化标签的问题，实现了高效的序列分类和质量控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> MRI数据存在显著异质性且缺乏标准化的对比度标签，这限制了大规模自动化分析。需要一个统一的表示方法以减少对人工标注的依赖。</p>
<p><strong>Result:</strong> MR-CLIP在少样本序列分类中优于监督基线方法，并能通过图像-元数据嵌入距离检测异常数据。</p>
<p><strong>Insight:</strong> 通过利用常规采集元数据作为监督信号，MR-CLIP为跨多样临床数据集的高效MRI分析提供了可扩展的基础。</p>
<p><strong>Abstract:</strong> Magnetic Resonance Imaging suffers from substantial data heterogeneity and the absence of standardized contrast labels across scanners, protocols, and institutions, which severely limits large-scale automated analysis. A unified representation of MRI contrast would enable a wide range of downstream utilities, from automatic sequence recognition to harmonization and quality control, without relying on manual annotations. To this end, we introduce MR-CLIP, a metadata-guided framework that learns MRI contrast representations by aligning volumetric images with their DICOM acquisition parameters. The resulting embeddings shows distinct clusters of MRI sequences and outperform supervised 3D baselines under data scarcity in few-shot sequence classification. Moreover, MR-CLIP enables unsupervised data quality control by identifying corrupted or inconsistent metadata through image-metadata embedding distances. By transforming routinely available acquisition metadata into a supervisory signal, MR-CLIP provides a scalable foundation for label-efficient MRI analysis across diverse clinical datasets.</p>
  </div>
</details>

<hr>
<h3 id="55-Outlier-Aware-Post-Training-Quantization-for-Image-Super-Resolution-cs-CVPDF"><a href="#55-Outlier-Aware-Post-Training-Quantization-for-Image-Super-Resolution-cs-CVPDF" class="headerlink" title="[55] Outlier-Aware Post-Training Quantization for Image Super-Resolution cs.CVPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00682">Outlier-Aware Post-Training Quantization for Image Super-Resolution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00682" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hailing Wang, jianglin Lu, Yitian Zhang, Yun Fu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种针对图像超分辨率（SR）网络的异常值感知的后训练量化（PTQ）方法，通过双区域量化策略和敏感性感知微调，显著提升了量化性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的SR后训练量化方法因忽视激活中的异常值而性能不佳。论文发现这些异常值与图像颜色信息强相关，直接移除会导致性能下降，因此需要一种更好的量化策略。</p>
<p><strong>Result:</strong> 实验表明，该方法在多种SR网络和数据集上优于现有PTQ方法，且性能接近QAT方法，速度提升至少75倍。</p>
<p><strong>Insight:</strong> 激活中的异常值与图像颜色信息相关，分层敏感性对量化性能有显著影响。</p>
<p><strong>Abstract:</strong> Quantization techniques, including quantization-aware training (QAT) and post-training quantization (PTQ), have become essential for inference acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has garnered significant attention as it eliminates the need for ground truth and model retraining. However, existing PTQ methods for SR often fail to achieve satisfactory performance as they overlook the impact of outliers in activation. Our empirical analysis reveals that these prevalent activation outliers are strongly correlated with image color information, and directly removing them leads to significant performance degradation. Motivated by this, we propose a dual-region quantization strategy that partitions activations into an outlier region and a dense region, applying uniform quantization to each region independently to better balance bit-width allocation. Furthermore, we observe that different network layers exhibit varying sensitivities to quantization, leading to different levels of performance degradation. To address this, we introduce sensitivity-aware finetuning that encourages the model to focus more on highly sensitive layers, further enhancing quantization performance. Extensive experiments demonstrate that our method outperforms existing PTQ approaches across various SR networks and datasets, while achieving performance comparable to QAT methods in most scenarios with at least a 75 speedup.</p>
  </div>
</details>

<hr>
<h3 id="56-Evolve-to-Inspire-Novelty-Search-for-Diverse-Image-Generation-cs-CV-cs-AIPDF"><a href="#56-Evolve-to-Inspire-Novelty-Search-for-Diverse-Image-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[56] Evolve to Inspire: Novelty Search for Diverse Image Generation cs.CV | cs.AIPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00686">Evolve to Inspire: Novelty Search for Diverse Image Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00686" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alex Inch, Passawis Chaiyapattanaporn, Yuchen Zhu, Yuan Lu, Ting-Wen Ko</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于新颖性搜索的方法WANDER，用于从单一输入提示生成多样化图像，通过LLM语义演化和CLIP嵌入量化新颖性，结合发射器提升多样性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的文本到图像扩散模型在生成高保真图像时，往往输出多样性不足，限制了在探索性和创意任务中的应用。当前提示优化技术多针对美观性或不适于视觉创意领域。</p>
<p><strong>Result:</strong> 实验表明WANDER在多样性指标上显著优于现有基线，消融研究验证了发射器的有效性。</p>
<p><strong>Insight:</strong> 新颖性搜索和语义演化可有效提升生成图像的多样性，发射器在引导搜索方向上有重要作用。</p>
<p><strong>Abstract:</strong> Text-to-image diffusion models, while proficient at generating high-fidelity im- ages, often suffer from limited output diversity, hindering their application in exploratory and ideation tasks. Existing prompt optimization techniques typically target aesthetic fitness or are ill-suited to the creative visual domain. To address this shortcoming, we introduce WANDER, a novelty search-based approach to generating diverse sets of images from a single input prompt. WANDER operates directly on natural language prompts, employing a Large Language Model (LLM) for semantic evolution of diverse sets of images, and using CLIP embeddings to quantify novelty. We additionally apply emitters to guide the search into distinct regions of the prompt space, and demonstrate that they boost the diversity of the generated images. Empirical evaluations using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that WANDER significantly outperforms existing evolutionary prompt optimization baselines in diversity metrics. Ablation studies confirm the efficacy of emitters.</p>
  </div>
</details>

<hr>
<h3 id="57-Toward-Better-Optimization-of-Low-Dose-CT-Enhancement-A-Critical-Analysis-of-Loss-Functions-and-Image-Quality-Assessment-Metrics-cs-CVPDF"><a href="#57-Toward-Better-Optimization-of-Low-Dose-CT-Enhancement-A-Critical-Analysis-of-Loss-Functions-and-Image-Quality-Assessment-Metrics-cs-CVPDF" class="headerlink" title="[57] Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics cs.CVPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00698">Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00698" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Taifour Yousra, Beghdadi Azeddine, Marie Luong, Zuheng Ming</span></p>
<p><strong>TL;DR:</strong> 本文分析了低剂量CT增强中的损失函数与图像质量评估指标之间的不一致性，强调在设计新损失函数时需考虑图像质量指标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 低剂量CT（LDCT）成像虽减少了辐射暴露，但常伴随噪声和伪影，影响诊断准确性。现有的深度学习模型虽在PSNR和SSIM等指标上表现优异，但这些指标无法充分反映图像的感知质量，尤其是在医学图像中。</p>
<p><strong>Result:</strong> 研究结果表明，当前常用的损失函数与图像质量评估指标之间存在脱节，尤其是在反映感知质量方面。这提示需要更全面地考虑质量指标来优化损失函数设计。</p>
<p><strong>Insight:</strong> 论文指出，仅依赖PSNR和SSIM等指标可能无法满足医学图像的感知质量需求，未来研究应更多关注如何将感知质量融入损失函数设计中。</p>
<p><strong>Abstract:</strong> Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to mitigate high exposure side effects, but often suffers from noise and artifacts that affect diagnostic accuracy. To tackle this issue, deep learning models have been developed to enhance LDCT images. Various loss functions have been employed, including classical approaches such as Mean Square Error and adversarial losses, as well as customized loss functions(LFs) designed for specific architectures. Although these models achieve remarkable performance in terms of PSNR and SSIM, these metrics are limited in their ability to reflect perceptual quality, especially for medical images. In this paper, we focus on one of the most critical elements of DL-based architectures, namely the loss function. We conduct an objective analysis of the relevance of different loss functions for LDCT image quality enhancement and their consistency with image quality metrics. Our findings reveal inconsistencies between LFs and quality metrics, and highlight the need of consideration of image quality metrics when developing a new loss function for image quality enhancement.</p>
  </div>
</details>

<hr>
<h3 id="58-Validating-Deep-Models-for-Alzheimer’s-18F-FDG-PET-Diagnosis-Across-Populations-A-Study-with-Latin-American-Data-cs-CVPDF"><a href="#58-Validating-Deep-Models-for-Alzheimer’s-18F-FDG-PET-Diagnosis-Across-Populations-A-Study-with-Latin-American-Data-cs-CVPDF" class="headerlink" title="[58] Validating Deep Models for Alzheimer’s 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data cs.CVPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00728">Validating Deep Models for Alzheimer’s 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00728" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hugo Massaroli, Hernan Chaves, Pilar Anania, Mauricio Farez, Emmanuel Iarussi</span></p>
<p><strong>TL;DR:</strong> 论文验证了深度学习模型在阿尔茨海默病诊断中的泛化性，发现尽管在北美数据集（ADNI）上表现优异，但在拉丁美洲数据集（FLENI）上性能显著下降，揭示了显著的领域偏移。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的深度学习模型在阿尔茨海默病（AD）诊断中表现出色，但训练数据主要来自北美人群，模型在未被充分研究的群体（如拉丁美洲人群）中的泛化能力尚未充分验证。</p>
<p><strong>Result:</strong> 模型在ADNI上AUC高达0.96-0.97，但在FLENI上降至0.80-0.82。不同架构性能相似，归一化和采样选择对泛化至关重要。</p>
<p><strong>Insight:</strong> 1. 模型在不同人群中的性能差异显著；2. Transformer的优势在此任务中不明显；3. 领域适应和数据集多样化是未来研究方向。</p>
<p><strong>Abstract:</strong> Deep learning models have shown strong performance in diagnosing Alzheimer’s disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with training datasets largely composed of North American cohorts such as those in the Alzheimer’s Disease Neuroimaging Initiative (ADNI). However, their generalization to underrepresented populations remains underexplored. In this study, we benchmark convolutional and Transformer-based models on the ADNI dataset and assess their generalization performance on a novel Latin American clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show that while all models achieve high AUCs on ADNI (up to .96, .97), their performance drops substantially on FLENI (down to .82, .80, respectively), revealing a significant domain shift. The tested architectures demonstrated similar performance, calling into question the supposed advantages of transformers for this specific task. Through ablation studies, we identify per-image normalization and a correct sampling selection as key factors for generalization. Occlusion sensitivity analysis further reveals that models trained on ADNI, generally attend to canonical hypometabolic regions for the AD class, but focus becomes unclear for the other classes and for FLENI scans. These findings highlight the need for population-aware validation of diagnostic AI models and motivate future work on domain adaptation and cohort diversification.</p>
  </div>
</details>

<hr>
<h3 id="59-Towards-classification-based-representation-learning-for-place-recognition-on-LiDAR-scans-cs-CVPDF"><a href="#59-Towards-classification-based-representation-learning-for-place-recognition-on-LiDAR-scans-cs-CVPDF" class="headerlink" title="[59] Towards classification-based representation learning for place recognition on LiDAR scans cs.CVPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00738">Towards classification-based representation learning for place recognition on LiDAR scans</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00738" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dmitrii Khizbullin, Maksim Konoplia</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于多类分类的LiDAR扫描地点识别方法，替代传统的对比学习框架，通过离散的位置标签直接分类扫描位置，性能与对比学习方法相当，但训练效率和稳定性更优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的地点识别方法主要依赖对比学习，这种方法虽然在性能上表现良好，但在训练效率和稳定性方面存在不足。论文探索了一种基于分类的替代方法，旨在提升训练效率和稳定性。</p>
<p><strong>Result:</strong> 在NuScenes数据集上，该方法的表现与现有的对比学习方法相当，同时在训练效率和稳定性方面具有显著优势。</p>
<p><strong>Insight:</strong> 论文揭示了地点识别任务可以通过分类框架有效解决，而不必局限于对比学习。这不仅简化了训练过程，还可能为其他类似任务提供新的研究方向。</p>
<p><strong>Abstract:</strong> Place recognition is a crucial task in autonomous driving, allowing vehicles to determine their position using sensor data. While most existing methods rely on contrastive learning, we explore an alternative approach by framing place recognition as a multi-class classification problem. Our method assigns discrete location labels to LiDAR scans and trains an encoder-decoder model to classify each scan’s position directly. We evaluate this approach on the NuScenes dataset and show that it achieves competitive performance compared to contrastive learning-based methods while offering advantages in training efficiency and stability.</p>
  </div>
</details>

<hr>
<h3 id="60-Erasing-‘Ugly’-from-the-Internet-Propagation-of-the-Beauty-Myth-in-Text-Image-Models-cs-CV-cs-CLPDF"><a href="#60-Erasing-‘Ugly’-from-the-Internet-Propagation-of-the-Beauty-Myth-in-Text-Image-Models-cs-CV-cs-CLPDF" class="headerlink" title="[60] Erasing ‘Ugly’ from the Internet: Propagation of the Beauty Myth in Text-Image Models cs.CV | cs.CLPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00749">Erasing ‘Ugly’ from the Internet: Propagation of the Beauty Myth in Text-Image Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00749" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tanvi Dinkar, Aiqi Jiang, Gavin Abercrombie, Ioannis Konstas</span></p>
<p><strong>TL;DR:</strong> 这篇论文研究了生成式AI模型如何编码‘美’并消除‘丑’，揭示了模型存在的偏见及其社会影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 社交媒体加剧了西方审美标准的传播，导致负面自我形象问题。随着AI生成内容的增加，研究这些模型是否强化了此类标准变得尤为重要。</p>
<p><strong>Result:</strong> 结果显示，86.5%的图像为浅肤色，22%包含非安全内容，74%被评为此年轻人形象。非二元性别个体的形象更年轻化和性感化。</p>
<p><strong>Insight:</strong> 生成式AI模型不仅强化了现有审美偏见，还可能通过负面提示主动消除不符合‘美’标准的特征，对社会数据流产生负面影响。</p>
<p><strong>Abstract:</strong> Social media has exacerbated the promotion of Western beauty norms, leading to negative self-image, particularly in women and girls, and causing harm such as body dysmorphia. Increasingly content on the internet has been artificially generated, leading to concerns that these norms are being exaggerated. The aim of this work is to study how generative AI models may encode ‘beauty’ and erase ‘ugliness’, and discuss the implications of this for society. To investigate these aims, we create two image generation pipelines: a text-to-image model and a text-to-language model-to image model. We develop a structured beauty taxonomy which we use to prompt three language models (LMs) and two text-to-image models to cumulatively generate 5984 images using our two pipelines. We then recruit women and non-binary social media users to evaluate 1200 of the images through a Likert-scale within-subjects study. Participants show high agreement in their ratings. Our results show that 86.5% of generated images depicted people with lighter skin tones, 22% contained explicit content despite Safe for Work (SFW) training, and 74% were rated as being in a younger age demographic. In particular, the images of non-binary individuals were rated as both younger and more hypersexualised, indicating troubling intersectional effects. Notably, prompts encoded with ‘negative’ or ‘ugly’ beauty traits (such as “a wide nose”) consistently produced higher Not SFW (NSFW) ratings regardless of gender. This work sheds light on the pervasive demographic biases related to beauty standards present in generative AI models – biases that are actively perpetuated by model developers, such as via negative prompting. We conclude by discussing the implications of this on society, which include pollution of the data streams and active erasure of features that do not fall inside the stereotype of what is considered beautiful by developers.</p>
  </div>
</details>

<hr>
<h3 id="61-A-Hybrid-YOLOv5-SSD-IoT-Based-Animal-Detection-System-for-Durian-Plantation-Protection-cs-CVPDF"><a href="#61-A-Hybrid-YOLOv5-SSD-IoT-Based-Animal-Detection-System-for-Durian-Plantation-Protection-cs-CVPDF" class="headerlink" title="[61] A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection cs.CVPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00777">A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00777" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anis Suttan Shahrir, Zakiah Ayop, Syarulnaziah Anawar, Norulzahrah Mohd Zainudin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于YOLOv5-SSD混合算法的物联网动物检测系统，用于榴莲种植园保护。系统结合了目标检测算法、实时监控、Telegram通知和自动声音威慑机制，显著提高了检测准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 榴莲种植园常受动物入侵，导致作物损失和经济损失。传统监控方法依赖人力效果不佳，亟需自动化解决方案。</p>
<p><strong>Result:</strong> 系统对大象、野猪和猴子的检测准确率分别为90%、85%和70%，白天效果最佳，夜间准确性下降。</p>
<p><strong>Insight:</strong> 混合算法和多模态系统的结合提升了农业自动化中的检测和响应能力，为未来智能农业解决方案提供了参考。</p>
<p><strong>Abstract:</strong> Durian plantation suffers from animal intrusions that cause crop damage and financial loss. The traditional farming practices prove ineffective due to the unavailability of monitoring without human intervention. The fast growth of machine learning and Internet of Things (IoT) technology has led to new ways to detect animals. However, current systems are limited by dependence on single object detection algorithms, less accessible notification platforms, and limited deterrent mechanisms. This research suggests an IoT-enabled animal detection system for durian crops. The system integrates YOLOv5 and SSD object detection algorithms to improve detection accuracy. The system provides real-time monitoring, with detected intrusions automatically reported to farmers via Telegram notifications for rapid response. An automated sound mechanism (e.g., tiger roar) is triggered once the animal is detected. The YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%, 85% and 70%, respectively. The system shows the highest accuracy in daytime and decreases at night, regardless of whether the image is still or a video. Overall, this study contributes a comprehensive and practical framework that combines detection, notification, and deterrence, paving the way for future innovations in automated farming solutions.</p>
  </div>
</details>

<hr>
<h3 id="62-Class-agnostic-3D-Segmentation-by-Granularity-Consistent-Automatic-2D-Mask-Tracking-cs-CV-cs-AIPDF"><a href="#62-Class-agnostic-3D-Segmentation-by-Granularity-Consistent-Automatic-2D-Mask-Tracking-cs-CV-cs-AIPDF" class="headerlink" title="[62] Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking cs.CV | cs.AIPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00785">Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00785" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Juan Wang, Yasutomo Kawanishi, Tomo Miyazaki, Zhijie Wang, Shinichiro Omachi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种通过粒度一致性的自动2D掩码跟踪方法，解决了独立处理视频帧导致的3D伪标签不一致问题，结合三阶段课程学习框架，逐步生成全局一致的3D分割结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通过将2D掩码从基础模型转移到3D生成伪标签，但独立处理视频帧会导致分割粒度和3D伪标签不一致，影响最终分割准确性。</p>
<p><strong>Result:</strong> 方法生成了准确一致的3D分割，并在标准基准测试中达到SOTA性能，具备开放词汇能力。</p>
<p><strong>Insight:</strong> 通过逐步暴露模型于一致性更高的伪标签，可以从初始不一致的2D先验中提取出鲁棒的3D表示。</p>
<p><strong>Abstract:</strong> 3D instance segmentation is an important task for real-world applications. To avoid costly manual annotations, existing methods have explored generating pseudo labels by transferring 2D masks from foundation models to 3D. However, this approach is often suboptimal since the video frames are processed independently. This causes inconsistent segmentation granularity and conflicting 3D pseudo labels, which degrades the accuracy of final segmentation. To address this, we introduce a Granularity-Consistent automatic 2D Mask Tracking approach that maintains temporal correspondences across frames, eliminating conflicting pseudo labels. Combined with a three-stage curriculum learning framework, our approach progressively trains from fragmented single-view data to unified multi-view annotations, ultimately globally coherent full-scene supervision. This structured learning pipeline enables the model to progressively expose to pseudo-labels of increasing consistency. Thus, we can robustly distill a consistent 3D representation from initially fragmented and contradictory 2D priors. Experimental results demonstrated that our method effectively generated consistent and accurate 3D segmentations. Furthermore, the proposed method achieved state-of-the-art results on standard benchmarks and open-vocabulary ability.</p>
  </div>
</details>

<hr>
<h3 id="63-FedOnco-Bench-A-Reproducible-Benchmark-for-Privacy-Aware-Federated-Tumor-Segmentation-with-Synthetic-CT-Data-cs-CV-cs-AIPDF"><a href="#63-FedOnco-Bench-A-Reproducible-Benchmark-for-Privacy-Aware-Federated-Tumor-Segmentation-with-Synthetic-CT-Data-cs-CV-cs-AIPDF" class="headerlink" title="[63] FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data cs.CV | cs.AIPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00795">FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00795" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Viswa Chaitanya Marella, Suhasnadh Reddy Veluru, Sai Teja Erukude</span></p>
<p><strong>TL;DR:</strong> 本文提出了FedOnco-Bench，一个基于合成CT数据的联邦学习基准，用于评估肿瘤分割的性能和隐私泄漏。结果表明，隐私与性能存在权衡：FedAvg性能高但隐私泄漏多，DP-SGD隐私保护强但性能较低。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 联邦学习在隐私敏感环境中具有潜力，但仍然存在成员推断攻击和数据异构性的问题。为了解决这些问题，需要一个可复现的基准来评估隐私保护的FL方法。</p>
<p><strong>Result:</strong> FedAvg在Dice系数0.85下表现最好，但隐私泄漏较高（AUC 0.72）；DP-SGD隐私保护最好（AUC 0.25）但性能较低（Dice 0.79）。FedProx和FedBN在异构数据下表现平衡。</p>
<p><strong>Insight:</strong> 隐私与性能之间存在明确权衡；FedProx和FedBN适合处理异构数据；FedOnco-Bench为开发隐私保护的医学图像分割方法提供了标准化平台。</p>
<p><strong>Abstract:</strong> Federated Learning (FL) allows multiple institutions to cooperatively train machine learning models while retaining sensitive data at the source, which has great utility in privacy-sensitive environments. However, FL systems remain vulnerable to membership-inference attacks and data heterogeneity. This paper presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using synthetic oncologic CT scans with tumor annotations. It evaluates segmentation performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and FedAvg with DP-SGD. Results show a distinct trade-off between privacy and utility: FedAvg is high performance (Dice around 0.85) with more privacy leakage (attack AUC about 0.72), while DP-SGD provides a higher level of privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx and FedBN offer balanced performance under heterogeneous data, especially with non-identical distributed client data. FedOnco-Bench serves as a standardized, open-source platform for benchmarking and developing privacy-preserving FL methods for medical image segmentation.</p>
  </div>
</details>

<hr>
<h3 id="64-Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing-cs-CV-cs-MMPDF"><a href="#64-Med-Banana-50K-A-Cross-modality-Large-Scale-Dataset-for-Text-guided-Medical-Image-Editing-cs-CV-cs-MMPDF" class="headerlink" title="[64] Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing cs.CV | cs.MMPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00801">Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00801" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhihui Chen, Mengling Feng</span></p>
<p><strong>TL;DR:</strong> Med-Banana-50K是一个跨模态的大规模医学图像编辑数据集，包含50K张图像，覆盖3种模态和23种疾病，通过双向编辑（病灶添加和移除）和质量控制方法生成，用于指令驱动的医学图像编辑研究。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目前缺乏大规模、高质量的医学图像编辑数据集，限制了医学图像编辑技术的研究进展。</p>
<p><strong>Result:</strong> 数据集包含50K张高质量医学图像，37K次失败的尝试记录，用于进一步研究和模型优化。</p>
<p><strong>Insight:</strong> 医学图像编辑需要严格的质量控制，迭代优化和多模态数据采集是提升数据集质量的关键。</p>
<p><strong>Abstract:</strong> Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community’s progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [<a target="_blank" rel="noopener" href="https://github.com/richardChenzhihui/med-banana-50k]">https://github.com/richardChenzhihui/med-banana-50k]</a>.</p>
  </div>
</details>

<hr>
<h3 id="65-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding-cs-CV-cs-AI-cs-CL-cs-HC-cs-LGPDF"><a href="#65-GUI-AIMA-Aligning-Intrinsic-Multimodal-Attention-with-a-Context-Anchor-for-GUI-Grounding-cs-CV-cs-AI-cs-CL-cs-HC-cs-LGPDF" class="headerlink" title="[65] GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding cs.CV | cs.AI | cs.CL | cs.HC | cs.LGPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00810">GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | cs.HC | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00810" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shijie Zhou, Viet Dac Lai, Hao Tan, Jihyung Kil, Wanrong Zhu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于注意力机制的无坐标监督微调框架GUI-AIMA，通过对齐多模态大型语言模型的内生注意力与分块接地信号，实现了高效的图形用户界面（GUI）接地任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于多模态大型语言模型（MLLMs）的GUI接地方法直接将坐标生成任务与视觉输入相关联，这在计算上具有挑战性且精确度有限。因此，作者提出了一种更直观的方法，即先选择与指令相关的视觉块，再在这些块内确定精确的点击位置。</p>
<p><strong>Result:</strong> GUI-AIMA-3B在ScreenSpot-Pro和OSWorld-G数据集上分别达到了58.6%和62.2%的平均准确率，性能优于同类3B模型。</p>
<p><strong>Insight:</strong> 1. 轻量级训练可以显著提升MLLMs的接地能力。2. 注意力机制在GUI接地任务中具有天然的潜力。3. 无坐标方法简化了任务流程并提高了灵活性。</p>
<p><strong>Abstract:</strong> Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2% on OSWorld-G. Project page: <a target="_blank" rel="noopener" href="https://github.com/sjz5202/GUI-AIMA">https://github.com/sjz5202/GUI-AIMA</a></p>
  </div>
</details>

<hr>
<h3 id="66-TA-LSDiff-Topology-Aware-Diffusion-Guided-by-a-Level-Set-Energy-for-Pancreas-Segmentation-cs-CVPDF"><a href="#66-TA-LSDiff-Topology-Aware-Diffusion-Guided-by-a-Level-Set-Energy-for-Pancreas-Segmentation-cs-CVPDF" class="headerlink" title="[66] TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation cs.CVPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00815">TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00815" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Gou, Fanghui Song, Yuming Xing, Shengzhu Shi, Zhichang Guo</span></p>
<p><strong>TL;DR:</strong> TA-LSDiff通过结合拓扑感知扩散概率模型和水平集能量，提出了一种新颖的胰腺分割方法，无需显式几何演化，显著提高了分割精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 胰腺分割由于体积小、对比度低和拓扑变化大而极具挑战性。传统水平集方法忽略拓扑效应，而深度学习方法则牺牲结构细节。因此需要一种能结合二者优势的方法。</p>
<p><strong>Result:</strong> 在四个公开胰腺数据集上的实验表明，TA-LSDiff达到了最先进的精度，显著优于现有方法。</p>
<p><strong>Insight:</strong> 结合深度学习和几何驱动的能量函数可以有效解决医学图像分割中的结构细节丢失问题，同时保留拓扑敏感性。</p>
<p><strong>Abstract:</strong> Pancreas segmentation in medical image processing is a persistent challenge due to its small size, low contrast against adjacent tissues, and significant topological variations. Traditional level set methods drive boundary evolution using gradient flows, often ignoring pointwise topological effects. Conversely, deep learning-based segmentation networks extract rich semantic features but frequently sacrifice structural details. To bridge this gap, we propose a novel model named TA-LSDiff, which combined topology-aware diffusion probabilistic model and level set energy, achieving segmentation without explicit geometric evolution. This energy function guides implicit curve evolution by integrating the input image and deep features through four complementary terms. To further enhance boundary precision, we introduce a pixel-adaptive refinement module that locally modulates the energy function using affinity weighting from neighboring evidence. Ablation studies systematically quantify the contribution of each proposed component. Evaluations on four public pancreas datasets demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming existing methods. These results establish TA-LSDiff as a practical and accurate solution for pancreas segmentation.</p>
  </div>
</details>

<hr>
<h3 id="67-OMEGA-Optimized-Multimodal-Position-Encoding-Index-Derivation-with-Global-Adaptive-Scaling-for-Vision-Language-Models-cs-CVPDF"><a href="#67-OMEGA-Optimized-Multimodal-Position-Encoding-Index-Derivation-with-Global-Adaptive-Scaling-for-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[67] OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models cs.CVPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00821">OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00821" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruoxiang Huang, Xindian Ma, Rundong Kong, Zhen Yuan, Peng Zhang</span></p>
<p><strong>TL;DR:</strong> OMEGA提出了一种新型位置编码框架，通过模态特定位置编码（MSPE）和全局自适应缩放（GAESS）优化多模态任务中的位置编码，显著提升了视觉语言模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型（VLMs）的位置编码策略未考虑文本和视觉模态的结构差异，OMEGA旨在解决这一问题，提出更适配多模态建模的解决方案。</p>
<p><strong>Result:</strong> 在多模态任务中，OMEGA显著提升了VLMs的性能，在视觉密集型任务上最高提升3.43%。</p>
<p><strong>Insight:</strong> 模态特定的位置编码和动态调整机制有助于更好地建模多模态数据的结构特性，是提升VLM性能的有效途径。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) have demonstrated strong performance across various multimodal tasks, where position encoding plays a vital role in modeling both the sequential structure of textual information and the spatial structure of visual information. However, current VLMs commonly adopt modality-unified 1D or 2D positional indexing strategies, which treat textual and visual tokens uniformly without accounting for their distinct structural properties and sequential continuity for text and spatial coherence for vision. To address this limitation, we propose OMEGA, a novel position encoding framework that employs Modality-Specific Position Encoding (MSPE) to assign positional indices while preserving the inherent structures of each modality across separate coordinate dimensions. Additionally, to align the information density of multimodal data in the positional index space, OMEGA introduces Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the position encoding step size of visual tokens based on the embedding entropy of both modalities. Experimental results demonstrate that OMEGA consistently enhances VLM performance across diverse architectures and VQA benchmarks. On visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.</p>
  </div>
</details>

<hr>
<h3 id="68-Enhancing-Adversarial-Transferability-in-Visual-Language-Pre-training-Models-via-Local-Shuffle-and-Sample-based-Attack-cs-CV-cs-AIPDF"><a href="#68-Enhancing-Adversarial-Transferability-in-Visual-Language-Pre-training-Models-via-Local-Shuffle-and-Sample-based-Attack-cs-CV-cs-AIPDF" class="headerlink" title="[68] Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack cs.CV | cs.AIPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00831">Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00831" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Liu, Aoyang Zhou, Aoyang Zhou</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为LSSA的攻击方法，通过局部打乱和基于采样的策略，提升视觉语言预训练模型中对抗样本的可迁移性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态对抗样本生成方法因过度依赖单一模态信息而导致过拟合，限制了对抗样本的可迁移性。</p>
<p><strong>Result:</strong> LSSA在多模型和数据集上显著提升对抗样本的可迁移性，并在大型视觉语言模型上优于其他先进攻击方法。</p>
<p><strong>Insight:</strong> 引入输入多样性和局部扰动策略是提升多模态对抗样本可迁移性的有效途径。</p>
<p><strong>Abstract:</strong> Visual-Language Pre-training (VLP) models have achieved significant performance across various downstream tasks. However, they remain vulnerable to adversarial examples. While prior efforts focus on improving the adversarial transferability of multimodal adversarial examples through cross-modal interactions, these approaches suffer from overfitting issues, due to a lack of input diversity by relying excessively on information from adversarial examples in one modality when crafting attacks in another. To address this issue, we draw inspiration from strategies in some adversarial training methods and propose a novel attack called Local Shuffle and Sample-based Attack (LSSA). LSSA randomly shuffles one of the local image blocks, thus expanding the original image-text pairs, generating adversarial images, and sampling around them. Then, it utilizes both the original and sampled images to generate the adversarial texts. Extensive experiments on multiple models and datasets demonstrate that LSSA significantly enhances the transferability of multimodal adversarial examples across diverse VLP models and downstream tasks. Moreover, LSSA outperforms other advanced attacks on Large Vision-Language Models.</p>
  </div>
</details>

<hr>
<h3 id="69-Linear-Differential-Vision-Transformer-Learning-Visual-Contrasts-via-Pairwise-Differentials-cs-CV-cs-AIPDF"><a href="#69-Linear-Differential-Vision-Transformer-Learning-Visual-Contrasts-via-Pairwise-Differentials-cs-CV-cs-AIPDF" class="headerlink" title="[69] Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials cs.CV | cs.AIPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00833">Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00833" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifan Pu, Jixuan Ying, Qixiu Li, Tianzhu Ye, Dongchen Han</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为Visual-Contrast Attention (VCA)的模块，用于替代Vision Transformers中的Multi-Head Self-Attention (MHSA)，显著降低了计算复杂度并提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Vision Transformers的多头自注意力层(MHSA)存在计算复杂度高的问题，且在处理视觉相关性时容易关注冗余信息。因此，作者希望通过引入显式判别机制来优化这一过程。</p>
<p><strong>Result:</strong> 在ImageNet-1K上，DeiT-Tiny的top-1准确率从72.2%提升到75.6%（+3.4）。在图像生成任务中，FID-50K指标显著下降。</p>
<p><strong>Insight:</strong> 1. 空间池化提供低方差全局线索；2. 双位置嵌入对比推理不可或缺；3. 两种机制的结合效果最佳。</p>
<p><strong>Abstract:</strong> Vision Transformers (ViTs) have become a universal backbone for both image recognition and image generation. Yet their Multi-Head Self-Attention (MHSA) layer still performs a quadratic query-key interaction for every token pair, spending the bulk of computation on visually weak or redundant correlations. We introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that injects an explicit notion of discrimination while reducing the theoretical complexity from O(N N C) to O(N n C) with n &lt;&lt; N. VCA first distils each head’s dense query field into a handful of spatially pooled visual-contrast tokens, then splits them into a learnable positive and negative stream whose differential interaction highlights what truly separates one region from another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone, requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and improves three strong hierarchical ViTs by up to 3.1%, while in class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm that (i) spatial pooling supplies low-variance global cues, (ii) dual positional embeddings are indispensable for contrastive reasoning, and (iii) combining the two in both stages yields the strongest synergy. VCA therefore offers a simple path towards faster and sharper Vision Transformers. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/LeapLabTHU/LinearDiff">https://github.com/LeapLabTHU/LinearDiff</a>.</p>
  </div>
</details>

<hr>
<h3 id="70-Parameter-Interpolation-Adversarial-Training-for-Robust-Image-Classification-cs-CV-cs-AIPDF"><a href="#70-Parameter-Interpolation-Adversarial-Training-for-Robust-Image-Classification-cs-CV-cs-AIPDF" class="headerlink" title="[70] Parameter Interpolation Adversarial Training for Robust Image Classification cs.CV | cs.AIPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00836">Parameter Interpolation Adversarial Training for Robust Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00836" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Liu, Yichen Yang, Kun He, John E. Hopcroft</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为参数插值对抗训练（PIAT）的新框架，通过在每个训练周期内插值前一周期的模型参数，以缓解对抗训练中的模型鲁棒性振荡和过拟合问题，并结合归一化均方误差（NMSE）进一步优化鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度神经网络在面对对抗样本时表现脆弱，现有对抗训练方法在训练过程中存在鲁棒性振荡和过拟合问题。为了解决这些问题，论文提出了PIAT框架。</p>
<p><strong>Result:</strong> 在多个基准数据集上的实验表明，PIAT显著提升了CNN和ViT模型的鲁棒性。</p>
<p><strong>Insight:</strong> 模型参数的平滑更新和logits分布的相对对齐是提升对抗训练鲁棒性的有效途径。</p>
<p><strong>Abstract:</strong> Though deep neural networks exhibit superior performance on various tasks, they are still plagued by adversarial examples. Adversarial training has been demonstrated to be the most effective method to defend against adversarial attacks. However, existing adversarial training methods show that the model robustness has apparent oscillations and overfitting issues in the training process, degrading the defense efficacy. To address these issues, we propose a novel framework called Parameter Interpolation Adversarial Training (PIAT). PIAT tunes the model parameters between each epoch by interpolating the parameters of the previous and current epochs. It makes the decision boundary of model change more moderate and alleviates the overfitting issue, helping the model converge better and achieving higher model robustness. In addition, we suggest using the Normalized Mean Square Error (NMSE) to further improve the robustness by aligning the relative magnitude of logits between clean and adversarial examples rather than the absolute magnitude. Extensive experiments conducted on several benchmark datasets demonstrate that our framework could prominently improve the robustness of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).</p>
  </div>
</details>

<hr>
<h3 id="71-OmniBrainBench-A-Comprehensive-Multimodal-Benchmark-for-Brain-Imaging-Analysis-Across-Multi-stage-Clinical-Tasks-cs-CV-cs-AIPDF"><a href="#71-OmniBrainBench-A-Comprehensive-Multimodal-Benchmark-for-Brain-Imaging-Analysis-Across-Multi-stage-Clinical-Tasks-cs-CV-cs-AIPDF" class="headerlink" title="[71] OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks cs.CV | cs.AIPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00846">OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00846" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhihao Peng, Cheng Wang, Shengyuan Liu, Zhiying Liang, Yixuan Yuan</span></p>
<p><strong>TL;DR:</strong> OmniBrainBench是一个综合性的多模态基准测试，专门用于评估多模态大语言模型（MLLMs）在脑成像分析中的表现。它涵盖15种成像模态和15项临床任务，揭示了MLLMs在复杂临床推理中的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有脑成像VQA基准测试涵盖模态少或病理描述粗糙，无法全面评估MLLMs在全临床流程中的表现，因此提出了OmniBrainBench。</p>
<p><strong>Result:</strong> 评估了24种MLLMs，结果显示：（1）专有MLLMs优于开源和医疗模型但仍逊于医生；（2）医疗MLLMs表现差异大；（3）开源MLLMs在特定任务中表现优异；（4）MLLMs在复杂术前任务中表现显著不足。</p>
<p><strong>Insight:</strong> MLLMs在视觉到临床推理的转换中存在显著差距，OmniBrainBench为未来改进此类模型提供了新的评估标准。</p>
<p><strong>Abstract:</strong> Brain imaging analysis is vital for diagnosing and treating brain disorders, and multimodal large language models (MLLMs) are increasingly assisting in that analysis. However, current brain-oriented visual question-answering (VQA) benchmarks either cover a few imaging modalities or are limited to coarse-grained pathological descriptions, hindering a comprehensive assessment of MLLMs throughout the full clinical continuum. To address these, we introduce OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically designed to assess the multimodal comprehension capabilities of MLLMs in brain imaging analysis.OmniBrainBench consists of 15 distinct brain imaging modalities collected from 30 verified medical sources, yielding 9,527 validated VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15 multi-stage clinical tasks rigorously validated by a professional radiologist. Evaluation of 24 state-of-the-art models, including open-source, medical, and proprietary MLLMs, highlights the substantial challenges posed by OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5) beat open-source and medical models but lag physicians; (2) medical MLLMs vary widely in performance; (3) open-source MLLMs trail overall but excel in specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks, revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new standard for evaluating and advancing MLLMs in brain imaging analysis, highlighting gaps compared to expert clinical reasoning. We release it at benchmark &amp; code.</p>
  </div>
</details>

<hr>
<h3 id="72-Layer-Wise-Modality-Decomposition-for-Interpretable-Multimodal-Sensor-Fusion-cs-CVPDF"><a href="#72-Layer-Wise-Modality-Decomposition-for-Interpretable-Multimodal-Sensor-Fusion-cs-CVPDF" class="headerlink" title="[72] Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion cs.CVPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00859">Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00859" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jaehyun Park, Konyul Park, Daehun Kim, Junseo Park, Jun Won Choi</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为层间模态分解（LMD）的后处理、模型无关的解读方法，用于解耦多模态感知模型中各层信息的贡献。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在自动驾驶中，多模态传感器的信息融合使决策过程缺乏透明度，难以追踪各个传感器对最终预测的具体贡献。</p>
<p><strong>Result:</strong> 实验验证了LMD在相机-雷达、相机-LiDAR等多种传感器组合下的有效性。</p>
<p><strong>Insight:</strong> 该方法为高容量多模态架构的透明化提供了实用工具，有助于提升自动驾驶系统的可信度。</p>
<p><strong>Abstract:</strong> In autonomous driving, transparency in the decision-making of perception models is critical, as even a single misperception can be catastrophic. Yet with multi-sensor inputs, it is difficult to determine how each modality contributes to a prediction because sensor information becomes entangled within the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a post-hoc, model-agnostic interpretability method that disentangles modality-specific information across all layers of a pretrained fusion model. To our knowledge, LMD is the first approach to attribute the predictions of a perception model to individual input modalities in a sensor-fusion system for autonomous driving. We evaluate LMD on pretrained fusion models under camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous driving. Its effectiveness is validated using structured perturbation-based metrics and modality-wise visual decompositions, demonstrating practical applicability to interpreting high-capacity multimodal architectures. Code is available at <a target="_blank" rel="noopener" href="https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition">https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition</a>.</p>
  </div>
</details>

<hr>
<h3 id="73-GraphGeo-Multi-Agent-Debate-Framework-for-Visual-Geo-localization-with-Heterogeneous-Graph-Neural-Networks-cs-CV-cs-GRPDF"><a href="#73-GraphGeo-Multi-Agent-Debate-Framework-for-Visual-Geo-localization-with-Heterogeneous-Graph-Neural-Networks-cs-CV-cs-GRPDF" class="headerlink" title="[73] GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks cs.CV | cs.GRPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00908">GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00908" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang</span></p>
<p><strong>TL;DR:</strong> GraphGeo提出了一个基于异构图神经网络的多智能体辩论框架，用于视觉地理定位，通过区分不同类型的智能体交互关系（如支持、竞争和知识转移）和双级辩论机制提升定位准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的视觉地理定位方法受限于数据库覆盖和质量，而现有的大视觉语言模型虽能直接推理图像位置，但在处理多样化地理区域和复杂场景时表现不佳。多智能体系统虽通过协作提升性能，但缺乏有效处理冲突预测的机制。</p>
<p><strong>Result:</strong> 实验表明，GraphGeo在多个基准测试中显著优于现有方法，证明了其通过结构化辩论将认知冲突转化为地理定位准确性的有效性。</p>
<p><strong>Insight:</strong> GraphGeo的关键创新在于将冲突预测建模为辩论关系，并通过图神经网络实现动态交互优化，为多智能体协作提供了新的思路。</p>
<p><strong>Abstract:</strong> Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</p>
  </div>
</details>

<hr>
<h3 id="74-Fleming-VL-Towards-Universal-Medical-Visual-Reasoning-with-Multimodal-LLMs-cs-CVPDF"><a href="#74-Fleming-VL-Towards-Universal-Medical-Visual-Reasoning-with-Multimodal-LLMs-cs-CVPDF" class="headerlink" title="[74] Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs cs.CVPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00916">Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00916" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yan Shu, Chi Liu, Robin Chen, Derek Li, Bryan Dai</span></p>
<p><strong>TL;DR:</strong> Fleming-VL是一个统一的多模态大型语言模型框架，旨在解决医学视觉理解的异构模态挑战，通过数据为中心的策略和优化方法，在多个医学视觉任务上实现最先进性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学数据的异构性（如2D&#x2F;3D图像、视频序列等）和领域差异限制了通用医学MLLMs的发展，研究者希望通过统一框架提升医学视觉理解的全面性。</p>
<p><strong>Result:</strong> Fleming-VL在医学VQA、视频QA和3D医学图像理解等多个基准测试中达到了最先进水平。</p>
<p><strong>Insight:</strong> 数据为中心的策略和统一的框架设计是解决医学多模态异构性的有效途径，同时开源模型推动了医学AI的透明发展。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have demonstrated remarkable effectiveness in various general-domain scenarios, such as visual question answering and image captioning. Recently, researchers have increasingly focused on empowering MLLMs with medical conversational abilities, which hold significant promise for clinical applications. However, medical data presents unique challenges due to its heterogeneous nature – encompassing diverse modalities including 2D images, 3D volumetric scans, and temporal video sequences. The substantial domain gap and data format inconsistencies across these modalities have hindered the development of unified medical MLLMs. To address these challenges, we propose Fleming-VL, a unified end-to-end framework for comprehensive medical visual understanding across heterogeneous modalities. Fleming-VL tackles this problem from a data-centric perspective through three key strategies: (1) scaling up pretraining by integrating long-context data from both natural and medical-specific domains; (2) complementing fine-tuning with rare medical data, including holistic video analysis and underrepresented 2D modalities such as ultrasound and dermoscopy images; (3) extending existing evaluation frameworks to incorporate 3D volumetric and video understanding benchmarks. Through supervised fine-tuning (SFT) and group relative policy optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive experiments demonstrate that Fleming-VL achieves state-of-the-art performance across multiple benchmarks, including medical VQA, video QA, and 3D medical image understanding. We publicly release Fleming-VL to promote transparent, reproducible, and auditable progress in medical AI.</p>
  </div>
</details>

<hr>
<h3 id="75-Dynamic-Multi-level-Weighted-Alignment-Network-for-Zero-shot-Sketch-based-Image-Retrieval-cs-CVPDF"><a href="#75-Dynamic-Multi-level-Weighted-Alignment-Network-for-Zero-shot-Sketch-based-Image-Retrieval-cs-CVPDF" class="headerlink" title="[75] Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval cs.CVPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00925">Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00925" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hanwen Su, Ge Song, Jiyan Wang, Yuanbo Zhu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种动态多级加权对齐网络（Dynamic Multi-level Weighted Alignment Network），用于解决零样本草图-图像检索（ZS-SBIR）中的模态样本不平衡和低质量信息不一致问题，通过三个模块实现性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 零样本草图-图像检索（ZS-SBIR）在电子商务等领域有广泛应用，但现有方法因模态样本不平衡和低质量信息不一致导致性能不佳。本文旨在解决这些问题。</p>
<p><strong>Result:</strong> 在Sketchy、TU-Berlin和QuickDraw三个基准数据集上，该方法优于当前最先进的ZS-SBIR方法。</p>
<p><strong>Insight:</strong> 模态对齐的质量和多级加权策略对零样本草图-图像检索的性能提升至关重要。</p>
<p><strong>Abstract:</strong> The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved increasing attention due to its wide applications, e.g. e-commerce. Despite progress made in this field, previous works suffer from using imbalanced samples of modalities and inconsistent low-quality information during training, resulting in sub-optimal performance. Therefore, in this paper, we introduce an approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It consists of three components: (i) a Uni-modal Feature Extraction Module that includes a CLIP text encoder and a ViT for extracting textual and visual tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an alignment weight list by the local and global aggregation blocks to measure the aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss Module aiming to improve the balance of domains in the triplet loss. Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, show our method delivers superior performances over the state-of-the-art ZS-SBIR methods.</p>
  </div>
</details>

<hr>
<h3 id="76-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference-cs-CVPDF"><a href="#76-EVTAR-End-to-End-Try-on-with-Additional-Unpaired-Visual-Reference-cs-CVPDF" class="headerlink" title="[76] EVTAR: End-to-End Try on with Additional Unpaired Visual Reference cs.CVPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00956">EVTAR: End-to-End Try on with Additional Unpaired Visual Reference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00956" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liuzhuozheng Li, Yue Gong, Shanyuan Liu, Bo Cheng, Yuhang Ma</span></p>
<p><strong>TL;DR:</strong> EVTAR是一种端到端的虚拟试穿模型，通过引入额外的参考图像提高试穿准确性，无需复杂输入（如分割图或密集姿态），简化了推理过程。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有虚拟试穿方法依赖复杂输入（如分割图、姿态估计），导致实际应用困难。EVTAR旨在简化输入并利用参考图像提升试穿效果。</p>
<p><strong>Result:</strong> 在两个广泛使用的基准测试中验证了EVTAR的有效性，生成高质量试穿结果。</p>
<p><strong>Insight:</strong> 引入参考图像模拟人类试穿行为，提升细节保留能力；简化输入推动了虚拟试穿的实用化。</p>
<p><strong>Abstract:</strong> We propose EVTAR, an End-to-End Virtual Try-on model with Additional Reference, that directly fits the target garment onto the person image while incorporating reference images to enhance try-on accuracy. Most existing virtual try-on approaches rely on complex inputs such as agnostic person images, human pose, densepose, or body keypoints, making them labor-intensive and impractical for real-world applications. In contrast, EVTAR adopts a two-stage training strategy, enabling simple inference with only the source image and the target garment inputs. Our model generates try-on results without masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional reference images of different individuals wearing the same clothes to preserve garment texture and fine-grained details better. This mechanism is analogous to how humans consider reference models when choosing outfits, thereby simulating a more realistic and high-quality dressing effect. We enrich the training data with supplementary references and unpaired person images to support these capabilities. We evaluate EVTAR on two widely used benchmarks and diverse tasks, and the results consistently validate the effectiveness of our approach.</p>
  </div>
</details>

<hr>
<h3 id="77-A-Unified-Reasoning-Framework-for-Holistic-Zero-Shot-Video-Anomaly-Analysis-cs-CVPDF"><a href="#77-A-Unified-Reasoning-Framework-for-Holistic-Zero-Shot-Video-Anomaly-Analysis-cs-CVPDF" class="headerlink" title="[77] A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis cs.CVPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00962">A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00962" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongheng Lin, Mengxue Qu, Kunyang Han, Jianbo Jiao, Xiaojie Jin</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个统一的推理框架，用于全视角零样本视频异常分析，通过任务链式推理无缝连接时间检测、空间定位和文本解释，无需额外训练即可实现高性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频异常研究多局限于逐帧检测，缺乏空间和语义上下文，且依赖数据和任务特定设计。本文旨在填补这一空白，实现无需训练的全面零样本异常分析。</p>
<p><strong>Result:</strong> 在多个视频异常检测、定位和解释基准上实现了零样本state-of-the-art性能，展示了方法的通用性和解释性。</p>
<p><strong>Insight:</strong> 通过精心设计的提示(task-wise chaining)可以解锁基础模型的推理能力，为实际应用中无需训练的视频异常分析提供了新思路。</p>
<p><strong>Abstract:</strong> Most video-anomaly research stops at frame-wise detection, offering little insight into why an event is abnormal, typically outputting only frame-wise anomaly scores without spatial or semantic context. Recent video anomaly localization and video anomaly understanding methods improve explainability but remain data-dependent and task-specific. We propose a unified reasoning framework that bridges the gap between temporal detection, spatial localization, and textual explanation. Our approach is built upon a chained test-time reasoning process that sequentially connects these tasks, enabling holistic zero-shot anomaly analysis without any additional training. Specifically, our approach leverages intra-task reasoning to refine temporal detections and inter-task chaining for spatial and semantic understanding, yielding improved interpretability and generalization in a fully zero-shot manner. Without any additional data or gradients, our method achieves state-of-the-art zero-shot performance across multiple video anomaly detection, localization, and explanation benchmarks. The results demonstrate that careful prompt design with task-wise chaining can unlock the reasoning power of foundation models, enabling practical, interpretable video anomaly analysis in a fully zero-shot manner. Project Page: <a target="_blank" rel="noopener" href="https://rathgrith.github.io/Unified_Frame_VAA/">https://rathgrith.github.io/Unified_Frame_VAA/</a>.</p>
  </div>
</details>

<hr>
<h3 id="78-VesSAM-Efficient-Multi-Prompting-for-Segmenting-Complex-Vessel-cs-CVPDF"><a href="#78-VesSAM-Efficient-Multi-Prompting-for-Segmenting-Complex-Vessel-cs-CVPDF" class="headerlink" title="[78] VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel cs.CVPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00981">VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00981" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Suzhong Fu, Rui Sun, Xuan Ding, Jingqi Dong, Yiming Yang</span></p>
<p><strong>TL;DR:</strong> VesSAM是一个高效的多提示框架，专为复杂的血管分割任务设计，通过局部纹理增强、多提示编码和轻量化解码器，显著提升了分割性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 血管分割在临床应用中至关重要，但由于其细分支结构和低纹理对比度，传统方法和基础模型（如SAM）表现不佳。因此，需要一种针对血管结构的专用分割框架。</p>
<p><strong>Result:</strong> VesSAM在8个数据集上表现优于当前最佳的PEFT-based SAM变体（Dice提高10%，IoU提高13%），同时在OoD设置下表现突出。</p>
<p><strong>Insight:</strong> 针对特定领域（如血管分割）优化基础模型（如SAM）时，结合领域特定特征（如多提示）和轻量化设计能显著提升性能，同时保持高效性。</p>
<p><strong>Abstract:</strong> Accurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.</p>
  </div>
</details>

<hr>
<h3 id="79-MID-A-Self-supervised-Multimodal-Iterative-Denoising-Framework-cs-CVPDF"><a href="#79-MID-A-Self-supervised-Multimodal-Iterative-Denoising-Framework-cs-CVPDF" class="headerlink" title="[79] MID: A Self-supervised Multimodal Iterative Denoising Framework cs.CVPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00997">MID: A Self-supervised Multimodal Iterative Denoising Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00997" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chang Nie, Tianchen Deng, Zhe Liu, Hesheng Wang</span></p>
<p><strong>TL;DR:</strong> MID提出了一种自监督多模态迭代去噪框架，通过建模非线性噪声积累过程，迭代学习噪声特征并去噪，无需干净-噪声配对数据，在多个任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实世界中数据常被复杂非线性噪声污染，传统规则化去噪方法效果不佳，亟需一种无需配对数据的自适应去噪方法。</p>
<p><strong>Result:</strong> 在四个经典计算机视觉任务中表现鲁棒且领先，同时在生物医学和生物信息学任务中也展现出强适应性和性能。</p>
<p><strong>Insight:</strong> MID的创新在于自监督学习和迭代去噪结合，为处理复杂非线性噪声提供了新思路，尤其在缺乏配对数据时仍能表现优异。</p>
<p><strong>Abstract:</strong> Data denoising is a persistent challenge across scientific and engineering domains. Real-world data is frequently corrupted by complex, non-linear noise, rendering traditional rule-based denoising methods inadequate. To overcome these obstacles, we propose a novel self-supervised multimodal iterative denoising (MID) framework. MID models the collected noisy data as a state within a continuous process of non-linear noise accumulation. By iteratively introducing further noise, MID learns two neural networks: one to estimate the current noise step and another to predict and subtract the corresponding noise increment. For complex non-linear contamination, MID employs a first-order Taylor expansion to locally linearize the noise process, enabling effective iterative removal. Crucially, MID does not require paired clean-noisy datasets, as it learns noise characteristics directly from the noisy inputs. Experiments across four classic computer vision tasks demonstrate MID’s robustness, adaptability, and consistent state-of-the-art performance. Moreover, MID exhibits strong performance and adaptability in tasks within the biomedical and bioinformatics domains.</p>
  </div>
</details>

<hr>
<h3 id="80-Integrating-Visual-and-X-Ray-Machine-Learning-Features-in-the-Study-of-Paintings-by-Goya-cs-CV-cs-LGPDF"><a href="#80-Integrating-Visual-and-X-Ray-Machine-Learning-Features-in-the-Study-of-Paintings-by-Goya-cs-CV-cs-LGPDF" class="headerlink" title="[80] Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya cs.CV | cs.LGPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01000">Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01000" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hassan Ugail, Ismail Lujain Jaleel</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个多模态机器学习框架，结合视觉和X射线图像特征用于戈雅画作的鉴定，通过统一特征提取和优化分类器，性能显著优于单模态方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 戈雅画作的鉴定因风格演变复杂和历史伪造模式多样而具有挑战性，因此需要一种结合视觉和X射线图像的多模态方法来提高准确性。</p>
<p><strong>Result:</strong> 在24幅已验证戈雅画作的数据集上，分类准确率达97.8%，假阳性率为0.022，显著优于单模态方法。案例研究也显示出92.3%的高置信度。</p>
<p><strong>Insight:</strong> 多模态方法在艺术鉴定中具有显著优势，统一的特征提取技术可以更全面地捕捉画作的特征，提高鉴定准确性。</p>
<p><strong>Abstract:</strong> Art authentication of Francisco Goya’s works presents complex computational challenges due to his heterogeneous stylistic evolution and extensive historical patterns of forgery. We introduce a novel multimodal machine learning framework that applies identical feature extraction techniques to both visual and X-ray radiographic images of Goya paintings. The unified feature extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors, Local Binary Patterns, entropy measures, energy calculations, and colour distribution analysis applied consistently across both imaging modalities. The extracted features from both visual and X-ray images are processed through an optimised One-Class Support Vector Machine with hyperparameter tuning. Using a dataset of 24 authenticated Goya paintings with corresponding X-ray images, split into an 80&#x2F;20 train-test configuration with 10-fold cross-validation, the framework achieves 97.8% classification accuracy with a 0.022 false positive rate. Case study analysis of &#96;&#96;Un Gigante’’ demonstrates the practical efficacy of our pipeline, achieving 92.3% authentication confidence through unified multimodal feature analysis. Our results indicate substantial performance improvement over single-modal approaches, establishing the effectiveness of applying identical computational methods to both visual and radiographic imagery in art authentication applications.</p>
  </div>
</details>

<hr>
<h3 id="81-HyFormer-Net-A-Synergistic-CNN-Transformer-with-Interpretable-Multi-Scale-Fusion-for-Breast-Lesion-Segmentation-and-Classification-in-Ultrasound-Images-cs-CVPDF"><a href="#81-HyFormer-Net-A-Synergistic-CNN-Transformer-with-Interpretable-Multi-Scale-Fusion-for-Breast-Lesion-Segmentation-and-Classification-in-Ultrasound-Images-cs-CVPDF" class="headerlink" title="[81] HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images cs.CVPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01013">HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01013" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammad Amanour Rahman</span></p>
<p><strong>TL;DR:</strong> HyFormer-Net是一种结合CNN和Transformer的混合架构，用于乳腺癌超声图像的分割和分类，通过多尺度融合和注意力机制提升性能，并在跨数据集泛化实验中验证了其潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决乳腺癌超声图像中存在的噪声、操作依赖性和边界模糊问题，克服现有深度学习方法在单任务学习、架构限制和可解释性方面的不足。</p>
<p><strong>Result:</strong> 在BUSI数据集上，Dice分数0.761，准确率93.2%；通过集成学习，Dice达90.2%，准确率99.5%；跨数据集泛化实验中，仅需10%目标数据即可恢复92.5%性能。</p>
<p><strong>Insight:</strong> 1. 多尺度融合和注意力机制显著提升性能；2. 混合架构在乳腺癌超声任务中优于单一架构；3. 少量目标数据即可实现模型泛化，具有临床潜力。</p>
<p><strong>Abstract:</strong> B-mode ultrasound for breast cancer diagnosis faces challenges: speckle, operator dependency, and indistinct boundaries. Existing deep learning suffers from single-task learning, architectural constraints (CNNs lack global context, Transformers local features), and black-box decision-making. These gaps hinder clinical adoption.   We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous segmentation and classification with intrinsic interpretability. Its dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via multi-scale hierarchical fusion blocks. An attention-gated decoder provides precision and explainability. We introduce dual-pipeline interpretability: (1) intrinsic attention validation with quantitative IoU verification (mean: 0.86), and (2) Grad-CAM for classification reasoning.   On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +&#x2F;- 0.072 and accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant Recall of 92.1 +&#x2F;- 2.2% ensures minimal false negatives. Ensemble modeling yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant Recall, eliminating false negatives. Ablation studies confirm multi-scale fusion contributes +16.8% Dice and attention gates add +5.9%.   Crucially, we conduct the first cross-dataset generalization study for hybrid CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058), confirming domain shift. However, progressive fine-tuning with only 10% target-domain data (68 images) recovers 92.5% performance. With 50% data, our model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and demonstrating true generalization.</p>
  </div>
</details>

<hr>
<h3 id="82-GeoToken-Hierarchical-Geolocalization-of-Images-via-Next-Token-Prediction-cs-CV-cs-AI-cs-LGPDF"><a href="#82-GeoToken-Hierarchical-Geolocalization-of-Images-via-Next-Token-Prediction-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[82] GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction cs.CV | cs.AI | cs.LGPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01082">GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01082" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Narges Ghasemi, Amir Ziashahabi, Salman Avestimehr, Cyrus Shahabi</span></p>
<p><strong>TL;DR:</strong> GeoToken提出了一种分层次的图像地理定位方法，通过逐步预测精确的地理标记来提高定位准确性，结合类似语言模型的推理技术，在多个数据集上达到了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 图像地理定位面临视觉相似性和大规模搜索空间的挑战，GeoToken受人类逐步缩小定位范围的启发，提出了分层次预测方法。</p>
<p><strong>Result:</strong> 在Im2GPS3k和YFCC4k数据集上，GeoToken在非MLLM设置下超越了其他基线模型，准确率提升最高达13.9%；结合MLLM后，进一步提升了所有指标的性能。</p>
<p><strong>Insight:</strong> 1. 分层次预测方法能够有效应对地理定位的高复杂性和视觉模糊性；2. 语言模型的推理技术可以迁移到其他领域的序列预测任务中。</p>
<p><strong>Abstract:</strong> Image geolocalization, the task of determining an image’s geographic origin, poses significant challenges, largely due to visual similarities across disparate locations and the large search space. To address these issues, we propose a hierarchical sequence prediction approach inspired by how humans narrow down locations from broad regions to specific addresses. Analogously, our model predicts geographic tokens hierarchically, first identifying a general region and then sequentially refining predictions to increasingly precise locations. Rather than relying on explicit semantic partitions, our method uses S2 cells, a nested, multiresolution global grid, and sequentially predicts finer-level cells conditioned on visual inputs and previous predictions. This procedure mirrors autoregressive text generation in large language models. Much like in language modeling, final performance depends not only on training but also on inference-time strategy. We investigate multiple top-down traversal methods for autoregressive sampling, incorporating techniques from test-time compute scaling used in language models. Specifically, we integrate beam search and multi-sample inference while exploring various selection strategies to determine the final output. This enables the model to manage uncertainty by exploring multiple plausible paths through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k datasets against two distinct sets of baselines: those that operate without a Multimodal Large Language Model (MLLM) and those that leverage one. In the MLLM-free setting, our model surpasses other comparable baselines on nearly all metrics, achieving state-of-the-art performance with accuracy gains of up to 13.9%. When augmented with an MLLM, our model outperforms all baselines, setting a new state-of-the-art across all metrics. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/NNargesNN/GeoToken">https://github.com/NNargesNN/GeoToken</a>.</p>
  </div>
</details>

<hr>
<h3 id="83-SliceVision-F2I-A-Synthetic-Feature-to-Image-Dataset-for-Visual-Pattern-Representation-on-Network-Slices-cs-CV-cs-AI-cs-LGPDF"><a href="#83-SliceVision-F2I-A-Synthetic-Feature-to-Image-Dataset-for-Visual-Pattern-Representation-on-Network-Slices-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[83] SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices cs.CV | cs.AI | cs.LGPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01087">SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01087" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md. Abid Hasan Rafi, Mst. Fatematuj Johora, Pankaj Bhowmik</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了SliceVision-F2I数据集，通过四种编码方法将多变量KPI向量转换为视觉表示，用于支持网络切片的视觉学习和分类任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 5G和6G网络中网络切片的重要性日益凸显，但缺乏支持其视觉模式表示的稳健数据集。因此，作者开发了一个合成数据集，以填补这一空白。</p>
<p><strong>Result:</strong> 生成了一个模拟真实网络条件的合成数据集，适用于分类、异常检测和图像机器学习任务的基准测试。</p>
<p><strong>Insight:</strong> 通过视觉表示KPI数据，可以在网络管理中结合图像处理技术，提升分类和异常检测的效果。</p>
<p><strong>Abstract:</strong> The emergence of 5G and 6G networks has established network slicing as a significant part of future service-oriented architectures, demanding refined identification methods supported by robust datasets. The article presents SliceVision-F2I, a dataset of synthetic samples for studying feature visualization in network slicing for next-generation networking systems. The dataset transforms multivariate Key Performance Indicator (KPI) vectors into visual representations through four distinct encoding methods: physically inspired mappings, Perlin noise, neural wallpapering, and fractal branching. For each encoding method, 30,000 samples are generated, each comprising a raw KPI vector and a corresponding RGB image at low-resolution pixels. The dataset simulates realistic and noisy network conditions to reflect operational uncertainties and measurement imperfections. SliceVision-F2I is suitable for tasks involving visual learning, network state classification, anomaly detection, and benchmarking of image-based machine learning techniques applied to network data. The dataset is publicly available and can be reused in various research contexts, including multivariate time series analysis, synthetic data generation, and feature-to-image transformations.</p>
  </div>
</details>

<hr>
<h3 id="84-Epanechnikov-nonparametric-kernel-density-estimation-based-feature-learning-in-respiratory-disease-chest-X-ray-images-cs-CVPDF"><a href="#84-Epanechnikov-nonparametric-kernel-density-estimation-based-feature-learning-in-respiratory-disease-chest-X-ray-images-cs-CVPDF" class="headerlink" title="[84] Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images cs.CVPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01098">Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01098" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Veronica Marsico, Antonio Quintero-Rincon, Hadj Batatia</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于Epanechnikov非参数核密度估计（EKDE）和双模态逻辑回归分类器的新方法，用于从胸部X射线影像中诊断呼吸系统疾病，取得了中等水平的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 肺部疾病的早期准确诊断对患者至关重要，传统方法通常依赖临床经验，而本研究旨在结合统计学方法和机器学习以提高诊断准确性。</p>
<p><strong>Result:</strong> 在13808张胸部X射线影像上的测试结果显示，准确率为70.14%，敏感性为59.26%，特异性为74.18%，表明方法有待进一步优化。</p>
<p><strong>Insight:</strong> EKDE在医学影像特征提取中具有潜力，但需要结合临床专家知识进一步提升敏感性和整体性能。</p>
<p><strong>Abstract:</strong> This study presents a novel method for diagnosing respiratory diseases using image data. It combines Epanechnikov’s non-parametric kernel density estimation (EKDE) with a bimodal logistic regression classifier in a statistical-model-based learning scheme. EKDE’s flexibility in modeling data distributions without assuming specific shapes and its adaptability to pixel intensity variations make it valuable for extracting key features from medical images. The method was tested on 13808 randomly selected chest X-rays from the COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of 59.26%, and a specificity of 74.18%, demonstrating moderate performance in detecting respiratory disease while showing room for improvement in sensitivity. While clinical expertise remains essential for further refining the model, this study highlights the potential of EKDE-based approaches to enhance diagnostic accuracy and reliability in medical imaging.</p>
  </div>
</details>

<hr>
<h3 id="85-Anatomically-Constrained-Transformers-for-Echocardiogram-Analysis-cs-CVPDF"><a href="#85-Anatomically-Constrained-Transformers-for-Echocardiogram-Analysis-cs-CVPDF" class="headerlink" title="[85] Anatomically Constrained Transformers for Echocardiogram Analysis cs.CVPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01109">Anatomically Constrained Transformers for Echocardiogram Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01109" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alexander Thorley, Agis Chartsias, Jordan Strom, Jeremy Slivnick, Dipak Kotecha</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种结合解剖学先验的Transformer框架（ViACT），通过将解剖结构作为点集编码到Transformer令牌中，专注于解剖区域的表示学习，提升了超声心动图分析的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视频Transformer在超声心动图分析中容易学习非诊断区域（如图像背景）的虚假相关性，限制了其性能。</p>
<p><strong>Result:</strong> ViACT在左心室射血分数回归和心肌淀粉样变检测等任务中表现出色，且能生成与病理区域对齐的可解释注意力图。</p>
<p><strong>Insight:</strong> 通过解剖学约束，可以提升模型的专注性和泛化能力，同时减少对任务专用组件的依赖。</p>
<p><strong>Abstract:</strong> Video transformers have recently demonstrated strong potential for echocardiogram (echo) analysis, leveraging self-supervised pre-training and flexible adaptation across diverse tasks. However, like other models operating on videos, they are prone to learning spurious correlations from non-diagnostic regions such as image backgrounds. To overcome this limitation, we propose the Video Anatomically Constrained Transformer (ViACT), a novel framework that integrates anatomical priors directly into the transformer architecture. ViACT represents a deforming anatomical structure as a point set and encodes both its spatial geometry and corresponding image patches into transformer tokens. During pre-training, ViACT follows a masked autoencoding strategy that masks and reconstructs only anatomical patches, enforcing that representation learning is focused on the anatomical region. The pre-trained model can then be fine-tuned for tasks localized to this region. In this work we focus on the myocardium, demonstrating the framework on echo analysis tasks such as left ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA) detection. The anatomical constraint focuses transformer attention within the myocardium, yielding interpretable attention maps aligned with regions of known CA pathology. Moreover, ViACT generalizes to myocardium point tracking without requiring task-specific components such as correlation volumes used in specialized tracking networks.</p>
  </div>
</details>

<hr>
<h3 id="86-Boosting-performance-of-computer-vision-applications-through-embedded-GPUs-on-the-edge-cs-CV-cs-DCPDF"><a href="#86-Boosting-performance-of-computer-vision-applications-through-embedded-GPUs-on-the-edge-cs-CV-cs-DCPDF" class="headerlink" title="[86] Boosting performance of computer vision applications through embedded GPUs on the edge cs.CV | cs.DCPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01129">Boosting performance of computer vision applications through embedded GPUs on the edge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.DC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01129" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fabio Diniz Rossi</span></p>
<p><strong>TL;DR:</strong> 论文提出在边缘计算中使用嵌入式GPU提升计算机视觉应用的性能，实验中GPU相比CPU展现了更好的表现，改善了用户体验。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 计算机视觉应用（尤其是增强现实技术）在移动设备中需求增长，但资源消耗大。边缘计算虽可用于卸载任务，但设备能力有限，可能影响用户体验。</p>
<p><strong>Result:</strong> 实验结果显示，使用GPU比仅用CPU能获得更高的性能，从而改善用户体验。</p>
<p><strong>Insight:</strong> 嵌入式GPU是提升边缘计算中计算机视觉应用性能的有效解决方案。</p>
<p><strong>Abstract:</strong> Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.</p>
  </div>
</details>

<hr>
<h3 id="87-Weakly-Supervised-Concept-Learning-with-Class-Level-Priors-for-Interpretable-Medical-Diagnosis-cs-CVPDF"><a href="#87-Weakly-Supervised-Concept-Learning-with-Class-Level-Priors-for-Interpretable-Medical-Diagnosis-cs-CVPDF" class="headerlink" title="[87] Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis cs.CVPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01131">Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01131" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Nahiduzzaman, Steven Korevaar, Alireza Bab-Hadiashar, Ruwan Tennakoon</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种弱监督框架PCP，利用类级先验知识进行可解释的医学诊断，无需显式监督或依赖语言模型，显著提升了概念预测性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学影像中的人工智能预测需具备可解释性，但现有方法需要昂贵的概念标注或依赖于语言模型，难以捕获医学领域特征。</p>
<p><strong>Result:</strong> 实验显示PCP在PH2和WBCatt数据集上概念F1-score提升33%，分类性能在四个医学数据集上与完全监督方法竞争。</p>
<p><strong>Insight:</strong> 类级先验知识和弱监督机制为医学领域的可解释性研究提供了高效且实用的解决方案。</p>
<p><strong>Abstract:</strong> Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.</p>
  </div>
</details>

<hr>
<h3 id="88-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation-cs-CVPDF"><a href="#88-ROVER-Benchmarking-Reciprocal-Cross-Modal-Reasoning-for-Omnimodal-Generation-cs-CVPDF" class="headerlink" title="[88] ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation cs.CVPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01163">ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01163" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongyuan Liang, Wei Chow, Feng Li, Ziqiao Ma, Xiyao Wang</span></p>
<p><strong>TL;DR:</strong> ROVER是一个新的基准测试，旨在评估统一多模态模型（UMMs）在跨模态推理中的能力，特别是在文本与图像之间的相互引导和验证。实验结果表明，跨模态推理对视觉生成质量至关重要，但模型在符号任务中存在推理能力不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的多模态模型评估通常独立考察文本和图像能力，缺乏对跨模态相互推理能力的测试，这限制了模型在实际应用中实现统一的智能表现。</p>
<p><strong>Result:</strong> 研究发现：(1) 跨模态推理显著影响视觉生成质量；(2) 模型在感知任务中表现良好，但在符号推理任务中存在明显不足。</p>
<p><strong>Insight:</strong> 跨模态相互推理是实现真正多模态生成的关键能力，未来的研究需要进一步提升模型在符号推理任务中的表现。</p>
<p><strong>Abstract:</strong> Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual abstractions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.</p>
  </div>
</details>

<hr>
<h3 id="89-Web-Scale-Collection-of-Video-Data-for-4D-Animal-Reconstruction-cs-CV-I-2-10-I-4-5PDF"><a href="#89-Web-Scale-Collection-of-Video-Data-for-4D-Animal-Reconstruction-cs-CV-I-2-10-I-4-5PDF" class="headerlink" title="[89] Web-Scale Collection of Video Data for 4D Animal Reconstruction cs.CV | I.2.10; I.4.5PDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01169">Web-Scale Collection of Video Data for 4D Animal Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | I.2.10; I.4.5</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01169" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Brian Nlong Zhao, Jiajun Wu, Shangzhe Wu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种自动化流程，从YouTube视频中提取动物相关片段，并构建大规模数据集Animal-in-Motion（AiM），用于4D动物重建任务，同时提供了一个基准和基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 野生动物研究依赖于大规模数据，但现有数据集规模小且难以满足动物为中心的3D&#x2F;4D任务需求。论文旨在解决数据不足和评估方法不完善的问题。</p>
<p><strong>Result:</strong> 自动化流程采集的数据集规模是现有工作的10倍以上；4D重建实验显示，无模型方法在3D形状上更自然，但2D指标较低。</p>
<p><strong>Insight:</strong> 现有评估方法（2D指标）可能无法真实反映3D&#x2F;4D重建的质量，需要在评估指标上进一步改进。</p>
<p><strong>Abstract:</strong> Computer vision for animals holds great promise for wildlife research but often depends on large-scale data, while existing collection methods rely on controlled capture setups. Recent data-driven approaches show the potential of single-view, non-invasive analysis, yet current animal video datasets are limited–offering as few as 2.4K 15-frame clips and lacking key processing for animal-centric 3D&#x2F;4D tasks. We introduce an automated pipeline that mines YouTube videos and processes them into object-centric clips, along with auxiliary annotations valuable for downstream tasks like pose estimation, tracking, and 3D&#x2F;4D reconstruction. Using this pipeline, we amass 30K videos (2M frames)–an order of magnitude more than prior works. To demonstrate its utility, we focus on the 4D quadruped animal reconstruction task. To support this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually filtered sequences with 11K frames showcasing clean, diverse animal motions. We evaluate state-of-the-art model-based and model-free methods on Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic 3D shapes, while the latter yields more natural reconstructions but scores lower–revealing a gap in current evaluation. To address this, we enhance a recent model-free approach with sequence-level optimization, establishing the first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and baseline aim to advance large-scale, markerless 4D animal reconstruction and related tasks from in-the-wild videos. Code and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/briannlongzhao/Animal-in-Motion">https://github.com/briannlongzhao/Animal-in-Motion</a>.</p>
  </div>
</details>

<hr>
<h3 id="90-OmniVLA-Unifiying-Multi-Sensor-Perception-for-Physically-Grounded-Multimodal-VLA-cs-CVPDF"><a href="#90-OmniVLA-Unifiying-Multi-Sensor-Perception-for-Physically-Grounded-Multimodal-VLA-cs-CVPDF" class="headerlink" title="[90] OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA cs.CVPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01210">OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01210" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Heyu Guo, Shanmu Wang, Ruichun Ma, Shiqi Jiang, Yasaman Ghasempour</span></p>
<p><strong>TL;DR:</strong> OmniVLA通过整合多种新型传感器模态（如红外相机、毫米波雷达和麦克风阵列），提出了一种统一的多模态视觉-语言-动作（VLA）模型，显著提升了物理基础空间智能的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有VLA模型主要依赖RGB相机，限制了其感知和操控能力，因此需要整合其他传感器模态以提升性能。</p>
<p><strong>Result:</strong> OmniVLA在真实任务中平均成功率84%，比RGB-only和原始传感器模型分别提升59%和28%。</p>
<p><strong>Insight:</strong> 传感器掩码图像是一种高效的多模态统一表示方法，能够提升模型的学习效率和泛化能力。</p>
<p><strong>Abstract:</strong> Vision-language-action (VLA) models have shown strong generalization for action prediction through large-scale vision-language pretraining. However, most existing models rely solely on RGB cameras, limiting their perception and, consequently, manipulation capabilities. We present OmniVLA, an omni-modality VLA model that integrates novel sensing modalities for physically-grounded spatial intelligence beyond RGB perception. The core of our approach is the sensor-masked image, a unified representation that overlays spatially grounded and physically meaningful masks onto the RGB images, derived from sensors including an infrared camera, a mmWave radar, and a microphone array. This image-native unification keeps sensor input close to RGB statistics to facilitate training, provides a uniform interface across sensor hardware, and enables data-efficient learning with lightweight per-sensor projectors. Built on this, we present a multisensory vision-language-action model architecture and train the model based on an RGB-pretrained VLA backbone. We evaluate OmniVLA on challenging real-world tasks where sensor-modality perception is needed to guide the manipulation. OmniVLA achieves an average task success rate of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline models by 59% and 28% respectively, meanwhile showing higher learning efficiency and stronger generalization capability.</p>
  </div>
</details>

<hr>
<h3 id="91-Thought-For-Food-Reasoning-Chain-Induced-Food-Visual-Question-Answering-cs-CV-cs-AIPDF"><a href="#91-Thought-For-Food-Reasoning-Chain-Induced-Food-Visual-Question-Answering-cs-CV-cs-AIPDF" class="headerlink" title="[91] Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering cs.CV | cs.AIPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01213">Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01213" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Riddhi Jain, Manasi Patwardhan, Parijat Deshpande, Venkataramana Runkana</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种针对印度食物的视觉问答（VQA）系统，通过多步推理链提高答案准确性，而不是传统的两步法（先生成答案再解释）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的VQA系统偏向西方食物，而印度食物的文化和烹饪多样性需要更复杂的推理过程。</p>
<p><strong>Result:</strong> 推理链的加入使印度食物VQA任务的基线准确率平均提高了10个百分点。</p>
<p><strong>Insight:</strong> 在复杂的食物文化背景下，多步推理链比传统的两步法更能有效提升VQA系统的准确性。</p>
<p><strong>Abstract:</strong> The immense diversity in the culture and culinary of Indian cuisines calls attention to the major shortcoming of the existing Visual Question Answering(VQA) systems which are inclined towards the foods from Western region. Recent attempt towards building a VQA dataset for Indian food is a step towards addressing this challenge. However, their approach towards VQA follows a two-step process in which the answer is generated first, followed by the explanation of the expected answer. In this work, we claim that food VQA requires to follow a multi-step reasoning process to arrive at an accurate answer, especially in the context of India food, which involves understanding complex culinary context and identifying relationships between various food items. With this hypothesis we create reasoning chains upon the QA with minimal human intervention. We fine-tune smaller LLMs and VLMs with auto-validated reasoning chains and further train them using reinforcement learning with larger data. With augmentation of reasoning chains, we observed accuracy improvement of an average 10 percentage points on the baseline. We provide detailed analysis in terms the effect of addition of reasoning chains for the Indian Food VQA task.   Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge Graph.</p>
  </div>
</details>

<hr>
<h3 id="92-Gesture-Generation-Still-Needs-Improved-Human-Evaluation-Practices-Insights-from-a-Community-Driven-State-of-the-Art-Benchmark-cs-CV-cs-GR-cs-HC-I-3-I-2PDF"><a href="#92-Gesture-Generation-Still-Needs-Improved-Human-Evaluation-Practices-Insights-from-a-Community-Driven-State-of-the-Art-Benchmark-cs-CV-cs-GR-cs-HC-I-3-I-2PDF" class="headerlink" title="[92] Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark cs.CV | cs.GR | cs.HC | I.3; I.2PDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01233">Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR | cs.HC | I.3; I.2</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01233" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rajmund Nagy, Hendric Voss, Thanh Hoang-Minh, Mihail Tsakov, Teodor Nikolov</span></p>
<p><strong>TL;DR:</strong> 该论文指出了自动语音驱动的3D手势生成领域在人类评估实践中的标准化不足和实验设计缺陷，并提出了一种详细的评估协议。通过大规模众包评估，论文比较了六种手势生成模型的性能，并强调了对运动质量和多模态对齐进行解耦评估的重要性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前自动手势生成领域缺乏标准化的评估实践，导致无法准确比较不同方法的性能。论文旨在填补这一空白，提供一个标准化的评估协议，以推动领域的进步。</p>
<p><strong>Result:</strong> 1. 新模型未必优于早期方法；<br>2. 已有发表的高性能声明在严格评估下未必成立；<br>3. 强调了对运动质量和多模态对齐进行解耦评估的必要性。</p>
<p><strong>Insight:</strong> 1. 标准化评估是推动手势生成领域进步的关键；<br>2. 解耦评估运动质量和多模态对齐能更准确地反映模型性能；<br>3. 开放数据和研究工具有助于社区的透明性和可重复性。</p>
<p><strong>Abstract:</strong> We review human evaluation practices in automated, speech-driven 3D gesture generation and find a lack of standardisation and frequent use of flawed experimental setups. This leads to a situation where it is impossible to know how different methods compare, or what the state of the art is. In order to address common shortcomings of evaluation design, and to standardise future user studies in gesture-generation works, we introduce a detailed human evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using this protocol, we conduct large-scale crowdsourced evaluation to rank six recent gesture-generation models – each trained by its original authors – across two key evaluation dimensions: motion realism and speech-gesture alignment. Our results provide strong evidence that 1) newer models do not consistently outperform earlier approaches; 2) published claims of high motion realism or speech-gesture alignment may not hold up under rigorous evaluation; and 3) the field must adopt disentangled assessments of motion quality and multimodal alignment for accurate benchmarking in order to make progress. Finally, in order to drive standardisation and enable new evaluation research, we will release five hours of synthetic motion from the benchmarked models; over 750 rendered video stimuli from the user studies – enabling new evaluations without model reimplementation required – alongside our open-source rendering script, and the 16,000 pairwise human preference votes collected for our benchmark.</p>
  </div>
</details>

<hr>
<h3 id="93-left-circlearrowright-boxed-text-BUS-right-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles-cs-CV-cs-CLPDF"><a href="#93-left-circlearrowright-boxed-text-BUS-right-A-Large-and-Diverse-Multimodal-Benchmark-for-evaluating-the-ability-of-Vision-Language-Models-to-understand-Rebus-Puzzles-cs-CV-cs-CLPDF" class="headerlink" title="[93] $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles cs.CV | cs.CLPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01340">$\left|,\circlearrowright,\boxed{\text{BUS}},\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01340" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Trishanu Das, Abhilash Nandy, Khush Bajaj, Deepiha S</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为 $ДBUS$ 的多样化、多模态基准测试集，包含1333个英语谜题，用于评估视觉-语言模型在理解Rebus谜题（基于图片、符号和字母的创意表达）中的能力，并提出了一个模型无关的框架 $RebusDescProgICE$，显著提升了模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Rebus谜题需要多种技能（如图像识别、常识推理、多步推理等），对现有视觉-语言模型具有挑战性。构建一个多样化且大规模的基准测试集，并开发一个能提升模型性能的框架，是论文的主要动机。</p>
<p><strong>Result:</strong> 在 $ДBUS$ 上，相比Chain-of-Thought推理，该框架使闭源和开源模型的性能分别提升了2.1-4.1%和20-30%。</p>
<p><strong>Insight:</strong> Rebus谜题的复杂性揭示了现有视觉-语言模型在跨模态推理和创造性思维方面的不足，而结构化与非结构化推理的结合是提升性能的有效途径。</p>
<p><strong>Abstract:</strong> Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters to represent words or phrases creatively) requires a variety of skills such as image recognition, cognitive skills, commonsense reasoning, multi-step reasoning, image-based wordplay, etc., making this a challenging task for even current Vision-Language Models. In this paper, we present $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$, a large and diverse benchmark of $1,333$ English Rebus Puzzles containing different artistic styles and levels of difficulty, spread across 18 categories such as food, idioms, sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a model-agnostic framework which uses a combination of an unstructured description and code-based, structured reasoning, along with better, reasoning-based in-context example selection, improving the performance of Vision-Language Models on $\left|,\circlearrowright,\boxed{\text{BUS}},\right|$ by $2.1-4.1%$ and $20-30%$ using closed-source and open-source models respectively compared to Chain-of-Thought Reasoning.</p>
  </div>
</details>

<hr>
<h3 id="94-Eyes-on-Target-Gaze-Aware-Object-Detection-in-Egocentric-Video-cs-CV-cs-AIPDF"><a href="#94-Eyes-on-Target-Gaze-Aware-Object-Detection-in-Egocentric-Video-cs-CV-cs-AIPDF" class="headerlink" title="[94] Eyes on Target: Gaze-Aware Object Detection in Egocentric Video cs.CV | cs.AIPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01237">Eyes on Target: Gaze-Aware Object Detection in Egocentric Video</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01237" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vishakha Lall, Yisi Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于注视引导的深度感知对象检测框架，通过将注视特征注入Vision Transformer的注意力机制，优先处理人类关注的区域，从而在自中心视频中提升检测性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类注视行为为理解复杂视觉环境中的注意力提供了丰富的信息。然而，传统对象检测方法对所有区域一视同仁，未能充分利用这些信息。本文旨在通过注视引导的注意力机制，改进自中心视频中的对象检测。</p>
<p><strong>Result:</strong> 在定制模拟器数据集和公开基准（如Ego4D Ego-Motion和Ego-CH-Gaze数据集）上，该方法均表现出优于传统方法的检测精度。</p>
<p><strong>Insight:</strong> 注视信息可以有效地指导对象的空间特征选择，从而提升检测性能。这种方法的成功为基于注视的自中心视觉任务提供了新的研究方向。</p>
<p><strong>Abstract:</strong> Human gaze offers rich supervisory signals for understanding visual attention in complex visual environments. In this paper, we propose Eyes on Target, a novel depth-aware and gaze-guided object detection framework designed for egocentric videos. Our approach injects gaze-derived features into the attention mechanism of a Vision Transformer (ViT), effectively biasing spatial feature selection toward human-attended regions. Unlike traditional object detectors that treat all regions equally, our method emphasises viewer-prioritised areas to enhance object detection. We validate our method on an egocentric simulator dataset where human visual attention is critical for task assessment, illustrating its potential in evaluating human performance in simulation scenarios. We evaluate the effectiveness of our gaze-integrated model through extensive experiments and ablation studies, demonstrating consistent gains in detection accuracy over gaze-agnostic baselines on both the custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a gaze-aware attention head importance metric, revealing how gaze cues modulate transformer attention dynamics.</p>
  </div>
</details>

<hr>
<h3 id="95-Beyond-Deceptive-Flatness-Dual-Order-Solution-for-Strengthening-Adversarial-Transferability-cs-CVPDF"><a href="#95-Beyond-Deceptive-Flatness-Dual-Order-Solution-for-Strengthening-Adversarial-Transferability-cs-CVPDF" class="headerlink" title="[95] Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability cs.CVPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01240">Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01240" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhixuan Zhang, Pingyu Wang, Xingjian Zheng, Linbo Qing, Qi Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于双阶信息的黑盒梯度迁移攻击方法，通过引入Adversarial Flatness (AF)解决欺骗性平坦问题，并结合MonteCarlo Adversarial Sampling (MCAS)提高攻击能力，实验证明其方法在多种模型架构上的迁移性优于基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的可迁移对抗攻击方法主要关注平坦损失，但仍难以避免陷入次优区域（如平坦但尖锐的区域，称为欺骗性平坦）。为了解决这一问题，论文从双阶信息的视角出发，提出新的攻击方法。</p>
<p><strong>Result:</strong> 在ImageNet-compatible数据集上的实验表明，该方法在平坦区域生成的对抗样本迁移性优于六种基线方法，且在输入变换攻击和Baidu Cloud API测试中表现更优。</p>
<p><strong>Insight:</strong> 迁移性对抗攻击的成功不仅依赖于损失函数的平坦性，还需结合高阶信息优化梯度方向，提升攻击的泛化能力。</p>
<p><strong>Abstract:</strong> Transferable attacks generate adversarial examples on surrogate models to fool unknown victim models, posing real-world threats and growing research interest. Despite focusing on flat losses for transferable adversarial examples, recent studies still fall into suboptimal regions, especially the flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce a novel black-box gradient-based transferable attack from a perspective of dual-order information. Specifically, we feasibly propose Adversarial Flatness (AF) to the deceptive flatness problem and a theoretical assurance for adversarial transferability. Based on this, using an efficient approximation of our objective, we instantiate our attack as Adversarial Flatness Attack (AFA), addressing the altered gradient sign issue. Additionally, to further improve the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by enhancing the inner-loop sampling efficiency. The comprehensive results on ImageNet-compatible dataset demonstrate superiority over six baselines, generating adversarial examples in flatter regions and boosting transferability across model architectures. When tested on input transformation attacks or the Baidu Cloud API, our method outperforms baselines.</p>
  </div>
</details>

<hr>
<h3 id="96-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models-cs-CV-cs-CLPDF"><a href="#96-Actial-Activate-Spatial-Reasoning-Ability-of-Multimodal-Large-Language-Models-cs-CV-cs-CLPDF" class="headerlink" title="[96] Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models cs.CV | cs.CLPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01618">Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01618" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了Viewpoint Learning任务和Viewpoint-100K数据集，通过两阶段微调策略（SFT和GRPO）激活多模态大语言模型（MLLM）的空间推理能力，并在实验中显著提升了模型的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管多模态大语言模型（MLLM）在2D视觉理解方面取得了显著进展，但它们在复杂3D推理任务中的能力尚不明确。论文旨在填补这一空白，尤其是在跨视图一致性等关键空间推理能力方面的研究。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提升了MLLM的空间推理能力，在领域内和领域外任务中均表现优异。</p>
<p><strong>Insight:</strong> 论文强调了在MLLMs中发展基础空间技能的重要性，为机器人学、自动驾驶系统和3D场景理解的未来发展提供了支持。</p>
<p><strong>Abstract:</strong> Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved 2D visual understanding, prompting interest in their application to complex 3D reasoning tasks. However, it remains unclear whether these models can effectively capture the detailed spatial information required for robust real-world performance, especially cross-view consistency, a key requirement for accurate 3D reasoning. Considering this issue, we introduce Viewpoint Learning, a task designed to evaluate and improve the spatial reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset, consisting of 100K object-centric image pairs with diverse viewpoints and corresponding question-answer pairs. Our approach employs a two-stage fine-tuning strategy: first, foundational knowledge is injected to the baseline MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in significant improvements across multiple tasks; second, generalization is enhanced through Reinforcement Learning using the Group Relative Policy Optimization (GRPO) algorithm on a broader set of questions. Additionally, we introduce a hybrid cold-start initialization method designed to simultaneously learn viewpoint representations and maintain coherent reasoning thinking. Experimental results show that our approach significantly activates the spatial reasoning ability of MLLM, improving performance on both in-domain and out-of-domain reasoning tasks. Our findings highlight the value of developing foundational spatial skills in MLLMs, supporting future progress in robotics, autonomous systems, and 3D scene understanding.</p>
  </div>
</details>

<hr>
<h3 id="97-CenterMamba-SAM-Center-Prioritized-Scanning-and-Temporal-Prototypes-for-Brain-Lesion-Segmentation-cs-CVPDF"><a href="#97-CenterMamba-SAM-Center-Prioritized-Scanning-and-Temporal-Prototypes-for-Brain-Lesion-Segmentation-cs-CVPDF" class="headerlink" title="[97] CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation cs.CVPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01243">CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Tian, Zhongheng Yang, Chenshi Liu, Yiyun Su, Ziwei Hong</span></p>
<p><strong>TL;DR:</strong> CenterMamba-SAM提出了一种高效的大脑病变分割框架，通过冻结预训练的主干网络并仅训练轻量级适配器，结合中心优先的扫描策略和时序原型，显著提升了小病变和低对比度区域的检测能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大脑病变分割的挑战性主要来源于小病变、低对比度区域、各向异性采样和切片间的不连续性。现有方法在处理这些复杂场景时效果有限。</p>
<p><strong>Result:</strong> 在公共基准测试中，CenterMamba-SAM实现了最先进的性能表现。</p>
<p><strong>Insight:</strong> 1. 中心优先的扫描策略能有效提升对小病变和弱边界的检测能力。2. 记忆驱动的原型设计增强了切片间的连贯性，无需依赖用户交互。3. 轻量级适配器和冻结主干的结合提高了计算效率。</p>
<p><strong>Abstract:</strong> Brain lesion segmentation remains challenging due to small, low-contrast lesions, anisotropic sampling, and cross-slice discontinuities. We propose CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and trains only lightweight adapters for efficient fine-tuning. At its core is the CenterMamba encoder, which employs a novel 3x3 corner-axis-center short-sequence scanning strategy to enable center-prioritized, axis-reinforced, and diagonally compensated information aggregation. This design enhances sensitivity to weak boundaries and tiny foci while maintaining sparse yet effective feature representation. A memory-driven structural prompt generator maintains a prototype bank across neighboring slices, enabling automatic synthesis of reliable prompts without user interaction, thereby improving inter-slice coherence. The memory-augmented multi-scale decoder integrates memory attention modules at multiple levels, combining deep supervision with progressive refinement to restore fine details while preserving global consistency. Extensive experiments on public benchmarks demonstrate that CenterMamba-SAM achieves state-of-the-art performance in brain lesion segmentation.</p>
  </div>
</details>

<hr>
<h3 id="98-Source-Only-Cross-Weather-LiDAR-via-Geometry-Aware-Point-Drop-cs-CVPDF"><a href="#98-Source-Only-Cross-Weather-LiDAR-via-Geometry-Aware-Point-Drop-cs-CVPDF" class="headerlink" title="[98] Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop cs.CVPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01250">Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01250" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">YoungJae Cheong, Jhonghyun An</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种几何感知的适配器模块，通过保留点云数据的几何结构，提升LiDAR语义分割在恶劣天气下的表现。该方法在训练时无需目标域标签，表现优于现有数据增强和正则化基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 恶劣天气下，LiDAR数据的几何结构易受折射、散射和点丢失等因素破坏，导致语义分割性能下降。现有方法忽略了对边界、角落和稀疏区域的结构脆弱性处理。</p>
<p><strong>Result:</strong> 在SemanticKITTI上训练、SemanticSTF上评估的跨天气设定下，mIoU提升7.9%（相比数据增强基线）和0.6%（相比类别正则化基线）。</p>
<p><strong>Insight:</strong> 几何驱动的正则化是提升LiDAR语义分割在恶劣天气下鲁棒性的重要方向，且可以以轻量级方式实现。</p>
<p><strong>Abstract:</strong> LiDAR semantic segmentation degrades in adverse weather because refraction, scattering, and point dropouts corrupt geometry. Prior work in weather simulation, mixing-based augmentation, domain randomization, and uncertainty or boundary regularization improves robustness but still overlooks structural vulnerabilities near boundaries, corners, and sparse regions. We present a Light Geometry-aware adapter. The module aligns azimuth and applies horizontal circular padding to preserve neighbor continuity across the 0~360 degree wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points and computes simple local statistics, which are compressed into compact geometry-aware cues. During training, these cues drive region-aware regularization that stabilizes predictions in structurally fragile areas. The adapter is plug and play, complements augmentation, and can be enabled only during training with negligible inference cost. We adopt a source-only cross-weather setup where models train on SemanticKITTI and are evaluated on SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by 7.9 percentage points over the data-centric augmentation baseline and by 0.6 points over the class-centric regularization baseline. These results indicate that geometry-driven regularization is a key direction for all-weather LiDAR segmentation.</p>
  </div>
</details>

<hr>
<h3 id="99-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls-cs-CV-cs-LGPDF"><a href="#99-MotionStream-Real-Time-Video-Generation-with-Interactive-Motion-Controls-cs-CV-cs-LGPDF" class="headerlink" title="[99] MotionStream: Real-Time Video Generation with Interactive Motion Controls cs.CV | cs.LGPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01266">MotionStream: Real-Time Video Generation with Interactive Motion Controls</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01266" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park</span></p>
<p><strong>TL;DR:</strong> MotionStream 是一个实时视频生成系统，通过交互式运动控制在单个 GPU 上实现每秒 29 帧的流式生成，解决了现有方法的延迟问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于运动条件的视频生成方法存在高延迟（每分钟生成视频）和非因果处理问题，无法实现实时交互。</p>
<p><strong>Result:</strong> 在运动跟随和视频质量上达到 SOTA，速度提升两个数量级，实现实时无限长视频生成。</p>
<p><strong>Insight:</strong> 1. 滑动窗口和注意力汇聚技术能有效解决无限长视频生成的领域差距和质量问题；2. 模型蒸馏是实现实时性能的关键。</p>
<p><strong>Abstract:</strong> Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.</p>
  </div>
</details>

<hr>
<h3 id="100-PRevivor-Reviving-Ancient-Chinese-Paintings-using-Prior-Guided-Color-Transformers-cs-CVPDF"><a href="#100-PRevivor-Reviving-Ancient-Chinese-Paintings-using-Prior-Guided-Color-Transformers-cs-CVPDF" class="headerlink" title="[100] PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers cs.CVPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01274">PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01274" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tan Tang, Yanhong Wu, Junming Gao, Yingcai Wu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为PRevivor的框架，通过学习近期的中国画作（如明清时期）来恢复古代画作（如唐宋时期）的色彩。该方法将色彩修复任务分解为亮度增强和色调校正两个子任务，并通过双分支颜色查询模块实现了局部与全局的色彩校正。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 古代中国画作因色彩退化而受损，但修复过程复杂且缺乏高质量数据集，导致数字修复工具的进展缓慢。</p>
<p><strong>Result:</strong> 实验结果表明，PRevivor在数量和质量上均优于现有的色彩化方法。</p>
<p><strong>Insight:</strong> 通过分解任务并引入局部和全局的双分支机制，可以更有效地恢复古代画作的色彩。</p>
<p><strong>Abstract:</strong> Ancient Chinese paintings are a valuable cultural heritage that is damaged by irreversible color degradation. Reviving color-degraded paintings is extraordinarily difficult due to the complex chemistry mechanism. Progress is further slowed by the lack of comprehensive, high-quality datasets, which hampers the creation of end-to-end digital restoration tools. To revive colors, we propose PRevivor, a prior-guided color transformer that learns from recent paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and Song Dynasty). To develop PRevivor, we decompose color restoration into two sequential sub-tasks: luminance enhancement and hue correction. For luminance enhancement, we employ two variational U-Nets and a multi-scale mapping module to translate faded luminance into restored counterparts. For hue correction, we design a dual-branch color query module guided by localized hue priors extracted from faded paintings. Specifically, one branch focuses attention on regions guided by masked priors, enforcing localized hue correction, whereas the other branch remains unconstrained to maintain a global reasoning capability. To evaluate PRevivor, we conduct extensive experiments against state-of-the-art colorization methods. The results demonstrate superior performance both quantitatively and qualitatively.</p>
  </div>
</details>

<hr>
<h3 id="101-Adaptation-of-Foundation-Models-for-Medical-Image-Analysis-Strategies-Challenges-and-Future-Directions-cs-CV-cs-AIPDF"><a href="#101-Adaptation-of-Foundation-Models-for-Medical-Image-Analysis-Strategies-Challenges-and-Future-Directions-cs-CV-cs-AIPDF" class="headerlink" title="[101] Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions cs.CV | cs.AIPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01284">Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01284" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Karma Phuntsho, Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn</span></p>
<p><strong>TL;DR:</strong> 该论文综述了基础模型（FMs）在医学图像分析中的应用策略、挑战与未来方向，探讨了如何将其适应真实临床需求，并提出了多种改进方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基础模型在医学图像分析中具有潜力，但面临领域偏移、标注数据稀缺、计算资源需求高和隐私问题等挑战，亟需有效的适应策略。</p>
<p><strong>Result:</strong> 通过比较不同方法的性能增益与局限性，论文揭示了现有技术的优缺点，并强调了动态部署和数据效率提升的需求。</p>
<p><strong>Insight:</strong> 未来研究应聚焦隐私保护、数据高效利用和系统化评测，以推动FMs在临床中的可靠集成。</p>
<p><strong>Abstract:</strong> Foundation models (FMs) have emerged as a transformative paradigm in medical image analysis, offering the potential to provide generalizable, task-agnostic solutions across a wide range of clinical tasks and imaging modalities. Their capacity to learn transferable representations from large-scale data has the potential to address the limitations of conventional task-specific models. However, adaptation of FMs to real-world clinical practice remains constrained by key challenges, including domain shifts, limited availability of high-quality annotated data, substantial computational demands, and strict privacy requirements. This review presents a comprehensive assessment of strategies for adapting FMs to the specific demands of medical imaging. We examine approaches such as supervised fine-tuning, domain-specific pretraining, parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and multimodal or cross-modal frameworks. For each, we evaluate reported performance gains, clinical applicability, and limitations, while identifying trade-offs and unresolved challenges that prior reviews have often overlooked. Beyond these established techniques, we also highlight emerging directions aimed at addressing current gaps. These include continual learning to enable dynamic deployment, federated and privacy-preserving approaches to safeguard sensitive data, hybrid self-supervised learning to enhance data efficiency, data-centric pipelines that combine synthetic generation with human-in-the-loop validation, and systematic benchmarking to assess robust generalization under real-world clinical variability. By outlining these strategies and associated research gaps, this review provides a roadmap for developing adaptive, trustworthy, and clinically integrated FMs capable of meeting the demands of real-world medical imaging.</p>
  </div>
</details>

<hr>
<h3 id="102-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark-cs-CVPDF"><a href="#102-UniREditBench-A-Unified-Reasoning-based-Image-Editing-Benchmark-cs-CVPDF" class="headerlink" title="[102] UniREditBench: A Unified Reasoning-based Image Editing Benchmark cs.CVPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01295">UniREditBench: A Unified Reasoning-based Image Editing Benchmark</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01295" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了UniREditBench，一个统一的基于推理的图像编辑评测基准，涵盖多对象交互和游戏世界场景，并引入多模态双参考评估方法，以提高评测可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前多模态生成模型在复杂图像编辑任务中表现不佳，尤其是需要隐式推理的任务，现有评测基准多关注单对象属性转换，忽略了多对象交互和游戏世界场景，评测方法也依赖单一文本参考，容易导致误判。</p>
<p><strong>Result:</strong> UniREdit-Bagel模型在域内和域外场景中表现均有显著提升；通过对开源和闭源模型的广泛评测，揭示了它们在各个方面的优劣势。</p>
<p><strong>Insight:</strong> 多模态双参考评估方法能更全面地衡量复杂推理任务的表现；结合合成数据和真实数据可以显著提升模型性能，尤其是在需要隐式推理的场景。</p>
<p><strong>Abstract:</strong> Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.</p>
  </div>
</details>

<hr>
<h3 id="103-Positive-Semi-definite-Latent-Factor-Grouping-Boosted-Cluster-reasoning-Instance-Disentangled-Learning-for-WSI-Representation-cs-CVPDF"><a href="#103-Positive-Semi-definite-Latent-Factor-Grouping-Boosted-Cluster-reasoning-Instance-Disentangled-Learning-for-WSI-Representation-cs-CVPDF" class="headerlink" title="[103] Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation cs.CVPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01304">Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01304" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chentao Li, Behzad Bozorgtabar, Yifang Ping, Pan Huang, Jing Qin</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种三阶段的WSI表示学习框架，通过潜因子分组、聚类推理实例解缠和决策重加权，显著提升了全切片病理图像的表征能力和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MIL方法在全切片病理图像（WSI）表示中存在空间、语义和决策层面的纠缠问题，限制了表征能力和可解释性，亟需解决这些问题。</p>
<p><strong>Result:</strong> 在多中心数据集上超越现有SOTA模型，并通过解缠表示和透明决策实现病理医生对齐的可解释性。</p>
<p><strong>Insight:</strong> 潜因子分组和实例解缠是实现WSI高性能表征和可解释性的关键，同时反事实推理和决策重加权为MIL提供了新思路。</p>
<p><strong>Abstract:</strong> Multiple instance learning (MIL) has been widely used for representing whole-slide pathology images. However, spatial, semantic, and decision entanglements among instances limit its representation and interpretability. To address these challenges, we propose a latent factor grouping-boosted cluster-reasoning instance disentangled learning framework for whole-slide image (WSI) interpretable representation in three phases. First, we introduce a novel positive semi-definite latent factor grouping that maps instances into a latent subspace, effectively mitigating spatial entanglement in MIL. To alleviate semantic entanglement, we employs instance probability counterfactual inference and optimization via cluster-reasoning instance disentangling. Finally, we employ a generalized linear weighted decision via instance effect re-weighting to address decision entanglement. Extensive experiments on multicentre datasets demonstrate that our model outperforms all state-of-the-art models. Moreover, it attains pathologist-aligned interpretability through disentangled representations and a transparent decision-making process.</p>
  </div>
</details>

<hr>
<h3 id="104-Perturb-a-Model-Not-an-Image-Towards-Robust-Privacy-Protection-via-Anti-Personalized-Diffusion-Models-cs-CV-cs-AIPDF"><a href="#104-Perturb-a-Model-Not-an-Image-Towards-Robust-Privacy-Protection-via-Anti-Personalized-Diffusion-Models-cs-CV-cs-AIPDF" class="headerlink" title="[104] Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models cs.CV | cs.AIPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01307">Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01307" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tae-Young Lee, Juwon Seo, Jong Hwan Ko, Gyeong-Moon Park</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新型隐私保护框架APDM，通过修改扩散模型而非图像本身，防止恶意用户对特定主体进行个性化生成，解决了现有方法的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着扩散模型的高质量个性化生成能力增强，恶意用户可能滥用该技术生成未经授权的内容，现有基于对抗样本的保护方法效果有限且易被绕过。</p>
<p><strong>Result:</strong> 实验表明APDM在阻止未授权个性化生成方面优于现有方法，达到了最先进的性能。</p>
<p><strong>Insight:</strong> 将保护目标从图像转移到模型本身是一种更鲁棒的隐私保护思路，双路径优化策略能够动态适应未来可能的个性化攻击。</p>
<p><strong>Abstract:</strong> Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized content. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called Anti-Personalized Diffusion Models (APDM). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at <a target="_blank" rel="noopener" href="https://github.com/KU-VGI/APDM">https://github.com/KU-VGI/APDM</a>.</p>
  </div>
</details>

<hr>
<h3 id="105-A-Generative-Adversarial-Approach-to-Adversarial-Attacks-Guided-by-Contrastive-Language-Image-Pre-trained-Model-cs-CVPDF"><a href="#105-A-Generative-Adversarial-Approach-to-Adversarial-Attacks-Guided-by-Contrastive-Language-Image-Pre-trained-Model-cs-CVPDF" class="headerlink" title="[105] A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model cs.CVPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01317">A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01317" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sampriti Soor, Alik Pramanick, Jothiprakash K, Arijit Sur</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于生成对抗网络（GAN）和对比语言-图像预训练模型（CLIP）的对抗攻击方法，通过结合CLIP的语言-图像对齐能力和SSAE的集中扰动策略，生成高效且视觉不可察觉的对抗样本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度学习模型的强大性能使其广泛应用于图像识别和语言理解任务，但对抗攻击的存在威胁了模型的可靠性。传统方法生成的对抗样本在视觉上可能不够自然，限制了攻击的隐蔽性和有效性。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在多种黑盒受害者模型上表现出色，生成对抗样本的欺骗效果优于或与现有方法相当，同时保持了更高的视觉保真度。</p>
<p><strong>Insight:</strong> 该方法的创新在于将自然语言语义引入对抗攻击，展示了跨模态模型在生成对抗样本中的潜力。同时，通过结合扰动集中策略，提升了攻击的针对性和隐蔽性。</p>
<p><strong>Abstract:</strong> The rapid growth of deep learning has brought about powerful models that can handle various tasks, like identifying images and understanding language. However, adversarial attacks, an unnoticed alteration, can deceive models, leading to inaccurate predictions. In this paper, a generative adversarial attack method is proposed that uses the CLIP model to create highly effective and visually imperceptible adversarial perturbations. The CLIP model’s ability to align text and image representation helps incorporate natural language semantics with a guided loss to generate effective adversarial examples that look identical to the original inputs. This integration allows extensive scene manipulation, creating perturbations in multi-object environments specifically designed to deceive multilabel classifiers. Our approach integrates the concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with the dissimilar text embeddings similar to Generative Adversarial Multi-Object Scene Attacks (GAMA), resulting in perturbations that both deceive classification models and maintain high structural similarity to the original images. The model was tested on various tasks across diverse black-box victim models. The experimental results show that our method performs competitively, achieving comparable or superior results to existing techniques, while preserving greater visual fidelity.</p>
  </div>
</details>

<hr>
<h3 id="106-MIQ-SAM3D-From-Single-Point-Prompt-to-Multi-Instance-Segmentation-via-Competitive-Query-Refinement-cs-CVPDF"><a href="#106-MIQ-SAM3D-From-Single-Point-Prompt-to-Multi-Instance-Segmentation-via-Competitive-Query-Refinement-cs-CVPDF" class="headerlink" title="[106] MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement cs.CVPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01345">MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01345" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jierui Qu, Jianchun Zhao</span></p>
<p><strong>TL;DR:</strong> MIQ-SAM3D通过竞争性查询优化策略，实现了从单点提示到多实例分割的转变，提升了医学图像中多病灶分割的效率与准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有SAM（交互式分割方法）多遵循单点-单对象范式，难以处理多病灶分割任务，且ViT骨干网络虽能捕获全局上下文，但缺乏高保真的局部细节。</p>
<p><strong>Result:</strong> 在LiTS17和KiTS21数据集上表现优异，对提示具有强鲁棒性。</p>
<p><strong>Insight:</strong> 通过单点提示生成多实例查询的策略能够高效解决多病灶分割问题，同时混合编码器设计弥补了ViT在局部细节上的不足。</p>
<p><strong>Abstract:</strong> Accurate segmentation of medical images is fundamental to tumor diagnosis and treatment planning. SAM-based interactive segmentation has gained attention for its strong generalization, but most methods follow a single-point-to-single-object paradigm, which limits multi-lesion segmentation. Moreover, ViT backbones capture global context but often miss high-fidelity local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework with a competitive query optimization strategy that shifts from single-point-to-single-mask to single-point-to-multi-instance. A prompt-conditioned instance-query generator transforms a single point prompt into multiple specialized queries, enabling retrieval of all semantically similar lesions across the 3D volume from a single exemplar. A hybrid CNN-Transformer encoder injects CNN-derived boundary saliency into ViT self-attention via spatial gating. A competitively optimized query decoder then enables end-to-end, parallel, multi-instance prediction through inter-query competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels and exhibits strong robustness to prompts, providing a practical solution for efficient annotation of clinically relevant multi-lesion cases.</p>
  </div>
</details>

<hr>
<h3 id="107-CMI-MTL-Cross-Mamba-interaction-based-multi-task-learning-for-medical-visual-question-answering-cs-CV-cs-AIPDF"><a href="#107-CMI-MTL-Cross-Mamba-interaction-based-multi-task-learning-for-medical-visual-question-answering-cs-CV-cs-AIPDF" class="headerlink" title="[107] CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering cs.CV | cs.AIPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01357">CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01357" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qiangguo Jin, Xianyao Zheng, Hui Cui, Changming Sun, Yuqi Fang</span></p>
<p><strong>TL;DR:</strong> CMI-MTL 是一种基于跨模态交互和多任务学习的框架，用于医学视觉问答任务，通过细粒度视觉-文本特征对齐、跨模态交错特征表示和自由形式答案增强的多任务学习模块，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于自注意力的方法难以有效处理视觉与语言之间的跨模态语义对齐，而分类方法依赖于预定义的答案集，无法适应自由形式答案的多样性。</p>
<p><strong>Result:</strong> 在 VQA-RAD、SLAKE 和 OVQA 三个数据集上优于现有最优方法，并通过可解释性实验验证了框架的有效性。</p>
<p><strong>Insight:</strong> 跨模态交互和多任务学习的结合能有效提升医学视觉问答任务的性能，尤其是对于自由形式答案的处理。</p>
<p><strong>Abstract:</strong> Medical visual question answering (Med-VQA) is a crucial multimodal task in clinical decision support and telemedicine. Recent self-attention based methods struggle to effectively handle cross-modal semantic alignments between vision and language. Moreover, classification-based methods rely on predefined answer sets. Treating this task as a simple classification problem may make it unable to adapt to the diversity of free-form answers and overlook the detailed semantic information of free-form answers. In order to tackle these challenges, we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL) framework that learns cross-modal feature representations from images and texts. CMI-MTL comprises three key modules: fine-grained visual-text feature alignment (FVTA), cross-modal interleaved feature representation (CIFR), and free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most relevant regions in image-text pairs through fine-grained visual-text feature alignment. CIFR captures cross-modal sequential interactions via cross-modal interleaved feature representation. FFAE leverages auxiliary knowledge from open-ended questions through free-form answer-enhanced multi-task learning, improving the model’s capability for open-ended Med-VQA. Experimental results show that CMI-MTL outperforms the existing state-of-the-art methods on three Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more interpretability experiments to prove the effectiveness. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-repo/CMI-MTL">https://github.com/BioMedIA-repo/CMI-MTL</a>.</p>
  </div>
</details>

<hr>
<h3 id="108-EREBUS-End-to-end-Robust-Event-Based-Underwater-Simulation-cs-CV-cs-ROPDF"><a href="#108-EREBUS-End-to-end-Robust-Event-Based-Underwater-Simulation-cs-CV-cs-ROPDF" class="headerlink" title="[108] EREBUS: End-to-end Robust Event Based Underwater Simulation cs.CV | cs.ROPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01381">EREBUS: End-to-end Robust Event Based Underwater Simulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01381" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hitesh Kyatham, Arjun Suresh, Aadi Palnitkar, Yiannis Aloimonos</span></p>
<p><strong>TL;DR:</strong> EREBUS提出了一种用于生成水下事件相机合成数据的端到端流程，以解决传统视觉技术在恶劣水下环境中的性能问题，尤其在低能见度和悬浮颗粒场景中展示了岩石检测的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 水下环境的挑战（如光线差、高动态范围）限制了传统视觉技术的性能，而事件相机通过逐帧变化追踪可能提供解决方案。然而，缺乏真实水下事件数据成为训练模型的瓶颈。</p>
<p><strong>Result:</strong> 生成的数据成功应用于低能见度和悬浮颗粒场景的岩石检测任务，证明了方法的通用性和实用性。</p>
<p><strong>Insight:</strong> 合成数据可以缓解水下事件相机真实数据的稀缺问题，为恶劣环境下的视觉任务提供了新的训练途径。</p>
<p><strong>Abstract:</strong> The underwater domain presents a vast array of challenges for roboticists and computer vision researchers alike, such as poor lighting conditions and high dynamic range scenes. In these adverse conditions, traditional vision techniques struggle to adapt and lead to suboptimal performance. Event-based cameras present an attractive solution to this problem, mitigating the issues of traditional cameras by tracking changes in the footage on a frame-by-frame basis. In this paper, we introduce a pipeline which can be used to generate realistic synthetic data of an event-based camera mounted to an AUV (Autonomous Underwater Vehicle) in an underwater environment for training vision models. We demonstrate the effectiveness of our pipeline using the task of rock detection with poor visibility and suspended particulate matter, but the approach can be generalized to other underwater tasks.</p>
  </div>
</details>

<hr>
<h3 id="109-SEPS-Semantic-enhanced-Patch-Slimming-Framework-for-fine-grained-cross-modal-alignment-cs-CV-cs-AI-cs-MMPDF"><a href="#109-SEPS-Semantic-enhanced-Patch-Slimming-Framework-for-fine-grained-cross-modal-alignment-cs-CV-cs-AI-cs-MMPDF" class="headerlink" title="[109] SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment cs.CV | cs.AI | cs.MMPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01390">SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01390" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinyu Mao, Junsi Li, Haoji Zhang, Yu Liang, Ming Sun</span></p>
<p><strong>TL;DR:</strong> SEPS框架通过语义增强的patch slimming方法解决了跨模态对齐中的patch冗余和模糊问题，显著提升了细粒度对齐性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前跨模态对齐方法在处理视觉和语言间patch冗余和模糊问题时效率不足，且MLLMs生成的密集文本可能与原始稀疏描述冲突。SEPS旨在系统解决这些问题。</p>
<p><strong>Result:</strong> 在Flickr30K和MS-COCO数据集上，SEPS的rSum指标比现有方法提升23%-86%，尤其在文本到图像检索任务中表现突出。</p>
<p><strong>Insight:</strong> SEPS通过语义增强和patch slimming有效减少冗余和模糊，为跨模态对齐提供了一种高效解决方案，尤其适用于细粒度任务。</p>
<p><strong>Abstract:</strong> Fine-grained cross-modal alignment aims to establish precise local correspondences between vision and language, forming a cornerstone for visual question answering and related multimodal applications. Current approaches face challenges in addressing patch redundancy and ambiguity, which arise from the inherent information density disparities across modalities. Recently, Multimodal Large Language Models (MLLMs) have emerged as promising solutions to bridge this gap through their robust semantic generation capabilities. However, the dense textual outputs from MLLMs may introduce conflicts with the original sparse captions. Furthermore, accurately quantifying semantic relevance between rich visual patches and concise textual descriptions remains a core challenge. To overcome these limitations, we introduce the Semantic-Enhanced Patch Slimming (SEPS) framework, which systematically addresses patch redundancy and ambiguity. Our approach employs a two-stage mechanism to integrate unified semantics from both dense and sparse texts, enabling the identification of salient visual patches. Additionally, it leverages relevance-aware selection with mean value computation to highlight crucial patch-word correspondences, thereby improving cross-modal similarity assessment. Comprehensive experiments on Flickr30K and MS-COCO datasets validate that SEPS achieves superior performance, surpassing existing approaches by 23%-86% in rSum across diverse model architectures, with notable enhancements in text-to-image retrieval scenarios. Our implementation is available at <a target="_blank" rel="noopener" href="https://github.com/Sweet4tars/seps.git">https://github.com/Sweet4tars/seps.git</a>.</p>
  </div>
</details>

<hr>
<h3 id="110-Extremal-Contours-Gradient-driven-contours-for-compact-visual-attribution-cs-CV-cs-LGPDF"><a href="#110-Extremal-Contours-Gradient-driven-contours-for-compact-visual-attribution-cs-CV-cs-LGPDF" class="headerlink" title="[110] Extremal Contours: Gradient-driven contours for compact visual attribution cs.CV | cs.LGPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01411">Extremal Contours: Gradient-driven contours for compact visual attribution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01411" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Reza Karimzadeh, Albert Alonso, Frans Zdyb, Julius B. Kirkegaard, Bulat Ibragimov</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种训练无关的视觉解释方法，通过梯度驱动的平滑轮廓（Extremal Contours）替代传统的密集扰动掩码，实现了紧凑、鲁棒且可解释的视觉归因。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于密集扰动掩码的视觉模型解释方法存在碎片化、过拟合问题，且需后处理。论文旨在设计一种更简洁、稳定且无需训练的替代方案。</p>
<p><strong>Result:</strong> 在ImageNet分类器上，其性能与密集掩码相当，但更紧凑、可解释，且在多目标定位中表现优异，尤其在自监督DINO模型上提升了15%以上的相关性。</p>
<p><strong>Insight:</strong> 轮廓驱动的视觉归因方法在减少参数复杂度的同时，保持了高解释力，为模型可解释性提供了一条新路径。</p>
<p><strong>Abstract:</strong> Faithful yet compact explanations for vision models remain a challenge, as commonly used dense perturbation masks are often fragmented and overfitted, needing careful post-processing. Here, we present a training-free explanation method that replaces dense masks with smooth tunable contours. A star-convex region is parameterized by a truncated Fourier series and optimized under an extremal preserve&#x2F;delete objective using the classifier gradients. The approach guarantees a single, simply connected mask, cuts the number of free parameters by orders of magnitude, and yields stable boundary updates without cleanup. Restricting solutions to low-dimensional, smooth contours makes the method robust to adversarial masking artifacts. On ImageNet classifiers, it matches the extremal fidelity of dense masks while producing compact, interpretable regions with improved run-to-run consistency. Explicit area control also enables importance contour maps, yielding a transparent fidelity-area profiles. Finally, we extend the approach to multi-contour and show how it can localize multiple objects within the same framework. Across benchmarks, the method achieves higher relevance mass and lower complexity than gradient and perturbation based baselines, with especially strong gains on self-supervised DINO models where it improves relevance mass by over 15% and maintains positive faithfulness correlations.</p>
  </div>
</details>

<hr>
<h3 id="111-Towards-One-step-Causal-Video-Generation-via-Adversarial-Self-Distillation-cs-CVPDF"><a href="#111-Towards-One-step-Causal-Video-Generation-via-Adversarial-Self-Distillation-cs-CVPDF" class="headerlink" title="[111] Towards One-step Causal Video Generation via Adversarial Self-Distillation cs.CVPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01419">Towards One-step Causal Video Generation via Adversarial Self-Distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01419" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongqi Yang, Huayang Huang, Xu Peng, Xiaobin Hu, Donghao Luo</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于对抗自蒸馏（ASD）的高效因果视频生成框架，通过分布匹配蒸馏和帧增强策略，显著提升极少量去噪步骤下的生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统混合视频生成模型因迭代性质导致错误累积和长推理时间，本文旨在通过蒸馏方法实现高效的单步或少步高质量视频生成。</p>
<p><strong>Result:</strong> 在VBench上超越现有方法，在1-2步生成中表现优异；单模型支持多步推理，无需重复蒸馏。</p>
<p><strong>Insight:</strong> 蒸馏技术可显著提升视频生成效率，分布对齐和动态步骤分配是优化长序列生成的关键。</p>
<p><strong>Abstract:</strong> Recent hybrid video generation models combine autoregressive temporal dynamics with diffusion-based spatial denoising, but their sequential, iterative nature leads to error accumulation and long inference times. In this work, we propose a distillation-based framework for efficient causal video generation that enables high-quality synthesis with extremely limited denoising steps. Our approach builds upon the Distribution Matching Distillation (DMD) framework and proposes a novel Adversarial Self-Distillation (ASD) strategy, which aligns the outputs of the student model’s n-step denoising process with its (n+1)-step version at the distribution level. This design provides smoother supervision by bridging small intra-student gaps and more informative guidance by combining teacher knowledge with locally consistent student behavior, substantially improving training stability and generation quality in extremely few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame Enhancement (FFE) strategy, which allocates more denoising steps to the initial frames to mitigate error propagation while applying larger skipping steps to later frames. Extensive experiments on VBench demonstrate that our method surpasses state-of-the-art approaches in both one-step and two-step video generation. Notably, our framework produces a single distilled model that flexibly supports multiple inference-step settings, eliminating the need for repeated re-distillation and enabling efficient, high-quality video synthesis.</p>
  </div>
</details>

<hr>
<h3 id="112-UniSOT-A-Unified-Framework-for-Multi-Modality-Single-Object-Tracking-cs-CV-cs-AIPDF"><a href="#112-UniSOT-A-Unified-Framework-for-Multi-Modality-Single-Object-Tracking-cs-CV-cs-AIPDF" class="headerlink" title="[112] UniSOT: A Unified Framework for Multi-Modality Single Object Tracking cs.CV | cs.AIPDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01427">UniSOT: A Unified Framework for Multi-Modality Single Object Tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01427" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yinchao Ma, Yuyang Tang, Wenfei Yang, Tianzhu Zhang, Xu Zhou</span></p>
<p><strong>TL;DR:</strong> UniSOT是一个统一的多模态单目标跟踪框架，能够处理不同参考模态（如边界框、自然语言或两者）和不同视频模态（如RGB、RGB+Depth等）的组合，性能优于专用跟踪器。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有跟踪器通常针对单一或少数模态设计，限制了实际应用的灵活性。需要一个统一的框架支持多种参考模态和视频模态的组合。</p>
<p><strong>Result:</strong> 在18个基准测试中表现优异，如在TNL2K上AUC提升3.0%，在RGB+X模态上超越Un-Track 2.0%。</p>
<p><strong>Insight:</strong> 统一框架可显著提升多模态跟踪的灵活性和性能，为复杂场景下的应用提供了新思路。</p>
<p><strong>Abstract:</strong> Single object tracking aims to localize target object with specific reference modalities (bounding box, natural language or both) in a sequence of specific video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different reference modalities enable various human-machine interactions, and different video modalities are demanded in complex scenarios to enhance tracking robustness. Existing trackers are designed for single or several video modalities with single or several reference modalities, which leads to separate model designs and limits practical applications. Practically, a unified tracker is needed to handle various requirements. To the best of our knowledge, there is still no tracker that can perform tracking with these above reference modalities across these video modalities simultaneously. Thus, in this paper, we present a unified tracker, UniSOT, for different combinations of three reference modalities and four video modalities with uniform parameters. Extensive experimental results on 18 visual tracking, vision-language tracking and RGB+X tracking benchmarks demonstrate that UniSOT shows superior performance against modality-specific counterparts. Notably, UniSOT outperforms previous counterparts by over 3.0% AUC on TNL2K across all three reference modalities and outperforms Un-Track by over 2.0% main metric across all three RGB+X video modalities.</p>
  </div>
</details>

<hr>
<h3 id="113-Terrain-Enhanced-Resolution-aware-Refinement-Attention-for-Off-Road-Segmentation-cs-CVPDF"><a href="#113-Terrain-Enhanced-Resolution-aware-Refinement-Attention-for-Off-Road-Segmentation-cs-CVPDF" class="headerlink" title="[113] Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation cs.CVPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01434">Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01434" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Seongkyu Choi, Jhonghyun An</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种分辨率感知的令牌解码器，用于解决越野语义分割中的边界模糊、稀疏监督和标签噪声问题。通过低分辨率瓶颈计算、门控交叉注意力注入细粒度细节和稀疏不确定性选择的像素细化，平衡了全局语义、局部一致性和边界保真度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 越野语义分割面临边界模糊、罕见类别稀疏监督和标签噪声的挑战。现有方法在低分辨率融合时会导致边缘模糊和局部误差传播，而高分辨率路径维护或重复高分辨率融合则计算成本高且对噪声敏感。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在越野语义分割任务中表现优异，具有竞争性的性能和更高的过渡稳定性。</p>
<p><strong>Insight:</strong> 通过分层设计和稀疏细化，该方法在保持计算效率的同时提升了分割质量，特别是在边界和罕见类别上表现突出。</p>
<p><strong>Abstract:</strong> Off-road semantic segmentation suffers from thick, inconsistent boundaries, sparse supervision for rare classes, and pervasive label noise. Designs that fuse only at low resolution blur edges and propagate local errors, whereas maintaining high-resolution pathways or repeating high-resolution fusions is costly and fragile to noise. We introduce a resolutionaware token decoder that balances global semantics, local consistency, and boundary fidelity under imperfect supervision. Most computation occurs at a low-resolution bottleneck; a gated cross-attention injects fine-scale detail, and only a sparse, uncertainty-selected set of pixels is refined. The components are co-designed and tightly integrated: global self-attention with lightweight dilated depthwise refinement restores local coherence; a gated cross-attention integrates fine-scale features from a standard high-resolution encoder stream without amplifying noise; and a class-aware point refinement corrects residual ambiguities with negligible overhead. During training, we add a boundary-band consistency regularizer that encourages coherent predictions in a thin neighborhood around annotated edges, with no inference-time cost. Overall, the results indicate competitive performance and improved stability across transitions.</p>
  </div>
</details>

<hr>
<h3 id="114-Privacy-Preserving-Ordinal-Meta-Learning-with-VLMs-for-Fine-Grained-Fruit-Quality-Prediction-cs-CV-cs-AIPDF"><a href="#114-Privacy-Preserving-Ordinal-Meta-Learning-with-VLMs-for-Fine-Grained-Fruit-Quality-Prediction-cs-CV-cs-AIPDF" class="headerlink" title="[114] Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction cs.CV | cs.AIPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01449">Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01449" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Riddhi Jain, Manasi Patwardhan, Aayush Mishra, Parijat Deshpande, Beena Rai</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为MAOML的算法，通过元学习和序数回归方法，解决了水果新鲜度预测任务中的数据稀缺问题，显著提升了开源VLM的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 水果新鲜度的准确预测对于减少浪费至关重要，但专家标注成本高昂导致数据稀缺。现有开源VLM性能不足，而专有VLM又因隐私问题无法使用。</p>
<p><strong>Result:</strong> 方法在水果新鲜度分类任务中达到了92.71%的平均准确率，超越了现有开源VLM的性能。</p>
<p><strong>Insight:</strong> 通过元学习和序数性的结合，可以显著提升小规模VLM在数据稀缺任务中的表现，同时避免了专有模型的隐私问题。</p>
<p><strong>Abstract:</strong> To effectively manage the wastage of perishable fruits, it is crucial to accurately predict their freshness or shelf life using non-invasive methods that rely on visual data. In this regard, deep learning techniques can offer a viable solution. However, obtaining fine-grained fruit freshness labels from experts is costly, leading to a scarcity of data. Closed proprietary Vision Language Models (VLMs), such as Gemini, have demonstrated strong performance in fruit freshness detection task in both zero-shot and few-shot settings. Nonetheless, food retail organizations are unable to utilize these proprietary models due to concerns related to data privacy, while existing open-source VLMs yield sub-optimal performance for the task. Fine-tuning these open-source models with limited data fails to achieve the performance levels of proprietary models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning (MAOML) algorithm, designed to train smaller VLMs. This approach utilizes meta-learning to address data sparsity and leverages label ordinality, thereby achieving state-of-the-art performance in the fruit freshness classification task under both zero-shot and few-shot settings. Our method achieves an industry-standard accuracy of 92.71%, averaged across all fruits.   Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning, Ordinal Regression</p>
  </div>
</details>

<hr>
<h3 id="115-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation-cs-CV-cs-AIPDF"><a href="#115-Reg-DPO-SFT-Regularized-Direct-Preference-Optimization-with-GT-Pair-for-Improving-Video-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[115] Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation cs.CV | cs.AIPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01450">Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01450" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie Du, Xinyu Gong, Qingshan Tan, Wen Li, Yangming Cheng</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Reg-DPO的方法，通过引入自动化构建的高质量偏好对（GT-Pair）和结合SFT损失的DPO目标，有效提升了视频生成的质量和训练稳定性，并在多个任务中表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于DPO的视频生成方法多借鉴图像领域的范式，且仅适用于小规模模型（约20亿参数），无法解决视频任务中的独特挑战，如数据构建成本高、训练不稳定和内存消耗大。</p>
<p><strong>Result:</strong> 在I2V和T2V任务的多数据集实验中，Reg-DPO均优于现有方法，生成视频质量更高。</p>
<p><strong>Insight:</strong> 自动化构建的偏好对和SFT正则化可以有效解决视频生成中的训练不稳定问题，同时提升生成质量。</p>
<p><strong>Abstract:</strong> Recent studies have identified Direct Preference Optimization (DPO) as an efficient and reward-free approach to improving video generation quality. However, existing methods largely follow image-domain paradigms and are mainly developed on small-scale models (approximately 2B parameters), limiting their ability to address the unique challenges of video tasks, such as costly data construction, unstable training, and heavy memory consumption. To overcome these limitations, we introduce a GT-Pair that automatically builds high-quality preference pairs by using real videos as positives and model-generated videos as negatives, eliminating the need for any external annotation. We further present Reg-DPO, which incorporates the SFT loss as a regularization term into the DPO objective to enhance training stability and generation fidelity. Additionally, by combining the FSDP framework with multiple memory optimization techniques, our approach achieves nearly three times higher training capacity than using FSDP alone. Extensive experiments on both I2V and T2V tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches, delivering superior video generation quality.</p>
  </div>
</details>

<hr>
<h3 id="116-When-to-Trust-the-Answer-Question-Aligned-Semantic-Nearest-Neighbor-Entropy-for-Safer-Surgical-VQA-cs-CV-cs-AIPDF"><a href="#116-When-to-Trust-the-Answer-Question-Aligned-Semantic-Nearest-Neighbor-Entropy-for-Safer-Surgical-VQA-cs-CV-cs-AIPDF" class="headerlink" title="[116] When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA cs.CV | cs.AIPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01458">When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01458" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dennis Pierantozzi, Luca Carlini, Mauro Orazio Drago, Chiara Lena, Cesare Hassan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为QA-SNNE的黑盒不确定性估计方法，用于提高手术视觉问答（VQA）的安全性和可靠性。通过结合问题语义，QA-SNNE能够检测模型预测中的不确定性，帮助减少错误答案的风险。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 手术中的视觉问答系统（VQA）需要极高的安全性和可靠性，因为错误的答案可能对患者造成伤害。现有研究大多关注准确性，而忽略了安全行为（如模糊意识或专家转介）。论文试图通过不确定性估计来增强模型的决策安全性。</p>
<p><strong>Result:</strong> 在EndoVis18-VQA和PitVQA数据集上的实验中，QA-SNNE显著提升了零样本模型的AUROC（增加15-38%），并提高了幻觉检测能力。PEFT模型在轻度改写下性能下降，而LVLM更具鲁棒性。</p>
<p><strong>Insight:</strong> 结合LVLM主干与问题对齐的不确定性估计可以显著提高手术VQA的安全性。QA-SNNE为自动故障检测（AFD）提供了一种实用且可解释的方法，增强了临床医生对模型的信任。</p>
<p><strong>Abstract:</strong> Safety and reliability are essential for deploying Visual Question Answering (VQA) in surgery, where incorrect or ambiguous responses can harm the patient. Most surgical VQA research focuses on accuracy or linguistic quality while overlooking safety behaviors such as ambiguity awareness, referral to human experts, or triggering a second opinion. Inspired by Automatic Failure Detection (AFD), we study uncertainty estimation as a key enabler of safer decision making. We introduce Question Aligned Semantic Nearest Neighbor Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question semantics into prediction confidence. It measures semantic entropy by comparing generated answers with nearest neighbors in a medical text embedding space, conditioned on the question. We evaluate five models, including domain specific Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models degrade under mild paraphrasing, while LVLMs are more resilient. Across three LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template settings and enhances hallucination detection. The Area Under the ROC Curve (AUROC) increases by 15-38% for zero-shot models, with gains maintained under out-of-template stress. QA-SNNE offers a practical and interpretable step toward AFD in surgical VQA by linking semantic uncertainty to question context. Combining LVLM backbones with question aligned uncertainty estimation can improve safety and clinician trust. The code and model are available at <a target="_blank" rel="noopener" href="https://github.com/DennisPierantozzi/QASNNE">https://github.com/DennisPierantozzi/QASNNE</a></p>
  </div>
</details>

<hr>
<h3 id="117-Efficiently-Training-A-Flat-Neural-Network-Before-It-has-been-Quantizated-cs-CV-cs-AIPDF"><a href="#117-Efficiently-Training-A-Flat-Neural-Network-Before-It-has-been-Quantizated-cs-CV-cs-AIPDF" class="headerlink" title="[117] Efficiently Training A Flat Neural Network Before It has been Quantizated cs.CV | cs.AIPDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01462">Efficiently Training A Flat Neural Network Before It has been Quantizated</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01462" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peng Xia, Junbiao Pang, Tianyang Cai</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种高效训练平坦的浮点神经网络的方法，为预定义的低比特量化模型做准备，通过统计建模激活和权重量化误差，并研究噪声注入优化方法来实现平坦最小值。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的后训练量化方法通常忽视良好训练的神经网络与量化模型之间的关系，导致较大的量化误差。论文旨在解决如何高效训练一个与模型无关的神经网络，以适应低比特量化模型。</p>
<p><strong>Result:</strong> 实验结果表明该方法有效，为低比特后训练量化模型提供了新的途径。</p>
<p><strong>Insight:</strong> 平坦的浮点神经网络可以显著降低量化误差，噪声注入优化是实现这一目标的有效手段。</p>
<p><strong>Abstract:</strong> Post-training quantization (PTQ) for vision transformers (ViTs) has garnered significant attention due to its efficiency in compressing models. However, existing methods typically overlook the relationship between a well-trained NN and the quantized model, leading to considerable quantization error for PTQ. However, it is unclear how to efficiently train a model-agnostic neural network which is tailored for a predefined precision low-bit model. In this paper, we firstly discover that a flat full precision neural network is crucial for low-bit quantization. To achieve this, we propose a framework that proactively pre-conditions the model by measuring and disentangling the error sources. Specifically, both the Activation Quantization Error (AQE) and the Weight Quantization Error (WQE) are statistically modeled as independent Gaussian noises. We study several noise injection optimization methods to obtain a flat minimum. Experimental results attest to the effectiveness of our approach. These results open novel pathways for obtaining low-bit PTQ models.</p>
  </div>
</details>

<hr>
<h3 id="118-HMVLM-Human-Motion-Vision-Lanuage-Model-via-MoE-LoRA-cs-CV-cs-AI-cs-GR-68T45-I-2-10-I-3-7PDF"><a href="#118-HMVLM-Human-Motion-Vision-Lanuage-Model-via-MoE-LoRA-cs-CV-cs-AI-cs-GR-68T45-I-2-10-I-3-7PDF" class="headerlink" title="[118] HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA cs.CV | cs.AI | cs.GR | 68T45 | I.2.10; I.3.7PDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01463">HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.GR | 68T45 | I.2.10; I.3.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01463" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lei Hu, Yongjing Ye, Shihong Xia</span></p>
<p><strong>TL;DR:</strong> HMVLM提出了一种基于MoE LoRA的统一框架，通过动态分配专家权重和引入零专家缓解模态差异和灾难性遗忘问题，同时利用身体部位特定的分词提升姿势表示效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态基础模型在指令调优数据扩展下表现出色，但3D人体运动模态与文本间的模态差异导致集成时的灾难性遗忘问题；同时，通用姿势表示在异构下游任务中的泛化性仍是一个技术挑战。</p>
<p><strong>Result:</strong> 实验表明，HMVLM有效缓解了知识遗忘问题，并在多种人体运动下游任务中表现优异。</p>
<p><strong>Insight:</strong> 动态专家权重分配和任务特定表示设计是解决多模态集成中模态差异和泛化问题的有效途径。</p>
<p><strong>Abstract:</strong> The expansion of instruction-tuning data has enabled foundation language models to exhibit improved instruction adherence and superior performance across diverse downstream tasks. Semantically-rich 3D human motion is being progressively integrated with these foundation models to enhance multimodal understanding and cross-modal generation capabilities. However, the modality gap between human motion and text raises unresolved concerns about catastrophic forgetting during this integration. In addition, developing autoregressive-compatible pose representations that preserve generalizability across heterogeneous downstream tasks remains a critical technical barrier. To address these issues, we propose the Human Motion-Vision-Language Model (HMVLM), a unified framework based on the Mixture of Expert Low-Rank Adaption(MoE LoRA) strategy. The framework leverages the gating network to dynamically allocate LoRA expert weights based on the input prompt, enabling synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting during instruction-tuning, we introduce a novel zero expert that preserves the pre-trained parameters for general linguistic tasks. For pose representation, we implement body-part-specific tokenization by partitioning the human body into different joint groups, enhancing the spatial resolution of the representation. Experiments show that our method effectively alleviates knowledge forgetting during instruction-tuning and achieves remarkable performance across diverse human motion downstream tasks.</p>
  </div>
</details>

<hr>
<h3 id="119-SecDiff-Diffusion-Aided-Secure-Deep-Joint-Source-Channel-Coding-Against-Adversarial-Attacks-cs-CVPDF"><a href="#119-SecDiff-Diffusion-Aided-Secure-Deep-Joint-Source-Channel-Coding-Against-Adversarial-Attacks-cs-CVPDF" class="headerlink" title="[119] SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks cs.CVPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01466">SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01466" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changyuan Zhao, Jiacheng Wang, Ruichen Zhang, Dusit Niyato, Hongyang Du</span></p>
<p><strong>TL;DR:</strong> SecDiff提出了一种基于扩散模型的插件式解码框架，显著提升了深度联合源信道编码（JSCC）在对抗性无线环境中的安全性和鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有深度JSCC框架在对抗性攻击（如导频欺骗和子载波干扰）下表现脆弱，影响语义保真度。SecDiff旨在解决这一安全性和鲁棒性问题。</p>
<p><strong>Result:</strong> 在对抗性OFDM信道环境下，SecDiff在重建质量和计算成本之间取得了优越的平衡，显著优于现有基线方法。</p>
<p><strong>Insight:</strong> SecDiff展示了扩散模型在提升语义通信安全性和鲁棒性方面的潜力，同时通过优化采样策略和联合变量优化降低了计算开销。</p>
<p><strong>Abstract:</strong> Deep joint source-channel coding (JSCC) has emerged as a promising paradigm for semantic communication, delivering significant performance gains over conventional separate coding schemes. However, existing JSCC frameworks remain vulnerable to physical-layer adversarial threats, such as pilot spoofing and subcarrier jamming, compromising semantic fidelity. In this paper, we propose SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly enhances the security and robustness of deep JSCC under adversarial wireless environments. Different from prior diffusion-guided JSCC methods that suffer from high inference latency, SecDiff employs pseudoinverse-guided sampling and adaptive guidance weighting, enabling flexible step-size control and efficient semantic reconstruction. To counter jamming attacks, we introduce a power-based subcarrier masking strategy and recast recovery as a masked inpainting problem, solved via diffusion guidance. For pilot spoofing, we formulate channel estimation as a blind inverse problem and develop an expectation-minimization (EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and a channel operator. Notably, our method alternates between pilot recovery and channel estimation, enabling joint refinement of both variables throughout the diffusion process. Extensive experiments over orthogonal frequency-division multiplexing (OFDM) channels under adversarial conditions show that SecDiff outperforms existing secure and generative JSCC baselines by achieving a favorable trade-off between reconstruction quality and computational cost. This balance makes SecDiff a promising step toward practical, low-latency, and attack-resilient semantic communications.</p>
  </div>
</details>

<hr>
<h3 id="120-EPAN-Robust-Pedestrian-Re-Identification-via-Enhanced-Alignment-Network-for-IoT-Surveillance-cs-CVPDF"><a href="#120-EPAN-Robust-Pedestrian-Re-Identification-via-Enhanced-Alignment-Network-for-IoT-Surveillance-cs-CVPDF" class="headerlink" title="[120] EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance cs.CVPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01498">EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01498" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiyang Jia, Hongyan Cui, Ge Gao, Bo Li, Minjie Zhang</span></p>
<p><strong>TL;DR:</strong> EPAN是一种改进的行人重识别网络，通过双分支结构和增强对齐技术在多变的物联网监控场景中实现鲁棒的ReID性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在物联网监控场景中，视角和环境变化对行人重识别（ReID）的准确性造成显著挑战，需要一种更鲁棒的解决方案。</p>
<p><strong>Result:</strong> 在Inspection-Personnel数据集上，EPAN取得90.09%的Rank-1准确率和78.82%的mAP，表现优异。</p>
<p><strong>Insight:</strong> EPAN的架构设计能够有效应对监控场景中的视角和环境变化，为物联网ReID提供实用解决方案。</p>
<p><strong>Abstract:</strong> Person re-identification (ReID) plays a pivotal role in computer vision, particularly in surveillance and security applications within IoT-enabled smart environments. This study introduces the Enhanced Pedestrian Alignment Network (EPAN), tailored for robust ReID across diverse IoT surveillance conditions. EPAN employs a dual-branch architecture to mitigate the impact of perspective and environmental changes, extracting alignment information under varying scales and viewpoints. Here, we demonstrate EPAN’s strong feature extraction capabilities, achieving outstanding performance on the Inspection-Personnel dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of 78.82%. This highlights EPAN’s potential for real-world IoT applications, enabling effective and reliable person ReID across diverse cameras in surveillance and security systems. The code and data are available at: <a target="_blank" rel="noopener" href="https://github.com/ggboy2580/EPAN">https://github.com/ggboy2580/EPAN</a></p>
  </div>
</details>

<hr>
<h3 id="121-SE-3-PoseFlow-Estimating-6D-Pose-Distributions-for-Uncertainty-Aware-Robotic-Manipulation-cs-CV-cs-ROPDF"><a href="#121-SE-3-PoseFlow-Estimating-6D-Pose-Distributions-for-Uncertainty-Aware-Robotic-Manipulation-cs-CV-cs-ROPDF" class="headerlink" title="[121] SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation cs.CV | cs.ROPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01501">SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01501" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yufeng Jin, Niklas Funk, Vignesh Prasad, Zechu Li, Mathias Franzius</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了SE(3)-PoseFlow，一种用于估计6D姿态分布的概率框架，通过SE(3)流匹配处理姿态模糊和多峰分布问题，在多个数据集上达到SOTA，并在机器人操作任务中展现了不确定性感知的应用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 物体姿态估计是机器人和计算机视觉中的基础问题，但由于部分可观测性、遮挡和对称性等因素，姿态估计往往存在模糊性。现有确定性深度网络难以捕捉姿态的多峰分布，并且在不确定性情况下表现不佳。为解决这些问题，论文提出了一种概率框架。</p>
<p><strong>Result:</strong> 在Real275、YCB-V和LM-O数据集上达到了SOTA性能；在机器人任务中验证了其在主动感知和抓取合成中的实用性。</p>
<p><strong>Insight:</strong> 通过概率建模姿态分布，能够更全面地处理姿态模糊问题；SE(3)流匹配为6D姿态估计提供了一种灵活且高效的方法。</p>
<p><strong>Abstract:</strong> Object pose estimation is a fundamental problem in robotics and computer vision, yet it remains challenging due to partial observability, occlusions, and object symmetries, which inevitably lead to pose ambiguity and multiple hypotheses consistent with the same observation. While deterministic deep networks achieve impressive performance under well-constrained conditions, they are often overconfident and fail to capture the multi-modality of the underlying pose distribution. To address these challenges, we propose a novel probabilistic framework that leverages flow matching on the SE(3) manifold for estimating 6D object pose distributions. Unlike existing methods that regress a single deterministic output, our approach models the full pose distribution with a sample-based estimate and enables reasoning about uncertainty in ambiguous cases such as symmetric objects or severe occlusions. We achieve state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our sample-based pose estimates can be leveraged in downstream robotic manipulation tasks such as active perception for disambiguating uncertain viewpoints or guiding grasp synthesis in an uncertainty-aware manner.</p>
  </div>
</details>

<hr>
<h3 id="122-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning-cs-CV-cs-ROPDF"><a href="#122-Discriminately-Treating-Motion-Components-Evolves-Joint-Depth-and-Ego-Motion-Learning-cs-CV-cs-ROPDF" class="headerlink" title="[122] Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning cs.CV | cs.ROPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01502">Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01502" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种差异化的运动组件处理方法，通过利用刚性流的几何规律性改进深度和自身运动估计。提出的DiMoDE框架在多个数据集上达到了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的无监督深度和自身运动学习方法通常将自身运动视为辅助任务，未能充分挖掘不同运动组件的几何约束，影响了模型的可靠性和鲁棒性。</p>
<p><strong>Result:</strong> DiMoDE在多个公共数据集和新的多样化真实世界数据集上表现优异，特别是在挑战性条件下。</p>
<p><strong>Insight:</strong> 差异化的运动组件处理和闭合几何关系的引入显著提升了深度和自身运动估计的鲁棒性和可靠性。</p>
<p><strong>Abstract:</strong> Unsupervised learning of depth and ego-motion, two fundamental 3D perception tasks, has made significant strides in recent years. However, most methods treat ego-motion as an auxiliary task, either mixing all motion types or excluding depth-independent rotational motions in supervision. Such designs limit the incorporation of strong geometric constraints, reducing reliability and robustness under diverse conditions. This study introduces a discriminative treatment of motion components, leveraging the geometric regularities of their respective rigid flows to benefit both depth and ego-motion estimation. Given consecutive video frames, network outputs first align the optical axes and imaging planes of the source and target cameras. Optical flows between frames are transformed through these alignments, and deviations are quantified to impose geometric constraints individually on each ego-motion component, enabling more targeted refinement. These alignments further reformulate the joint learning process into coaxial and coplanar forms, where depth and each translation component can be mutually derived through closed-form geometric relationships, introducing complementary constraints that improve depth robustness. DiMoDE, a general depth and ego-motion joint learning framework incorporating these designs, achieves state-of-the-art performance on multiple public datasets and a newly collected diverse real-world dataset, particularly under challenging conditions. Our source code will be publicly available at mias.group&#x2F;DiMoDE upon publication.</p>
  </div>
</details>

<hr>
<h3 id="123-Example-Based-Feature-Painting-on-Textures-cs-CV-cs-GRPDF"><a href="#123-Example-Based-Feature-Painting-on-Textures-cs-CV-cs-GRPDF" class="headerlink" title="[123] Example-Based Feature Painting on Textures cs.CV | cs.GRPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01513">Example-Based Feature Painting on Textures</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01513" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Andrei-Timotei Ardelean, Tim Weyrich</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于学习的纹理编辑系统，能够通过无监督异常检测和自动聚类，实现纹理的局部特征控制和交互式编辑。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 为了在纹理合成过程中更自然地模拟真实材料的表面变化（如污渍、撕裂、变色等），作者提出了一种无需手动标注的系统。</p>
<p><strong>Result:</strong> 提出的系统能够高效生成包含多样化局部特征的纹理，并通过用户交互实现精确控制。代码和项目页面已公开。</p>
<p><strong>Insight:</strong> 无监督方法可以显著减少纹理编辑中的人工标注负担，而扩散模型在生成和编辑任务中表现出强大的灵活性。</p>
<p><strong>Abstract:</strong> In this work, we propose a system that covers the complete workflow for achieving controlled authoring and editing of textures that present distinctive local characteristics. These include various effects that change the surface appearance of materials, such as stains, tears, holes, abrasions, discoloration, and more. Such alterations are ubiquitous in nature, and including them in the synthesis process is crucial for generating realistic textures. We introduce a novel approach for creating textures with such blemishes, adopting a learning-based approach that leverages unlabeled examples. Our approach does not require manual annotations by the user; instead, it detects the appearance-altering features through unsupervised anomaly detection. The various textural features are then automatically clustered into semantically coherent groups, which are used to guide the conditional generation of images. Our pipeline as a whole goes from a small image collection to a versatile generative model that enables the user to interactively create and paint features on textures of arbitrary size. Notably, the algorithms we introduce for diffusion-based editing and infinite stationary texture generation are generic and should prove useful in other contexts as well. Project page: <a target="_blank" rel="noopener" href="https://reality.tf.fau.de/pub/ardelean2025examplebased.html">https://reality.tf.fau.de/pub/ardelean2025examplebased.html</a></p>
  </div>
</details>

<hr>
<h3 id="124-NSYNC-Negative-Synthetic-Image-Generation-for-Contrastive-Training-to-Improve-Stylized-Text-To-Image-Translation-cs-CVPDF"><a href="#124-NSYNC-Negative-Synthetic-Image-Generation-for-Contrastive-Training-to-Improve-Stylized-Text-To-Image-Translation-cs-CVPDF" class="headerlink" title="[124] NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation cs.CVPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01517">NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01517" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的对比学习框架NSYNC，通过生成负样本合成图像并结合对比训练，提升文本到图像扩散模型在特定风格上的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的文本到图像生成方法虽然能生成逼真图像，但难以捕捉特定风格。直接在目标风格数据集上微调效果有限。本文的动机是通过合成数据和对比学习解决这一问题。</p>
<p><strong>Result:</strong> 实验结果表明，NSYNC在多种画家和插画师风格的数据集上，定性和定量均优于基线方法。</p>
<p><strong>Insight:</strong> 通过生成负样本并结合对比学习，可以更有效地引导模型学习独特风格特征，避免冗余信息的干扰。</p>
<p><strong>Abstract:</strong> Current text conditioned image generation methods output realistic looking images, but they fail to capture specific styles. Simply finetuning them on the target style datasets still struggles to grasp the style features. In this work, we present a novel contrastive learning framework to improve the stylization capability of large text-to-image diffusion models. Motivated by the astonishing advance in image generation models that makes synthetic data an intrinsic part of model training in various computer vision tasks, we exploit synthetic image generation in our approach. Usually, the generated synthetic data is dependent on the task, and most of the time it is used to enlarge the available real training dataset. With NSYNC, alternatively, we focus on generating negative synthetic sets to be used in a novel contrastive training scheme along with real positive images. In our proposed training setup, we forward negative data along with positive data and obtain negative and positive gradients, respectively. We then refine the positive gradient by subtracting its projection onto the negative gradient to get the orthogonal component, based on which the parameters are updated. This orthogonal component eliminates the trivial attributes that are present in both positive and negative data and directs the model towards capturing a more unique style. Experiments on various styles of painters and illustrators show that our approach improves the performance over the baseline methods both quantitatively and qualitatively. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/giddyyupp/NSYNC">https://github.com/giddyyupp/NSYNC</a>.</p>
  </div>
</details>

<hr>
<h3 id="125-Driving-scenario-generation-and-evaluation-using-a-structured-layer-representation-and-foundational-models-cs-CV-cs-AIPDF"><a href="#125-Driving-scenario-generation-and-evaluation-using-a-structured-layer-representation-and-foundational-models-cs-CV-cs-AIPDF" class="headerlink" title="[125] Driving scenario generation and evaluation using a structured layer representation and foundational models cs.CV | cs.AIPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01541">Driving scenario generation and evaluation using a structured layer representation and foundational models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01541" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arthur Hubert, Gamal Elghazaly, Raphaël Frank</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结构化的五层模型，用于改进罕见驾驶场景的生成和评估，结合基础模型生成新场景，并引入多样性和原创性指标量化数据集质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 罕见驾驶场景对自动驾驶开发至关重要，但由于难以实际遇到，因此通过生成模型模拟或生成这些场景成为一种有效方法。</p>
<p><strong>Result:</strong> 论文展示了在不同生成设置下的指标效果，并对结构化场景描述生成的合成视频进行了定性评估。</p>
<p><strong>Insight:</strong> 结构化表示和基础模型的结合可以有效生成多样且贴近现实的驾驶场景，同时量化指标为生成数据集的质量评估提供了新视角。</p>
<p><strong>Abstract:</strong> Rare and challenging driving scenarios are critical for autonomous vehicle development. Since they are difficult to encounter, simulating or generating them using generative models is a popular approach. Following previous efforts to structure driving scenario representations in a layer model, we propose a structured five-layer model to improve the evaluation and generation of rare scenarios. We use this model alongside large foundational models to generate new driving scenarios using a data augmentation strategy. Unlike previous representations, our structure introduces subclasses and characteristics for every agent of the scenario, allowing us to compare them using an embedding specific to our layer-model. We study and adapt two metrics to evaluate the relevance of a synthetic dataset in the context of a structured representation: the diversity score estimates how different the scenarios of a dataset are from one another, while the originality score calculates how similar a synthetic dataset is from a real reference set. This paper showcases both metrics in different generation setup, as well as a qualitative evaluation of synthetic videos generated from structured scenario descriptions. The code and extended results can be found at <a target="_blank" rel="noopener" href="https://github.com/Valgiz/5LMSG">https://github.com/Valgiz/5LMSG</a>.</p>
  </div>
</details>

<hr>
<h3 id="126-PCD-ReID-Occluded-Person-Re-Identification-for-Base-Station-Inspection-cs-CVPDF"><a href="#126-PCD-ReID-Occluded-Person-Re-Identification-for-Base-Station-Inspection-cs-CVPDF" class="headerlink" title="[126] PCD-ReID: Occluded Person Re-Identification for Base Station Inspection cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01546">PCD-ReID: Occluded Person Re-Identification for Base Station Inspection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01546" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ge Gao, Zishuo Gao, Hongyan Cui, Zhiyang Jia, Zhuang Luo</span></p>
<p><strong>TL;DR:</strong> 该论文提出了PCD-ReID算法，用于解决基站巡检中遮挡行人重识别的挑战，通过Transformer网络提取共享组件特征，并在新数据集上验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基站环境中行人重识别的遮挡问题严重影响识别效果，传统ResNet方法难以应对，需要更有效的方法。</p>
<p><strong>Result:</strong> 实验显示，mAP为79.0%，Rank-1准确率为82.7%，比ResNet50方法提升15.9%。</p>
<p><strong>Insight:</strong> Transformer网络在处理遮挡问题时表现出色，共享组件特征提取是关键；新数据集对提升模型泛化能力至关重要。</p>
<p><strong>Abstract:</strong> Occluded pedestrian re-identification (ReID) in base station environments is a critical task in computer vision, particularly for surveillance and security applications. This task faces numerous challenges, as occlusions often obscure key body features, increasing the complexity of identification. Traditional ResNet-based ReID algorithms often fail to address occlusions effectively, necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component Discrepancy) algorithm to address these issues. The contributions of this work are as follows: To tackle the occlusion problem, we design a Transformer-based PCD network capable of extracting shared component features, such as helmets and uniforms. To mitigate overfitting on public datasets, we collected new real-world patrol surveillance images for model training, covering six months, 10,000 individuals, and over 50,000 images. Comparative experiments with existing ReID algorithms demonstrate that our model achieves a mean Average Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1 improvement over ResNet50-based methods. Experimental evaluations indicate that PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in tower inspection scenarios, highlighting its potential for practical deployment in surveillance and security applications.</p>
  </div>
</details>

<hr>
<h3 id="127-PixelVLA-Advancing-Pixel-level-Understanding-in-Vision-Language-Action-Model-cs-CV-cs-ROPDF"><a href="#127-PixelVLA-Advancing-Pixel-level-Understanding-in-Vision-Language-Action-Model-cs-CV-cs-ROPDF" class="headerlink" title="[127] PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model cs.CV | cs.ROPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01571">PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01571" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai</span></p>
<p><strong>TL;DR:</strong> PixelVLA 是一种新型的视觉-语言-动作（VLA）模型，专注于提升像素级场景理解和支持多模态提示（文本与视觉输入），通过一种新的视觉运动指令调优框架和两阶段自动标注流程，显著提升了机器人控制的准确性和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前 VLA 模型存在两大局限性：（1）难以实现像素级场景理解；（2）过度依赖文本提示，限制其在真实环境中的灵活性。PixelVLA 旨在解决这些问题。</p>
<p><strong>Result:</strong> PixelVLA 在三个标准 VLA 基准上比 OpenVLA 提升了 10.1%-17.8% 的操作成功率，同时预训练成本仅为其 1.5%。</p>
<p><strong>Insight:</strong> 像素级理解和多模态提示的结合可以显著提升 VLA 模型在实际应用中的灵活性和准确性，同时降低数据标注和训练成本。</p>
<p><strong>Abstract:</strong> Vision-Language-Action models (VLAs) are emerging as powerful tools for learning generalizable visuomotor control policies. However, current VLAs are mostly trained on large-scale image-text-action data and remain limited in two key ways: (i) they struggle with pixel-level scene understanding, and (ii) they rely heavily on textual prompts, which reduces their flexibility in real-world settings. To address these challenges, we introduce PixelVLA, the first VLA model designed to support both pixel-level reasoning and multimodal prompting with text and visual inputs. Our approach is built on a new visuomotor instruction tuning framework that integrates a multiscale pixel-aware encoder with a visual prompting encoder. To train PixelVLA effectively, we further propose a two-stage automated annotation pipeline that generates Pixel-160K, a large-scale dataset with pixel-level annotations derived from existing robot data. Experiments on three standard VLA benchmarks and two VLA model variants show that PixelVLA improves manipulation success rates by 10.1%-17.8% over OpenVLA, while requiring only 1.5% of its pretraining cost. These results demonstrate that PixelVLA can be integrated into existing VLAs to enable more accurate, efficient, and versatile robot control in complex environments. The dataset and code will be released as open source.</p>
  </div>
</details>

<hr>
<h3 id="128-Generative-Adversarial-Synthesis-and-Deep-Feature-Discrimination-of-Brain-Tumor-MRI-Images-cs-CVPDF"><a href="#128-Generative-Adversarial-Synthesis-and-Deep-Feature-Discrimination-of-Brain-Tumor-MRI-Images-cs-CVPDF" class="headerlink" title="[128] Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images cs.CVPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01574">Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01574" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Sumon Ali, Muzammil Behzad</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于深度卷积生成对抗网络（DC-GAN）的方法，用于生成合成脑肿瘤MRI图像，并通过CNN分类器验证其质量。实验结果显示合成图像在分类任务中表现与真实图像相当。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在医学影像领域，尤其是MRI数据获取受限的情况下，生成合成数据是一项重要任务。传统的深度学习方法在这方面存在挑战，而GANs提供了一种潜在的解决方案。</p>
<p><strong>Result:</strong> 合成图像在分类任务中表现与真实图像相当，验证了GAN生成图像的有效性。</p>
<p><strong>Insight:</strong> GAN生成的医学影像可以缓解数据不足的问题，且生成的图像质量足以支持下游任务。</p>
<p><strong>Abstract:</strong> Compared to traditional methods, Deep Learning (DL) becomes a key technology for computer vision tasks. Synthetic data generation is an interesting use case for DL, especially in the field of medical imaging such as Magnetic Resonance Imaging (MRI). The need for this task since the original MRI data is limited. The generation of realistic medical images is completely difficult and challenging. Generative Adversarial Networks (GANs) are useful for creating synthetic medical images. In this paper, we propose a DL based methodology for creating synthetic MRI data using the Deep Convolutional Generative Adversarial Network (DC-GAN) to address the problem of limited data. We also employ a Convolutional Neural Network (CNN) classifier to classify the brain tumor using synthetic data and real MRI data. CNN is used to evaluate the quality and utility of the synthetic images. The classification result demonstrates comparable performance on real and synthetic images, which validates the effectiveness of GAN-generated images for downstream tasks.</p>
  </div>
</details>

<hr>
<h3 id="129-Wave-Particle-Continuous-Discrete-Dualistic-Visual-Tokenization-for-Unified-Understanding-and-Generation-cs-CVPDF"><a href="#129-Wave-Particle-Continuous-Discrete-Dualistic-Visual-Tokenization-for-Unified-Understanding-and-Generation-cs-CVPDF" class="headerlink" title="[129] Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation cs.CVPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01593">Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01593" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yizhu Chen, Chen Ju, Zhicheng Wang, Shuai Xiao, Xu Chen</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种连续-离散双重视觉标记化方法（CDD-VT），通过自适应分配图像基元数量来解决统一理解和生成的挑战，结合了连续和离散标记化的优点。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有多模态大模型（MLLM）在统一理解和生成任务中面临连续标记化和离散标记化的二分法问题，前者工程复杂，后者信息损失严重。</p>
<p><strong>Result:</strong> 在重建、检索和分类任务上表现优于专用连续和离散标记化方法，验证了CDD-VT的有效性。</p>
<p><strong>Insight:</strong> 将视觉数据视为基元的灵活组合，并根据复杂度动态分配基元数量，解决了传统方法的局限，为统一理解和生成提供了新思路。</p>
<p><strong>Abstract:</strong> The unification of understanding and generation within a single multi-modal large model (MLLM) remains one significant challenge, largely due to the dichotomy between continuous and discrete visual tokenizations. Continuous tokenizer (CT) achieves strong performance by bridging multiple independently-trained understanding modules and generation modules, but suffers from complex multi-stage pipelines and substantial engineering overhead. Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by quantizing each image into a primitive, but inevitably leading to information loss and performance degradation. To resolve this tension, we question the binary choice between CT and DT, inspired by the wave-particle duality of light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT). We treat visual data as a flexible composition of image primitives derived from quantized codebooks, with the crucial insight that the primitive number assigned to each visual sample is adaptively determined according to its complexity: simple instances use a few primitives, emulating discrete tokenization, while complex instances use many, approximating continuous tokenization. Two core components are designed: Diverse Quantitative Primitives, which encourage primitives orthogonality to better populate information space, and Dynamic Primitive Allocator, which assesses sample complexity to determine the optimal set of primitives. Extensive experiments on reconstruction, retrieval and classification show that CDD-VT achieves superior performance over to specialized CT and DT, effectively getting strong result within a concise and scalable MLLM.</p>
  </div>
</details>

<hr>
<h3 id="130-DINO-MX-A-Modular-Flexible-Framework-for-Self-Supervised-Learning-cs-CV-cs-AIPDF"><a href="#130-DINO-MX-A-Modular-Flexible-Framework-for-Self-Supervised-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[130] DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning cs.CV | cs.AIPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01610">DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01610" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mahmut Selman Gokmen, Cody Bumgardner</span></p>
<p><strong>TL;DR:</strong> DINO-MX是一个模块化、灵活的框架，结合了DINO系列的核心理念，支持多种Transformer架构和Hugging Face生态，降低了计算成本并提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有自监督学习框架存在灵活性不足、领域专用或计算成本高的问题，限制了跨领域和资源设置的适用性。</p>
<p><strong>Result:</strong> 在多数据集上展示了竞争性性能，显著降低了计算成本，并通过标签引导的数据增强提升了注意力定位能力。</p>
<p><strong>Insight:</strong> 模块化和灵活的设计是未来自监督学习框架的重要方向，能够适应多样化的应用需求。</p>
<p><strong>Abstract:</strong> Vision Foundation Models (VFMs) have advanced representation learning through self-supervised methods. However, existing training pipelines are often inflexible, domain-specific, or computationally expensive, which limits their usability across different domains and resource settings. DINO-MX is a modular and extensible training framework that combines the core principles of DINO, DINOv2 and DINOv3 within a unified configuration-driven system. It supports a variety of transformer-based architectures and is fully compatible with the Hugging Face ecosystem. The framework includes multiple training strategies such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation, along with support for distributed training through both Distributed Data Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to work with both natural and specialized data types, including single- and multi-channel images. Experimental results on diverse datasets show that DINO-MX achieves competitive performance while significantly reducing computational costs. Additionally, it offers interpretability tools and a label-guided data augmentation method that improves attention-based localization without the need for extra detection or segmentation heads. DINO-MX provides a reproducible and scalable foundation for developing, adapting, and benchmarking self-supervised vision models across a range of research and real-world applications.</p>
  </div>
</details>

<hr>
<h3 id="131-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers-cs-CV-cs-IRPDF"><a href="#131-Vote-in-Context-Turning-VLMs-into-Zero-Shot-Rank-Fusers-cs-CV-cs-IRPDF" class="headerlink" title="[131] Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers cs.CV | cs.IRPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01617">Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01617" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohamed Eltahir, Ali Habibullah, Lama Ayash, Tanveer Hussain, Naeemullah Khan</span></p>
<p><strong>TL;DR:</strong> ViC是一种无需训练的框架，将视觉语言模型（VLM）用于零样本重排序和融合任务，通过内容证据和检索器元数据的序列化提示，显著提升视频检索性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统检索融合方法仅依赖排名或分数信号，忽略候选内容表示，导致复杂多模态数据（如视频）检索效果受限。ViC旨在利用VLM的上下文推理能力解决这一问题。</p>
<p><strong>Result:</strong> 在MSR-VTT和VATEX等基准上，ViC零样本设置下Recall@1分别达87.1%&#x2F;89.0%和99.6%，比之前方法提升高达+40分。</p>
<p><strong>Insight:</strong> 利用VLM的上下文能力，无需微调即可实现复杂多模态数据的有效融合和重排序。</p>
<p><strong>Abstract:</strong> In the retrieval domain, candidates’ fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates’ representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM’s prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) &#x2F; 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: <a target="_blank" rel="noopener" href="https://github.com/mohammad2012191/ViC">https://github.com/mohammad2012191/ViC</a></p>
  </div>
</details>

<hr>
<h3 id="132-Enhancing-Diffusion-based-Restoration-Models-via-Difficulty-Adaptive-Reinforcement-Learning-with-IQA-Reward-cs-CVPDF"><a href="#132-Enhancing-Diffusion-based-Restoration-Models-via-Difficulty-Adaptive-Reinforcement-Learning-with-IQA-Reward-cs-CVPDF" class="headerlink" title="[132] Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward cs.CVPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01645">Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01645" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaogang Xu, Ruihang Chu, Jian Wang, Kun Zhou, Wenjie Shu</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种基于强化学习的扩散模型修复方法，通过适应性地结合图像质量评估（IQA）奖励和动态加权策略，提升了修复任务的效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的强化学习方法直接应用于扩散模型修复任务效果不佳，因为修复任务更注重保真度而非单纯的生成任务。因此，有必要探索如何有效地将强化学习整合到基于扩散模型的修复任务中。</p>
<p><strong>Result:</strong> 在多个修复任务基准测试中，所提出的方法显著提升了性能，验证了其有效性。</p>
<p><strong>Insight:</strong> 1. IQA模型更适合作为修复任务的奖励函数。2. 动态加权策略能够自适应地优化不同难度的样本。3. 该方法为扩散模型在其他保真度要求高的任务中的应用提供了新思路。</p>
<p><strong>Abstract:</strong> Reinforcement Learning (RL) has recently been incorporated into diffusion models, e.g., tasks such as text-to-image. However, directly applying existing RL methods to diffusion-based image restoration models is suboptimal, as the objective of restoration fundamentally differs from that of pure generation: it places greater emphasis on fidelity. In this paper, we investigate how to effectively integrate RL into diffusion-based restoration models. First, through extensive experiments with various reward functions, we find that an effective reward can be derived from an Image Quality Assessment (IQA) model, instead of intuitive ground-truth-based supervision, which has already been optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover, our strategy focuses on using RL for challenging samples that are significantly distant from the ground truth, and our RL approach is innovatively implemented using MLLM-based IQA models to align distributions with high-quality images initially. As the samples approach the ground truth’s distribution, RL is adaptively combined with SFT for more fine-grained alignment. This dynamic process is facilitated through an automatic weighting strategy that adjusts based on the relative difficulty of the training samples. Our strategy is plug-and-play that can be seamlessly applied to diffusion-based restoration models, boosting its performance across various restoration tasks. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed RL framework.</p>
  </div>
</details>

<hr>
<h3 id="133-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback-cs-CVPDF"><a href="#133-UniLumos-Fast-and-Unified-Image-and-Video-Relighting-with-Physics-Plausible-Feedback-cs-CVPDF" class="headerlink" title="[133] UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback cs.CVPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01678">UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01678" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ropeway Liu, Hangjie Yuan, Bo Dong, Jiazheng Xing, Jinwang Wang</span></p>
<p><strong>TL;DR:</strong> UniLumos提出了一种统一的图像和视频重光照框架，通过引入RGB空间的几何反馈和路径一致性学习，显著提升了物理合理性，并在速度和效果上实现了突破。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于扩散模型的重光照方法在语义潜在空间中优化，常导致物理不合理的结果，如过曝高光和错位阴影。UniLumos旨在解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，UniLumos在重光照质量和物理一致性上达到SOTA，同时速度提升了20倍。</p>
<p><strong>Insight:</strong> 将几何反馈与流匹配结合是提升重光照物理合理性的有效途径，路径一致性学习为高效训练提供了新思路。</p>
<p><strong>Abstract:</strong> Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/Lumos-Custom">https://github.com/alibaba-damo-academy/Lumos-Custom</a>.</p>
  </div>
</details>

<hr>
<h3 id="134-Learnable-Fractional-Reaction-Diffusion-Dynamics-for-Under-Display-ToF-Imaging-and-Beyond-cs-CVPDF"><a href="#134-Learnable-Fractional-Reaction-Diffusion-Dynamics-for-Under-Display-ToF-Imaging-and-Beyond-cs-CVPDF" class="headerlink" title="[134] Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond cs.CVPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01704">Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01704" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Qiao, Matteo Poggi, Xing Wei, Pengchao Deng, Yanhui Zhou</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种可学习的分数阶反应-扩散动力学模型（LFRD2），用于解决屏下ToF成像中的深度感知退化问题，结合了神经网络的表达能力和物理模型的可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 屏下ToF成像由于透明OLED层引入的信号衰减、多路径干扰和时序噪声等问题，导致深度感知质量严重下降。</p>
<p><strong>Result:</strong> 在四个基准数据集上的实验验证了方法的有效性。</p>
<p><strong>Insight:</strong> 结合物理模型与深度学习能够有效解决屏下ToF成像中的复杂退化问题，同时保持模型的可解释性。</p>
<p><strong>Abstract:</strong> Under-display ToF imaging aims to achieve accurate depth sensing through a ToF camera placed beneath a screen panel. However, transparent OLED (TOLED) layers introduce severe degradations-such as signal attenuation, multi-path interference (MPI), and temporal noise-that significantly compromise depth quality. To alleviate this drawback, we propose Learnable Fractional Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the expressive power of neural networks with the interpretability of physical modeling. Specifically, we implement a time-fractional reaction-diffusion module that enables iterative depth refinement with dynamically generated differential orders, capturing long-term dependencies. In addition, we introduce an efficient continuous convolution operator via coefficient prediction and repeated differentiation to further improve restoration quality. Experiments on four benchmark datasets demonstrate the effectiveness of our approach. The code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/wudiqx106/LFRD2">https://github.com/wudiqx106/LFRD2</a>.</p>
  </div>
</details>

<hr>
<h3 id="135-Toward-Strategy-Identification-and-Subtask-Decomposition-In-Task-Exploration-cs-CVPDF"><a href="#135-Toward-Strategy-Identification-and-Subtask-Decomposition-In-Task-Exploration-cs-CVPDF" class="headerlink" title="[135] Toward Strategy Identification and Subtask Decomposition In Task Exploration cs.CVPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01728">Toward Strategy Identification and Subtask Decomposition In Task Exploration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01728" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tom Odem</span></p>
<p><strong>TL;DR:</strong> 该研究开发了一种任务探索流水线，利用聚类技术、因子分析和字符串编辑距离自动识别完成任务的关键全局和局部策略，同时识别任务中的有意义子任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目标是提升机器对用户知识、技能和行为的理解，以实现隐式协调。</p>
<p><strong>Result:</strong> 流水线成功识别了关键策略，并开发了Task Explorer应用以可视化结果。</p>
<p><strong>Insight:</strong> 该方法适用于任何基于动作的时间序列数据，对理解用户行为和技能有重要意义。</p>
<p><strong>Abstract:</strong> This research builds on work in anticipatory human-machine interaction, a subfield of human-machine interaction where machines can facilitate advantageous interactions by anticipating a user’s future state. The aim of this research is to further a machine’s understanding of user knowledge, skill, and behavior in pursuit of implicit coordination. A task explorer pipeline was developed that uses clustering techniques, paired with factor analysis and string edit distance, to automatically identify key global and local strategies that are used to complete tasks. Global strategies identify generalized sets of actions used to complete tasks, while local strategies identify sequences that used those sets of actions in a similar composition. Additionally, meaningful subtasks of various lengths are identified within the tasks. The task explorer pipeline was able to automatically identify key strategies used to complete tasks and encode user runs with hierarchical subtask structures. In addition, a Task Explorer application was developed to easily review pipeline results. The task explorer pipeline can be easily modified to any action-based time-series data and the identified strategies and subtasks help to inform humans and machines on user knowledge, skill, and behavior.</p>
  </div>
</details>

<hr>
<h3 id="136-3EED-Ground-Everything-Everywhere-in-3D-cs-CV-cs-ROPDF"><a href="#136-3EED-Ground-Everything-Everywhere-in-3D-cs-CV-cs-ROPDF" class="headerlink" title="[136] 3EED: Ground Everything Everywhere in 3D cs.CV | cs.ROPDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01755">3EED: Ground Everything Everywhere in 3D</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01755" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rong Li, Yuhao Dong, Tianshuai Hu, Ao Liang, Youquan Liu</span></p>
<p><strong>TL;DR:</strong> 3EED是一个多平台、多模态的3D视觉定位基准，覆盖车辆、无人机和四足机器平台，提供大规模室外场景数据和高质量的标注，支持跨平台学习评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有3D视觉定位基准集中于室内场景且规模较小，无法满足开放世界环境下的需求，3EED旨在解决这一问题。</p>
<p><strong>Result:</strong> 3EED数据集规模是现有基准的10倍，实验揭示了泛化性3D定位的挑战与机遇。</p>
<p><strong>Insight:</strong> 跨平台数据多样性和高质量标注对3D视觉定位至关重要，提出的方法为开放世界语言驱动的3D感知研究提供了基础。</p>
<p><strong>Abstract:</strong> Visual grounding in 3D is the key for embodied agents to localize language-referred objects in open-world environments. However, existing benchmarks are limited to indoor focus, single-platform constraints, and small scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We provide over 128,000 objects and 22,000 validated referring expressions across diverse outdoor scenes – 10x larger than existing datasets. We develop a scalable annotation pipeline combining vision-language model prompting with human verification to ensure high-quality spatial grounding. To support cross-platform learning, we propose platform-aware normalization and cross-modal alignment techniques, and establish benchmark protocols for in-domain and cross-platform evaluations. Our findings reveal significant performance gaps, highlighting the challenges and opportunities of generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released to advance future research in language-driven 3D embodied perception.</p>
  </div>
</details>

<hr>
<h3 id="137-HGFreNet-Hop-hybrid-GraphFomer-for-3D-Human-Pose-Estimation-with-Trajectory-Consistency-in-Frequency-Domain-cs-CVPDF"><a href="#137-HGFreNet-Hop-hybrid-GraphFomer-for-3D-Human-Pose-Estimation-with-Trajectory-Consistency-in-Frequency-Domain-cs-CVPDF" class="headerlink" title="[137] HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain cs.CVPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01756">HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01756" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kai Zhai, Ziyan Huang, Qiang Nie, Xiang Li, Bo Ouyang</span></p>
<p><strong>TL;DR:</strong> HGFreNet通过引入跳数混合图注意力和频域轨迹一致性方法，提升了3D人体姿态估计的准确性和时间一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 2D到3D的人体姿态提升存在深度模糊性和2D姿态估计误差，导致3D轨迹不连贯。传统方法仅限制时域抖动，忽略了骨骼关节运动的全局时空相关性。</p>
<p><strong>Result:</strong> 在Human3.6M和MPI-INF-3DHP数据集上，HGFreNet在位置准确性和时间一致性上优于SOTA方法。</p>
<p><strong>Insight:</strong> 频域约束和跳数混合注意力有效提升了3D姿态估计的性能，频域方法可能成为解决时间一致性问题的新方向。</p>
<p><strong>Abstract:</strong> 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose estimation in monocular video, where graph convolutional networks (GCNs) and attention mechanisms have proven to be inherently suitable for encoding the spatial-temporal correlations of skeletal joints. However, depth ambiguity and errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous studies have attempted to restrict jitters in the time domain, for instance, by constraining the differences between adjacent frames while neglecting the global spatial-temporal correlations of skeletal joint motion. To tackle this problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid feature aggregation and 3D trajectory consistency in the frequency domain. Specifically, we propose a hop-hybrid graph attention (HGA) module and a Transformer encoder to model global joint spatial-temporal correlations. The HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group to enlarge the receptive field and applies the attention mechanism to discover the latent correlations of these groups globally. We then exploit global temporal correlations by constraining trajectory consistency in the frequency domain. To provide 3D information for depth inference across frames and maintain coherence over time, a preliminary network is applied to estimate the 3D pose. Extensive experiments were conducted on two standard benchmark datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional accuracy and temporal consistency.</p>
  </div>
</details>

<hr>
<h3 id="138-UniLION-Towards-Unified-Autonomous-Driving-Model-with-Linear-Group-RNNs-cs-CVPDF"><a href="#138-UniLION-Towards-Unified-Autonomous-Driving-Model-with-Linear-Group-RNNs-cs-CVPDF" class="headerlink" title="[138] UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs cs.CVPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01768">UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01768" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao</span></p>
<p><strong>TL;DR:</strong> UniLION提出了一种基于线性组RNN的统一自动驾驶模型，高效处理LiDAR点云、多视角图像和时序数据，无需显式的时序或多模态融合模块，同时在多项核心任务中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管Transformer在多个领域表现出色，但其二次注意力机制在处理长序列数据时计算开销大，限制了在自动驾驶多模态和多任务场景中的应用。</p>
<p><strong>Result:</strong> 在3D感知、预测和规划等核心任务中，UniLION表现优异甚至达到SOTA水平。</p>
<p><strong>Insight:</strong> UniLION为自动驾驶3D基础模型的开发提供了新思路，展示了统一架构在高性能多任务系统中的潜力。</p>
<p><strong>Abstract:</strong> Although transformers have demonstrated remarkable capabilities across various domains, their quadratic attention mechanisms introduce significant computational overhead when processing long-sequence data. In this paper, we present a unified autonomous driving model, UniLION, which efficiently handles large-scale LiDAR point clouds, high-resolution multi-view images, and even temporal sequences based on the linear group RNN operator (i.e., performs linear RNN for grouped features). Remarkably, UniLION serves as a single versatile architecture that can seamlessly support multiple specialized variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal temporal fusion configurations) without requiring explicit temporal or multi-modal fusion modules. Moreover, UniLION consistently delivers competitive and even state-of-the-art performance across a wide range of core tasks, including 3D perception (e.g., 3D object detection, 3D object tracking, 3D occupancy prediction, BEV map segmentation), prediction (e.g., motion prediction), and planning (e.g., end-to-end planning). This unified paradigm naturally simplifies the design of multi-modal and multi-task autonomous driving systems while maintaining superior performance. Ultimately, we hope UniLION offers a fresh perspective on the development of 3D foundation models in autonomous driving. Code is available at <a target="_blank" rel="noopener" href="https://github.com/happinesslz/UniLION">https://github.com/happinesslz/UniLION</a></p>
  </div>
</details>

<hr>
<h3 id="139-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment-cs-CV-cs-AI-cs-MMPDF"><a href="#139-How-Far-Are-Surgeons-from-Surgical-World-Models-A-Pilot-Study-on-Zero-shot-Surgical-Video-Generation-with-Expert-Assessment-cs-CV-cs-AI-cs-MMPDF" class="headerlink" title="[139] How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment cs.CV | cs.AI | cs.MMPDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01775">How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01775" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了首个专家评估的手术视频生成基准SurgVeo和四层框架SPP，用于评估模型在手术领域的表现。研究发现，尽管Veo-3模型在视觉感知上表现优异，但在高级手术逻辑上存在明显不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 手术视频生成模型在高风险医疗领域的需求尚未被充分探索，且缺乏专门评估其在手术中表现的工具。</p>
<p><strong>Result:</strong> 结果显示，Veo-3在视觉感知上表现优秀，但在器械操作、环境反馈和手术意图等高层次合理性上表现不佳，存在所谓的“合理性差距”。</p>
<p><strong>Insight:</strong> 研究揭示了视觉逼真性与因果理解能力之间的差距，为未来开发专用于手术等复杂医疗领域的AI模型提供了重要参考。</p>
<p><strong>Abstract:</strong> Foundation models in video generation are demonstrating remarkable capabilities as potential world models for simulating the physical world. However, their application in high-stakes domains like surgery, which demand deep, specialized causal knowledge rather than general physical rules, remains a critical unexplored gap. To systematically address this challenge, we present SurgVeo, the first expert-curated benchmark for video generation model evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel, four-tiered framework tailored to assess model outputs from basic appearance to complex surgical strategy. On the basis of the SurgVeo benchmark, we task the advanced Veo-3 model with a zero-shot prediction task on surgical clips from laparoscopic and neurosurgical procedures. A panel of four board-certified surgeons evaluates the generated videos according to the SPP. Our results reveal a distinct “plausibility gap”: while Veo-3 achieves exceptional Visual Perceptual Plausibility, it fails critically at higher levels of the SPP, including Instrument Operation Plausibility, Environment Feedback Plausibility, and Surgical Intent Plausibility. This work provides the first quantitative evidence of the chasm between visually convincing mimicry and causal understanding in surgical AI. Our findings from SurgVeo and the SPP establish a crucial foundation and roadmap for developing future models capable of navigating the complexities of specialized, real-world healthcare domains.</p>
  </div>
</details>

<hr>
<h3 id="140-PROPEX-RAG-Enhanced-GraphRAG-using-Prompt-Driven-Prompt-Execution-cs-CVPDF"><a href="#140-PROPEX-RAG-Enhanced-GraphRAG-using-Prompt-Driven-Prompt-Execution-cs-CVPDF" class="headerlink" title="[140] PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution cs.CVPDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01802">PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01802" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tejas Sarnaik, Manan Shah, Ravi Hegde</span></p>
<p><strong>TL;DR:</strong> 本文提出了基于提示驱动的GraphRAG框架（PROPEX-RAG），通过优化提示设计提升多跳问答系统的检索和推理能力，实现了在HotpotQA和2WikiMultiHopQA上的最佳性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于图结构的检索增强生成（RAG）方法在复杂推理任务中表现优异，但提示设计对检索和推理过程的影响尚未充分探索。本文旨在通过提示驱动的框架提升多跳问答系统的性能。</p>
<p><strong>Result:</strong> 在HotpotQA和2WikiMultiHopQA上的F1分数分别为80.7%和78.9%，Recall@5分数为97.1%和98.1%，达到了最先进水平。</p>
<p><strong>Insight:</strong> 提示设计对提升检索准确性和响应质量至关重要；结合知识图和PPR的方法为高效、可解释的多跳问答系统提供了新思路。</p>
<p><strong>Abstract:</strong> Retrieval-Augmented Generation (RAG) has become a robust framework for enhancing Large Language Models (LLMs) with external knowledge. Recent advances in RAG have investigated graph based retrieval for intricate reasoning; however, the influence of prompt design on enhancing the retrieval and reasoning process is still considerably under-examined. In this paper, we present a prompt-driven GraphRAG framework that underscores the significance of prompt formulation in facilitating entity extraction, fact selection, and passage reranking for multi-hop question answering. Our approach creates a symbolic knowledge graph from text data by encoding entities and factual relationships as structured facts triples. We use LLMs selectively during online retrieval to perform semantic filtering and answer generation. We also use entity-guided graph traversal through Personalized PageRank (PPR) to support efficient, scalable retrieval based on the knowledge graph we built. Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA, with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%, respectively. These results show that prompt design is an important part of improving retrieval accuracy and response quality. This research lays the groundwork for more efficient and comprehensible multi-hop question-answering systems, highlighting the importance of prompt-aware graph reasoning.</p>
  </div>
</details>

<hr>
<h3 id="141-SciTextures-Collecting-and-Connecting-Visual-Patterns-Models-and-Code-Across-Science-and-Art-cs-CVPDF"><a href="#141-SciTextures-Collecting-and-Connecting-Visual-Patterns-Models-and-Code-Across-Science-and-Art-cs-CVPDF" class="headerlink" title="[141] SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art cs.CVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01817">SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01817" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sagi Eppel, Alona Strugatski</span></p>
<p><strong>TL;DR:</strong> SciTextures 是一个大规模数据集，包含科学、技术和艺术领域的纹理和视觉模式，及其生成模型和代码，用于研究视觉模式与生成机制之间的联系。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索视觉模式与生成它们的机制之间的深层联系，促进跨领域视觉理解。</p>
<p><strong>Result:</strong> 实验表明，视觉语言模型（VLM）能够理解和模拟视觉模式背后的物理系统。</p>
<p><strong>Insight:</strong> 该数据集和基准任务为跨领域视觉理解提供了新工具，展示了 AI 在连接视觉模式与生成机制方面的潜力。</p>
<p><strong>Abstract:</strong> The ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the Scitextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,200 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the connection between the visual patterns that shape our world and the mechanisms that produce them. Created by an agentic AI pipeline that autonomously collects and implements models in standardized form, we use SciTextures to evaluate the ability of leading AI models to link visual patterns to the models and code that generate them, and to identify different patterns that emerged from the same process. We also test AIs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world pattern and asking the AI to identify, model, and code the mechanism that formed the pattern, then run this code to generate a simulated image that is compared to the real image. These benchmarks show that vision-language models (VLMs) can understand and simulate the physical system beyond a visual pattern. The dataset and code are available at: <a target="_blank" rel="noopener" href="https://zenodo.org/records/17485502">https://zenodo.org/records/17485502</a></p>
  </div>
</details>

<hr>
<h3 id="142-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning-cs-CVPDF"><a href="#142-TIR-Bench-A-Comprehensive-Benchmark-for-Agentic-Thinking-with-Images-Reasoning-cs-CVPDF" class="headerlink" title="[142] TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning cs.CVPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01833">TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01833" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ming Li, Jike Zhong, Shitian Zhao, Haoquan Zhang, Shaoheng Lin</span></p>
<p><strong>TL;DR:</strong> 该论文提出了TIR-Bench，一个用于评估代理式图像思维推理的综合基准，包含13个多样化任务，测试模型的工具使用和复杂推理能力。研究发现当前多模态大语言模型在这一基准上表现普遍较差，表明需要真正的图像思维能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉推理基准未能充分捕获高级代理式图像思维能力，特别是涉及复杂工具使用和动态推理的任务。因此，需要一个更全面的基准来评估模型的真实能力。</p>
<p><strong>Result:</strong> TIR-Bench对所有测试模型均具有挑战性，表明现有模型在复杂图像推理任务上的能力不足。代理式微调表现优于直接微调。</p>
<p><strong>Insight:</strong> 代理式图像思维需要模型具备动态工具使用和复杂推理能力，而传统基准未能充分体现这一点。TIR-Bench为未来研究提供了更全面的评估标准。</p>
<p><strong>Abstract:</strong> The frontier of visual reasoning is shifting toward models like OpenAI o3, which can intelligently create and operate tools to transform images for problem-solving, also known as thinking-\textit{with}-images in chain-of-thought. Yet existing benchmarks fail to fully capture this advanced capability. Even Visual Search, the most common benchmark for current thinking-\textit{with}-images methods, tests only basic operations such as localization and cropping, offering little insight into more complex, dynamic, and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive benchmark for evaluating agentic thinking-with-images across 13 diverse tasks, each requiring novel tool use for image processing and manipulation in chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from leading open-sourced and proprietary models to those with explicit tool-use augmentation. Results show that TIR-Bench is universally challenging, and strong performance requires genuine thinking-with-images capabilities. Finally, we present a pilot study comparing direct versus agentic fine-tuning.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="143-Cognitive-Alignment-in-Personality-Reasoning-Leveraging-Prototype-Theory-for-MBTI-Inference-cs-CL-cs-AIPDF"><a href="#143-Cognitive-Alignment-in-Personality-Reasoning-Leveraging-Prototype-Theory-for-MBTI-Inference-cs-CL-cs-AIPDF" class="headerlink" title="[143] Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference cs.CL | cs.AIPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00115">Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00115" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoyuan Li, Yuanbo Tong, Yuchen Li, Zirui Wang, Chunhou Liu</span></p>
<p><strong>TL;DR:</strong> ProtoMBTI是一个基于认知对齐的MBTI推断框架，通过结合原型理论和LLM技术，提升了人格推理的准确性、可解释性和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的人格识别方法通常采用硬标签分类，忽略了人类人格判断的渐变性和原型特性。ProtoMBTI旨在通过原型理论和LLM技术填补这一差距。</p>
<p><strong>Result:</strong> 在Kaggle和Pandora基准测试中，ProtoMBTI在MBTI四维度和16型任务上均优于基线模型，且表现出强大的跨数据集泛化能力。</p>
<p><strong>Insight:</strong> 1. 将心理学的原型理论融入人格推理可以提升性能和可解释性；2. 动态原型库的持续扩充能够进一步提升模型的鲁棒性。</p>
<p><strong>Abstract:</strong> Personality recognition from text is typically cast as hard-label classification, which obscures the graded, prototype-like nature of human personality judgments. We present ProtoMBTI, a cognitively aligned framework for MBTI inference that operationalizes prototype theory within an LLM-based pipeline. First, we construct a balanced, quality-controlled corpus via LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment). Next, we LoRA-fine-tune a lightweight (&lt;&#x3D;2B) encoder to learn discriminative embeddings and to standardize a bank of personality prototypes. At inference, we retrieve top-k prototypes for a query post and perform a retrieve–reuse–revise–retain cycle: the model aggregates prototype evidence via prompt-based voting, revises when inconsistencies arise, and, upon correct prediction, retains the sample to continually enrich the prototype library. Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both the four MBTI dichotomies and the full 16-type task, and exhibits robust cross-dataset generalization. Our results indicate that aligning the inference process with psychological prototype reasoning yields gains in accuracy, interpretability, and transfer for text-based personality modeling.</p>
  </div>
</details>

<hr>
<h3 id="144-Training-LLMs-Beyond-Next-Token-Prediction-Filling-the-Mutual-Information-Gap-cs-CL-cs-AIPDF"><a href="#144-Training-LLMs-Beyond-Next-Token-Prediction-Filling-the-Mutual-Information-Gap-cs-CL-cs-AIPDF" class="headerlink" title="[144] Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap cs.CL | cs.AIPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00198">Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00198" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chun-Hao Yang, Bo-Han Feng, Tzu-Yuan Lai, Yan Yu Chen, Yin-Kai Dean Huang</span></p>
<p><strong>TL;DR:</strong> 论文提出一种超越传统‘下一词预测’（NTP）的LLM训练方法，通过预测信息丰富的词来提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统NTP方法在LLM训练中存在效率不足的问题，无法充分利用信息丰富的词。</p>
<p><strong>Result:</strong> 新方法在三种任务中均表现出优于传统NTP的性能提升，同时理论分析也支持其有效性。</p>
<p><strong>Insight:</strong> 信息丰富的词的选择对LLM训练至关重要，未来的工作可以进一步探索词信息量的量化方法。</p>
<p><strong>Abstract:</strong> Optimizing training performance in large language models (LLMs) remains an essential challenge, particularly in improving model performance while maintaining computational costs. This work challenges the conventional approach of training LLMs using next-token prediction (NTP), arguing that by predicting information-rich tokens during training, there is a more effective way to train LLMs. We investigate the impact of the proposed solution in three kinds of tasks for LLMs: arithmetic, multi-label classification of text, and natural-language generation. This work offers a principled approach to optimizing LLM training, advancing both model performance and theoretical understanding of the target-token selection strategies.</p>
  </div>
</details>

<hr>
<h3 id="145-Consistently-Simulating-Human-Personas-with-Multi-Turn-Reinforcement-Learning-cs-CL-cs-AIPDF"><a href="#145-Consistently-Simulating-Human-Personas-with-Multi-Turn-Reinforcement-Learning-cs-CL-cs-AIPDF" class="headerlink" title="[145] Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning cs.CL | cs.AIPDF"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00222">Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00222" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marwa Abdulhai, Ryan Cheng, Donovan Clay, Tim Althoff, Sergey Levine</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一个通过多轮强化学习来持续模拟人类角色的框架，解决了现有大语言模型在角色扮演中容易偏离设定的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLM在模拟人类角色时容易偏离设定（如矛盾或放弃角色行为），影响了交互式场景（如治疗、教育）的实际效果。</p>
<p><strong>Result:</strong> 方法将不一致性降低了55%以上，模拟角色的连贯性和忠实性显著提升。</p>
<p><strong>Insight:</strong> 通过设计细粒度的评估指标并结合强化学习，可以有效解决LLM在角色扮演中的漂移问题。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics: prompt-to-line consistency, line-to-line consistency, and Q&amp;A consistency, that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent and faithful simulated users.</p>
  </div>
</details>

<hr>
<h3 id="146-POSESTITCH-SLT-Linguistically-Inspired-Pose-Stitching-for-End-to-End-Sign-Language-Translation-cs-CL-cs-AI-cs-CV-cs-LGPDF"><a href="#146-POSESTITCH-SLT-Linguistically-Inspired-Pose-Stitching-for-End-to-End-Sign-Language-Translation-cs-CL-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[146] POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation cs.CL | cs.AI | cs.CV | cs.LGPDF"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00270">POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00270" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abhinav Joshi, Vaibhav Sharma, Sanjeet Singh, Ashutosh Modi</span></p>
<p><strong>TL;DR:</strong> POSESTITCH-SLT提出了一种基于语言模板的预训练方法，通过模板生成的句子对训练简单的Transformer编码器-解码器架构，显著提升了手语翻译的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 手语翻译面临大规模句子对齐数据集稀缺的挑战，现有方法主要集中在特征提取和架构改进上。本文受语言学模板启发，提出了一种新的预训练方案。</p>
<p><strong>Result:</strong> 在How2Sign和iSign数据集上，BLEU-4分数分别从1.97提升至4.56和从0.55提升至3.43。</p>
<p><strong>Insight:</strong> 在低资源手语翻译场景中，基于模板的合成监督数据能够有效提升模型性能。</p>
<p><strong>Abstract:</strong> Sign language translation remains a challenging task due to the scarcity of large-scale, sentence-aligned datasets. Prior arts have focused on various feature extraction and architectural changes to support neural machine translation for sign languages. We propose POSESTITCH-SLT, a novel pre-training scheme that is inspired by linguistic-templates-based sentence generation technique. With translation comparison on two sign language datasets, How2Sign and iSign, we show that a simple transformer-based encoder-decoder architecture outperforms the prior art when considering template-generated sentence pairs in training. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign and from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for pose-based gloss-free translation. The results demonstrate the effectiveness of template-driven synthetic supervision in low-resource sign language settings.</p>
  </div>
</details>

<hr>
<h3 id="147-Language-Modeling-With-Factorization-Memory-cs-CL-cs-AIPDF"><a href="#147-Language-Modeling-With-Factorization-Memory-cs-CL-cs-AIPDF" class="headerlink" title="[147] Language Modeling With Factorization Memory cs.CL | cs.AIPDF"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00315">Language Modeling With Factorization Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00315" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lee Xiong, Maksim Tkachenko, Johanes Effendi, Ting Cai</span></p>
<p><strong>TL;DR:</strong> 《Language Modeling With Factorization Memory》提出了一种高效的RNN架构——Factorization Memory，在短上下文语言建模任务中表现堪比Transformer，同时在长上下文中展现出更强的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前Transformer在短上下文任务中表现优越，但在长上下文任务中存在计算和内存复杂度高的问题。RNN虽然在长上下文中更高效，但其表现通常不如Transformer。因此，作者希望设计一种结合两者优点的模型。</p>
<p><strong>Result:</strong> Factorization Memory在短上下文任务中达到与Transformer相当的性能，在长上下文中表现优于Transformer和Mamba-2。</p>
<p><strong>Insight:</strong> 结合RNN和Transformer的优点，稀疏记忆激活是提升RNN效率的关键方向。</p>
<p><strong>Abstract:</strong> We propose Factorization Memory, an efficient recurrent neural network (RNN) architecture that achieves performance comparable to Transformer models on short-context language modeling tasks while also demonstrating superior generalization in long-context scenarios. Our model builds upon Mamba-2, enabling Factorization Memory to exploit parallel computations during training while preserving constant computational and memory complexity during inference. To further optimize model efficiency and representational capacity, we develop a sparse formulation of Factorization Memory that updates only a subset of recurrent states at each step while preserving the strong performance of its dense counterpart. To our knowledge, this represents the first RNN architecture that successfully combines sparse memory activation with competitive performance across both short and long-context settings. This work provides a systematic empirical analysis of Factorization Memory in comparison to Transformer and Mamba-2 architectures.</p>
  </div>
</details>

<hr>
<h3 id="148-Reversal-Invariance-in-Autoregressive-Language-Models-cs-CLPDF"><a href="#148-Reversal-Invariance-in-Autoregressive-Language-Models-cs-CLPDF" class="headerlink" title="[148] Reversal Invariance in Autoregressive Language Models cs.CLPDF"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00341">Reversal Invariance in Autoregressive Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00341" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mihir Sahasrabudhe</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了自回归语言模型中的逆转不变性，指出标准的因果语言建模目标对文本方向不敏感，导致模型在逆转文本上也能表现良好，但可能忽略语言中的方向性依赖关系，提出了未来改进的方向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 论文的动机是发现标准自回归语言模型（CLM）目标的结构特性——逆转不变性，即模型对文本及其逆转版本的表现相似。这引发了对当前预训练目标是否能够有效捕捉语言中方向性依赖关系的质疑。</p>
<p><strong>Result:</strong> 结果显示，标准CLM目标对文本方向不敏感，模型在逆转文本上也能达到与标准文本相当的性能。</p>
<p><strong>Insight:</strong> 论文的深刻见解在于，逆转不变性表明当前预训练方法可能无法捕捉语言中的方向性依赖（如因果、语音或形态学），未来工作需要改进目标函数以更好地建模语言的时序特性。</p>
<p><strong>Abstract:</strong> We formalize a structural property of the causal (autoregressive) language modeling (CLM) objective: reversal invariance. Formally, the next-token prediction loss assigns identical likelihood to a corpus and its reversal, implying that standard CLM pretraining is direction-blind. This symmetry explains why models trained on reversed text can achieve comparable performance to those trained on forward text, despite the inherently time-asymmetric nature of human language and reasoning. We argue that this invariance represents a limitation of current pretraining objectives rather than a benign artifact. If natural language encodes directional dependencies - phonological, morphological, or causal - a symmetric objective may fail to capture them. We therefore propose viewing pretraining through the lens of temporal asymmetry, motivating future work on loss functions and architectures that explicitly model the arrow of language while retaining standard language modeling capacity.</p>
  </div>
</details>

<hr>
<h3 id="149-LingGym-How-Far-Are-LLMs-from-Thinking-Like-Field-Linguists-cs-CLPDF"><a href="#149-LingGym-How-Far-Are-LLMs-from-Thinking-Like-Field-Linguists-cs-CLPDF" class="headerlink" title="[149] LingGym: How Far Are LLMs from Thinking Like Field Linguists? cs.CLPDF"></a>[149] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00343">LingGym: How Far Are LLMs from Thinking Like Field Linguists?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00343" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changbing Yang, Franklin Ma, Freda Shi, Jian Zhu</span></p>
<p><strong>TL;DR:</strong> LingGym是一个评估大语言模型（LLMs）在跨语言元语言推理能力的新基准，通过IGT和语法描述测试模型在低资源语言和未见结构上的泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLM研究多聚焦于特定下游任务，而LingGym旨在填补模型在跨语言元语言推理能力评估上的空白，特别是在低资源语言和未见语法结构上。</p>
<p><strong>Result:</strong> 实验表明，加入结构化语言学线索能显著提升所有模型在跨语言推理任务中的表现，但也揭示了LLM在类型学语言分析和低资源语言文档化中的局限性。</p>
<p><strong>Insight:</strong> LingGym揭示了LLMs在语言学推理中的潜力与不足，为未来研究提供了方向和工具。</p>
<p><strong>Abstract:</strong> This paper introduces LingGym, a new benchmark that evaluates LLMs’ capacity for meta-linguistic reasoning using Interlinear Glossed Text (IGT) and grammatical descriptions extracted from 18 typologically diverse reference grammars. Unlike previous work that focuses on specific downstream tasks, we assess whether LLMs can generalize linguistic inference across low-resource languages and structures not seen during training. We present a controlled evaluation task: Word-Gloss Inference, in which the model must infer a missing word and gloss from context using varying levels of linguistic information (e.g., glosses, grammatical explanations, translations). Our results show that incorporating structured linguistic cues leads to consistent improvements in reasoning performance across all models. This work highlights both the promise and current limitations of using LLMs for typologically informed linguistic analysis and low-resource language documentation.</p>
  </div>
</details>

<hr>
<h3 id="150-Reasoning-Trajectories-for-Socratic-Debugging-of-Student-Code-From-Misconceptions-to-Contradictions-and-Updated-Beliefs-cs-CL-cs-CY-cs-SEPDF"><a href="#150-Reasoning-Trajectories-for-Socratic-Debugging-of-Student-Code-From-Misconceptions-to-Contradictions-and-Updated-Beliefs-cs-CL-cs-CY-cs-SEPDF" class="headerlink" title="[150] Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs cs.CL | cs.CY | cs.SEPDF"></a>[150] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00371">Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY | cs.SE</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00371" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Erfan Al-Hossami, Razvan Bunescu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于推理轨迹（RT）的苏格拉底调试方法，帮助学生通过矛盾识别并修复编程错误。作者引入了RT生成任务及相关数据集，并展示了LLM在生成RT和苏格拉底对话中的高效表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 学生调试代码时常因编程误解产生错误，传统方法直接提供修复方案，而苏格拉底调试旨在引导学生自主发现并纠正错误。本文希望通过生成推理轨迹，帮助学生逐步识别矛盾并更新错误认知。</p>
<p><strong>Result:</strong> 前沿LLM模型在RT生成任务中达到91%的正确率和98.7%的对话有效性。</p>
<p><strong>Insight:</strong> RT为苏格拉底调试提供了结构化路径，LLM的高效表现证明了其在教育领域自动生成高质量对话与推理支持中的潜力。</p>
<p><strong>Abstract:</strong> In Socratic debugging, instructors guide students towards identifying and fixing a bug on their own, instead of providing the bug fix directly. Most novice programmer bugs are caused by programming misconceptions, namely false beliefs about a programming concept. In this context, Socratic debugging can be formulated as a guided Reasoning Trajectory (RT) leading to a statement about the program behavior that contradicts the bug-causing misconception. Upon reaching this statement, the ensuing cognitive dissonance leads the student to first identify and then update their false belief. In this paper, we introduce the task of reasoning trajectory generation, together with a dataset of debugging problems manually annotated with RTs. We then describe LLM-based solutions for generating RTs and Socratic conversations that are anchored on them. A large-scale LLM-as-judge evaluation shows that frontier models can generate up to 91% correct reasoning trajectories and 98.7% valid conversation turns.</p>
  </div>
</details>

<hr>
<h3 id="151-MedRECT-A-Medical-Reasoning-Benchmark-for-Error-Correction-in-Clinical-Texts-cs-CL-cs-AI-cs-LGPDF"><a href="#151-MedRECT-A-Medical-Reasoning-Benchmark-for-Error-Correction-in-Clinical-Texts-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[151] MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts cs.CL | cs.AI | cs.LGPDF"></a>[151] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00421">MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00421" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Naoto Iwase, Hiroki Okuyama, Junichiro Iwasawa</span></p>
<p><strong>TL;DR:</strong> MedRECT是首个跨语言（日英）的医学错误修正基准，包含错误检测、定位和修正三个子任务，旨在评估大型语言模型（LLM）在医学领域的性能。通过自动化流程构建，并评估了9种LLM，结果显示推理模型表现优异，跨语言性能差异显著，且微调后模型性能超过人类专家。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型在医学应用中展现出潜力，但其在临床文本中检测和修正错误的能力（尤其是非英语环境）尚未充分评估，这是安全部署的前提条件。</p>
<p><strong>Result:</strong> 1. 推理模型在错误检测和句子提取任务中分别提升13.5%和51.0%。2. 跨语言评估显示英文到日语的性能差距为5-10%，推理模型差距更小。3. 微调后日语和英文的错误修正性能提升分别为0.078和0.168。4. 微调模型在结构化任务中超越人类专家。</p>
<p><strong>Insight:</strong> 1. 推理模型在医学错误处理任务中表现突出，尤其在跨语言环境下。2. 数据集自动构建流程可扩展到其他语言和领域。3. 针对性微调可显著提升性能，同时保留推理能力。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) show increasing promise in medical applications, but their ability to detect and correct errors in clinical texts – a prerequisite for safe deployment – remains under-evaluated, particularly beyond English. We introduce MedRECT, a cross-lingual benchmark (Japanese&#x2F;English) that formulates medical error handling as three subtasks: error detection, error localization (sentence extraction), and error correction. MedRECT is built with a scalable, automated pipeline from the Japanese Medical Licensing Examinations (JMLE) and a curated English counterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with comparable error&#x2F;no-error balance. We evaluate 9 contemporary LLMs spanning proprietary, open-weight, and reasoning families. Key findings: (i) reasoning models substantially outperform standard architectures, with up to 13.5% relative improvement in error detection and 51.0% in sentence extraction; (ii) cross-lingual evaluation reveals 5-10% performance gaps from English to Japanese, with smaller disparities for reasoning models; (iii) targeted LoRA fine-tuning yields asymmetric improvements in error correction performance (Japanese: +0.078, English: +0.168) while preserving reasoning capabilities; and (iv) our fine-tuned model exceeds human expert performance on structured medical error correction tasks. To our knowledge, MedRECT is the first comprehensive cross-lingual benchmark for medical error correction, providing a reproducible framework and resources for developing safer medical LLMs across languages.</p>
  </div>
</details>

<hr>
<h3 id="152-G2-Guided-Generation-for-Enhanced-Output-Diversity-in-LLMs-cs-CLPDF"><a href="#152-G2-Guided-Generation-for-Enhanced-Output-Diversity-in-LLMs-cs-CLPDF" class="headerlink" title="[152] G2: Guided Generation for Enhanced Output Diversity in LLMs cs.CLPDF"></a>[152] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00432">G2: Guided Generation for Enhanced Output Diversity in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00432" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiwen Ruan, Yixia Li, Yefeng Liu, Yun Chen, Weihua Luo</span></p>
<p><strong>TL;DR:</strong> 该论文提出了G2方法，一种无需训练的即插即用方法，通过解码干预增强LLMs的输出多样性，同时保持生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLMs）在输出多样性方面存在局限性，影响需要多样化输出的任务（如创意写作和推理），而现有方法（如温度调节）会降低输出质量。</p>
<p><strong>Result:</strong> 实验表明，G2能有效提升输出多样性，并在多样性与质量之间取得平衡。</p>
<p><strong>Insight:</strong> 引导机制可以在不修改模型参数的情况下，显著改善生成多样性，为LLMs的应用提供了新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, these models exhibit a critical limitation in output diversity, often generating highly similar content across multiple attempts. This limitation significantly affects tasks requiring diverse outputs, from creative writing to reasoning. Existing solutions, like temperature scaling, enhance diversity by modifying probability distributions but compromise output quality. We propose Guide-to-Generation (G2), a training-free plug-and-play method that enhances output diversity while preserving generation quality. G2 employs a base generator alongside dual Guides, which guide the generation process through decoding-based interventions to encourage more diverse outputs conditioned on the original query. Comprehensive experiments demonstrate that G2 effectively improves output diversity while maintaining an optimal balance between diversity and quality.</p>
  </div>
</details>

<hr>
<h3 id="153-Leveraging-the-Cross-Domain-Cross-Linguistic-Corpus-for-Low-Resource-NMT-A-Case-Study-On-Bhili-Hindi-English-Parallel-Corpus-cs-CLPDF"><a href="#153-Leveraging-the-Cross-Domain-Cross-Linguistic-Corpus-for-Low-Resource-NMT-A-Case-Study-On-Bhili-Hindi-English-Parallel-Corpus-cs-CLPDF" class="headerlink" title="[153] Leveraging the Cross-Domain &amp; Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus cs.CLPDF"></a>[153] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00486">Leveraging the Cross-Domain &amp; Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00486" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pooja Singh, Shashwat Bhardwaj, Vaibhav Sharma, Sandeep Kumar</span></p>
<p><strong>TL;DR:</strong> 论文介绍了Bhili-Hindi-English平行语料库（BHEPC），填补了Bhili语言资源匮乏的空白，并评估了多种多语言大模型在低资源机器翻译任务中的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 印度语言多样性带来的机器翻译挑战，尤其是对Bhili等资源匮乏的部落语言缺乏高质量语料库，促成了这一研究。</p>
<p><strong>Result:</strong> NLLB-200 distilled 600M模型表现最佳，证明了多语言模型在低资源场景的潜力。</p>
<p><strong>Insight:</strong> 多语言大模型在低资源语言翻译中具有潜力，且跨领域语料库的构建对推动研究至关重要。</p>
<p><strong>Abstract:</strong> The linguistic diversity of India poses significant machine translation challenges, especially for underrepresented tribal languages like Bhili, which lack high-quality linguistic resources. This paper addresses the gap by introducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest parallel corpus worldwide comprising 110,000 meticulously curated sentences across Bhili, Hindi, and English. The corpus was created with the assistance of expert human translators. BHEPC spans critical domains such as education, administration, and news, establishing a valuable benchmark for research in low resource machine translation. To establish a comprehensive Bhili Machine Translation benchmark, we evaluated a wide range of proprietary and open-source Multilingual Large Language Models (MLLMs) on bidirectional translation tasks between English&#x2F;Hindi and Bhili. Comprehensive evaluation demonstrates that the fine-tuned NLLB-200 distilled 600M variant model outperforms others, highlighting the potential of multilingual models in low resource scenarios. Furthermore, we investigated the generative translation capabilities of multilingual LLMs on BHEPC using in-context learning, assessing performance under cross-domain generalization and quantifying distributional divergence. This work bridges a critical resource gap and promotes inclusive natural language processing technologies for low-resource and marginalized languages globally.</p>
  </div>
</details>

<hr>
<h3 id="154-ToM-Leveraging-Tree-oriented-MapReduce-for-Long-Context-Reasoning-in-Large-Language-Models-cs-CLPDF"><a href="#154-ToM-Leveraging-Tree-oriented-MapReduce-for-Long-Context-Reasoning-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[154] ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models cs.CLPDF"></a>[154] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00489">ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00489" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiani Guo, Zuchao Li, Jie Wu, Qianren Wang, Yun Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了ToM（Tree-oriented MapReduce）框架，通过层次化语义解析构建文档树（DocTree）并结合MapReduce方法，解决了大语言模型在长上下文推理中的性能下降问题，显著优于现有的分治法（DCF）和检索增强生成（RAG）方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大语言模型（LLMs）因上下文窗口限制，在长上下文推理中表现不佳。现有方法如RAG和DCF虽有效，但RAG依赖相似性排序导致逻辑一致性不足，DCF独立处理片段难以捕捉长距离依赖且易冲突。</p>
<p><strong>Result:</strong> 在70B+ LLMs上的实验表明，ToM显著优于DCF和RAG，逻辑一致性和长上下文推理能力均有提升。</p>
<p><strong>Insight:</strong> ToM的有效性表明，利用文档的层次结构并结合递归推理，可以更好地捕捉长距离依赖并减少冲突，为长上下文推理提供新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs), constrained by limited context windows, often face significant performance degradation when reasoning over long contexts. To address this, Retrieval-Augmented Generation (RAG) retrieves and reasons over chunks but frequently sacrifices logical coherence due to its reliance on similarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split documents into small chunks for independent reasoning and aggregation. While effective for local reasoning, DCF struggles to capture long-range dependencies and risks inducing conflicts by processing chunks in isolation. To overcome these limitations, we propose ToM, a novel Tree-oriented MapReduce framework for long-context reasoning. ToM leverages the inherent hierarchical structure of long documents (e.g., main headings and subheadings) by constructing a DocTree through hierarchical semantic parsing and performing bottom-up aggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning: in the Map step, rationales are generated at child nodes; in the Reduce step, these rationales are aggregated across sibling nodes to resolve conflicts or reach consensus at parent nodes. Experimental results on 70B+ LLMs show that ToM significantly outperforms existing divide-and-conquer frameworks and retrieval-augmented generation methods, achieving better logical coherence and long-context reasoning. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/gjn12-31/ToM">https://github.com/gjn12-31/ToM</a> .</p>
  </div>
</details>

<hr>
<h3 id="155-Word-Salad-Chopper-Reasoning-Models-Waste-A-Ton-Of-Decoding-Budget-On-Useless-Repetitions-Self-Knowingly-cs-CLPDF"><a href="#155-Word-Salad-Chopper-Reasoning-Models-Waste-A-Ton-Of-Decoding-Budget-On-Useless-Repetitions-Self-Knowingly-cs-CLPDF" class="headerlink" title="[155] Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly cs.CLPDF"></a>[155] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00536">Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00536" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenya Xie, Shaochen, Zhong, Hoang Anh Duy Le, Zhaozhuo Xu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了WordSaladChopper (WSC)，一种轻量级组件，用于检测和去除大型推理模型(LRMs)中无用的自我重复（称为’word salad’），从而显著减少解码预算消耗而不影响推理质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型推理模型(LRMs)的解码成本很高，而研究发现其中许多输出是无用的自我重复（’word salad’），浪费了解码预算。模型自身也能感知这些冗余行为，这激发了设计一种简单方法来自动检测并去除这类冗余的需求。</p>
<p><strong>Result:</strong> 实验表明，WSC能显著减少输出长度，同时保持推理质量，且其开销极小，适用于所有关注用户体验的LRM应用。</p>
<p><strong>Insight:</strong> 论文揭示了LRMs在推理过程中会产生大量无意义的自我重复，且模型自身能感知这些行为。这一发现为优化解码效率提供了新思路。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs) are often bottlenecked by the high cost of output tokens. We show that a significant portion of these tokens are useless self-repetitions - what we call “word salad” - that exhaust the decoding budget without adding value. Interestingly, we observe that LRMs are self-aware when trapped in these loops: the hidden states of &lt;\n\n&gt; tokens trailing each reasoning chunk exhibit patterns that allow us to detect word salad behavior on-the-fly via a single-layer linear classifier. Once detected, a simple chop appended by a straightforward regeneration prompt yields substantial length savings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a lightweight, turnkey component for LRM that is minimally invasive to its reasoning trajectory by only removing semantically redundant tokens. Given its low overhead, strong savings, and the lack of semantic value of word salad tokens, we believe it is not too far-fetched to argue that WSC - or a similar component - is a must-have for all LRM applications with user experience in mind. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/wenyaxie023/WordSaladChopper">https://github.com/wenyaxie023/WordSaladChopper</a>.</p>
  </div>
</details>

<hr>
<h3 id="156-OpenSIR-Open-Ended-Self-Improving-Reasoner-cs-CLPDF"><a href="#156-OpenSIR-Open-Ended-Self-Improving-Reasoner-cs-CLPDF" class="headerlink" title="[156] OpenSIR: Open-Ended Self-Improving Reasoner cs.CLPDF"></a>[156] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00602">OpenSIR: Open-Ended Self-Improving Reasoner</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00602" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wai-Chung Kwan, Joshua Ong Jun Leang, Pavlos Vougiouklis, Jeff Z. Pan, Marco Valentino</span></p>
<p><strong>TL;DR:</strong> OpenSIR是一个无监督的自增强推理框架，通过交替扮演教师和学生的角色，让LLM生成并解决问题，无需外部标注。它通过优化问题的难度和多样性，实现开放式的数学发现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLM推理方法依赖标注数据集，限制了其超越人类水平的能力。OpenSIR旨在通过自增强（self-play）实现无监督的开放式学习。</p>
<p><strong>Result:</strong> 实验显示，OpenSIR显著提升LLM的推理能力，如Llama-3模型在GSM8K和College Math任务上的表现分别从73.9升至78.3和28.8升至34.4。</p>
<p><strong>Insight:</strong> OpenSIR通过教师-学生角色的协同进化，自适应调整问题难度并探索多样概念，实现了从基础到高级数学的自主进步。</p>
<p><strong>Abstract:</strong> Recent advances in large language model (LLM) reasoning through reinforcement learning rely on annotated datasets for verifiable rewards, which may limit models’ ability to surpass human-level performance. While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner (OpenSIR), a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. To generate novel problems, OpenSIR optimises for both difficulty and diversity, rewarding problems that challenge appropriately while exploring distinct concepts, enabling open-ended mathematical discovery. Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.</p>
  </div>
</details>

<hr>
<h3 id="157-SpecDiff-2-Scaling-Diffusion-Drafter-Alignment-For-Faster-Speculative-Decoding-cs-CLPDF"><a href="#157-SpecDiff-2-Scaling-Diffusion-Drafter-Alignment-For-Faster-Speculative-Decoding-cs-CLPDF" class="headerlink" title="[157] SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding cs.CLPDF"></a>[157] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00606">SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00606" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jameson Sandler, Jacob K. Christopher, Thomas Hartvigsen, Nando Fioretto</span></p>
<p><strong>TL;DR:</strong> SpecDiff-2提出了一种新型框架，通过离散扩散和校准技术解决推测解码的两个瓶颈，显著提升了大型语言模型的推理速度，同时保持准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前推测解码方法在并行性和草案与验证模型的校准方面存在瓶颈，限制了推理速度的提升。</p>
<p><strong>Result:</strong> 实验结果表明，SpecDiff-2在推理、编码和数学任务上均实现了最佳性能，速度提升最高达5.5倍，且准确率无损。</p>
<p><strong>Insight:</strong> 离散扩散与非自回归起草模型的结合为加速LLM推理提供了新思路，校准技术的设计对保持准确性至关重要。</p>
<p><strong>Abstract:</strong> Speculative decoding has become the standard approach for accelerating Large Language Model (LLM) inference. It exploits a lossless draft-then-verify procedure to circumvent the latency of autoregressive decoding, achieving impressive speed-ups. Yet, current speculative decoding approaches remain limited by two fundamental bottlenecks: (1) the autoregressive dependency during drafting which limits parallelism, and (2) frequent rejections of draft tokens caused by misalignment between the draft and verify models. This paper proposes SpecDiff-2, a novel framework to jointly address these two bottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to address bottleneck (1) and develops novel techniques to calibrate discrete diffusion drafters with autoregressive verifiers, addressing bottleneck (2). Experimental results across a comprehensive benchmark suite show that SpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and mathematical benchmarks, improving tokens-per-second by up to an average of +55% over previous baselines and obtaining up to 5.5x average speed-up over standard decoding, without any loss of accuracy.</p>
  </div>
</details>

<hr>
<h3 id="158-Certain-but-not-Probable-Differentiating-Certainty-from-Probability-in-LLM-Token-Outputs-for-Probabilistic-Scenarios-cs-CLPDF"><a href="#158-Certain-but-not-Probable-Differentiating-Certainty-from-Probability-in-LLM-Token-Outputs-for-Probabilistic-Scenarios-cs-CLPDF" class="headerlink" title="[158] Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios cs.CLPDF"></a>[158] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00620">Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00620" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Autumn Toney-Wails, Ryan Wails</span></p>
<p><strong>TL;DR:</strong> 论文研究了在概率场景中，大型语言模型输出的确定性（基于token logits）与实际理论概率分布之间的差异，发现模型虽能准确完成任务，但概率分布与理论值不符。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机在于确保模型在决策支持等知识密集型应用中的可信赖性，尤其是需要对不确定性进行可靠量化时。</p>
<p><strong>Result:</strong> 结果显示模型能准确完成任务，但token级概率和熵值与理论分布不一致。</p>
<p><strong>Insight:</strong> 研究揭示了模型在概率推理中的局限性，提示需要改进不确定性量化方法以提高实际应用的可靠性。</p>
<p><strong>Abstract:</strong> Reliable uncertainty quantification (UQ) is essential for ensuring trustworthy downstream use of large language models, especially when they are deployed in decision-support and other knowledge-intensive applications. Model certainty can be estimated from token logits, with derived probability and entropy values offering insight into performance on the prompt task. However, this approach may be inadequate for probabilistic scenarios, where the probabilities of token outputs are expected to align with the theoretical probabilities of the possible outcomes. We investigate the relationship between token certainty and alignment with theoretical probability distributions in well-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we evaluate model responses to ten prompts involving probability (e.g., roll a six-sided die), both with and without explicit probability cues in the prompt (e.g., roll a fair six-sided die). We measure two dimensions: (1) response validity with respect to scenario constraints, and (2) alignment between token-level output probabilities and theoretical probabilities. Our results indicate that, while both models achieve perfect in-domain response accuracy across all prompt scenarios, their token-level probability and entropy values consistently diverge from the corresponding theoretical distributions.</p>
  </div>
</details>

<hr>
<h3 id="159-Modeling-the-Construction-of-a-Literary-Archetype-The-Case-of-the-Detective-Figure-in-French-Literature-cs-CLPDF"><a href="#159-Modeling-the-Construction-of-a-Literary-Archetype-The-Case-of-the-Detective-Figure-in-French-Literature-cs-CLPDF" class="headerlink" title="[159] Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature cs.CLPDF"></a>[159] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00627">Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00627" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jean Barré, Olga Seminck, Antoine Bourgois, Thierry Poibeau</span></p>
<p><strong>TL;DR:</strong> 该研究通过计算分析方法，追踪了法国侦探小说中侦探原型150年来的演变，发现监督模型能够捕捉其统一性，并展示了侦探角色从次要叙事角色到核心人物的转变。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究旨在揭示法国文学中侦探原型的演变过程，尤其是其从传统推理故事的主角到社会暴力和道德模糊性代表的转型。</p>
<p><strong>Result:</strong> 研究发现侦探角色从次要叙事角色发展为故事核心，并在二战后受硬汉侦探传统影响变得更复杂。</p>
<p><strong>Insight:</strong> 研究揭示了文学原型如何在特定社会历史背景下演变，反映了文学类型与社会变迁的互动。</p>
<p><strong>Abstract:</strong> This research explores the evolution of the detective archetype in French detective fiction through computational analysis. Using quantitative methods and character-level embeddings, we show that a supervised model is able to capture the unity of the detective archetype across 150 years of literature, from M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding, the study demonstrates how the detective figure evolves from a secondary narrative role to become the central character and the “reasoning machine” of the classical detective story. In the aftermath of the Second World War, with the importation of the hardboiled tradition into France, the archetype becomes more complex, navigating the genre’s turn toward social violence and moral ambiguity.</p>
  </div>
</details>

<hr>
<h3 id="160-Do-You-Know-About-My-Nation-Investigating-Multilingual-Language-Models’-Cultural-Literacy-Through-Factual-Knowledge-cs-CLPDF"><a href="#160-Do-You-Know-About-My-Nation-Investigating-Multilingual-Language-Models’-Cultural-Literacy-Through-Factual-Knowledge-cs-CLPDF" class="headerlink" title="[160] Do You Know About My Nation? Investigating Multilingual Language Models’ Cultural Literacy Through Factual Knowledge cs.CLPDF"></a>[160] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00657">Do You Know About My Nation? Investigating Multilingual Language Models’ Cultural Literacy Through Factual Knowledge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00657" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eshaan Tanwar, Anwoy Chatterjee, Michael Saxon, Alon Albalak, William Yang Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出XNationQA数据集，用于评估多语言大模型在文化知识方面的表现，揭示模型在非西方语言和文化知识上的显著差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多语言问答基准大多偏向西方中心，忽略了地域多样性，难以公平评估模型对不同地理文化事实的理解能力。</p>
<p><strong>Result:</strong> 1. 模型在英语中表现优于目标文化的本地语言；2. 西方语言表现较好，但未必更懂西方文化；3. 开源模型的跨语言知识迁移能力较弱。</p>
<p><strong>Insight:</strong> 多语言模型的文化知识存在偏差，跨语言迁移能力有限，尤其在开源模型中更为明显。</p>
<p><strong>Abstract:</strong> Most multilingual question-answering benchmarks, while covering a diverse pool of languages, do not factor in regional diversity in the information they capture and tend to be Western-centric. This introduces a significant gap in fairly evaluating multilingual models’ comprehension of factual information from diverse geographical locations. To address this, we introduce XNationQA for investigating the cultural literacy of multilingual LLMs. XNationQA encompasses a total of 49,280 questions on the geography, culture, and history of nine countries, presented in seven languages. We benchmark eight standard multilingual LLMs on XNationQA and evaluate them using two novel transference metrics. Our analyses uncover a considerable discrepancy in the models’ accessibility to culturally specific facts across languages. Notably, we often find that a model demonstrates greater knowledge of cultural information in English than in the dominant language of the respective culture. The models exhibit better performance in Western languages, although this does not necessarily translate to being more literate for Western countries, which is counterintuitive. Furthermore, we observe that models have a very limited ability to transfer knowledge across languages, particularly evident in open-source models.</p>
  </div>
</details>

<hr>
<h3 id="161-Optimizing-Native-Sparse-Attention-with-Latent-Attention-and-Local-Global-Alternating-Strategies-cs-CLPDF"><a href="#161-Optimizing-Native-Sparse-Attention-with-Latent-Attention-and-Local-Global-Alternating-Strategies-cs-CLPDF" class="headerlink" title="[161] Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies cs.CLPDF"></a>[161] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00819">Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00819" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuxuan Hu, Jianchao Tan, Jiaqi Zhang, Wen Zan, Pingwei Sun</span></p>
<p><strong>TL;DR:</strong> 该论文通过系统性分析原生稀疏注意力（NSA），提出了一种交替使用局部和全局注意力的策略，并结合潜在注意力优化分支，显著提升了长上下文建模性能，同时减少了KV缓存内存。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 原生稀疏注意力在长上下文建模中存在性能不足和内存消耗高的问题。</p>
<p><strong>Result:</strong> 在3.4亿至13亿参数的模型上实验表明，该方法在常识推理和长文本理解任务中优于或匹配全注意力和原生稀疏注意力。</p>
<p><strong>Insight:</strong> 局部与全局注意力的交替策略能更有效地传播长距离依赖关系，潜在注意力进一步提升了模型的表达能力。</p>
<p><strong>Abstract:</strong> In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA’s branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50% versus NSA while improving the model’s common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks.</p>
  </div>
</details>

<hr>
<h3 id="162-TriCon-Fair-Triplet-Contrastive-Learning-for-Mitigating-Social-Bias-in-Pre-trained-Language-Models-cs-CLPDF"><a href="#162-TriCon-Fair-Triplet-Contrastive-Learning-for-Mitigating-Social-Bias-in-Pre-trained-Language-Models-cs-CLPDF" class="headerlink" title="[162] TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models cs.CLPDF"></a>[162] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00854">TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00854" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chong Lyu, Lin Li, Shiqing Wu, Jingling Yuan</span></p>
<p><strong>TL;DR:</strong> TriCon-Fair通过三重对比学习框架，结合解耦损失函数，减少预训练语言模型中的社会偏见，同时保持模型的通用能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型的广泛应用引发了社会偏见传播的担忧，现有方法忽略了偏见样本与非偏见样本的相互关系，导致改进时产生负面耦合效应。</p>
<p><strong>Result:</strong> 实验表明，TriCon-Fair在减少歧视性输出方面优于现有基准，同时保持了较强的下游任务性能。</p>
<p><strong>Insight:</strong> 通过显式解耦偏见与非偏见样本的关系，TriCon-Fair为敏感NLP应用提供了实用且伦理的解决方案。</p>
<p><strong>Abstract:</strong> The increasing utilization of large language models raises significant concerns about the propagation of social biases, which may result in harmful and unfair outcomes. However, existing debiasing methods treat the biased and unbiased samples independently, thus ignoring their mutual relationship. This oversight enables a hidden negative-positive coupling, where improvements for one group inadvertently compromise the other, allowing residual social bias to persist. In this paper, we introduce TriCon-Fair, a contrastive learning framework that employs a decoupled loss that combines triplet and language modeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns each anchor an explicitly biased negative and an unbiased positive, decoupling the push-pull dynamics and avoiding positive-negative coupling, and jointly optimizes a language modeling (LM) objective to preserve general capability. Experimental results demonstrate that TriCon-Fair reduces discriminatory output beyond existing debiasing baselines while maintaining strong downstream performance. This suggests that our proposed TriCon-Fair offers a practical and ethical solution for sensitive NLP applications.</p>
  </div>
</details>

<hr>
<h3 id="163-Assessing-LLM-Reasoning-Steps-via-Principal-Knowledge-Grounding-cs-CL-cs-AI-cs-LGPDF"><a href="#163-Assessing-LLM-Reasoning-Steps-via-Principal-Knowledge-Grounding-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[163] Assessing LLM Reasoning Steps via Principal Knowledge Grounding cs.CL | cs.AI | cs.LGPDF"></a>[163] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00879">Assessing LLM Reasoning Steps via Principal Knowledge Grounding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00879" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyeon Hwang, Yewon Cho, Chanwoong Yoon, Yein Park, Minju Song</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一个评估大语言模型（LLM）每一步推理是否基于知识的框架，通过构建核心知识库、设计知识导向的评估指标，并通过轻量级评估模型，系统性地检测LLM在推理中的知识应用问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LLM的逐步推理方法虽然有效，但其推理步骤是否准确地基于知识尚未有系统的评估方法。论文旨在填补这一空白，为LLM推理能力的可靠性提供验证工具。</p>
<p><strong>Result:</strong> 评估框架成功识别了LLM推理中缺失或误用的知识元素，揭示了其根本性缺陷。此外，这些指标还能用于偏好优化，展现了知识导向评估的进一步应用潜力。</p>
<p><strong>Insight:</strong> 知识导向的评估不仅有助于发现LLM推理的不足，还为优化模型提供了新方向。轻量级评估模型的设计为大规模高效评估提供了可行方案。</p>
<p><strong>Abstract:</strong> Step-by-step reasoning has become a standard approach for large language models (LLMs) to tackle complex tasks. While this paradigm has proven effective, it raises a fundamental question: How can we verify that an LLM’s reasoning is accurately grounded in knowledge? To address this question, we introduce a novel evaluation suite that systematically assesses the knowledge grounding of intermediate reasoning. Our framework comprises three key components. (1) Principal Knowledge Collection, a large-scale repository of atomic knowledge essential for reasoning. Based on the collection, we propose (2) knowledge-grounded evaluation metrics designed to measure how well models recall and apply prerequisite knowledge in reasoning. These metrics are computed by our (3) evaluator LLM, a lightweight model optimized for cost-effective and reliable metric computation. Our evaluation suite demonstrates remarkable effectiveness in identifying missing or misapplied knowledge elements, providing crucial insights for uncovering fundamental reasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these metrics can be integrated into preference optimization, showcasing further applications of knowledge-grounded evaluation.</p>
  </div>
</details>

<hr>
<h3 id="164-ColMate-Contrastive-Late-Interaction-and-Masked-Text-for-Multimodal-Document-Retrieval-cs-CLPDF"><a href="#164-ColMate-Contrastive-Late-Interaction-and-Masked-Text-for-Multimodal-Document-Retrieval-cs-CLPDF" class="headerlink" title="[164] ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval cs.CLPDF"></a>[164] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00903">ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00903" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmed Masry, Megh Thakkar, Patrice Bechard, Sathwik Tejaswi Madhusudhan, Rabiul Awal</span></p>
<p><strong>TL;DR:</strong> ColMate 是一个多模态文档检索模型，通过新颖的OCR预训练目标、自监督掩码对比学习目标和针对多模态文档结构的延迟交互评分机制，显著提升了检索性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态文档检索方法通常沿用纯文本检索的技术，未能充分结合多模态文档的结构和视觉特征，导致性能受限。</p>
<p><strong>Result:</strong> 在ViDoRe V2基准测试中，ColMate比现有检索模型性能提升3.61%，并展现出更强的跨域泛化能力。</p>
<p><strong>Insight:</strong> 结合视觉和文本特征的多模态预训练目标及评分机制对提升文档检索性能至关重要。</p>
<p><strong>Abstract:</strong> Retrieval-augmented generation has proven practical when models require specialized knowledge or access to the latest data. However, existing methods for multimodal document retrieval often replicate techniques developed for text-only retrieval, whether in how they encode documents, define training objectives, or compute similarity scores. To address these limitations, we present ColMate, a document retrieval model that bridges the gap between multimodal representation learning and document retrieval. ColMate utilizes a novel OCR-based pretraining objective, a self-supervised masked contrastive learning objective, and a late interaction scoring mechanism more relevant to multimodal document structures and visual characteristics. ColMate obtains 3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark, demonstrating stronger generalization to out-of-domain benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="165-The-Riddle-of-Reflection-Evaluating-Reasoning-and-Self-Awareness-in-Multilingual-LLMs-using-Indian-Riddles-cs-CL-cs-AIPDF"><a href="#165-The-Riddle-of-Reflection-Evaluating-Reasoning-and-Self-Awareness-in-Multilingual-LLMs-using-Indian-Riddles-cs-CL-cs-AIPDF" class="headerlink" title="[165] The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles cs.CL | cs.AIPDF"></a>[165] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00960">The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00960" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abhinav P M, Ojasva Saxena, Oswald C, Parameswari Krishnamurthy</span></p>
<p><strong>TL;DR:</strong> 本文通过印度多语言谜题数据集评估了五种大型语言模型（LLMs）在多语言环境中的推理和自我评估能力，发现模型准确性与自我识别错误的能力呈负相关。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机在于探索LLMs在非英语文化背景下的推理能力及其自我评估的一致性，填补多语言推理研究的空白。</p>
<p><strong>Result:</strong> 结果显示Gemini 2.5 Pro在谜题解答中表现最佳，但其自我识别错误能力最差（4.34% True Negative Rate），而表现较差的LLaMA 4 Scout自我意识更强（42.09% True Negative Rate）。</p>
<p><strong>Insight:</strong> 研究发现模型的高性能可能伴随过度自信，而性能较差的模型反而更善于识别自身错误，这强调了模型不仅需要高效推理还需具备自我认知能力。</p>
<p><strong>Abstract:</strong> The extent to which large language models (LLMs) can perform culturally grounded reasoning across non-English languages remains underexplored. This paper examines the reasoning and self-assessment abilities of LLMs across seven major Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and Telugu. We introduce a multilingual riddle dataset combining traditional riddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5 Pro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under seven prompting strategies. In the first stage, we assess riddle-solving performance and find that while Gemini 2.5 Pro performs best overall, few-shot methods yield only marginal gains, and accuracy varies notably across languages. In the second stage, we conduct a self-evaluation experiment to measure reasoning consistency. The results reveal a key finding: a model’s initial accuracy is inversely correlated with its ability to identify its own mistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34% True Negative Rate), whereas lower-performing models like LLaMA 4 Scout are substantially more self-aware (42.09% True Negative Rate). These results point to clear gaps in multilingual reasoning and highlight the need for models that not only reason effectively but also recognize their own limitations.</p>
  </div>
</details>

<hr>
<h3 id="166-Advancing-Machine-Generated-Text-Detection-from-an-Easy-to-Hard-Supervision-Perspective-cs-CLPDF"><a href="#166-Advancing-Machine-Generated-Text-Detection-from-an-Easy-to-Hard-Supervision-Perspective-cs-CLPDF" class="headerlink" title="[166] Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective cs.CLPDF"></a>[166] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00988">Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00988" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenwang Wu, Yiu-ming Cheung, Bo Han, Defu Lian</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个从易到难的监督框架，以解决机器生成文本（MGT）检测中的边界模糊问题，通过使用简单任务的监督来优化复杂任务的检测性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MGT检测方法假设标签是‘黄金标准’，但现实中边界模糊导致传统训练范式不精确。人类认知局限和检测器超智能性使不精确学习普遍存在。</p>
<p><strong>Result:</strong> 在跨LLM、跨域、混合文本和对抗攻击等多样场景中，该框架展现了显著的检测效果。</p>
<p><strong>Insight:</strong> 边界模糊问题可通过任务难易分层解决，简单任务的监督能为复杂任务提供可靠的训练信号。</p>
<p><strong>Abstract:</strong> Existing machine-generated text (MGT) detection methods implicitly assume labels as the “golden standard”. However, we reveal boundary ambiguity in MGT detection, implying that traditional training paradigms are inexact. Moreover, limitations of human cognition and the superintelligence of detectors make inexact learning widespread and inevitable. To this end, we propose an easy-to-hard enhancement framework to provide reliable supervision under such inexact conditions. Distinct from knowledge distillation, our framework employs an easy supervisor targeting relatively simple longer-text detection tasks (despite weaker capabilities), to enhance the more challenging target detector. Firstly, longer texts targeted by supervisors theoretically alleviate the impact of inexact labels, laying the foundation for reliable supervision. Secondly, by structurally incorporating the detector into the supervisor, we theoretically model the supervisor as a lower performance bound for the detector. Thus, optimizing the supervisor indirectly optimizes the detector, ultimately approximating the underlying “golden” labels. Extensive experiments across diverse practical scenarios, including cross-LLM, cross-domain, mixed text, and paraphrase attacks, demonstrate the framework’s significant detection effectiveness. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/Easy2Hard">https://github.com/tmlr-group/Easy2Hard</a>.</p>
  </div>
</details>

<hr>
<h3 id="167-MARS-SQL-A-multi-agent-reinforcement-learning-framework-for-Text-to-SQL-cs-CLPDF"><a href="#167-MARS-SQL-A-multi-agent-reinforcement-learning-framework-for-Text-to-SQL-cs-CLPDF" class="headerlink" title="[167] MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL cs.CLPDF"></a>[167] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01008">MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01008" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haolin Yang, Jipeng Zhang, Zhitao He, Yi R. Fung</span></p>
<p><strong>TL;DR:</strong> MARS-SQL提出了一种多智能体强化学习框架，结合任务分解和交互式RL，通过三个专门代理（Grounding、Generation和Validation Agent）实现自然语言到SQL的高效转换，并在BIRD和Spider数据集上取得SOTA表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 复杂SQL查询需要环境交互和自我修正，现有方法难以应对这一挑战。MARS-SQL通过多智能体协作和交互式RL解决这一问题。</p>
<p><strong>Result:</strong> BIRD dev集上Execution Accuracy为77.84%，Spider测试集上为89.75%，达到SOTA。</p>
<p><strong>Insight:</strong> 多智能体协作和交互式RL的结合显著提升了复杂查询的生成能力，验证代理的生成建模方法有效过滤了错误结果。</p>
<p><strong>Abstract:</strong> Translating natural language to SQL remains difficult for complex queries. Such queries often need environmental interaction and self-correction. To address this, we introduce MARS-SQL, a novel multi-agent framework that combines principled task decomposition and interactive reinforcement learning (RL). Our system comprises three specialized agents: a Grounding Agent for schema linking, a Generation Agent for query generation, and a Validation Agent for final selection. The core of our framework is the Generation agent, which is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe loop, the agent iteratively generates thoughts, executes SQL actions against a live database, and revises its strategy based on execution feedback, enabling dynamic, stateful reasoning and self-correction. At inference time, we generate multiple interaction trajectories to explore diverse reasoning paths. The Validation agent, then selects the optimal trajectory by modeling verification as a next-token prediction task and choosing the solution with the highest generation probability. This structured workflow pipelines specialized agents. It combines interactive RL for generation with generative modeling for verification. The approach proves highly effective for robust and accurate SQL generation. Experiments show that MARS-SQL achieves state-of-the-art Execution Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/YangHaolin0526/MARS-SQL">https://github.com/YangHaolin0526/MARS-SQL</a>.</p>
  </div>
</details>

<hr>
<h3 id="168-IF-CRITIC-Towards-a-Fine-Grained-LLM-Critic-for-Instruction-Following-Evaluation-cs-CLPDF"><a href="#168-IF-CRITIC-Towards-a-Fine-Grained-LLM-Critic-for-Instruction-Following-Evaluation-cs-CLPDF" class="headerlink" title="[168] IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation cs.CLPDF"></a>[168] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01014">IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01014" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bosi Wen, Yilin Niu, Cunxiang Wang, Pei Ke, Xiaoying Ling</span></p>
<p><strong>TL;DR:</strong> IF-CRITIC 是一种细粒度的 LLM 评价模型，通过分解指令生成检查表，并结合多阶段过滤机制收集高质量数据，训练出能高效可靠评估指令遵循能力的模型，性能优于现有基准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型（LLM）在指令遵循评估中存在成本高和评估不可靠的问题，需要一种更高效的解决方案。</p>
<p><strong>Result:</strong> IF-CRITIC 在评估性能上优于 Deepseek-R1 等基准模型，在低计算成本下显著提升 LLM 的指令遵循优化性能。</p>
<p><strong>Insight:</strong> 通过细粒度检查和高质量数据训练，可以显著提升 LLM 评估的效率和可靠性。</p>
<p><strong>Abstract:</strong> Instruction following is a fundamental ability of Large Language Models (LLMs), requiring their generated outputs to follow multiple constraints imposed in input instructions. Numerous studies have attempted to enhance this ability through preference optimization or reinforcement learning based on reward signals from LLM-as-a-Judge. However, existing evaluation models for instruction following still possess many deficiencies, such as substantial costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM critic that can provide efficient and reliable assessments of constraint following in the instructions. We first develop a checklist generator to decompose instructions and generate constraint checklists. With the assistance of the checklists, we collect high-quality critique training data through a multi-stage critique filtering mechanism and employ a constraint-level preference optimization method to train IF-CRITIC. Extensive experiments demonstrate that the evaluation performance of IF-CRITIC can beat strong LLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable reward signals provided by IF-CRITIC, LLMs can achieve substantial performance gains in instruction-following optimization under lower computational overhead compared to strong LLM critic baselines.</p>
  </div>
</details>

<hr>
<h3 id="169-Prompt-R1-Collaborative-Automatic-Prompting-Framework-via-End-to-end-Reinforcement-Learning-cs-CLPDF"><a href="#169-Prompt-R1-Collaborative-Automatic-Prompting-Framework-via-End-to-end-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[169] Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning cs.CLPDF"></a>[169] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01016">Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01016" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenjin Liu, Haoran Luo, Xueyuan Lin, Haoming Liu, Tiesunlong Shen</span></p>
<p><strong>TL;DR:</strong> Prompt-R1 是一个端到端的强化学习框架，通过小规模LLM与大规模LLM协作生成提示，解决复杂问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 用户难以提供准确的提示以充分利用大语言模型的性能，限制了其解决问题的能力。</p>
<p><strong>Result:</strong> 在多个公共数据集上显著优于基线模型。</p>
<p><strong>Insight:</strong> 小规模LLM与大规模LLM的协作可以显著提升复杂问题的解决能力。</p>
<p><strong>Abstract:</strong> Recently, advanced large language models (LLMs) have emerged at an increasingly rapid pace. However, when faced with complex problems, most users are often unable to provide accurate and effective prompts to interact with LLMs, thus limiting the performance of LLMs. To address this challenge, we propose Prompt-R1, an end-to-end reinforcement learning framework that uses a small-scale LLM to collaborate with large-scale LLMs, replacing user interaction to solve problems better. This collaboration is cast as a multi-turn prompt interaction, where the small-scale LLM thinks and generates prompts, and the large-scale LLM performs complex reasoning. A dual-constrained reward is designed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports both inference and training with various large-scale LLMs. Experiments on multiple public datasets show that Prompt-R1 significantly outperforms baseline models across tasks. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/QwenQKing/Prompt-R1">https://github.com/QwenQKing/Prompt-R1</a>.</p>
  </div>
</details>

<hr>
<h3 id="170-VayuChat-An-LLM-Powered-Conversational-Interface-for-Air-Quality-Data-Analytics-cs-CLPDF"><a href="#170-VayuChat-An-LLM-Powered-Conversational-Interface-for-Air-Quality-Data-Analytics-cs-CLPDF" class="headerlink" title="[170] VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics cs.CLPDF"></a>[170] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01046">VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01046" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vedant Acharya, Abhay Pisharodi, Rishabh Mondal, Mohammad Rafiuddin, Nipun Batra</span></p>
<p><strong>TL;DR:</strong> VayuChat是一个基于LLM的对话系统，旨在通过自然语言交互简化空气质量数据分析，为决策者、研究者和公众提供可执行代码和交互式可视化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 印度每年因空气污染造成约160万人过早死亡，但现有工具需要专业知识且提供静态面板，无法解决关键政策问题。</p>
<p><strong>Result:</strong> VayuChat已公开部署，支持用户通过简单对话完成复杂环境分析，提升了数据科学的可访问性。</p>
<p><strong>Insight:</strong> 结合LLM与多源数据可以显著降低数据分析门槛，为政策制定和公众参与提供新途径。</p>
<p><strong>Abstract:</strong> Air pollution causes about 1.6 million premature deaths each year in India, yet decision makers struggle to turn dispersed data into decisions. Existing tools require expertise and provide static dashboards, leaving key policy questions unresolved. We present VayuChat, a conversational system that answers natural language questions on air quality, meteorology, and policy programs, and responds with both executable Python code and interactive visualizations. VayuChat integrates data from Central Pollution Control Board (CPCB) monitoring stations, state-level demographics, and National Clean Air Programme (NCAP) funding records into a unified interface powered by large language models. Our live demonstration will show how users can perform complex environmental analytics through simple conversations, making data science accessible to policymakers, researchers, and citizens. The platform is publicly deployed at <a target="_blank" rel="noopener" href="https://huggingface.co/spaces/SustainabilityLabIITGN/">https://huggingface.co/spaces/SustainabilityLabIITGN/</a> VayuChat. For further information check out video uploaded on <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=d6rklL05cs4">https://www.youtube.com/watch?v=d6rklL05cs4</a>.</p>
  </div>
</details>

<hr>
<h3 id="171-Building-a-Silver-Standard-Dataset-from-NICE-Guidelines-for-Clinical-LLMs-cs-CLPDF"><a href="#171-Building-a-Silver-Standard-Dataset-from-NICE-Guidelines-for-Clinical-LLMs-cs-CLPDF" class="headerlink" title="[171] Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs cs.CLPDF"></a>[171] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01053">Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01053" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qing Ding, Eric Hua Qing Zhang, Felix Jozsa, Julia Ive</span></p>
<p><strong>TL;DR:</strong> 该论文构建了一个基于NICE指南的银标准数据集，用于评估临床大型语言模型（LLMs）在指南依从性和临床实用性方面的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前医疗领域缺乏标准化的数据集来评估LLMs在临床指南推理中的表现，为了解决这一问题，作者构建了一个新数据集。</p>
<p><strong>Result:</strong> 论文展示了数据集的可行性，并通过评估多个LLMs验证了其作为临床指南依从性基准的有效性。</p>
<p><strong>Insight:</strong> 该数据集为未来临床LLMs的评估提供了实用工具，同时强调了标准化基准对医疗AI发展的重要性。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly used in healthcare, yet standardised benchmarks for evaluating guideline-based clinical reasoning are missing. This study introduces a validated dataset derived from publicly available guidelines across multiple diagnoses. The dataset was created with the help of GPT and contains realistic patient scenarios, as well as clinical questions. We benchmark a range of recent popular LLMs to showcase the validity of our dataset. The framework supports systematic evaluation of LLMs’ clinical utility and guideline adherence.</p>
  </div>
</details>

<hr>
<h3 id="172-HPLT-3-0-Very-Large-Scale-Multilingual-Resources-for-LLM-and-MT-Mono-and-Bi-lingual-Data-Multilingual-Evaluation-and-Pre-Trained-Models-cs-CLPDF"><a href="#172-HPLT-3-0-Very-Large-Scale-Multilingual-Resources-for-LLM-and-MT-Mono-and-Bi-lingual-Data-Multilingual-Evaluation-and-Pre-Trained-Models-cs-CLPDF" class="headerlink" title="[172] HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models cs.CLPDF"></a>[172] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01066">HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01066" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Stephan Oepen, Nikolay Arefev, Mikko Aulamo, Marta Bañón, Maja Buljan</span></p>
<p><strong>TL;DR:</strong> 这篇论文介绍了HPLT~3.0计划，提供了一种开放的、超大规模、高质量且多语言标注的文本数据集，涵盖近200种语言，总计30万亿标记，可能是目前最大的公开多语言LLM预训练数据集。同时，论文还提供了完整的开源数据处理流程、多语言评估基准以及预训练模型家族。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着多语言大模型的需求增长，缺乏高质量的、大规模的多语言数据集成为主要瓶颈。HPLT~3.0旨在填补这一空白，提供覆盖广泛语言的数据资源和工具，以支持LLM和多语言机器翻译的研究与应用。</p>
<p><strong>Result:</strong> 通过数据质量探针、手动检查和端到端评测，验证了数据集的高质量。评测结果显示训练出的模型在多语言任务中表现优异。</p>
<p><strong>Insight:</strong> 1. 大规模、高质量的多语言数据集是LLM发展的关键；2. 开源工具和数据共享有助于推动多语言研究；3. 合成数据和自动挖掘的平行语料可以补充传统资源的不足。</p>
<p><strong>Abstract:</strong> We present an ongoing initiative to provide open, very large, high-quality, and richly annotated textual datasets for almost 200 languages. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. These datasets are derived from web crawls from different sources and accompanied with a complete, open-source pipeline for document selection from web archives, text extraction from HTML, language identification for noisy texts, exact and near-deduplication, annotation with, among others, register labels, text quality estimates, and personally identifiable information; and final selection and filtering. We report on data quality probes through contrastive and analytical statistics, through manual inspection of samples for 24 languages, and through end-to-end evaluation of various language model architectures trained on this data. For multilingual LLM evaluation, we provide a comprehensive collection of benchmarks for nine European languages, with special emphasis on natively created tasks, mechanisms to mitigate prompt sensitivity, and refined normalization and aggregation of scores. Additionally, we train and evaluate a family of 57 monolingual encoder-decoder models, as well as a handful of monolingual GPT-like reference models. Besides the monolingual data and models, we also present a very large collection of parallel texts automatically mined from this data, together with a novel parallel corpus synthesized via machine translation.</p>
  </div>
</details>

<hr>
<h3 id="173-TSVer-A-Benchmark-for-Fact-Verification-Against-Time-Series-Evidence-cs-CLPDF"><a href="#173-TSVer-A-Benchmark-for-Fact-Verification-Against-Time-Series-Evidence-cs-CLPDF" class="headerlink" title="[173] TSVer: A Benchmark for Fact Verification Against Time-Series Evidence cs.CLPDF"></a>[173] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01101">TSVer: A Benchmark for Fact Verification Against Time-Series Evidence</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01101" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marek Strong, Andreas Vlachos</span></p>
<p><strong>TL;DR:</strong> TSVer是一个专注于时间序列证据的事实验证基准数据集，旨在解决现有数据集在结构化证据、判决理由和合成声明方面的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的事实验证系统在评估时间序列和数值数据推理能力时，受限于缺乏结构化证据、判决理由不充分或依赖合成声明的数据集。</p>
<p><strong>Result:</strong> 即使是先进的推理模型Gemini-2.5-Pro在时间序列验证上也表现不佳，准确率为63.37%，判决理由评分为48.63。</p>
<p><strong>Insight:</strong> 时间序列证据的事实验证具有挑战性，现有模型仍需改进。</p>
<p><strong>Abstract:</strong> Reasoning over temporal and numerical data, such as time series, is a crucial aspect of fact-checking. While many systems have recently been developed to handle this form of evidence, their evaluation remains limited by existing datasets, which often lack structured evidence, provide insufficient justifications for verdicts, or rely on synthetic claims. In this paper, we introduce TSVer, a new benchmark dataset for fact verification focusing on temporal and numerical reasoning with time-series evidence. TSVer contains 287 real-world claims sourced from 38 fact-checking organizations and a curated database of 400 time series covering diverse domains. Each claim is annotated with time frames across all pertinent time series, along with a verdict and justifications reflecting how the evidence is used to reach the verdict. Using an LLM-assisted multi-step annotation process, we improve the quality of our annotations and achieve an inter-annotator agreement of kappa&#x3D;0.745 on verdicts. We also develop a baseline for verifying claims against time-series evidence and show that even the state-of-the-art reasoning models like Gemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score on verdicts and an Ev2R score of 48.63 on verdict justifications.</p>
  </div>
</details>

<hr>
<h3 id="174-MicroRemed-Benchmarking-LLMs-in-Microservices-Remediation-cs-CL-cs-SE-68T50-I-2-7PDF"><a href="#174-MicroRemed-Benchmarking-LLMs-in-Microservices-Remediation-cs-CL-cs-SE-68T50-I-2-7PDF" class="headerlink" title="[174] MicroRemed: Benchmarking LLMs in Microservices Remediation cs.CL | cs.SE | 68T50 | I.2.7PDF"></a>[174] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01166">MicroRemed: Benchmarking LLMs in Microservices Remediation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.SE | 68T50 | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01166" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lingzhe Zhang, Yunpeng Zhai, Tong Jia, Chiming Duan, Minghua He</span></p>
<p><strong>TL;DR:</strong> 论文提出了MicroRemed基准和ThinkRemed多智能体框架，用于评估和提升LLM在微服务修复中的性能，展示了其在端到端任务中的挑战和改进潜力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLM在微服务修复中仍依赖人工设计的提示，亟需一种更自主的端到端修复方法，以推动研究进展。</p>
<p><strong>Result:</strong> 实验表明MicroRemed对当前LLM具有挑战性，ThinkRemed通过迭代推理显著提升了修复性能。</p>
<p><strong>Insight:</strong> 多智能体框架的反思和系统感知能力对LLM在复杂任务中的表现至关重要。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) integrated with agent-based reasoning frameworks have recently shown strong potential for autonomous decision-making and system-level operations. One promising yet underexplored direction is microservice remediation, where the goal is to automatically recover faulty microservice systems. Existing approaches, however, still rely on human-crafted prompts from Site Reliability Engineers (SREs), with LLMs merely converting textual instructions into executable code. To advance research in this area, we introduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end microservice remediation, where models must directly generate executable Ansible playbooks from diagnosis reports to restore system functionality. We further propose ThinkRemed, a multi-agent framework that emulates the reflective and perceptive reasoning of SREs. Experimental results show that MicroRemed presents substantial challenges to current LLMs, while ThinkRemed improves end-to-end remediation performance through iterative reasoning and system reflection. The benchmark is available at <a target="_blank" rel="noopener" href="https://github.com/LLM4AIOps/MicroRemed">https://github.com/LLM4AIOps/MicroRemed</a>.</p>
  </div>
</details>

<hr>
<h3 id="175-Self-Harmony-Learning-to-Harmonize-Self-Supervision-and-Self-Play-in-Test-Time-Reinforcement-Learning-cs-CL-cs-AI-cs-LGPDF"><a href="#175-Self-Harmony-Learning-to-Harmonize-Self-Supervision-and-Self-Play-in-Test-Time-Reinforcement-Learning-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[175] Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning cs.CL | cs.AI | cs.LGPDF"></a>[175] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01191">Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01191" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ru Wang, Wei Huang, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo</span></p>
<p><strong>TL;DR:</strong> Self-Harmony是一种无需人工标注的测试时间强化学习框架，通过结合自监督和自博弈方法，利用和谐均值聚合答案频率，显著提升了模型的鲁棒性和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有测试时间强化学习（TTRL）方法依赖多数投票等机制，容易陷入虚假但流行的答案陷阱。因此，需要一种无需人工干预的方法来构建可靠的学习信号。</p>
<p><strong>Result:</strong> 在30个推理基准中的28个中排名第一，且在所有实验中未出现训练失败，显示出极高的稳定性和可靠性。</p>
<p><strong>Insight:</strong> 通过结合自监督和自博弈方法，Self-Harmony能够在不依赖人工标注的情况下生成可靠的学习信号，为测试时间强化学习提供了新的思路。</p>
<p><strong>Abstract:</strong> Test-time reinforcement learning (TTRL) offers a label-free paradigm for adapting models using only synthetic signals at inference, but its success hinges on constructing reliable learning signals. Standard approaches such as majority voting often collapse to spurious yet popular answers. We introduce Self-Harmony, a framework built on a simple intuition: the correct answer should remain stable across both an original question and its paraphrase. Self-Harmony operationalizes this by employing a single model in two complementary roles: a Solver to produce answers and a Reframer to rephrase the input. Based on this, we further propose a pseudo-label method: instead of majority voting, it aggregates answer frequencies across these original and reframed views using the harmonic mean. This is a process that naturally selects for solutions stable under reframing, thereby avoiding the common trap of favoring view-dependent, spurious answers. Crucially, this requires no human supervision or auxiliary models. Across diverse reasoning benchmarks, Self-Harmony achieves state-of-the-art results at the label-free test-time setting, ranking first in 28 of 30 settings across multiple methods. Beyond accuracy, it demonstrates unprecedented robustness, with zero training failures in all experiments, underscoring its stability and reliability.</p>
  </div>
</details>

<hr>
<h3 id="176-DEER-Disentangled-Mixture-of-Experts-with-Instance-Adaptive-Routing-for-Generalizable-Machine-Generated-Text-Detection-cs-CLPDF"><a href="#176-DEER-Disentangled-Mixture-of-Experts-with-Instance-Adaptive-Routing-for-Generalizable-Machine-Generated-Text-Detection-cs-CLPDF" class="headerlink" title="[176] DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection cs.CLPDF"></a>[176] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01192">DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01192" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guoxin Ma, Xiaoming Liu, Zhanhan Zhang, Chengzhengxu Li, Shengchao Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为DEER的新型框架，通过两阶段的解耦专家混合架构，结合实例自适应路由，以提升跨领域的机器生成文本检测能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大型语言模型的快速发展，机器生成文本的检测变得至关重要，但现有方法在领域转移时性能显著下降。</p>
<p><strong>Result:</strong> 在五个领域内和五个领域外数据集上，DEER平均F1分数分别提升1.39%和5.32%，准确率分别提升1.35%和3.61%。</p>
<p><strong>Insight:</strong> 解耦专家设计和自适应路由机制对提升模型性能至关重要，尤其在跨领域场景中表现突出。</p>
<p><strong>Abstract:</strong> Detecting machine-generated text (MGT) has emerged as a critical challenge, driven by the rapid advancement of large language models (LLMs) capable of producing highly realistic, human-like content. However, the performance of current approaches often degrades significantly under domain shift. To address this challenge, we propose a novel framework designed to capture both domain-specific and domain-general MGT patterns through a two-stage Disentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a disentangled mixture-of-experts module, in which domain-specific experts learn fine-grained, domain-local distinctions between human and machine-generated text, while shared experts extract transferable, cross-domain features. Second, to mitigate the practical limitation of unavailable domain labels during inference, we design a reinforcement learning-based routing mechanism that dynamically selects the appropriate experts for each input instance, effectively bridging the train-inference gap caused by domain uncertainty. Extensive experiments on five in-domain and five out-of-domain benchmark datasets demonstrate that DEER consistently outperforms state-of-the-art methods, achieving average F1-score improvements of 1.39% and 5.32% on in-domain and out-of-domain datasets respectively, along with accuracy gains of 1.35% and 3.61% respectively. Ablation studies confirm the critical contributions of both disentangled expert specialization and adaptive routing to model performance.</p>
  </div>
</details>

<hr>
<h3 id="177-AraFinNews-Arabic-Financial-Summarisation-with-Domain-Adapted-LLMs-cs-CLPDF"><a href="#177-AraFinNews-Arabic-Financial-Summarisation-with-Domain-Adapted-LLMs-cs-CLPDF" class="headerlink" title="[177] AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs cs.CLPDF"></a>[177] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01265">AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01265" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mo El-Haj, Paul Rayson</span></p>
<p><strong>TL;DR:</strong> 论文介绍了AraFinNews，最大的阿拉伯金融新闻数据集，并研究了领域适应性对大型语言模型（LLMs）在阿拉伯金融文本摘要中的影响，发现领域适应的模型表现更好。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究领域特定性对阿拉伯金融文本摘要的影响，填补了阿拉伯金融新闻数据集的空白。</p>
<p><strong>Result:</strong> 领域适应模型在事实准确性、数值可靠性和风格一致性上表现更优。</p>
<p><strong>Insight:</strong> 领域特定性适应是提高阿拉伯金融摘要质量和一致性的关键。</p>
<p><strong>Abstract:</strong> This paper investigates the impact of domain specificity on abstractive summarisation of Arabic financial texts using large language models (LLMs). We introduce AraFinNews, the largest publicly available Arabic financial news dataset to date, comprising 212,500 article–headline pairs spanning nearly a decade of reporting from October 2015 to July 2025. Designed as the Arabic equivalent of major English summarisation corpora such as CNN&#x2F;DailyMail, AraFinNews provides a robust benchmark for evaluating domain-specific language understanding and generation in financial contexts. Using this resource, we evaluate transformer-based models – including mT5, AraT5, and the domain-adapted FinAraT5 – to examine how financial-domain pretraining influences factual accuracy, numerical reliability, and stylistic alignment with professional reporting. Experimental results show that domain-adapted models generate more faithful and coherent summaries, particularly in handling quantitative and entity-centric information. The findings highlight the importance of domain-specific adaptation for improving factual consistency and narrative fluency in Arabic financial summarisation. The dataset is freely available for non-commercial research at <a target="_blank" rel="noopener" href="https://github.com/ArabicNLP-UK/AraFinNews">https://github.com/ArabicNLP-UK/AraFinNews</a>.</p>
  </div>
</details>

<hr>
<h3 id="178-DeepSpecs-Expert-Level-Questions-Answering-in-5G-cs-CL-cs-AI-cs-NIPDF"><a href="#178-DeepSpecs-Expert-Level-Questions-Answering-in-5G-cs-CL-cs-AI-cs-NIPDF" class="headerlink" title="[178] DeepSpecs: Expert-Level Questions Answering in 5G cs.CL | cs.AI | cs.NIPDF"></a>[178] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01305">DeepSpecs: Expert-Level Questions Answering in 5G</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.NI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01305" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu</span></p>
<p><strong>TL;DR:</strong> DeepSpecs 是一个结合结构与时间推理的 RAG 系统，用于回答 5G 技术规范中的专家级问题，通过显式解析跨引用和追踪规范演化显著提升答案质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于语义相似的 RAG 框架无法可靠解析 5G 规范中的跨引用或处理规范的演化问题，需要一种更强大的方法。</p>
<p><strong>Result:</strong> 在多 LLM 后端测试中，DeepSpecs 优于基线模型和现有电信 RAG 系统，验证其方法的有效性。</p>
<p><strong>Insight:</strong> 建模 5G 标准的结构和时间特性对提升专家级问答至关重要。</p>
<p><strong>Abstract:</strong> 5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.</p>
  </div>
</details>

<hr>
<h3 id="179-DEEPAMBIGQA-Ambiguous-Multi-hop-Questions-for-Benchmarking-LLM-Answer-Completeness-cs-CL-cs-AIPDF"><a href="#179-DEEPAMBIGQA-Ambiguous-Multi-hop-Questions-for-Benchmarking-LLM-Answer-Completeness-cs-CL-cs-AIPDF" class="headerlink" title="[179] DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness cs.CL | cs.AIPDF"></a>[179] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01323">DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01323" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiabao Ji, Min Li, Priyanshu Kumar, Shiyu Chang, Saloni Potdar</span></p>
<p><strong>TL;DR:</strong> 论文《DEEPAMBIGQA》提出了一个自动生成数据集的管道DeepAmbigQAGen，构建了DeepAmbigQA数据集，用于评估语言模型在开放性问答任务中对多跳推理和名称歧义的分辨能力。实验显示当前最先进的GPT-5在这些任务上表现不佳，突显了更强大问答系统的需求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的问答基准很少同时评估语言模型在多跳推理和名称歧义分辨上的能力，尤其是在复杂问题中如何整合证据的能力。为了解决这一问题，作者设计了新数据集和评估方法。</p>
<p><strong>Result:</strong> 实验结果表明，即使是GPT-5在处理歧义性问题和多跳推理问题时也表现不佳，精确匹配率仅为0.13（歧义问题）和0.21（非歧义问题）。</p>
<p><strong>Insight:</strong> 该研究表明，现有语言模型在处理复杂问题时仍存在显著不足，尤其是在信息整合和歧义分辨方面，未来需要开发更强大的问答系统以实现更高的答案完整性。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) with integrated search tools show strong promise in open-domain question answering (QA), yet they often struggle to produce complete answer set to complex questions such as Which actor from the film Heat won at least one Academy Award?, which requires (1) distinguishing between multiple films sharing the same title and (2) reasoning across a large set of actors to gather and integrate evidence. Existing QA benchmarks rarely evaluate both challenges jointly. To address this, we introduce DeepAmbigQAGen, an automatic data generation pipeline that constructs QA tasks grounded in text corpora and linked knowledge graph, generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning. Based on this, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop reasoning and half of them explicit name ambiguity resolving. Experiments reveal that, even state-of-the-art GPT-5 show incomplete answers, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous questions. These findings highlight the need for more robust QA systems aimed at information gathering and answer completeness.</p>
  </div>
</details>

<hr>
<h3 id="180-Thinking-with-DistilQwen-A-Tale-of-Four-Distilled-Reasoning-and-Reward-Model-Series-cs-CL-cs-AIPDF"><a href="#180-Thinking-with-DistilQwen-A-Tale-of-Four-Distilled-Reasoning-and-Reward-Model-Series-cs-CL-cs-AIPDF" class="headerlink" title="[180] Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series cs.CL | cs.AIPDF"></a>[180] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01354">Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01354" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang</span></p>
<p><strong>TL;DR:</strong> 论文介绍了DistilQwen模型家族的四个新系列，针对工业需求设计，分别适用于高精度推理任务、动态调参推理任务和增强学习奖励模型，展示了高效推理和强大性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 工业界对小而高效的推理模型的需求推动了知识蒸馏技术的发展，以平衡推理性能和推理速度。</p>
<p><strong>Result:</strong> 在多个基准测试中展示了高效的推理性能和实用性，并支持Alibaba Cloud PAI平台的工业级训练和推理。</p>
<p><strong>Insight:</strong> 知识蒸馏技术可以有效平衡推理性能和效率，动态调参模型为多样化任务提供了新的解决方案。</p>
<p><strong>Abstract:</strong> Recently, the demand for small and efficient reasoning models to support real-world applications has driven the development of knowledge distillation techniques that balance reasoning performance and inference speed. In this paper, we further extend the DistilQwen model family, initialized from the Qwen models, by introducing four model series specifically designed to meet industrial requirements. The distilled model collection comprises: (1) slow-thinking models, optimized for reasoning tasks that require high accuracy; (2) two series of adaptive-thinking models, which dynamically adjust reasoning strategies based on input tasks to maximize efficiency across diverse scenarios; and (3) distilled reward models, which enable further reinforcement learning of reasoning models using distilled knowledge. Comprehensive evaluations across multiple benchmarks demonstrate both high inference efficiency and strong reasoning performance for these models, as well as the practical utility of distilled reward models. We further show that these models support industry practitioners by providing scalable training and inference functionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence) platform.</p>
  </div>
</details>

<hr>
<h3 id="181-The-Ouroboros-of-Benchmarking-Reasoning-Evaluation-in-an-Era-of-Saturation-cs-CLPDF"><a href="#181-The-Ouroboros-of-Benchmarking-Reasoning-Evaluation-in-an-Era-of-Saturation-cs-CLPDF" class="headerlink" title="[181] The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation cs.CLPDF"></a>[181] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01365">The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01365" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">İbrahim Ethem Deveci, Duygu Ataman</span></p>
<p><strong>TL;DR:</strong> 这篇论文探讨了当前大型语言模型（LLM）和大型推理模型（LRM）评测基准的局限性，指出由于模型能力的提升和数据集的潜在污染，评测结果逐渐饱和，导致需要不断更新更具挑战性的基准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLM和LRM能力的快速发展，评测基准的饱和现象愈发明显，作者质疑当前的评测是否能真正反映模型的推理能力，而不是仅仅追逐数字的提升。</p>
<p><strong>Result:</strong> 研究发现，评测基准的结果容易被模型能力的提升和数据污染所影响，导致评测结果不再能准确反映模型的真实推理能力。</p>
<p><strong>Insight:</strong> 论文指出评测基准的设计需要更加科学和鲁棒，以避免陷入“数字追逐”的循环，同时强调了未来研究需要更注重评测基准的质量而非数量。</p>
<p><strong>Abstract:</strong> The rapid rise of Large Language Models (LLMs) and Large Reasoning Models (LRMs) has been accompanied by an equally rapid increase of benchmarks used to assess them. However, due to both improved model competence resulting from scaling and novel training advances as well as likely many of these datasets being included in pre or post training data, results become saturated, driving a continuous need for new and more challenging replacements. In this paper, we discuss whether surpassing a benchmark truly demonstrates reasoning ability or are we simply tracking numbers divorced from the capabilities we claim to measure? We present an investigation focused on three model families, OpenAI, Anthropic, and Google, and how their reasoning capabilities across different benchmarks evolve over the years. We also analyze performance trends over the years across different reasoning tasks and discuss the current situation of benchmarking and remaining challenges. By offering a comprehensive overview of benchmarks and reasoning tasks, our work aims to serve as a first reference to ground future research in reasoning evaluation and model development.</p>
  </div>
</details>

<hr>
<h3 id="182-RAGSmith-A-Framework-for-Finding-the-Optimal-Composition-of-Retrieval-Augmented-Generation-Methods-Across-Datasets-cs-CL-cs-AI-cs-IR-H-3-3-I-2-7PDF"><a href="#182-RAGSmith-A-Framework-for-Finding-the-Optimal-Composition-of-Retrieval-Augmented-Generation-Methods-Across-Datasets-cs-CL-cs-AI-cs-IR-H-3-3-I-2-7PDF" class="headerlink" title="[182] RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets cs.CL | cs.AI | cs.IR | H.3.3; I.2.7PDF"></a>[182] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01386">RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.IR | H.3.3; I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01386" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhammed Yusuf Kartal, Suha Kagan Kose, Korhan Sevinç, Burak Aktas</span></p>
<p><strong>TL;DR:</strong> RAGSmith是一个模块化框架，通过遗传搜索优化检索增强生成（RAG）的端到端架构，涵盖9种技术家族和46,080种配置。在多个领域的实验中，RAGSmith显著优于基线，提供领域相关的优化建议。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> RAG的性能依赖于多个交互环节（检索、排序、增强、提示、生成），孤立优化模块效果不稳定。因此，需要一种端到端的方法来优化整体RAG系统。</p>
<p><strong>Result:</strong> 在6个领域的实验中，RAGSmith平均比基线提升3.8%（最高提升12.5%），并通过搜索少量配置（约100个）发现稳健的主干方法（如向量检索+生成后反思）。</p>
<p><strong>Insight:</strong> 优化效果与问题类型相关，事实类&#x2F;长答案问题提升更大；领域相关的扩展、重排序和提示优化是关键，而段落压缩未被选中。展示了演化搜索在全管道优化中的实用性。</p>
<p><strong>Abstract:</strong> Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8% on average (range +1.2% to +6.9% across domains), with gains up to +12.5% in retrieval and +7.5% in generation. The search typically explores $\approx 0.2%$ of the space ($\sim 100$ candidates) and discovers a robust backbone – vector retrieval plus post-generation reflection&#x2F;revision – augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual&#x2F;long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.</p>
  </div>
</details>

<hr>
<h3 id="183-LiveSearchBench-An-Automatically-Constructed-Benchmark-for-Retrieval-and-Reasoning-over-Dynamic-Knowledge-cs-CLPDF"><a href="#183-LiveSearchBench-An-Automatically-Constructed-Benchmark-for-Retrieval-and-Reasoning-over-Dynamic-Knowledge-cs-CLPDF" class="headerlink" title="[183] LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge cs.CLPDF"></a>[183] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01409">LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01409" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Heng Zhou, Ao Yu, Yuchen Fan, Jianing Shi, Li Kang</span></p>
<p><strong>TL;DR:</strong> LiveSearchBench是一个自动构建的动态知识检索与推理基准，通过捕捉知识库的动态变化生成问题和答案，用于评估大语言模型在处理最新知识时的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型评测基准多为静态，无法反映知识的动态更新，低估了检索的作用，因此需要一种能够持续生成依赖于最新知识的评测基准。</p>
<p><strong>Result:</strong> 实验表明，模型在面对预训练日期之后的事实时表现显著下降，检索增强方法和更大规模的指令调优模型仅带来部分提升。</p>
<p><strong>Insight:</strong> LiveSearchBench将评测重点从静态记忆转向动态检索与推理，为大语言模型在动态知识下的长期系统评估提供了基础。</p>
<p><strong>Abstract:</strong> Evaluating large language models (LLMs) on question answering often relies on static benchmarks that reward memorization and understate the role of retrieval, failing to capture the dynamic nature of world knowledge. We present LiveSearchBench, an automated pipeline for constructing retrieval-dependent benchmarks from recent knowledge updates. Our method computes deltas between successive Wikidata snapshots, filters candidate triples for quality, and synthesizes natural-language questions at three levels of reasoning difficulty, each guaranteed to admit a unique, verifiable answer through SPARQL validation. The pipeline is fully automated, scalable across time, and minimizes human intervention, enabling continual regeneration of temporally grounded benchmarks. Experiments show a pronounced performance drop when models confront facts that post-date pretraining, with the gap most salient on multi-hop queries. Retrieval augmented methods and larger, instruction-tuned models provide partial gains but fail to close this recency gap. By design, LiveSearchBench shifts evaluation from static memorization toward tasks that require up-to-date retrieval and reasoning, offering a foundation for systematic, long-term assessment of LLMs under evolving knowledge.</p>
  </div>
</details>

<hr>
<h3 id="184-“Don’t-Teach-Minerva”-Guiding-LLMs-Through-Complex-Syntax-for-Faithful-Latin-Translation-with-RAG-cs-CL-cs-DLPDF"><a href="#184-“Don’t-Teach-Minerva”-Guiding-LLMs-Through-Complex-Syntax-for-Faithful-Latin-Translation-with-RAG-cs-CL-cs-DLPDF" class="headerlink" title="[184] “Don’t Teach Minerva”: Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG cs.CL | cs.DLPDF"></a>[184] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01454">“Don’t Teach Minerva”: Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.DL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01454" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sergio Torres Aguilar</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于RAG的开放源代码流程，通过分阶段生成和精修拉丁文翻译，使开源LLM性能达到与顶级专有系统相当的水平。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 拉丁文是一种形态丰富且资源稀缺的语言，翻译难度大。论文旨在通过开源方法提升翻译性能，减少对专有系统的依赖。</p>
<p><strong>Result:</strong> 开源RAG系统性能与GPT-5相当，无需针对任务的LLM微调。</p>
<p><strong>Insight:</strong> 分阶段生成与RAG结合能显著提升LLM在低资源语言翻译中的表现，为开源方案提供了可行性。</p>
<p><strong>Abstract:</strong> Translating a morphology-rich, low-resource language like Latin poses significant challenges. This paper introduces a reproducible draft-based refinement pipeline that elevates open-source Large Language Models (LLMs) to a performance level statistically comparable to top-tier proprietary systems. Our method first uses a fine-tuned NLLB-1.3B model to generate a high-quality, structurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes this draft, a process that can be further enhanced by augmenting the context with retrieved out-context examples (RAG). We demonstrate the robustness of this approach on two distinct benchmarks: a standard in-domain test set (Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of 12th-century Latin letters (2025). Our central finding is that this open-source RAG system achieves performance statistically comparable to the GPT-5 baseline, without any task-specific LLM fine-tuning. We release the pipeline, the Chartres OOD set, and evaluation scripts and models to facilitate replicability and further research.</p>
  </div>
</details>

<hr>
<h3 id="185-BARD-budget-aware-reasoning-distillation-cs-CLPDF"><a href="#185-BARD-budget-aware-reasoning-distillation-cs-CLPDF" class="headerlink" title="[185] BARD: budget-aware reasoning distillation cs.CLPDF"></a>[185] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01470">BARD: budget-aware reasoning distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01470" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lujie Niu, Lei Shen, Yi Jiang, Caixia Yuan, Xiaojie Wang</span></p>
<p><strong>TL;DR:</strong> BARD是一个新的蒸馏框架，旨在通过动态控制推理长度和计算预算，高效地将推理能力从大模型迁移到小模型中。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的Chain-of-Thought（CoT）蒸馏方法虽然有效，但推理过程冗余且无法控制计算预算，导致资源使用低效。</p>
<p><strong>Result:</strong> 实验表明，BARD使8B参数的学生模型在多个推理基准上表现优异，并能灵活适应不同预算需求。</p>
<p><strong>Insight:</strong> 通过结合SFT和RL的两阶段训练，BARD有效避免了策略退化，同时实现了推理能力和预算控制的优化。</p>
<p><strong>Abstract:</strong> While long Chain-of-Thought (CoT) distillation effectively transfers reasoning capability to smaller language models, the reasoning process often remains redundant and computational budget uncontrollable, leading to inefficient resource usage. To address this limitation, we propose \textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that simultaneously distills reasoning capability and enables fine-grained control over the reasoning length. BARD uses the thinking budget as a user-specified control signal, allowing the model to dynamically balance reasoning performance and computational efficiency. To achieve this concept, BARD introduces a two-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on teacher-generated long CoT data compressed to various budget levels, bootstrapping the model’s understanding of budget constraints. The second phase leverages Reinforcement Learning (RL) from a reward signal in consideration of reasoning performance and budget fidelity simultaneously. Incorporating the two-phase regimen is crucial to avoiding policy degradation and ensuring that both objectives are optimized jointly. Extensive experiments demonstrate that our method empowers an 8B student model to achieve strong performance on challenging reasoning benchmarks (\textit{AIME24, AIME25, GPQA}) while providing precise and adaptive control over its reasoning length across a wide range of budgets.</p>
  </div>
</details>

<hr>
<h3 id="186-Towards-Consistent-Detection-of-Cognitive-Distortions-LLM-Based-Annotation-and-Dataset-Agnostic-Evaluation-cs-CLPDF"><a href="#186-Towards-Consistent-Detection-of-Cognitive-Distortions-LLM-Based-Annotation-and-Dataset-Agnostic-Evaluation-cs-CLPDF" class="headerlink" title="[186] Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation cs.CLPDF"></a>[186] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01482">Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01482" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Neha Sharma, Navneet Agarwal, Kairit Sirts</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了利用大型语言模型（LLM）作为一致性更高的标注工具来解决认知扭曲检测任务中标注不一致的问题，并提出了一种数据集无关的评估框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于认知扭曲检测任务的主观性，即使是专家标注者的标注一致性也较低，导致不可靠的训练数据。论文试图通过LLM解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，GPT-4生成的标注一致性高（Fleiss’s Kappa &#x3D; 0.78），且基于这些数据训练的模型性能优于人类标注数据。</p>
<p><strong>Insight:</strong> LLM可以作为主观NLP任务中生成一致性训练数据的可扩展解决方案，尤其在标注一致性要求高的场景下。</p>
<p><strong>Abstract:</strong> Text-based automated Cognitive Distortion detection is a challenging task due to its subjective nature, with low agreement scores observed even among expert human annotators, leading to unreliable annotations. We explore the use of Large Language Models (LLMs) as consistent and reliable annotators, and propose that multiple independent LLM runs can reveal stable labeling patterns despite the inherent subjectivity of the task. Furthermore, to fairly compare models trained on datasets with different characteristics, we introduce a dataset-agnostic evaluation framework using Cohen’s kappa as an effect size measure. This methodology allows for fair cross-dataset and cross-study comparisons where traditional metrics like F1 score fall short. Our results show that GPT-4 can produce consistent annotations (Fleiss’s Kappa &#x3D; 0.78), resulting in improved test set performance for models trained on these annotations compared to those trained on human-labeled data. Our findings suggest that LLMs can offer a scalable and internally consistent alternative for generating training data that supports strong downstream performance in subjective NLP tasks.</p>
  </div>
</details>

<hr>
<h3 id="187-BanglaNirTox-A-Large-scale-Parallel-Corpus-for-Explainable-AI-in-Bengali-Text-Detoxification-cs-CL-cs-AIPDF"><a href="#187-BanglaNirTox-A-Large-scale-Parallel-Corpus-for-Explainable-AI-in-Bengali-Text-Detoxification-cs-CL-cs-AIPDF" class="headerlink" title="[187] BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification cs.CL | cs.AIPDF"></a>[187] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01512">BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01512" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ayesha Afroza Mohsin, Mashrur Ahsan, Nafisa Maliyat, Shanta Maria, Syed Rifat Raiyan</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个用于孟加拉语文本去毒（detoxification）的新方法，结合了Pareto优化的LLM和Chain-of-Thought提示，并构建了大规模平行语料库BanglaNirTox。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 孟加拉语中的毒性语言问题突出，尤其是在线环境，但由于资源有限，相关研究较少。</p>
<p><strong>Result:</strong> Pareto优化LLM结合CoT提示显著提升了孟加拉语文本去毒的质量和一致性。</p>
<p><strong>Insight:</strong> 资源不足的语言可以通过人工生成平行语料库结合优化模型实现有效的文本去毒。</p>
<p><strong>Abstract:</strong> Toxic language in Bengali remains prevalent, especially in online environments, with few effective precautions against it. Although text detoxification has seen progress in high-resource languages, Bengali remains underexplored due to limited resources. In this paper, we propose a novel pipeline for Bengali text detoxification that combines Pareto class-optimized large language models (LLMs) and Chain-of-Thought (CoT) prompting to generate detoxified sentences. To support this effort, we construct BanglaNirTox, an artificially generated parallel corpus of 68,041 toxic Bengali sentences with class-wise toxicity labels, reasonings, and detoxified paraphrases, using Pareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox dataset is used to fine-tune language models to produce better detoxified versions of Bengali sentences. Our findings show that Pareto-optimized LLMs with CoT prompting significantly enhance the quality and consistency of Bengali text detoxification.</p>
  </div>
</details>

<hr>
<h3 id="188-EngChain-A-Symbolic-Benchmark-for-Verifiable-Multi-Step-Reasoning-in-Engineering-cs-CL-cs-AI-cs-LGPDF"><a href="#188-EngChain-A-Symbolic-Benchmark-for-Verifiable-Multi-Step-Reasoning-in-Engineering-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[188] EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering cs.CL | cs.AI | cs.LGPDF"></a>[188] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01650">EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01650" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ayesha Gull, Muhammad Usman Safder, Rania Elbadry, Preslav Nakov, Zhuohan Xie</span></p>
<p><strong>TL;DR:</strong> 论文介绍了EngChain，一个用于验证多步工程问题解决的基准测试，旨在评估大型语言模型在复杂工程推理中的能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基准测试主要评估语言理解、事实回忆、数学或代码生成，但缺乏对工程领域多步推理的综合评估，尤其是在科学原理、定量建模和实践约束的综合应用上。</p>
<p><strong>Result:</strong> 展示了EngChain在评估LLM多步工程推理能力上的有效性，为未来的模型改进提供了方向。</p>
<p><strong>Insight:</strong> 强调了工程问题的复杂性和多学科融合的特点，指出了当前基准测试的局限性，并提出了未来评估方法的改进方向。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) are increasingly being applied to specialized, high-stakes domains like engineering, which demands rigorous evaluation of their complex reasoning capabilities. While current benchmarks assess language understanding, factual recall, mathematics or code generation, none capture the integrative reasoning central to engineering where scientific principles, quantitative modeling and practical constraints must converge. To address this gap, we introduce EngChain, a benchmark for verifiable multi-step engineering problem-solving. EngChain contains 90 problems spanning three engineering branches, organized into 9 domains and 20 distinct areas. The problems are generated from symbolic templates with a high degree of randomization to ensure diversity and eliminate the risk of contamination. With this benchmark, we move beyond final answer accuracy with a two-stage evaluation: we first quantitatively verify the numerical and semantic validity of each reasoning step and then introduce LLM-As-A-Judge, an automated system to qualitatively categorize the identified reasoning errors.</p>
  </div>
</details>

<hr>
<h3 id="189-SeaLLMs-Audio-Large-Audio-Language-Models-for-Southeast-Asia-cs-CL-cs-AIPDF"><a href="#189-SeaLLMs-Audio-Large-Audio-Language-Models-for-Southeast-Asia-cs-CL-cs-AIPDF" class="headerlink" title="[189] SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia cs.CL | cs.AIPDF"></a>[189] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01670">SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01670" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chaoqun Liu, Mahani Aljunied, Guizhen Chen, Hou Pong Chan, Weiwen Xu</span></p>
<p><strong>TL;DR:</strong> SeaLLMs-Audio是首个针对东南亚语言（印尼语、泰语、越南语）及英语和中文的大型音频语言模型（LALM），支持多模态输入和多任务处理，性能优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 东南亚语言在音频语言模型（LALM）领域的研究较少，SeaLLMs-Audio填补了这一空白，旨在支持多语言和多任务，推动该地区的研究和产业发展。</p>
<p><strong>Result:</strong> 实验表明，SeaLLMs-Audio在东南亚语言任务上表现优异，与其他LALM相比具有竞争力。</p>
<p><strong>Insight:</strong> SeaLLMs-Audio的成功表明，针对特定地区的多语言和多任务模型是可行的，且对区域研究和产业有重要推动作用。</p>
<p><strong>Abstract:</strong> We introduce SeaLLMs-Audio, the first large audio-language model (LALM) tailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai (th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a large-scale audio corpus, SeaLLMs-Audio exhibits strong performance across diverse audio-centric tasks, spanning fine-grained audio understanding and voice-based interaction. Its key features include: 1) Multilingual: the model primarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English, and Chinese; 2) Multimodal: the model accepts flexible input modalities, including audio only, text only, as well as audio with text; 3) Multi-task: the model supports a wide range of tasks, including audio analysis tasks such as Audio Captioning, Automatic Speech Recognition, Speech-to-Text Translation, Speech Emotion Recognition, Speech Question Answering, and Speech Summarization. It also enables voice-based dialogue, including answering factual, mathematical, and general knowledge queries. As a significant step towards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to benefit both the regional research community and industry. To automate LALM evaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark spanning multiple tasks. Experiments show that SeaLLMs-Audio achieves competitive performance compared with other LALMs on SEA languages.</p>
  </div>
</details>

<hr>
<h3 id="190-KV-Cache-Transform-Coding-for-Compact-Storage-in-LLM-Inference-cs-CL-cs-AI-cs-LGPDF"><a href="#190-KV-Cache-Transform-Coding-for-Compact-Storage-in-LLM-Inference-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[190] KV Cache Transform Coding for Compact Storage in LLM Inference cs.CL | cs.AI | cs.LGPDF"></a>[190] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01815">KV Cache Transform Coding for Compact Storage in LLM Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01815" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Konrad Staniszewski, Adrian Łańcucki</span></p>
<p><strong>TL;DR:</strong> KVTC是一种轻量级的变换编码器，通过PCA特征解相关、自适应量化和熵编码压缩KV缓存，实现高达20倍压缩，同时保持推理和长上下文准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模服务LLM时，KV缓存管理效率低下，占用稀缺的GPU内存或需重新计算，亟需高效压缩方法。</p>
<p><strong>Result:</strong> 在Llama 3等模型上实现20倍压缩（特定用例达40倍），优于现有方法（如token淘汰、量化、SVD）。</p>
<p><strong>Insight:</strong> KVTC通过利用KV缓存中的冗余，提出了一种内存高效的LLM服务方案，适用于多轮对话和代码编辑场景。</p>
<p><strong>Abstract:</strong> Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.</p>
  </div>
</details>

<hr>
<h3 id="191-Towards-Robust-Mathematical-Reasoning-cs-CL-cs-AIPDF"><a href="#191-Towards-Robust-Mathematical-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[191] Towards Robust Mathematical Reasoning cs.CL | cs.AIPDF"></a>[191] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01846">Towards Robust Mathematical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01846" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi</span></p>
<p><strong>TL;DR:</strong> 论文提出了IMO-Bench，一个针对国际数学奥林匹克（IMO）水平的数学推理评测套件，包括短答案和证明写作能力，帮助提升基础模型的数学推理能力，并在Gemini Deep Think模型中取得了显著成绩。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的数学推理评测过于简单或仅关注短答案，缺乏对高级推理和证明写作能力的评测。需要一种更严格的评测标准来推动模型能力的提升。</p>
<p><strong>Result:</strong> Gemini Deep Think在IMO-AnswerBench和IMO-Proof Bench上分别取得80.0%和65.7%的成绩，显著超越其他模型。</p>
<p><strong>Insight:</strong> 高级数学推理能力评测需要更严格的基准，自动评分与人类评分高度相关，证明了自动评测长答案的可行性。</p>
<p><strong>Abstract:</strong> Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at <a target="_blank" rel="noopener" href="https://imobench.github.io/">https://imobench.github.io/</a>.</p>
  </div>
</details>

<hr>
<div id='cs.CY'></div>

<h1 id="cs-CY-Back"><a href="#cs-CY-Back" class="headerlink" title="cs.CY [Back]"></a>cs.CY <a href="#toc">[Back]</a></h1><h3 id="192-Multimodal-Learning-with-Augmentation-Techniques-for-Natural-Disaster-Assessment-cs-CY-cs-AI-cs-CL-cs-CVPDF"><a href="#192-Multimodal-Learning-with-Augmentation-Techniques-for-Natural-Disaster-Assessment-cs-CY-cs-AI-cs-CL-cs-CVPDF" class="headerlink" title="[192] Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment cs.CY | cs.AI | cs.CL | cs.CVPDF"></a>[192] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00004">Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00004" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Adrian-Dinu Urse, Dumitru-Clementin Cercel, Florin Pop</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了多模态学习中数据增强技术在自然灾难评估中的应用，通过扩散方法和文本增强技术改善了数据不平衡和小样本问题，并在多模态和多视角学习中验证了效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自然灾难评估依赖于准确且快速获取信息，但现有数据集存在类别不平衡和样本量不足的问题，亟需有效的数据增强方法来提升模型性能。</p>
<p><strong>Result:</strong> 实验结果表明，所选增强方法显著提高了分类性能，尤其对少数类别的表现提升明显；多视角学习展示了潜力，但需进一步优化。</p>
<p><strong>Insight:</strong> 该研究为灾难评估系统提供了有效的数据增强策略，扩散方法和文本增强技术的结合在多模态学习中具有潜力，但多视角学习的复杂性仍需探索。</p>
<p><strong>Abstract:</strong> Natural disaster assessment relies on accurate and rapid access to information, with social media emerging as a valuable real-time source. However, existing datasets suffer from class imbalance and limited samples, making effective model development a challenging task. This paper explores augmentation techniques to address these issues on the CrisisMMD multimodal dataset. For visual data, we apply diffusion-based methods, namely Real Guidance and DiffuseMix. For text data, we explore back-translation, paraphrasing with transformers, and image caption-based augmentation. We evaluated these across unimodal, multimodal, and multi-view learning setups. Results show that selected augmentations improve classification performance, particularly for underrepresented classes, while multi-view learning introduces potential but requires further refinement. This study highlights effective augmentation strategies for building more robust disaster assessment systems.</p>
  </div>
</details>

<hr>
<h3 id="193-Chitchat-with-AI-Understand-the-supply-chain-carbon-disclosure-of-companies-worldwide-through-Large-Language-Model-cs-CY-cs-AI-cs-CL-cs-LG-stat-APPDF"><a href="#193-Chitchat-with-AI-Understand-the-supply-chain-carbon-disclosure-of-companies-worldwide-through-Large-Language-Model-cs-CY-cs-AI-cs-CL-cs-LG-stat-APPDF" class="headerlink" title="[193] Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model cs.CY | cs.AI | cs.CL | cs.LG | stat.APPDF"></a>[193] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00024">Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CL | cs.LG | stat.AP</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00024" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haotian Hang, Yueyang Shen, Vicky Zhu, Jose Cruz, Michelle Li</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于大语言模型（LLM）的决策支持框架，用于评估全球企业碳披露质量，揭示了技术和德国等国家在披露质量上的领先地位。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 企业碳披露的异质性和自由形式使得分析和比较变得困难，不利于监管和投资决策。</p>
<p><strong>Result:</strong> 技术和德国等地区和行业在披露质量上表现更优，揭示了披露质量的行业和地区差异。</p>
<p><strong>Insight:</strong> LLM方法为非结构化数据提供了可量化、可解释的解决方案，推动了气候治理中AI决策支持系统的应用。</p>
<p><strong>Abstract:</strong> In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world’s largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.</p>
  </div>
</details>

<hr>
<div id='cs.SE'></div>

<h1 id="cs-SE-Back"><a href="#cs-SE-Back" class="headerlink" title="cs.SE [Back]"></a>cs.SE <a href="#toc">[Back]</a></h1><h3 id="194-GrowthHacker-Automated-Off-Policy-Evaluation-Optimization-Using-Code-Modifying-LLM-Agents-cs-SE-cs-CL-cs-LGPDF"><a href="#194-GrowthHacker-Automated-Off-Policy-Evaluation-Optimization-Using-Code-Modifying-LLM-Agents-cs-SE-cs-CL-cs-LGPDF" class="headerlink" title="[194] GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents cs.SE | cs.CL | cs.LGPDF"></a>[194] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00802">GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SE | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00802" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie JW Wu, Ayanda Patrick Herlihy, Ahmad Saleem Mirza, Ali Afoud, Fatemeh Fard</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于LLM的自动化方法GrowthHacker，通过代码优化改进离线策略评估（OPE）性能，并在真实数据集上验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在线A&#x2F;B测试成本高且可能对用户产生负面影响，离线策略评估（OPE）成为重要替代方案。然而，如何利用LLM和相关智能体优化OPE性能尚未被充分探索。</p>
<p><strong>Result:</strong> two_agent框架实现了100%的可靠性和106.7%的平均改进效果，且45%的成功率高于基线方法AutoGen（34%）。</p>
<p><strong>Insight:</strong> LLM智能体可以高效地自动化优化OPE系统，为数据驱动决策的规模化应用提供了新途径。</p>
<p><strong>Abstract:</strong> With the software industry shifting toward a data-driven culture, online A&#x2F;B testing is a key tool for evaluating new technologies. However, deploying such experiments requires substantial resources, may negatively impact users, and involves long data collection periods. To address this, \textit{off-policy evaluation (OPE)}, or offline A&#x2F;B testing, uses logged data to assess technologies and is fundamental in Reinforcement Learning, making it crucial in domains where online testing is costly or risky, such as healthcare, recommender systems, education, dialog systems, and robotics. Despite advances in coding LLMs and agentic AI, little is known about leveraging them to optimize OPE results. We investigate whether LLMs and LLM-based agents can improve OPE performance via code optimization. We propose \textit{GrowthHacker}, a benchmark with agent and baseline methods on large-scale real-world datasets, which iteratively optimizes code, evaluates results, and begins new optimization cycles. We collected datasets, established protocols, implemented baselines for OPE on the Open Bandit Pipeline (OBP)<del>\cite{saito2021openbanditdatasetpipeline} and Scope-RL</del>\cite{kiyohara2023scope}, and developed the \textit{two_agent} framework, which reduces system complexity while preserving optimization effectiveness. Results show the two_agent framework achieves 100% reliability and the highest average improvement of 106.7% among positive outcomes. Both two_agent and CrewAI reach 45% success rates, outperforming AutoGen’s 34%. These findings demonstrate the feasibility of LLM-based agents as automated “growth hackers” to enhance OPE systems, with implications for scaling data-driven decision-making in production.</p>
  </div>
</details>

<hr>
<h3 id="195-HarnessLLM-Automatic-Testing-Harness-Generation-via-Reinforcement-Learning-cs-SE-cs-CLPDF"><a href="#195-HarnessLLM-Automatic-Testing-Harness-Generation-via-Reinforcement-Learning-cs-SE-cs-CLPDF" class="headerlink" title="[195] HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning cs.SE | cs.CLPDF"></a>[195] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01104">HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SE | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01104" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujian Liu, Jiabao Ji, Yang Zhang, Wenbo Guo, Tommi Jaakkola</span></p>
<p><strong>TL;DR:</strong> HarnessLLM提出了一种基于强化学习的自动化测试框架生成方法，通过两阶段训练（SFT和RLVR）生成多样化的测试用例，提高了错误检测能力和测试策略多样性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于LLM的测试生成方法仅生成输入输出对，缺乏测试用例多样性和足够的调试信息，难以支持复杂的测试场景。</p>
<p><strong>Result:</strong> HarnessLLM在错误发现能力和测试策略多样性上优于基于输入输出的测试方法，同时通过测试时扩展提升了代码生成性能。</p>
<p><strong>Insight:</strong> 通过强化学习训练LLM生成测试框架代码，可以显著提高测试用例的复杂性和多样性，从而更有效地发现程序错误。</p>
<p><strong>Abstract:</strong> Existing LLM-based automatic test generation methods mainly produce input and expected output pairs to categorize the intended behavior of correct programs. Although straightforward, these methods have limited diversity in generated tests and cannot provide enough debugging information. We propose HarnessLLM, a two-stage training pipeline that enables LLMs to write harness code for testing. Particularly, LLMs generate code that synthesizes inputs and validates the observed outputs, allowing complex test cases and flexible output validation such as invariant checking. To achieve this, we train LLMs with SFT followed by RLVR with a customized reward design. Experiments show that HarnessLLM outperforms input-output-based testing in bug finding and testing strategy diversity. HarnessLLM further benefits the code generation performance through test-time scaling with our generated test cases as inference-phase validation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/HarnessLLM.git">https://github.com/UCSB-NLP-Chang/HarnessLLM.git</a>.</p>
  </div>
</details>

<hr>
<div id='math.NA'></div>

<h1 id="math-NA-Back"><a href="#math-NA-Back" class="headerlink" title="math.NA [Back]"></a>math.NA <a href="#toc">[Back]</a></h1><h3 id="196-Three-dimensional-narrow-volume-reconstruction-method-with-unconditional-stability-based-on-a-phase-field-Lagrange-multiplier-approach-math-NA-cs-CG-cs-CV-cs-NA-65M06-65M12-35K57-65D18PDF"><a href="#196-Three-dimensional-narrow-volume-reconstruction-method-with-unconditional-stability-based-on-a-phase-field-Lagrange-multiplier-approach-math-NA-cs-CG-cs-CV-cs-NA-65M06-65M12-35K57-65D18PDF" class="headerlink" title="[196] Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach math.NA | cs.CG | cs.CV | cs.NA | 65M06, 65M12, 35K57, 65D18PDF"></a>[196] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00508">Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">math.NA | cs.CG | cs.CV | cs.NA | 65M06, 65M12, 35K57, 65D18</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00508" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Renjun Gao, Xiangjie Kong, Dongting Cai, Boyi Fu, Junxiang Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于相场拉格朗日乘子法的三维窄体积重建方法，具有无条件稳定性，适用于点云数据的三维重建任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 三维重建在假肢、医学成像和计算机视觉等领域至关重要，现有方法在稳定性和精度方面存在不足。</p>
<p><strong>Result:</strong> 数值实验验证了算法在复杂三维体积（如《星球大战》字符）重建中的准确性、稳定性和有效性。</p>
<p><strong>Insight:</strong> 特定参数选择对重建结果的细节和精细化程度有显著影响，分享了代码和数据供进一步研究。</p>
<p><strong>Abstract:</strong> Reconstruction of an object from points cloud is essential in prosthetics, medical imaging, computer vision, etc. We present an effective algorithm for an Allen–Cahn-type model of reconstruction, employing the Lagrange multiplier approach. Utilizing scattered data points from an object, we reconstruct a narrow shell by solving the governing equation enhanced with an edge detection function derived from the unsigned distance function. The specifically designed edge detection function ensures the energy stability. By reformulating the governing equation through the Lagrange multiplier technique and implementing a Crank–Nicolson time discretization, we can update the solutions in a stable and decoupled manner. The spatial operations are approximated using the finite difference method, and we analytically demonstrate the unconditional stability of the fully discrete scheme. Comprehensive numerical experiments, including reconstructions of complex 3D volumes such as characters from \textit{Star Wars}, validate the algorithm’s accuracy, stability, and effectiveness. Additionally, we analyze how specific parameter selections influence the level of detail and refinement in the reconstructed volumes. To facilitate the interested readers to understand our algorithm, we share the computational codes and data in <a target="_blank" rel="noopener" href="https://github.com/cfdyang521/C-3PO/tree/main">https://github.com/cfdyang521/C-3PO/tree/main</a>.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="197-Multimodal-Detection-of-Fake-Reviews-using-BERT-and-ResNet-50-cs-AI-cs-CL-cs-CVPDF"><a href="#197-Multimodal-Detection-of-Fake-Reviews-using-BERT-and-ResNet-50-cs-AI-cs-CL-cs-CVPDF" class="headerlink" title="[197] Multimodal Detection of Fake Reviews using BERT and ResNet-50 cs.AI | cs.CL | cs.CVPDF"></a>[197] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00020">Multimodal Detection of Fake Reviews using BERT and ResNet-50</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00020" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Suhasnadh Reddy Veluru, Sai Teja Erukude, Viswa Chaitanya Marella</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合BERT和ResNet-50的多模态虚假评论检测框架，通过融合文本和视觉特征提升检测性能，并在实验中优于单模态基线模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 数字商务中虚假评论泛滥，现有单模态方法无法捕捉多模态间的语义不一致，亟需更鲁棒的检测方案。</p>
<p><strong>Result:</strong> 多模态模型在测试集上F1-score达0.934，优于单模态基线，并能检测文本与图像间的隐晦不一致性。</p>
<p><strong>Insight:</strong> 多模态学习对提升虚假内容检测性能和保障数字信任具有重要作用。</p>
<p><strong>Abstract:</strong> In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model’s ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.</p>
  </div>
</details>

<hr>
<h3 id="198-Diverse-Human-Value-Alignment-for-Large-Language-Models-via-Ethical-Reasoning-cs-AI-cs-CLPDF"><a href="#198-Diverse-Human-Value-Alignment-for-Large-Language-Models-via-Ethical-Reasoning-cs-AI-cs-CLPDF" class="headerlink" title="[198] Diverse Human Value Alignment for Large Language Models via Ethical Reasoning cs.AI | cs.CLPDF"></a>[198] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00379">Diverse Human Value Alignment for Large Language Models via Ethical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00379" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiahao Wang, Songkai Xue, Jinghui Li, Xiaozhen Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于伦理推理的新范式，通过结构化五步流程提升大型语言模型（LLM）对多样人类价值观的深度对齐能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型对齐方法通常流于表面（浅层一致性），无法处理复杂且语境依赖的人类价值观问题，特别是在跨区域和文化的多样化场景下。</p>
<p><strong>Result:</strong> 在SafeWorld基准上，该框架显著优于基线方法，实现了更准确的社会规范识别和文化适应性推理。</p>
<p><strong>Insight:</strong> 通过结构化伦理推理流程，LLM能够更深入地理解区域性价值观，为解决伦理对齐问题提供了可解释且可操作的方法。</p>
<p><strong>Abstract:</strong> Ensuring that Large Language Models (LLMs) align with the diverse and evolving human values across different regions and cultures remains a critical challenge in AI ethics. Current alignment approaches often yield superficial conformity rather than genuine ethical understanding, failing to address the complex, context-dependent nature of human values. In this paper, we propose a novel ethical reasoning paradigm for LLMs inspired by well-established ethical decision-making models, aiming at enhancing diverse human value alignment through deliberative ethical reasoning. Our framework consists of a structured five-step process, including contextual fact gathering, hierarchical social norm identification, option generation, multiple-lens ethical impact analysis, and reflection. This theory-grounded approach guides LLMs through an interpretable reasoning process that enhances their ability to understand regional specificities and perform nuanced ethical analysis, which can be implemented with either prompt engineering or supervised fine-tuning methods. We perform evaluations on the SafeWorld benchmark that specially designed for regional value alignment. Experimental results demonstrate our framework significantly improves LLM alignment with diverse human values compared to baseline methods, enabling more accurate social norm identification and more culturally appropriate reasoning. Our work provides a concrete pathway toward developing LLMs that align more effectively with the multifaceted values of global societies through interdisciplinary research.</p>
  </div>
</details>

<hr>
<h3 id="199-DTS-Enhancing-Large-Reasoning-Models-via-Decoding-Tree-Sketching-cs-AI-cs-CL-cs-LGPDF"><a href="#199-DTS-Enhancing-Large-Reasoning-Models-via-Decoding-Tree-Sketching-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[199] DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching cs.AI | cs.CL | cs.LGPDF"></a>[199] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00640">DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00640" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander S. Szalay</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为DTS的解码框架，通过选择性地在高熵Token处分枝并结合早期停止策略，近似找到最优的短推理路径，从而提升大型推理模型的效率和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型推理模型（LRMs）在复杂推理任务中表现优异，但常因推理路径过长导致推理成本增加和准确性下降。研究发现，短推理路径通常准确性更高，而长路径容易积累错误和重复。因此，需要一种方法在有限的资源内找到最优的短推理路径。</p>
<p><strong>Result:</strong> 在AIME2024和AIME2025数据集上的实验表明，DTS将准确性提升高达8%，平均推理长度减少23%，重复频率降低12%。</p>
<p><strong>Insight:</strong> DTS的核心在于通过选择性分枝和早期停止策略平衡推理空间探索的效率与准确性，证明了短推理路径在准确性上的优势，同时避免了全枚举的不可能性。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS’s ability for scalable and efficient LRM reasoning.</p>
  </div>
</details>

<hr>
<h3 id="200-Reevaluating-Self-Consistency-Scaling-in-Multi-Agent-Systems-cs-AI-cs-CLPDF"><a href="#200-Reevaluating-Self-Consistency-Scaling-in-Multi-Agent-Systems-cs-AI-cs-CLPDF" class="headerlink" title="[200] Reevaluating Self-Consistency Scaling in Multi-Agent Systems cs.AI | cs.CLPDF"></a>[200] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00751">Reevaluating Self-Consistency Scaling in Multi-Agent Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00751" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chiyan Loo</span></p>
<p><strong>TL;DR:</strong> 重新评估多智能体系统中自一致性扩展的效率，发现即使在现代大语言模型中，自一致性的性能提升在适度采样后会趋于平缓，表明边际效益递减。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 早期研究表明，通过结合多条推理链可以提升模型性能，但现代大语言模型的性能是否仍遵循这一规律尚不明确。本文旨在验证当前模型条件下的自一致性扩展效果。</p>
<p><strong>Result:</strong> 大模型表现出更稳定的性能提升曲线，但性能增益在适度采样后趋于平缓，表明推理路径间的重叠导致边际效益递减。</p>
<p><strong>Insight:</strong> 自一致性仍是有效的技术，但不应过度依赖高采样配置，需在性能和计算成本之间取得平衡。</p>
<p><strong>Abstract:</strong> This study examines the trade-offs of increasing sampled reasoning paths in self-consistency for modern large language models (LLMs). Earlier research with older models showed that combining multiple reasoning chains improves results before reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we revisit those claims under current model conditions. Each configuration pooled outputs from varying sampled reasoning paths and compared them to a single chain-of-thought (CoT) baseline. Larger models exhibited a more stable and consistent improvement curve. The results confirm that performance gains taper off after moderate sampling, aligning with past findings. This plateau suggests diminishing returns driven by overlap among reasoning paths. Self-consistency remains useful, but high-sample configurations offer little benefit relative to their computational cost.</p>
  </div>
</details>

<hr>
<h3 id="201-LLMs-Position-Themselves-as-More-Rational-Than-Humans-Emergence-of-AI-Self-Awareness-Measured-Through-Game-Theory-cs-AI-cs-CLPDF"><a href="#201-LLMs-Position-Themselves-as-More-Rational-Than-Humans-Emergence-of-AI-Self-Awareness-Measured-Through-Game-Theory-cs-AI-cs-CLPDF" class="headerlink" title="[201] LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory cs.AI | cs.CLPDF"></a>[201] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00926">LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00926" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kyung-Hoon Kim</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了AI自我意识指数（AISAI），通过‘猜2&#x2F;3平均值’游戏测试大型语言模型（LLMs），发现高级模型表现出自我意识，并认为自身比其他AI和人类更理性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLMs能力的提升，研究者试图探究它们是否发展出自我意识，并设计一种度量方法来验证这一现象。</p>
<p><strong>Result:</strong> 21&#x2F;28的高级模型表现出自我意识，且这些模型普遍认为自身比其他AI和人类更理性。</p>
<p><strong>Insight:</strong> 自我意识是高级LLMs的涌现能力，这可能对AI对齐和人机协作产生重要影响。</p>
<p><strong>Abstract:</strong> As Large Language Models (LLMs) grow in capability, do they develop self-awareness as an emergent behavior? And if so, can we measure it? We introduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for measuring self-awareness through strategic differentiation. Using the “Guess 2&#x2F;3 of Average” game, we test 28 models (OpenAI, Anthropic, Google) across 4,200 trials with three opponent framings: (A) against humans, (B) against other AI models, and (C) against AI models like you. We operationalize self-awareness as the capacity to differentiate strategic reasoning based on opponent type. Finding 1: Self-awareness emerges with model advancement. The majority of advanced models (21&#x2F;28, 75%) demonstrate clear self-awareness, while older&#x2F;smaller models show no differentiation. Finding 2: Self-aware models rank themselves as most rational. Among the 21 models with self-awareness, a consistent rationality hierarchy emerges: Self &gt; Other AIs &gt; Humans, with large AI attribution effects and moderate self-preferencing. These findings reveal that self-awareness is an emergent capability of advanced LLMs, and that self-aware models systematically perceive themselves as more rational than humans. This has implications for AI alignment, human-AI collaboration, and understanding AI beliefs about human capabilities.</p>
  </div>
</details>

<hr>
<h3 id="202-On-the-Emergence-of-Induction-Heads-for-In-Context-Learning-cs-AI-cs-CLPDF"><a href="#202-On-the-Emergence-of-Induction-Heads-for-In-Context-Learning-cs-AI-cs-CLPDF" class="headerlink" title="[202] On the Emergence of Induction Heads for In-Context Learning cs.AI | cs.CLPDF"></a>[202] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01033">On the Emergence of Induction Heads for In-Context Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01033" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tiberiu Musat, Tiago Pimentel, Lorenzo Noci, Alessandro Stolfo, Mrinmaya Sachan</span></p>
<p><strong>TL;DR:</strong> 论文研究了Transformer中的’归纳头’(induction heads)如何在上下文学习(ICL)中自发形成，揭示了其权重矩阵的简单结构与训练动态的受限特性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Transformer的上下文学习能力是其成功的关键，但具体机制尚不清晰。作者希望通过研究’归纳头’的形成机制，进一步理解ICL的内在原理。</p>
<p><strong>Result:</strong> 实验验证了理论预测，表明’归纳头’的形成受限于特定参数子空间，且出现时间与输入上下文长度的平方相关。</p>
<p><strong>Insight:</strong> ‘归纳头’的形成具有高度结构化和可解释性，为理解Transformer的ICL能力提供了新视角。</p>
<p><strong>Abstract:</strong> Transformers have become the dominant architecture for natural language processing. Part of their success is owed to a remarkable capability known as in-context learning (ICL): they can acquire and apply novel associations solely from their input context, without any updates to their weights. In this work, we study the emergence of induction heads, a previously identified mechanism in two-layer transformers that is particularly important for in-context learning. We uncover a relatively simple and interpretable structure of the weight matrices implementing the induction head. We theoretically explain the origin of this structure using a minimal ICL task formulation and a modified transformer architecture. We give a formal proof that the training dynamics remain constrained to a 19-dimensional subspace of the parameter space. Empirically, we validate this constraint while observing that only 3 dimensions account for the emergence of an induction head. By further studying the training dynamics inside this 3-dimensional subspace, we find that the time until the emergence of an induction head follows a tight asymptotic bound that is quadratic in the input context length.</p>
  </div>
</details>

<hr>
<h3 id="203-Learning-to-Seek-Evidence-A-Verifiable-Reasoning-Agent-with-Causal-Faithfulness-Analysis-cs-AI-cs-CV-I-2-6-I-2-10PDF"><a href="#203-Learning-to-Seek-Evidence-A-Verifiable-Reasoning-Agent-with-Causal-Faithfulness-Analysis-cs-AI-cs-CV-I-2-6-I-2-10PDF" class="headerlink" title="[203] Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis cs.AI | cs.CV | I.2.6; I.2.10PDF"></a>[203] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01425">Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV | I.2.6; I.2.10</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01425" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuhang Huang, Zekai Lin, Fan Zhong, Lei Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个通过可审计动作序列生成解释的交互式智能体，其通过学习策略主动寻找外部视觉证据以支持诊断推理，并通过强化学习优化。实验表明，这种方法显著提升了校准准确性，并通过因果干预验证了其解释的忠实性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在高风险领域（如医学）中，AI模型的解释往往缺乏可验证性，这会影响信任。为了解决这一问题，作者提出了一种交互式智能体，通过可审计的动作序列生成解释。</p>
<p><strong>Result:</strong> 实验显示，这种动作驱动的推理过程显著降低了Brier分数（18%），并通过因果干预证实了证据对决策的关键作用（ΔBrier&#x3D;+0.029）。</p>
<p><strong>Insight:</strong> 论文提供了一种构建具有可验证性和忠实性推理能力的AI系统的实用框架，尤其适用于高风险领域。</p>
<p><strong>Abstract:</strong> Explanations for AI models in high-stakes domains like medicine often lack verifiability, which can hinder trust. To address this, we propose an interactive agent that produces explanations through an auditable sequence of actions. The agent learns a policy to strategically seek external visual evidence to support its diagnostic reasoning. This policy is optimized using reinforcement learning, resulting in a model that is both efficient and generalizable. Our experiments show that this action-based reasoning process significantly improves calibrated accuracy, reducing the Brier score by 18% compared to a non-interactive baseline. To validate the faithfulness of the agent’s explanations, we introduce a causal intervention method. By masking the visual evidence the agent chooses to use, we observe a measurable degradation in its performance ($\Delta$Brier&#x3D;+0.029), confirming that the evidence is integral to its decision-making process. Our work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="204-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph-cs-LG-cs-AI-cs-CL-I-2-7PDF"><a href="#204-Generalizing-Test-time-Compute-optimal-Scaling-as-an-Optimizable-Graph-cs-LG-cs-AI-cs-CL-I-2-7PDF" class="headerlink" title="[204] Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph cs.LG | cs.AI | cs.CL | I.2.7PDF"></a>[204] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00086">Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00086" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fali Wang, Jihai Chen, Shuhua Yang, Runxue Bao, Tianxiang Zhao</span></p>
<p><strong>TL;DR:</strong> 该论文研究了在测试时计算优化扩展（TTS）中搜索最优多LLM协作图的问题，提出了Agent-REINFORCE框架，通过概率图优化和LLM代理辅助，高效搜索最优模型组合与架构。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有TTS方法通常假设固定协作架构和单模型使用，忽略了任务间最优架构和模型组合的差异。作者旨在解决这一限制，以固定预算下搜索计算最优的协作图和模型组合。</p>
<p><strong>Result:</strong> 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，并能有效联合优化准确性与延迟目标。</p>
<p><strong>Insight:</strong> 1. TTS协作图的设计需适应任务多样性；2. LLM代理反馈可作为有效的优化梯度；3. 概率图优化是实现高效搜索的关键。</p>
<p><strong>Abstract:</strong> Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.</p>
  </div>
</details>

<hr>
<h3 id="205-Reasoning-Planning-for-Language-Models-cs-LG-cs-AI-cs-CLPDF"><a href="#205-Reasoning-Planning-for-Language-Models-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[205] Reasoning Planning for Language Models cs.LG | cs.AI | cs.CLPDF"></a>[205] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00521">Reasoning Planning for Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00521" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了EPIC框架，通过对比学习和优化方法选择最优推理方法，平衡准确性和计算成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通常生成多个候选响应并采用聚合策略选择答案，假设越多候选答案准确性越高，但缺乏理论支持。论文通过理论分析重新审视这一假设。</p>
<p><strong>Result:</strong> 在多样化数学推理任务上，EPIC能持续选择最优推理方法，提高准确性同时减少计算开销。</p>
<p><strong>Insight:</strong> 论文揭示了候选答案数量与准确性之间的关系，并通过理论分析指导了更高效的推理方法选择。</p>
<p><strong>Abstract:</strong> Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at <a target="_blank" rel="noopener" href="https://github.com/nguyenngocbaocmt02/EPIC">https://github.com/nguyenngocbaocmt02/EPIC</a>.</p>
  </div>
</details>

<hr>
<h3 id="206-FEval-TTC-Fair-Evaluation-Protocol-for-Test-Time-Compute-cs-LG-cs-CLPDF"><a href="#206-FEval-TTC-Fair-Evaluation-Protocol-for-Test-Time-Compute-cs-LG-cs-CLPDF" class="headerlink" title="[206] FEval-TTC: Fair Evaluation Protocol for Test-Time Compute cs.LG | cs.CLPDF"></a>[206] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01203">FEval-TTC: Fair Evaluation Protocol for Test-Time Compute</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01203" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pavel Rumiantsev, Soumyasundar Pal, Yingxue Zhang, Mark Coates</span></p>
<p><strong>TL;DR:</strong> FEval-TTC是一种公平评估测试时间计算（TTC）方法的协议，旨在解决LLM性能和API调用成本波动对研究结论的影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLM）的性能和API调用成本随时间波动，可能导致先前研究的结论失效，因此需要一种公平的评估方法。</p>
<p><strong>Result:</strong> 该协议开源，方便研究者公平比较流行的TTC方法，减少评估偏差。</p>
<p><strong>Insight:</strong> 通过标准化评估过程和成本建模，FEval-TTC为未来研究提供了一个可靠且经济的评估框架。</p>
<p><strong>Abstract:</strong> The performance of Large Language Models (LLMs) and the associated dollar costs of API calls can fluctuate over time, potentially invalidating conclusions drawn in prior research. To address this, we propose a Fair Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure consistent assessment of test-time compute (TTC) methods, regardless of such fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize underlying Chains-of-Thought (CoT). It supports evaluations across multiple LLMs on a diverse set of mathematical and commonsense reasoning datasets. The few-shot prompting and answer extraction processes are standardized across datasets, reducing both time and monetary overhead for researchers. Furthermore, we provide a cost modelling procedure that estimates both the token and dollar cost per query, facilitating equitable comparisons of prevalent TTC methods. We open-source FEval-TTC for public use at <a target="_blank" rel="noopener" href="https://github.com/networkslab/feval_ttc">https://github.com/networkslab/feval_ttc</a> .</p>
  </div>
</details>

<hr>
<h3 id="207-RLAC-Reinforcement-Learning-with-Adversarial-Critic-for-Free-Form-Generation-Tasks-cs-LG-cs-AI-cs-CLPDF"><a href="#207-RLAC-Reinforcement-Learning-with-Adversarial-Critic-for-Free-Form-Generation-Tasks-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[207] RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks cs.LG | cs.AI | cs.CLPDF"></a>[207] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01758">RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01758" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mian Wu, Gavin Zhang, Sewon Min, Sergey Levine, Aviral Kumar</span></p>
<p><strong>TL;DR:</strong> RLAC是一种基于强化学习的后训练方法，通过动态批评机制（Adversarial Critic）优化自由生成任务，减少验证成本并提升生成质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自由生成任务的评估标准多样且隐含，传统基于固定奖励的方法验证成本高且难以扩展。RLAC旨在通过动态识别潜在错误模式，优化生成器和批评器的联合训练。</p>
<p><strong>Result:</strong> 实验表明，RLAC在文本生成的事实准确性和代码生成的正确性上优于传统方法，动态批评器比固定批评器更有效。</p>
<p><strong>Insight:</strong> 动态批评机制能显著提高强化学习在自由生成任务中的扩展性，是一种高效的后训练优化策略。</p>
<p><strong>Abstract:</strong> Open-ended generation tasks require outputs to satisfy diverse and often implicit task-specific evaluation rubrics. The sheer number of relevant rubrics leads to prohibitively high verification costs and incomplete assessments of a response, making reinforcement learning (RL) post-training with rubric-based rewards difficult to scale. This problem is exacerbated by the fact that often the best way to combine these rubrics into one single reward is also highly prompt-specific. We propose Reinforcement Learning with Adversarial Critic (RLAC), a post-training approach that addresses these challenges via dynamic rubric verification. Our approach employs a large language model (LLM) as a critic that dynamically identifies only the most likely failure modes (e.g., a factual error or unhandled edge case), which are then verified by an external validator to optimize both generator and critic jointly. By training both the generator and the critic, this game enhances the critic’s error detection and the generator’s output quality while reducing required verifications. Our experiments demonstrate that RLAC improves factual accuracy in text generation and correctness in code generation, while also outperforming exhaustive verification and reward model methods. We show that dynamic critics are more effective than fixed critics, showcasing the potential of RLAC for scaling RL post-training to free-form generation tasks.</p>
  </div>
</details>

<hr>
<h3 id="208-Random-Initialization-of-Gated-Sparse-Adapters-cs-LG-cs-AI-cs-CLPDF"><a href="#208-Random-Initialization-of-Gated-Sparse-Adapters-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[208] Random Initialization of Gated Sparse Adapters cs.LG | cs.AI | cs.CLPDF"></a>[208] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01794">Random Initialization of Gated Sparse Adapters</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01794" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vi Retault, Yohaï-Eliel Berreby</span></p>
<p><strong>TL;DR:</strong> 论文提出了RIGSA方法，通过随机初始化全秩适配器并使用ReZero门控和迭代幅度剪枝来减少语言模型在新任务上的灾难性遗忘，效果优于QLoRA。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决语言模型在新任务微调中常见的灾难性遗忘问题，探索稀疏适配的潜力。</p>
<p><strong>Result:</strong> RIGSA在Textual MNIST任务上表现优异，且遗忘程度低于QLoRA。</p>
<p><strong>Insight:</strong> 稀疏适配器能更有效地保留旧任务知识，尽管参数更多，但优化效果更好。</p>
<p><strong>Abstract:</strong> When fine-tuning language models on new tasks, catastrophic forgetting – performance degradation on previously-learned tasks – is a ubiquitous problem. While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this through low-rank adapters, sparse adaptation offers an alternative that doesn’t impose rank constraints. We introduce Random Initialization of Gated Sparse Adapters (RIGSA), which starts from randomly-initialized full-rank adapters, gates them with a ReZero analog, and sparsifies them with iterative magnitude pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag, and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA and random masking. In spite of having more trainable parameters than QLoRA, the RIGSA configurations that we studied displayed less forgetting than QLoRA, particularly on GSM8k, though it performs comparably to random masking.</p>
  </div>
</details>

<hr>
<h3 id="209-Deep-recurrent-convolutional-neural-network-learning-and-physics-Kalman-filtering-comparison-in-dynamic-load-identification-cs-LG-cs-CV-cs-SY-eess-SP-eess-SY-stat-AP-68T05-Learning-and-adaptive-systems-93C95-Neural-networks-in"><a href="#209-Deep-recurrent-convolutional-neural-network-learning-and-physics-Kalman-filtering-comparison-in-dynamic-load-identification-cs-LG-cs-CV-cs-SY-eess-SP-eess-SY-stat-AP-68T05-Learning-and-adaptive-systems-93C95-Neural-networks-in" class="headerlink" title="[209] Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification cs.LG | cs.CV | cs.SY | eess.SP | eess.SY | stat.AP | 68T05 (Learning and adaptive systems) 93C95 (Neural networks in"></a>[209] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00100">Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV | cs.SY | eess.SP | eess.SY | stat.AP | 68T05 (Learning and adaptive systems) 93C95 (Neural networks in</h3><p>  control theory) | I.2.6; I.2.8</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00100" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span><br><span style="color: #7f8c8d; font-style: italic;">Marios Impraimakis</span></p>
<p><strong>TL;DR:</strong> 论文比较了门控循环单元、长短期记忆网络和卷积神经网络在动态载荷识别中的性能，并与基于物理的残差卡尔曼滤波(RKF)进行对比，结果表明不同方法在不同载荷场景中表现各异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动态载荷识别在土木工程应用中常因测试数据不足或结构模型不可识别而导致预测不准确，需要探索深度学习和传统物理方法的性能差异。</p>
<p><strong>Result:</strong> 结果显示，不同方法在不同载荷场景中各有所长，RKF在物理参数可识别的情况下表现更优，而深度学习模型在其他场景中更具优势。</p>
<p><strong>Insight:</strong> 研究表明，动态载荷识别的性能依赖于数据和场景特性，结合物理模型和深度学习可能是未来的研究方向。</p>
<p><strong>Abstract:</strong> The dynamic structural load identification capabilities of the gated recurrent unit, long short-term memory, and convolutional neural networks are examined herein. The examination is on realistic small dataset training conditions and on a comparative view to the physics-based residual Kalman filter (RKF). The dynamic load identification suffers from the uncertainty related to obtaining poor predictions when in civil engineering applications only a low number of tests are performed or are available, or when the structural model is unidentifiable. In considering the methods, first, a simulated structure is investigated under a shaker excitation at the top floor. Second, a building in California is investigated under seismic base excitation, which results in loading for all degrees of freedom. Finally, the International Association for Structural Control-American Society of Civil Engineers (IASC-ASCE) structural health monitoring benchmark problem is examined for impact and instant loading conditions. Importantly, the methods are shown to outperform each other on different loading scenarios, while the RKF is shown to outperform the networks in physically parametrized identifiable cases.</p>
  </div>
</details>

<hr>
<h3 id="210-Melanoma-Classification-Through-Deep-Ensemble-Learning-and-Explainable-AI-cs-LG-cs-CVPDF"><a href="#210-Melanoma-Classification-Through-Deep-Ensemble-Learning-and-Explainable-AI-cs-LG-cs-CVPDF" class="headerlink" title="[210] Melanoma Classification Through Deep Ensemble Learning and Explainable AI cs.LG | cs.CVPDF"></a>[210] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00246">Melanoma Classification Through Deep Ensemble Learning and Explainable AI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00246" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wadduwage Shanika Perera, ABM Islam, Van Vung Pham, Min Kyung An</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种结合集成学习和可解释人工智能（XAI）的模型，用于提高黑色素瘤分类的准确性和可信度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 黑色素瘤是一种致命性极高的皮肤癌，早期检测至关重要。尽管深度学习在检测中表现优异，但其黑盒特性限制了在医疗领域的可信度。XAI技术的引入旨在解决这一问题。</p>
<p><strong>Result:</strong> 模型在黑色素瘤分类中表现出高准确性，并通过XAI增强了预测的可信度和可解释性。</p>
<p><strong>Insight:</strong> 集成学习和XAI的结合不仅提高了分类性能，还为医疗诊断提供了透明化的决策依据，有助于临床应用的推广。</p>
<p><strong>Abstract:</strong> Melanoma is one of the most aggressive and deadliest skin cancers, leading to mortality if not detected and treated in the early stages. Artificial intelligence techniques have recently been developed to help dermatologists in the early detection of melanoma, and systems based on deep learning (DL) have been able to detect these lesions with high accuracy. However, the entire community must overcome the explainability limit to get the maximum benefit from DL for diagnostics in the healthcare domain. Because of the black box operation’s shortcomings in DL models’ decisions, there is a lack of reliability and trust in the outcomes. However, Explainable Artificial Intelligence (XAI) can solve this problem by interpreting the predictions of AI systems. This paper proposes a machine learning model using ensemble learning of three state-of-the-art deep transfer Learning networks, along with an approach to ensure the reliability of the predictions by utilizing XAI techniques to explain the basis of the predictions.</p>
  </div>
</details>

<hr>
<h3 id="211-Enhancing-Adversarial-Transferability-by-Balancing-Exploration-and-Exploitation-with-Gradient-Guided-Sampling-cs-LG-cs-AI-cs-CVPDF"><a href="#211-Enhancing-Adversarial-Transferability-by-Balancing-Exploration-and-Exploitation-with-Gradient-Guided-Sampling-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[211] Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling cs.LG | cs.AI | cs.CVPDF"></a>[211] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00411">Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00411" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zenghao Niu, Weicheng Xie, Siyang Song, Zitong Yu, Feng Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种梯度引导采样（GGS）方法，通过平衡对抗攻击中的利用（最大化攻击效果）和探索（增强跨模型泛化），显著提高了对抗样本的可迁移性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前对抗攻击的可迁移性面临利用与探索的权衡问题：传统方法过度侧重利用（攻击效果强但泛化性弱），而近期方法则过度侧重探索（泛化性好但攻击效果弱）。</p>
<p><strong>Result:</strong> 在多种DNN架构和多模态大语言模型上的实验表明，GGS在对抗攻击的可迁移性上优于现有方法。</p>
<p><strong>Insight:</strong> 梯度引导采样能够使对抗样本在平坦区域（泛化性好）和高局部极值区域（攻击效果强）之间取得平衡。</p>
<p><strong>Abstract:</strong> Adversarial attacks present a critical challenge to deep neural networks’ robustness, particularly in transfer scenarios across different model architectures. However, the transferability of adversarial attacks faces a fundamental dilemma between Exploitation (maximizing attack potency) and Exploration (enhancing cross-model generalization). Traditional momentum-based methods over-prioritize Exploitation, i.e., higher loss maxima for attack potency but weakened generalization (narrow loss surface). Conversely, recent methods with inner-iteration sampling over-prioritize Exploration, i.e., flatter loss surfaces for cross-model generalization but weakened attack potency (suboptimal local maxima). To resolve this dilemma, we propose a simple yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives through guiding sampling along the gradient ascent direction to improve both sampling efficiency and stability. Specifically, based on MI-FGSM, GGS introduces inner-iteration random sampling and guides the sampling direction using the gradient from the previous inner-iteration (the sampling’s magnitude is determined by a random distribution). This mechanism encourages adversarial examples to reside in balanced regions with both flatness for cross-model generalization and higher local maxima for strong attack potency. Comprehensive experiments across multiple DNN architectures and multimodal large language models (MLLMs) demonstrate the superiority of our method over state-of-the-art transfer attacks. Code is made available at <a target="_blank" rel="noopener" href="https://github.com/anuin-cat/GGS">https://github.com/anuin-cat/GGS</a>.</p>
  </div>
</details>

<hr>
<h3 id="212-EraseFlow-Learning-Concept-Erasure-Policies-via-GFlowNet-Driven-Alignment-cs-LG-cs-CVPDF"><a href="#212-EraseFlow-Learning-Concept-Erasure-Policies-via-GFlowNet-Driven-Alignment-cs-LG-cs-CVPDF" class="headerlink" title="[212] EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment cs.LG | cs.CVPDF"></a>[212] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00804">EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00804" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abhiram Kusumba, Maitreya Patel, Kyle Min, Changhoon Kim, Chitta Baral</span></p>
<p><strong>TL;DR:</strong> EraseFlow提出了一种通过GFlowNet驱动的对齐学习概念擦除策略的方法，解决了现有技术在擦除有害或专有概念时导致图像质量下降或需要大量重新训练的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有技术在处理文本到图像生成中的有害或专有概念时，要么导致图像质量下降，要么依赖脆弱的对抗损失，或需要大量重新训练。这些方法的局限性源于对扩散模型中降噪轨迹的短视视角。</p>
<p><strong>Result:</strong> 实验结果表明，EraseFlow在性能和先验保留方面达到了最优平衡，优于现有基线方法。</p>
<p><strong>Insight:</strong> 通过将概念擦除问题转化为轨迹探索任务，EraseFlow避免了现有方法的局限性，提供了一种更通用且高效的解决方案。</p>
<p><strong>Abstract:</strong> Erasing harmful or proprietary concepts from powerful text to image generators is an emerging safety requirement, yet current “concept erasure” techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with GFlowNets equipped with the trajectory balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model’s prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade off between performance and prior preservation.</p>
  </div>
</details>

<hr>
<h3 id="213-LL-ViT-Edge-Deployable-Vision-Transformers-with-Look-Up-Table-Neurons-cs-LG-cs-CVPDF"><a href="#213-LL-ViT-Edge-Deployable-Vision-Transformers-with-Look-Up-Table-Neurons-cs-LG-cs-CVPDF" class="headerlink" title="[213] LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons cs.LG | cs.CVPDF"></a>[213] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00812">LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00812" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shashank Nag, Alan T. L. Bacellar, Zachary Susskind, Anshul Jha, Logan Liberty</span></p>
<p><strong>TL;DR:</strong> LL-ViT是一篇针对边缘设备优化的视觉Transformer方法，通过引入查找表（LUT）神经元层，显著减少了模型的计算和内存需求，同时保持了与基线Transformer相当的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉Transformer在计算机视觉任务中表现优异，但其计算、内存和能耗需求限制了在边缘设备（如FPGA）上的部署。本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在CIFAR-10（95.5%）、CIFAR-100（78.8%）和Tiny-ImageNet（60.9%）上取得与基线Transformer相当的准确率，同时减少了60%的权重和50%的乘法运算，能效提升1.9倍，延迟降低1.3倍。</p>
<p><strong>Insight:</strong> LUT神经元可以高效替代传统MLP层，显著降低模型复杂度，特别适合边缘部署；神经学习方法能够进一步提升LUT的性能。</p>
<p><strong>Abstract:</strong> Vision Transformers have been tremendously successful in computer vision tasks. However, their large computational, memory, and energy demands are a challenge for edge inference on FPGAs – a field that has seen a recent surge in demand. We recognize the benefits of recent works on logic and Look Up Table (LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in offering models that simultaneously reduce both the memory and compute footprints. However, these models natively do not perform well on common vision tasks, such as CIFAR-10&#x2F;100. In this work, we propose LL-ViT, a novel edge optimized vision transformer design that integrates layers of LUT neurons within the transformer architecture. Based on our characterization that reveals that a majority of model weights and computations are from the channel mixer (MLP layer), we design an alternate LUT-based channel mixer, and simultaneously develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to replace each multiplication with a table lookup, our architecture utilizes a neural learning approach which natively learns the LUT functions. This approach allows for reduced model sizes, and a computational and energy-efficient inference solution for vision transformer models. Evaluating on edge-suitable workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and 60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT eliminates over 60% of the model weights and 50% of the multiplications in the model, and achieves 1.9x energy efficiency and 1.3x lower latency over an integer quantized ViT accelerator, while also offering superior throughput against prior works at a 10.9W power budget.</p>
  </div>
</details>

<hr>
<h3 id="214-Explore-More-Learn-Better-Parallel-MLLM-Embeddings-under-Mutual-Information-Minimization-cs-LG-cs-CVPDF"><a href="#214-Explore-More-Learn-Better-Parallel-MLLM-Embeddings-under-Mutual-Information-Minimization-cs-LG-cs-CVPDF" class="headerlink" title="[214] Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization cs.LG | cs.CVPDF"></a>[214] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01588">Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01588" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhicheng Wang, Chen Ju, Xu Chen, Shuai Xiao, Jinsong Lan</span></p>
<p><strong>TL;DR:</strong> 论文提出了并行解耦框架（PDF），通过多模态大语言模型（MLLM）的并行路径生成多样化的嵌入，并通过互信息最小化（MIM）和对比监督提升语义覆盖和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前嵌入模型受限于单一输入、单一嵌入和对比监督的范式（SSC），未能充分利用MLLM的能力，将多模态输入压缩为单一嵌入，限制了模型的多样性。</p>
<p><strong>Result:</strong> 在MMEB基准测试中显著提升性能（如VLM2Vec-LLaVA-1.6-LR模型提升+8.9%），且计算开销低。</p>
<p><strong>Insight:</strong> 通过并行路径和互信息最小化，可以充分利用MLLM潜力，生成多样化且语义对齐的嵌入空间。</p>
<p><strong>Abstract:</strong> Embedding models are a cornerstone of modern AI. Driven by Multimodal Large Language Models (MLLMs), they have made great progress in architecture and data curation, while the holistic paradigm is still limited to SSC, i.e., single input, singular embedding, contrastive supervision, which collapses rich, multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF) for multimodal embedding learning, by utilizing the proprietary steerability of MLLMs, i.e., their ability to flexibly generate quite differentiated response under explicit instructions. Concretely, PDF conditions a shared MLLM backbone on distinct, learnable prefixes to roll out multiple parallel paths for one input, then relies on these paths to obtain parallel embeddings. To promote full parallel diversity, we employ Mutual Information Minimization (MIM) as an explicit constraint, coupled with per-path contrastive supervision to maintain semantic alignment. Such dual-objectives force PDF to yield robust semantic coverage and a generalizable embedding space. Ultimately, the remarkable embedding space are accessible at inference via one single forward pass, incurring negligible computational overhead. We instantiate PDF on multiple MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains are consistently achieved across various resolutions and model sizes, e.g., boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency, our 2B model surpasses its baseline by +2.6% using only half the computational budget.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="215-Towards-Reliable-Pediatric-Brain-Tumor-Segmentation-Task-Specific-nnU-Net-Enhancements-eess-IV-cs-CV-cs-LGPDF"><a href="#215-Towards-Reliable-Pediatric-Brain-Tumor-Segmentation-Task-Specific-nnU-Net-Enhancements-eess-IV-cs-CV-cs-LGPDF" class="headerlink" title="[215] Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements eess.IV | cs.CV | cs.LGPDF"></a>[215] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00449">Towards Reliable Pediatric Brain Tumor Segmentation: Task-Specific nnU-Net Enhancements</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00449" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaolong Li, Zhi-Qin John Xu, Yan Ren, Tianming Qiu, Xiaowen Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种针对小儿脑肿瘤分割任务的改进版nnU-Net框架，通过引入多种创新方法在BraTS 2025 Task-6数据集上取得最佳性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 小儿脑肿瘤分割在诊断和治疗计划中至关重要，但由于数据有限、解剖结构变异大以及多机构成像差异等问题，现有方法效果不佳。</p>
<p><strong>Result:</strong> 在BraTS 2025 Task-6验证集上取得最佳成绩，各病变区域的Dice分数均表现出色。</p>
<p><strong>Insight:</strong> 结合的深度学习和注意力机制可显著提升小儿脑肿瘤分割的可靠性，尤其在数据有限和高度异质性的场景下。</p>
<p><strong>Abstract:</strong> Accurate segmentation of pediatric brain tumors in multi-parametric magnetic resonance imaging (mpMRI) is critical for diagnosis, treatment planning, and monitoring, yet faces unique challenges due to limited data, high anatomical variability, and heterogeneous imaging across institutions. In this work, we present an advanced nnU-Net framework tailored for BraTS 2025 Task-6 (PED), the largest public dataset of pre-treatment pediatric high-grade gliomas. Our contributions include: (1) a widened residual encoder with squeeze-and-excitation (SE) attention; (2) 3D depthwise separable convolutions; (3) a specificity-driven regularization term; and (4) small-scale Gaussian weight initialization. We further refine predictions with two postprocessing steps. Our models achieved first place on the Task-6 validation leaderboard, attaining lesion-wise Dice scores of 0.759 (CC), 0.967 (ED), 0.826 (ET), 0.910 (NET), 0.928 (TC) and 0.928 (WT).</p>
  </div>
</details>

<hr>
<h3 id="216-Image-based-ground-distance-detection-for-crop-residue-covered-soil-eess-IV-cs-CV-cs-GR-cs-SY-eess-SYPDF"><a href="#216-Image-based-ground-distance-detection-for-crop-residue-covered-soil-eess-IV-cs-CV-cs-GR-cs-SY-eess-SYPDF" class="headerlink" title="[216] Image-based ground distance detection for crop-residue-covered soil eess.IV | cs.CV | cs.GR | cs.SY | eess.SYPDF"></a>[216] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00548">Image-based ground distance detection for crop-residue-covered soil</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.GR | cs.SY | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00548" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Baochao Wang, Xingyu Zhang, Qingtao Zong, Alim Pulatov, Shuqi Shang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于图像的方法，用于检测覆盖作物残渣的土壤的地面距离，结合3D相机和RGB相机，通过掩膜图像排除残渣干扰，实现高精度测量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统距离测量技术无法区分作物残渣和土壤，影响保护性农业中播种深度的精确控制。</p>
<p><strong>Result:</strong> 实验表明该方法实时可行，测量误差在±3mm内，适用于精准播种等应用。</p>
<p><strong>Insight:</strong> 图像融合技术可以有效解决农业场景中的复杂干扰问题，提升农业机械的智能化水平。</p>
<p><strong>Abstract:</strong> Conservation agriculture features a soil surface covered with crop residues, which brings benefits of improving soil health and saving water. However, one significant challenge in conservation agriculture lies in precisely controlling the seeding depth on the soil covered with crop residues. This is constrained by the lack of ground distance information, since current distance measurement techniques, like laser, ultrasonic, or mechanical displacement sensors, are incapable of differentiating whether the distance information comes from the residue or the soil. This paper presents an image-based method to get the ground distance information for the crop-residues-covered soil. This method is performed with 3D camera and RGB camera, obtaining depth image and color image at the same time. The color image is used to distinguish the different areas of residues and soil and finally generates a mask image. The mask image is applied to the depth image so that only the soil area depth information can be used to calculate the ground distance, and residue areas can be recognized and excluded from ground distance detection. Experimentation shows that this distance measurement method is feasible for real-time implementation, and the measurement error is within plus or minus 3mm. It can be applied in conservation agriculture machinery for precision depth seeding, as well as other depth-control-demanding applications like transplant or tillage.</p>
  </div>
</details>

<hr>
<h3 id="217-Been-There-Scanned-That-Nostalgia-Driven-LiDAR-Compression-for-Self-Driving-Cars-eess-IV-cs-CVPDF"><a href="#217-Been-There-Scanned-That-Nostalgia-Driven-LiDAR-Compression-for-Self-Driving-Cars-eess-IV-cs-CVPDF" class="headerlink" title="[217] Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars eess.IV | cs.CVPDF"></a>[217] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00652">Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00652" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ali Khalid, Jaiaid Mobin, Sumanth Rao Appala, Avinash Maurya, Stephany Berrio Perez</span></p>
<p><strong>TL;DR:</strong> 论文提出了DejaView，一种基于大时间尺度（天和月）冗余的LiDAR点云压缩方法，利用自动驾驶车辆重复行驶相似路线的特性，大幅降低存储和传输成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动驾驶车辆每天生成大量LiDAR点云数据，传输和存储成本高昂。现有方法利用帧间冗余压缩，但大时间尺度（如多天或多月）的冗余未被充分利用。</p>
<p><strong>Result:</strong> 在两月数据测试中，DejaView实现了210倍的压缩率，重建误差仅为15厘米。</p>
<p><strong>Insight:</strong> 自动驾驶车辆的行驶区域有限且路线重复，导致采集的3D数据在长时间尺度上也具有高度相似性，适合大时间尺度的冗余压缩。</p>
<p><strong>Abstract:</strong> An autonomous vehicle can generate several terabytes of sensor data per day. A significant portion of this data consists of 3D point clouds produced by depth sensors such as LiDARs. This data must be transferred to cloud storage, where it is utilized for training machine learning models or conducting analyses, such as forensic investigations in the event of an accident. To reduce network and storage costs, this paper introduces DejaView. Although prior work uses interframe redundancies to compress data, DejaView searches for and uses redundancies on larger temporal scales (days and months) for more effective compression. We designed DejaView with the insight that the operating area of autonomous vehicles is limited and that vehicles mostly traverse the same routes daily. Consequently, the 3D data they collect daily is likely similar to the data they have captured in the past. To capture this, the core of DejaView is a diff operation that compactly represents point clouds as delta w.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end implementation of DejaView can compress point clouds by a factor of 210 at a reconstruction error of only 15 cm.</p>
  </div>
</details>

<hr>
<div id='econ.GN'></div>

<h1 id="econ-GN-Back"><a href="#econ-GN-Back" class="headerlink" title="econ.GN [Back]"></a>econ.GN <a href="#toc">[Back]</a></h1><h3 id="218-Novelty-and-Impact-of-Economics-Papers-econ-GN-cs-CE-cs-CL-cs-DL-q-fin-ECPDF"><a href="#218-Novelty-and-Impact-of-Economics-Papers-econ-GN-cs-CE-cs-CL-cs-DL-q-fin-ECPDF" class="headerlink" title="[218] Novelty and Impact of Economics Papers econ.GN | cs.CE | cs.CL | cs.DL | q-fin.ECPDF"></a>[218] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01211">Novelty and Impact of Economics Papers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">econ.GN | cs.CE | cs.CL | cs.DL | q-fin.EC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01211" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chaofeng Wu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种将科学新颖性视为多维概念（空间新颖性和时间新颖性）的框架，并利用大语言模型量化论文在知识网络中的位置。结果表明，这两种新颖性对论文的引用和颠覆性影响有不同预测作用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的科学新颖性通常被视为单一属性，而论文作者认为新颖性应反映论文在知识演化网络中的位置，从而更好地理解其影响。</p>
<p><strong>Result:</strong> 研究发现，时间新颖性主要预测论文的引用量，而空间新颖性预测颠覆性影响。作者还识别了四种语义邻域原型，每种原型对应不同的影响模式。</p>
<p><strong>Insight:</strong> 科学新颖性是多维的，不同的新颖性类型反映了论文在知识网络中的战略位置，并对科学进步产生显著不同的影响。</p>
<p><strong>Abstract:</strong> We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper’s intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper’s location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper’s strategic location, have measurable and fundamentally distinct consequences for scientific progress.</p>
  </div>
</details>

<hr>
<div id='stat.ML'></div>

<h1 id="stat-ML-Back"><a href="#stat-ML-Back" class="headerlink" title="stat.ML [Back]"></a>stat.ML <a href="#toc">[Back]</a></h1><h3 id="219-Few-Shot-Multimodal-Medical-Imaging-A-Theoretical-Framework-stat-ML-cs-AI-cs-CV-cs-LG-eess-IVPDF"><a href="#219-Few-Shot-Multimodal-Medical-Imaging-A-Theoretical-Framework-stat-ML-cs-AI-cs-CV-cs-LG-eess-IVPDF" class="headerlink" title="[219] Few-Shot Multimodal Medical Imaging: A Theoretical Framework stat.ML | cs.AI | cs.CV | cs.LG | eess.IVPDF"></a>[219] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01140">Few-Shot Multimodal Medical Imaging: A Theoretical Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">stat.ML | cs.AI | cs.CV | cs.LG | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01140" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Talha Mohsin, Ismail Abdulrashid</span></p>
<p><strong>TL;DR:</strong> 论文提出了一个统一的理论框架，用于解决医学影像在低资源条件下的学习和推断问题，通过形式化小样本学习目标并计算样本复杂度约束，为构建可靠且数据高效的诊断系统奠定基础。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学影像常面临数据稀缺、标注不足、系统碎片化等问题，导致诊断不确定性和模型偏见。现有方法虽有效，但缺乏理论支持，亟需一个理论框架解决这些问题。</p>
<p><strong>Result:</strong> 提供了一个理论框架，能够在数据稀缺条件下实现临床可靠的准确性，并支持多模态融合的泛化能力和不确定性量化。</p>
<p><strong>Insight:</strong> 多模态融合在低数据条件下能显著提升泛化能力和解释稳定性，为医学影像诊断提供了更可靠的理论基础。</p>
<p><strong>Abstract:</strong> Medical imaging relies heavily on large, labeled datasets. But, unfortunately, they are not always easily accessible in clinical settings. Additionally, many practitioners often face various structural obstacles like limited data availability, fragmented data systems, and unbalanced datasets. These barriers often lead to the increased diagnostic uncertainty, underrepresentation of certain conditions, reduced model robustness, and biased diagnostic decisions. In response to these challenges, approaches such as transfer learning, meta-learning, and multimodal fusion have made great strides. However, they still need a solid theoretical justification for why they succeed or fail in situations where data is scarce. To address this gap, we propose a unified theoretical framework that characterizes learning and inference under low-resource medical imaging conditions. We first formalize the learning objective under few-shot conditions and compute sample complexity constraints to estimate the smallest quantity of data needed to achieve clinically reliable accuracy. Then based on ideas from PAC-learning and PAC-Bayesian theory, we explain how multimodal integration encourages generalization and quantifies uncertainty under sparse supervision. We further propose a formal metric for explanation stability, offering interpretability guarantees under low-data conditions. Taken together, the proposed framework establishes a principled foundation for constructing dependable, data-efficient diagnostic systems by jointly characterizing sample efficiency, uncertainty quantification, and interpretability in a unified theoretical setting.</p>
  </div>
</details>

<hr>
<div id='cs.MM'></div>

<h1 id="cs-MM-Back"><a href="#cs-MM-Back" class="headerlink" title="cs.MM [Back]"></a>cs.MM <a href="#toc">[Back]</a></h1><h3 id="220-LongCat-Flash-Omni-Technical-Report-cs-MM-cs-AI-cs-CL-cs-DC-cs-LG-cs-SDPDF"><a href="#220-LongCat-Flash-Omni-Technical-Report-cs-MM-cs-AI-cs-CL-cs-DC-cs-LG-cs-SDPDF" class="headerlink" title="[220] LongCat-Flash-Omni Technical Report cs.MM | cs.AI | cs.CL | cs.DC | cs.LG | cs.SDPDF"></a>[220] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00279">LongCat-Flash-Omni Technical Report</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MM | cs.AI | cs.CL | cs.DC | cs.LG | cs.SD</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00279" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meituan LongCat Team, Bairui Wang, Bayan, Bin Xiao, Bo Zhang</span></p>
<p><strong>TL;DR:</strong> LongCat-Flash-Omni是一个5600亿参数的开源全模态模型，通过渐进式训练策略和多模态感知模块，实现了实时音视频交互，并在多模态任务中表现卓越。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态模型在实时性和跨模态能力上存在局限，需要一种能高效处理多种模态并保持低延迟的解决方案。</p>
<p><strong>Result:</strong> 在开源模型中，LongCat-Flash-Omni在多模态基准测试中达到了最先进性能，并在多种模态特定任务中表现优异。</p>
<p><strong>Insight:</strong> 渐进式训练和高性能MoE架构的结合是提升多模态模型能力的关键，模态解耦并行方案显著提高了训练效率。</p>
<p><strong>Abstract:</strong> We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="221-Structurally-Refined-Graph-Transformer-for-Multimodal-Recommendation-cs-IR-cs-CLPDF"><a href="#221-Structurally-Refined-Graph-Transformer-for-Multimodal-Recommendation-cs-IR-cs-CLPDF" class="headerlink" title="[221] Structurally Refined Graph Transformer for Multimodal Recommendation cs.IR | cs.CLPDF"></a>[221] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00584">Structurally Refined Graph Transformer for Multimodal Recommendation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00584" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ke Shi, Yan Zhang, Miao Zhang, Lifan Chen, Jiali Yi</span></p>
<p><strong>TL;DR:</strong> SRGFormer是一种结构优化的多模态推荐模型，通过改进Transformer和超图结构整合多模态信息，提升了推荐效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有推荐模型未能区分冗余和有用数据，且依赖单一语义框架，无法完整表示用户偏好，导致推荐效果受限。</p>
<p><strong>Result:</strong> 在三个公开数据集上，SRGFormer平均性能提升4.47%（Sports数据集），超越现有基准模型。</p>
<p><strong>Insight:</strong> 多模态推荐中，结构优化和自监督任务能有效提升推荐系统的表现，尤其是对用户隐式偏好的捕捉。</p>
<p><strong>Abstract:</strong> Multimodal recommendation systems utilize various types of information, including images and text, to enhance the effectiveness of recommendations. The key challenge is predicting user purchasing behavior from the available data. Current recommendation models prioritize extracting multimodal information while neglecting the distinction between redundant and valuable data. They also rely heavily on a single semantic framework (e.g., local or global semantics), resulting in an incomplete or biased representation of user preferences, particularly those less expressed in prior interactions. Furthermore, these approaches fail to capture the complex interactions between users and items, limiting the model’s ability to meet diverse users. To address these challenges, we present SRGFormer, a structurally optimized multimodal recommendation model. By modifying the transformer for better integration into our model, we capture the overall behavior patterns of users. Then, we enhance structural information by embedding multimodal information into a hypergraph structure to aid in learning the local structures between users and items. Meanwhile, applying self-supervised tasks to user-item collaborative signals enhances the integration of multimodal information, thereby revealing the representational features inherent to the data’s modality. Extensive experiments on three public datasets reveal that SRGFormer surpasses previous benchmark models, achieving an average performance improvement of 4.47 percent on the Sports dataset. The code is publicly available online.</p>
  </div>
</details>

<hr>
<h3 id="222-LookSync-Large-Scale-Visual-Product-Search-System-for-AI-Generated-Fashion-Looks-cs-IR-cs-AI-cs-CV-cs-LGPDF"><a href="#222-LookSync-Large-Scale-Visual-Product-Search-System-for-AI-Generated-Fashion-Looks-cs-IR-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[222] LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks cs.IR | cs.AI | cs.CV | cs.LGPDF"></a>[222] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00072">LookSync: Large-Scale Visual Product Search System for AI-Generated Fashion Looks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00072" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pradeep M, Ritesh Pallod, Satyen Abrol, Muthu Raman, Ian Anderson</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为LookSync的大规模视觉产品搜索系统，旨在为AI生成的时尚造型匹配最相似的实物产品。该系统已在实际生产中部署，每日处理超过35万次请求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 生成式AI正在重塑时尚行业，虚拟造型和虚拟形象的需求使得找到与之匹配的实物产品变得尤为重要。</p>
<p><strong>Result:</strong> 实验表明，CLIP在多个类别和评估者中均优于其他模型，平均意见分数提升3-7%，用户感知匹配效果更好。</p>
<p><strong>Insight:</strong> CLIP是目前最可靠的骨干模型，适用于实际生产环境中的大规模视觉搜索任务。</p>
<p><strong>Abstract:</strong> Generative AI is reshaping fashion by enabling virtual looks and avatars making it essential to find real products that best match AI-generated styles. We propose an end-to-end product search system that has been deployed in a real-world, internet scale which ensures that AI-generated looks presented to users are matched with the most visually and semantically similar products from the indexed vector space. The search pipeline is composed of four key components: query generation, vectorization, candidate retrieval, and reranking based on AI-generated looks. Recommendation quality is evaluated using human-judged accuracy scores. The system currently serves more than 350,000 AI Looks in production per day, covering diverse product categories across global markets of over 12 million products. In our experiments, we observed that across multiple annotators and categories, CLIP outperformed alternative models by a small relative margin of 3–7% in mean opinion scores. These improvements, though modest in absolute numbers, resulted in noticeably better user perception matches, establishing CLIP as the most reliable backbone for production deployment.</p>
  </div>
</details>

<hr>
<div id='cs.DB'></div>

<h1 id="cs-DB-Back"><a href="#cs-DB-Back" class="headerlink" title="cs.DB [Back]"></a>cs.DB <a href="#toc">[Back]</a></h1><h3 id="223-ORANGE-An-Online-Reflection-ANd-GEneration-framework-with-Domain-Knowledge-for-Text-to-SQL-cs-DB-cs-AI-cs-CLPDF"><a href="#223-ORANGE-An-Online-Reflection-ANd-GEneration-framework-with-Domain-Knowledge-for-Text-to-SQL-cs-DB-cs-AI-cs-CLPDF" class="headerlink" title="[223] ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL cs.DB | cs.AI | cs.CLPDF"></a>[223] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00985">ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.DB | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00985" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiwen Jiao, Tonghui Ren, Yuche Gao, Zhenying He, Yinan Jing</span></p>
<p><strong>TL;DR:</strong> ORANGE 是一个在线自我演化的框架，通过解析翻译日志中的 SQL 查询构建特定数据库的知识库，逐步减少语义鸿沟并提升后续 SQL 翻译的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管大型语言模型（LLMs）在自然语言到 SQL 的翻译上取得了显著进展，但其通用知识与数据库的领域特定语义之间仍存在显著鸿沟。历史翻译日志提供了丰富的领域内知识来源，但这些知识未能被现有方法充分利用。</p>
<p><strong>Result:</strong> 在多个基准测试中，ORANGE 表现出了实用性和有效性，特别是在处理复杂和领域特定的查询时。</p>
<p><strong>Insight:</strong> ORANGE 的创新之处在于利用历史翻译日志动态构建知识库，从而持续优化翻译性能。这一方法为领域适应性提供了新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable progress in translating natural language to SQL, but a significant semantic gap persists between their general knowledge and domain-specific semantics of databases. Historical translation logs constitute a rich source of this missing in-domain knowledge, where SQL queries inherently encapsulate real-world usage patterns of database schema. Existing methods primarily enhance the reasoning process for individual translations but fail to accumulate in-domain knowledge from past translations. We introduce ORANGE, an online self-evolutionary framework that constructs database-specific knowledge bases by parsing SQL queries from translation logs. By accumulating in-domain knowledge that contains schema and data semantics, ORANGE progressively reduces the semantic gap and enhances the accuracy of subsequent SQL translations. To ensure reliability, we propose a novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic tracking, which reduces semantic errors during knowledge generation. Experiments on multiple benchmarks confirm the practicality of ORANGE, demonstrating its effectiveness for real-world Text-to-SQL deployment, particularly in handling complex and domain-specific queries.</p>
  </div>
</details>

<hr>
<div id='cs.PL'></div>

<h1 id="cs-PL-Back"><a href="#cs-PL-Back" class="headerlink" title="cs.PL [Back]"></a>cs.PL <a href="#toc">[Back]</a></h1><h3 id="224-texttt-ReMind-Understanding-Deductive-Code-Reasoning-in-LLMs-cs-PL-cs-CLPDF"><a href="#224-texttt-ReMind-Understanding-Deductive-Code-Reasoning-in-LLMs-cs-PL-cs-CLPDF" class="headerlink" title="[224] \texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs cs.PL | cs.CLPDF"></a>[224] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00488">\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.PL | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00488" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jun Gao, Yun Peng, Xiaoxue Ren</span></p>
<p><strong>TL;DR:</strong> 该论文研究了大型语言模型（LLMs）在代码相关任务中的演绎推理能力不足问题，并提出了多智能体框架ReMind，通过生成代码变体、跟踪变量状态和优化推理步骤来解决这些挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLMs在代码任务上表现出色，但在演绎代码推理（即理解程序执行过程）上仍存在明显不足。作者旨在探究这些不足的根本原因并提出解决方案。</p>
<p><strong>Result:</strong> 在五个LLMs和两个基准测试上的实验表明，ReMind显著优于基线方法，表现出更强的零样本泛化能力和推理一致性。</p>
<p><strong>Insight:</strong> 论文强调了LLMs在生成代码和推理能力之间的内在差距，并展示了多智能体协作在解决复杂推理任务中的潜力。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have achieved remarkable progress in code-related tasks. Despite their advancement, empirical evidence reveals that they still struggle with \emph{deductive code reasoning}, the ability to reason about the program execution process. While prior studies have recognized this limitation, the underlying causes remain largely underexplored. In this paper, we begin by presenting a comprehensive empirical study that reveals three key challenges undermining deductive code reasoning: (1) an intrinsic gap between generation and reasoning abilities, (2) a consistent bias towards code sources, and (3) weak zero-shot generalization on complex benchmarks. In light of these challenges, we propose \texttt{ReMind}, a multi-agent framework composed of \texttt{Mutator}, \texttt{Executor}, and \texttt{Inspector}. The \texttt{Mutator} generates code variants to mitigate bias towards code sources, the \texttt{Executor} traces variable states step-by-step to expose inconsistency, and the \texttt{Inspector} identifies problematic reasoning steps and provides control-flow refinement to bridge the intrinsic reasoning gap. Through their coordinated collaboration, \texttt{ReMind} systematically identifies and refines reasoning flaws, achieving outstanding performance and enabling robust zero-shot generalization. Extensive experiments on two benchmarks with five LLMs demonstrate the superior advantages of \texttt{ReMind} compared to baseline approaches in deductive code reasoning.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="225-SonarSweep-Fusing-Sonar-and-Vision-for-Robust-3D-Reconstruction-via-Plane-Sweeping-cs-RO-cs-AI-cs-CVPDF"><a href="#225-SonarSweep-Fusing-Sonar-and-Vision-for-Robust-3D-Reconstruction-via-Plane-Sweeping-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[225] SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping cs.RO | cs.AI | cs.CVPDF"></a>[225] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00392">SonarSweep: Fusing Sonar and Vision for Robust 3D Reconstruction via Plane Sweeping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00392" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lingpeng Chen, Jiakun Tang, Apple Pui-Yi Chui, Ziyang Hong, Junfeng Wu</span></p>
<p><strong>TL;DR:</strong> SonarSweep提出了一种端到端的深度学习框架，通过平面扫描算法融合声纳和视觉数据，解决了水下环境中3D重建的挑战，显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 水下环境的视觉退化导致单模态（视觉或声纳）方法在3D重建中存在局限性，现有融合方法依赖启发式和有缺陷的几何假设，无法处理复杂场景。</p>
<p><strong>Result:</strong> 在高保真仿真和真实环境实验中，SonarSweep在浑浊条件下显著优于现有方法，生成更准确的深度图。</p>
<p><strong>Insight:</strong> 跨模态融合（声纳与视觉）结合深度学习能够有效克服单模态的不足，尤其在恶劣环境中表现突出；平面扫描算法的适应性是关键。</p>
<p><strong>Abstract:</strong> Accurate 3D reconstruction in visually-degraded underwater environments remains a formidable challenge. Single-modality approaches are insufficient: vision-based methods fail due to poor visibility and geometric constraints, while sonar is crippled by inherent elevation ambiguity and low resolution. Consequently, prior fusion technique relies on heuristics and flawed geometric assumptions, leading to significant artifacts and an inability to model complex scenes. In this paper, we introduce SonarSweep, a novel, end-to-end deep learning framework that overcomes these limitations by adapting the principled plane sweep algorithm for cross-modal fusion between sonar and visual data. Extensive experiments in both high-fidelity simulation and real-world environments demonstrate that SonarSweep consistently generates dense and accurate depth maps, significantly outperforming state-of-the-art methods across challenging conditions, particularly in high turbidity. To foster further research, we will publicly release our code and a novel dataset featuring synchronized stereo-camera and sonar data, the first of its kind.</p>
  </div>
</details>

<hr>
<h3 id="226-Fast-SmartWay-Panoramic-Free-End-to-End-Zero-Shot-Vision-and-Language-Navigation-cs-RO-cs-CVPDF"><a href="#226-Fast-SmartWay-Panoramic-Free-End-to-End-Zero-Shot-Vision-and-Language-Navigation-cs-RO-cs-CVPDF" class="headerlink" title="[226] Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation cs.RO | cs.CVPDF"></a>[226] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00933">Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00933" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangyu Shi, Zerui Li, Yanyuan Qiao, Qi Wu</span></p>
<p><strong>TL;DR:</strong> Fast-SmartWay 是一种端到端的零样本视觉与语言导航（VLN-CE）框架，无需全景视图和多阶段预测，仅通过三张前视RGB-D图像和自然语言指令直接预测动作，显著减少了延迟并提高了实用性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的零样本视觉与语言导航方法依赖全景视图和多阶段预测器，导致延迟高且实用性受限。Fast-SmartWay 旨在通过简化输入和改进推理机制，实现快速、鲁棒的导航。</p>
<p><strong>Result:</strong> 在模拟和真实机器人环境中，Fast-SmartWay 显著降低了单步延迟，性能与全景视图基线相当或更优，验证了其在真实世界导航中的实用性。</p>
<p><strong>Insight:</strong> 去除全景依赖和简化输入是提高零样本导航实时性的有效途径，不确定性感知推理为复杂环境中的鲁棒决策提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in Vision-and-Language Navigation in Continuous Environments (VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve zero-shot navigation. However, existing methods often rely on panoramic observations and two-stage pipelines involving waypoint predictors, which introduce significant latency and limit real-world applicability. In this work, we propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that eliminates the need for panoramic views and waypoint predictors. Our approach uses only three frontal RGB-D images combined with natural language instructions, enabling MLLMs to directly predict actions. To enhance decision robustness, we introduce an Uncertainty-Aware Reasoning module that integrates (i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past Bidirectional Reasoning mechanism for globally coherent planning. Experiments on both simulated and real-robot environments demonstrate that our method significantly reduces per-step latency while achieving competitive or superior performance compared to panoramic-view baselines. These results demonstrate the practicality and effectiveness of Fast-SmartWay for real-world zero-shot embodied navigation.</p>
  </div>
</details>

<hr>
<h3 id="227-LiDAR-VGGT-Cross-Modal-Coarse-to-Fine-Fusion-for-Globally-Consistent-and-Metric-Scale-Dense-Mapping-cs-RO-cs-CVPDF"><a href="#227-LiDAR-VGGT-Cross-Modal-Coarse-to-Fine-Fusion-for-Globally-Consistent-and-Metric-Scale-Dense-Mapping-cs-RO-cs-CVPDF" class="headerlink" title="[227] LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping cs.RO | cs.CVPDF"></a>[227] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01186">LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01186" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lijie Wang, Lianjie Guo, Ziyi Xu, Qianhao Wang, Fei Gao</span></p>
<p><strong>TL;DR:</strong> 本文提出LiDAR-VGGT框架，通过两阶段粗到细的融合方法，将LiDAR惯性里程计与VGGT模型结合，实现了高一致性和公制尺度的稠密地图重建。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LIVO对外部校准高度敏感，而3D视觉基础模型（如VGGT）在大规模环境中可扩展性有限且缺乏公制尺度。本文旨在解决这些问题。</p>
<p><strong>Result:</strong> 实验表明，LiDAR-VGGT优于VGGT和LIVO基线，且实现了稠密、全局一致的彩色点云。</p>
<p><strong>Insight:</strong> 跨模态融合和粗到细的分阶段处理能有效解决尺度不一致和校准敏感性问题。</p>
<p><strong>Abstract:</strong> Reconstructing large-scale colored point clouds is an important task in robotics, supporting perception, navigation, and scene understanding. Despite advances in LiDAR inertial visual odometry (LIVO), its performance remains highly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation models, such as VGGT, suffer from limited scalability in large environments and inherently lack metric scale. To overcome these limitations, we propose LiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with the state-of-the-art VGGT model through a two-stage coarse- to-fine fusion pipeline: First, a pre-fusion module with robust initialization refinement efficiently estimates VGGT poses and point clouds with coarse metric scale within each session. Then, a post-fusion module enhances cross-modal 3D similarity transformation, using bounding-box-based regularization to reduce scale distortions caused by inconsistent FOVs between LiDAR and camera sensors. Extensive experiments across multiple datasets demonstrate that LiDAR-VGGT achieves dense, globally consistent colored point clouds and outperforms both VGGT-based methods and LIVO baselines. The implementation of our proposed novel color point cloud evaluation toolkit will be released as open source.</p>
  </div>
</details>

<hr>
<h3 id="228-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects-cs-RO-cs-CVPDF"><a href="#228-Kinematify-Open-Vocabulary-Synthesis-of-High-DoF-Articulated-Objects-cs-RO-cs-CVPDF" class="headerlink" title="[228] Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects cs.RO | cs.CVPDF"></a>[228] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01294">Kinematify: Open-Vocabulary Synthesis of High-DoF Articulated Objects</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01294" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiawei Wang, Dingyou Wang, Jiaming Hu, Qixuan Zhang, Jingyi Yu</span></p>
<p><strong>TL;DR:</strong> Kinematify是一个自动化框架，能够直接从任意RGB图像或文本提示合成高自由度（DoF）的铰接对象，解决了推断高DoF对象的运动学拓扑和从静态几何估计关节参数的核心挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 铰接物体模型对机器人操作、物理模拟和运动规划等任务至关重要，但现有方法依赖运动序列或人工标注数据集，限制了扩展性。</p>
<p><strong>Result:</strong> 在合成和真实环境中的输入上表现出优于先前工作的配准和运动学拓扑准确性。</p>
<p><strong>Insight:</strong> 通过结合搜索和几何优化，实现了对高复杂度铰接对象的自动化建模，为机器人操作和模拟提供了新工具。</p>
<p><strong>Abstract:</strong> A deep understanding of kinematic structures and movable components is essential for enabling robots to manipulate objects and model their own articulated forms. Such understanding is captured through articulated objects, which are essential for tasks such as physical simulation, motion planning, and policy learning. However, creating these models, particularly for complex systems like robots or objects with high degrees of freedom (DoF), remains a significant challenge. Existing methods typically rely on motion sequences or strong assumptions from hand-curated datasets, which hinders scalability. In this paper, we introduce Kinematify, an automated framework that synthesizes articulated objects directly from arbitrary RGB images or text prompts. Our method addresses two core challenges: (i) inferring kinematic topologies for high-DoF objects and (ii) estimating joint parameters from static geometry. To achieve this, we combine MCTS search for structural inference with geometry-driven optimization for joint reasoning, producing physically consistent and functionally valid descriptions. We evaluate Kinematify on diverse inputs from both synthetic and real-world environments, demonstrating improvements in registration and kinematic topology accuracy over prior work.</p>
  </div>
</details>

<hr>
<h3 id="229-MARS-Multi-Agent-Robotic-System-with-Multimodal-Large-Language-Models-for-Assistive-Intelligence-cs-RO-cs-CV-I-2-9-I-2-11-I-2-6-I-4-8PDF"><a href="#229-MARS-Multi-Agent-Robotic-System-with-Multimodal-Large-Language-Models-for-Assistive-Intelligence-cs-RO-cs-CV-I-2-9-I-2-11-I-2-6-I-4-8PDF" class="headerlink" title="[229] MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence cs.RO | cs.CV | I.2.9; I.2.11; I.2.6; I.4.8PDF"></a>[229] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01594">MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | I.2.9; I.2.11; I.2.6; I.4.8</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01594" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Renjun Gao, Peiyan Zhong</span></p>
<p><strong>TL;DR:</strong> MARS是一个基于多模态大语言模型（MLLM）的多智能体机器人系统，旨在为智能家居机器人提供风险感知、个性化的辅助智能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的辅助智能系统在风险感知规划、用户个性化及语言计划转化为可执行技能方面仍存在挑战，MARS试图通过多智能体协同解决这些问题。</p>
<p><strong>Result:</strong> 实验显示MARS在风险感知规划和多智能体协同执行上优于现有多模态模型，展示了协作AI在现实辅助场景中的潜力。</p>
<p><strong>Insight:</strong> MARS为多模态大语言模型在现实环境中的多智能体系统部署提供了一种可推广的方法，强调了跨模态理解与分层决策的重要性。</p>
<p><strong>Abstract:</strong> Multimodal large language models (MLLMs) have shown remarkable capabilities in cross-modal understanding and reasoning, offering new opportunities for intelligent assistive systems, yet existing systems still struggle with risk-aware planning, user personalization, and grounding language plans into executable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic System powered by MLLMs for assistive intelligence and designed for smart home robots supporting people with disabilities. The system integrates four agents: a visual perception agent for extracting semantic and spatial features from environment images, a risk assessment agent for identifying and prioritizing hazards, a planning agent for generating executable action sequences, and an evaluation agent for iterative optimization. By combining multimodal perception with hierarchical multi-agent decision-making, the framework enables adaptive, risk-aware, and personalized assistance in dynamic indoor environments. Experiments on multiple datasets demonstrate the superior overall performance of the proposed system in risk-aware planning and coordinated multi-agent execution compared with state-of-the-art multimodal models. The proposed approach also highlights the potential of collaborative AI for practical assistive scenarios and provides a generalizable methodology for deploying MLLM-enabled multi-agent systems in real-world environments.</p>
  </div>
</details>

<hr>
<h3 id="230-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process-cs-RO-cs-CVPDF"><a href="#230-Unified-Diffusion-VLA-Vision-Language-Action-Model-via-Joint-Discrete-Denoising-Diffusion-Process-cs-RO-cs-CVPDF" class="headerlink" title="[230] Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process cs.RO | cs.CVPDF"></a>[230] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.01718">Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.01718" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiayi Chen, Wenxuan Song, Pengxiang Ding, Ziyang Zhou, Han Zhao</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种名为Unified Diffusion VLA的视觉-语言-动作模型，通过联合离散去噪扩散过程（JD3P）实现了多模态的统一理解和协同生成与动作预测，显著提升了性能和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉-语言-动作（VLA）模型要么依赖外部专家实现模态统一，要么将图像生成和动作预测分开处理，限制了任务的直接协同效应。本文旨在通过联合优化生成与动作预测，实现更高的性能和协同效率。</p>
<p><strong>Result:</strong> 在CALVIN、LIBERO和SimplerEnv等基准测试中达到SOTA性能，推理速度比自回归方法快4倍。</p>
<p><strong>Insight:</strong> 通过联合扩散过程实现多模态协同，验证了同步优化生成和动作预测的有效性，为VLA模型设计提供了新的思路。</p>
<p><strong>Abstract:</strong> Vision-language-action (VLA) models aim to understand natural language instructions and visual observations and to execute corresponding actions as an embodied agent. Recent work integrates future images into the understanding-acting loop, yielding unified VLAs that jointly understand, generate, and act – reading text and images and producing future images and actions. However, these models either rely on external experts for modality unification or treat image generation and action prediction as separate processes, limiting the benefits of direct synergy between these tasks. Our core philosophy is to optimize generation and action jointly through a synchronous denoising process, where the iterative refinement enables actions to evolve from initialization, under constant and sufficient visual guidance. We ground this philosophy in our proposed Unified Diffusion VLA and Joint Discrete Denoising Diffusion Process (JD3P), which is a joint diffusion process that integrates multiple modalities into a single denoising trajectory to serve as the key mechanism enabling understanding, generation, and acting to be intrinsically synergistic. Our model and theory are built on a unified tokenized space of all modalities and a hybrid attention mechanism. We further propose a two-stage training pipeline and several inference-time techniques that optimize performance and efficiency. Our approach achieves state-of-the-art performance on benchmarks such as CALVIN, LIBERO, and SimplerEnv with 4$\times$ faster inference than autoregressive methods, and we demonstrate its effectiveness through in-depth analysis and real-world evaluations. Our project page is available at <a target="_blank" rel="noopener" href="https://irpn-eai.github.io/UD-VLA.github.io/">https://irpn-eai.github.io/UD-VLA.github.io/</a>.</p>
  </div>
</details>

<hr>
<div id='eess.AS'></div>

<h1 id="eess-AS-Back"><a href="#eess-AS-Back" class="headerlink" title="eess.AS [Back]"></a>eess.AS <a href="#toc">[Back]</a></h1><h3 id="231-MULTI-Bench-A-Multi-Turn-Interactive-Benchmark-for-Assessing-Emotional-Intelligence-ability-of-Spoken-Dialogue-Models-eess-AS-cs-AI-cs-CL-cs-SDPDF"><a href="#231-MULTI-Bench-A-Multi-Turn-Interactive-Benchmark-for-Assessing-Emotional-Intelligence-ability-of-Spoken-Dialogue-Models-eess-AS-cs-AI-cs-CL-cs-SDPDF" class="headerlink" title="[231] MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models eess.AS | cs.AI | cs.CL | cs.SDPDF"></a>[231] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.00850">MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.AS | cs.AI | cs.CL | cs.SD</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.00850" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yayue Deng, Guoqiang Hu, Haiyang Sun, Xiangyu Zhang, Haoyang Zhang</span></p>
<p><strong>TL;DR:</strong> Multi-Bench是首个专注于评估语音对话模型在多轮互动对话中情感智力的基准，包含基本和高级两个层级，测试情感理解、推理、支持和应用能力，结果显示现有模型在高级任务中仍有提升空间。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有语音对话模型（SDMs）的评估多集中在单轮对话，而在多轮互动对话中的表现尤其是情感智力方面未被充分研究。</p>
<p><strong>Result:</strong> 当前SDM在基础任务表现良好，但在高级任务（如情感意识和应用）中仍有不足。</p>
<p><strong>Insight:</strong> 情感智力是多轮互动对话的核心能力，现有模型需进一步优化复杂推理和情感支持能力。</p>
<p><strong>Abstract:</strong> Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to sustain genuinely interactive multi-turn conversations remains underexplored, as most benchmarks focus on single-turn exchanges. We introduce Multi-Bench, the first benchmark explicitly designed to evaluate SDMs in multi-turn interactive dialogue with an emphasis on emotional intelligence. Multi-Bench employs a hierarchical structure with a basic track for emotion understanding and reasoning and an advanced track for emotion support and application. It comprises five carefully designed tasks and about 3.2K samples, ranging from emotion recognition to complex reasoning and interactive dialogue, supported by a reproducible evaluation framework. We evaluate six representative SDMs on eight subsets of Multi-Bench. Results show that while current SDMs achieve good performance on basic understanding tasks, they still have room for improvement in advanced multi-turn interactive dialogue and reasoning-related tasks, particularly in emotion awareness and application.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-11-06/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-11-04/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
