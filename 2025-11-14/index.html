<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-11-14 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-11-14
            
            
        </div>
        <span class="post-date">
            Nov 14, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 12]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 9]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Case-Study-Transformer-Based-Solution-for-the-Automatic-Digitization-of-Gas-Plants-cs-CV-cs-AI-cs-LGPDF"><a href="#1-Case-Study-Transformer-Based-Solution-for-the-Automatic-Digitization-of-Gas-Plants-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[1] Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants cs.CV | cs.AI | cs.LGPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08609">Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08609" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">I. Bailo, F. Buonora, G. Ciarfaglia, L. T. Consoli, A. Evangelista</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于Transformer的解决方案，用于自动化燃气工厂的数字化过程，结合OCR、Vision LLM等技术，实现了高精度的数据提取和拓扑结构重建。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 燃气工厂的数字化过程复杂且缺乏标准化，传统方法效率低下。本文旨在通过AI技术（如Transformer架构）自动化数据提取和结构重建，提升效率和准确性。</p>
<p><strong>Result:</strong> 文本数据提取准确率91%，组件识别率93%，拓扑结构提取准确率80%。</p>
<p><strong>Insight:</strong> Transformer架构在处理复杂关系时表现出色，多技术协同可有效解决数据多样化问题，为工业数字化提供了一种高效解决方案。</p>
<p><strong>Abstract:</strong> The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&amp;ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant’s components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80%.</p>
  </div>
</details>

<hr>
<h3 id="2-Assessing-Identity-Leakage-in-Talking-Face-Generation-Metrics-and-Evaluation-Framework-cs-CV-eess-IVPDF"><a href="#2-Assessing-Identity-Leakage-in-Talking-Face-Generation-Metrics-and-Evaluation-Framework-cs-CV-eess-IVPDF" class="headerlink" title="[2] Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework cs.CV | eess.IVPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08613">Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08613" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dogucan Yaman, Fevziye Irem Eyiokur, Hazım Kemal Ekenel, Alexander Waibel</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种系统化的评估方法来分析和量化基于语音驱动的对话人脸生成中的嘴唇泄漏问题，并通过互补的测试设置和衍生指标提供了一个可靠的基准。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基于语音驱动的对话人脸生成方法可能在生成过程中引入嘴唇泄漏问题，即生成的嘴唇受参考图像影响而非仅由驱动音频决定。传统的评估方法和测试设置难以检测此类问题。</p>
<p><strong>Result:</strong> 该方法能够有效检测和量化嘴唇泄漏问题，并提供了一个模型无关的可靠基准。</p>
<p><strong>Insight:</strong> 研究表明，身份参考的选择对泄漏问题有显著影响，这为未来的参考图像设计提供了重要指导。</p>
<p><strong>Abstract:</strong> Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.</p>
  </div>
</details>

<hr>
<h3 id="3-Time-to-Move-Training-Free-Motion-Controlled-Video-Generation-via-Dual-Clock-Denoising-cs-CV-cs-AI-cs-GR-cs-LG-cs-MMPDF"><a href="#3-Time-to-Move-Training-Free-Motion-Controlled-Video-Generation-via-Dual-Clock-Denoising-cs-CV-cs-AI-cs-GR-cs-LG-cs-MMPDF" class="headerlink" title="[3] Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising cs.CV | cs.AI | cs.GR | cs.LG | cs.MMPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08633">Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.GR | cs.LG | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08633" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Assaf Singer, Noam Rotstein, Amir Mann, Ron Kimmel, Or Litany</span></p>
<p><strong>TL;DR:</strong> 论文提出了Time-to-Move (TTM)，一种无需训练的即插即用框架，用于实现基于图像到视频（I2V）扩散模型的运动控制和外观控制的视频生成。通过双时钟去噪策略，TTM在指定运动区域强制对齐，同时在其他区域保持灵活性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于图像和文本的扩散视频生成方法缺乏精确的运动控制，而传统方法需要针对特定模型进行微调，计算成本高且受限。TTM旨在实现无需训练的运动控制视频生成。</p>
<p><strong>Result:</strong> TTM在物体和相机运动基准测试中表现优异，与基于训练的基线方法相当或更优。</p>
<p><strong>Insight:</strong> 通过粗略动画和图像条件的结合，TTM展示了无需训练即可实现精确运动控制的潜力，扩展了扩散模型的应用范围。</p>
<p><strong>Abstract:</strong> Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit’s use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: <a target="_blank" rel="noopener" href="https://time-to-move.github.io/">https://time-to-move.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="4-CADIC-Continual-Anomaly-Detection-Based-on-Incremental-Coreset-cs-CV-cs-AIPDF"><a href="#4-CADIC-Continual-Anomaly-Detection-Based-on-Incremental-Coreset-cs-CV-cs-AIPDF" class="headerlink" title="[4] CADIC: Continual Anomaly Detection Based on Incremental Coreset cs.CV | cs.AIPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08634">CADIC: Continual Anomaly Detection Based on Incremental Coreset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08634" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gen Yang, Zhipeng Deng, Junfeng Man</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于增量核心集的持续异常检测方法CADIC，通过统一内存库和固定大小核心集的增量更新，解决了现有方法中任务特定内存碎片化的问题，并在多个数据集上实现了优异的检测精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有持续异常检测方法需要为每个任务构建特定的子内存库，限制了灵活性和可扩展性。为了解决这一问题，论文提出了一种共享统一内存库的方法，避免内存碎片化。</p>
<p><strong>Result:</strong> 在MVTec AD和Visa数据集上，CADIC的平均图像级AUROC分别达到0.972和0.891；在真实电子纸数据集上实现了100%的异常检测准确率。</p>
<p><strong>Insight:</strong> 核心集技术在持续学习中能够有效避免内存碎片化，统一内存库的设计提升了方法的灵活性和可扩展性。</p>
<p><strong>Abstract:</strong> The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.</p>
  </div>
</details>

<hr>
<h3 id="5-Predict-and-Resist-Long-Term-Accident-Anticipation-under-Sensor-Noise-cs-CV-cs-AIPDF"><a href="#5-Predict-and-Resist-Long-Term-Accident-Anticipation-under-Sensor-Noise-cs-CV-cs-AIPDF" class="headerlink" title="[5] Predict and Resist: Long-Term Accident Anticipation under Sensor Noise cs.CV | cs.AIPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08640">Predict and Resist: Long-Term Accident Anticipation under Sensor Noise</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08640" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xingcheng Liu, Bin Rao, Yanchen Guan, Chengyue Wang, Haicheng Liao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合扩散去噪和时域演员-评论家模型的框架，用于长期事故预测，以应对传感器噪声和早期警报的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动驾驶中事故预测的关键在于及时可靠的预警，但传感器噪声和早期预测与可靠性平衡的问题阻碍了实际部署。</p>
<p><strong>Result:</strong> 在多个基准数据集上取得最先进性能，平均事故时间显著提升，噪声环境下表现稳健。</p>
<p><strong>Insight:</strong> 扩散去噪与时域强化学习的结合为噪声环境下的长期预测提供了新思路，模型预测更加稳定且符合人类行为。</p>
<p><strong>Abstract:</strong> Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.</p>
  </div>
</details>

<hr>
<h3 id="6-RS-Net-Context-Aware-Relation-Scoring-for-Dynamic-Scene-Graph-Generation-cs-CV-cs-AIPDF"><a href="#6-RS-Net-Context-Aware-Relation-Scoring-for-Dynamic-Scene-Graph-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[6] RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation cs.CV | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08651">RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08651" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hae-Won Jo, Yeong-Jun Cho</span></p>
<p><strong>TL;DR:</strong> RS-Net是一个用于动态场景图生成的模块化框架，通过空间交互和长距离时序上下文评估对象对的重要性，显著提高了关系预测的精度和召回率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的动态场景图生成（DSGG）方法仅针对标注的对象对进行训练，缺乏对非相关对的指导，导致推理时难以识别有意义的关系。</p>
<p><strong>Result:</strong> 在Action Genome数据集上，RS-Net显著提升了召回率和精度，尤其是在长尾关系分布中表现突出，同时保持了较高的效率。</p>
<p><strong>Insight:</strong> RS-Net无需更改架构即可整合到现有DSGG模型中，为动态场景图中关系的动态演化提供了更全面的上下文建模能力。</p>
<p><strong>Abstract:</strong> Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="7-Privacy-Beyond-Pixels-Latent-Anonymization-for-Privacy-Preserving-Video-Understanding-cs-CVPDF"><a href="#7-Privacy-Beyond-Pixels-Latent-Anonymization-for-Privacy-Preserving-Video-Understanding-cs-CVPDF" class="headerlink" title="[7] Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08666">Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08666" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah</span></p>
<p><strong>TL;DR:</strong> 論文提出了一種新型的視頻隱私保護方法，通過在潛在空間中操作，避免了像素級匿名化的局限，同時保持下游任務的效用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 現有視頻隱私保護方法主要集中在像素級匿名化，這需要重新訓練整個模型，且針對特定任務，不適用於視頻基礎模型。這些方法也無法解決潛在空間中隱私洩露的問題。</p>
<p><strong>Result:</strong> 實驗顯示，隱私洩露減少了35%，同時在多個任務（如動作識別、時序動作檢測、異常檢測）中保持了接近基線的性能。此外，方法有效地減少了性別偏見。</p>
<p><strong>Insight:</strong> 潛在空間匿名化是一種有效的隱私保護途徑，尤其適用於視頻基礎模型。通過設計多任務訓練目標，可以同時解決隱私保護和任務性能的平衡問題。</p>
<p><strong>Abstract:</strong> We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.</p>
  </div>
</details>

<hr>
<h3 id="8-Rethinking-generative-image-pretraining-How-far-are-we-from-scaling-up-next-pixel-prediction-cs-CV-cs-LGPDF"><a href="#8-Rethinking-generative-image-pretraining-How-far-are-we-from-scaling-up-next-pixel-prediction-cs-CV-cs-LGPDF" class="headerlink" title="[8] Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction? cs.CV | cs.LGPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08704">Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08704" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinchen Yan, Chen Liang, Lijun Yu, Adams Wei Yu, Yifeng Lu</span></p>
<p><strong>TL;DR:</strong> 该论文研究了自回归逐像素预测的扩展特性，发现分类和生成任务的最优扩展策略存在显著差异，且计算能力是主要瓶颈而非数据量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索自回归逐像素预测作为一种简单但未充分研究的统一视觉模型框架在不同任务（分类和生成）中的扩展特性。</p>
<p><strong>Result:</strong> 发现分类和生成任务的最优扩展策略不同，计算能力是限制因素，预计五年内可实现逐像素建模。</p>
<p><strong>Insight:</strong> 未来研究应关注计算效率的提升，而非单纯增加数据量。</p>
<p><strong>Abstract:</strong> This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr’echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.</p>
  </div>
</details>

<hr>
<h3 id="9-Harnessing-Diffusion-Generated-Synthetic-Images-for-Fair-Image-Classification-cs-CVPDF"><a href="#9-Harnessing-Diffusion-Generated-Synthetic-Images-for-Fair-Image-Classification-cs-CVPDF" class="headerlink" title="[9] Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification cs.CVPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08711">Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08711" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abhipsa Basu, Aviral Gupta, Abhijnya Bhat, R. Venkatesh Babu</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了如何利用扩散模型生成平衡的训练数据以减少图像分类中的偏见。通过LoRA和DreamBooth等技术微调扩散模型，并结合聚类方法提升生成数据的准确性。实验表明，该方法优于普通Stable Diffusion，并在高偏置数据集上超越了Group-DRO等先进方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 图像分类系统常因训练数据中不同群体的不平衡表示而产生偏见，例如金发头发与女性的过度关联。如何生成更平衡的数据以减少偏见是一个重要问题。</p>
<p><strong>Result:</strong> 实验表明，提出的微调方法优于普通Stable Diffusion，并在多个基准测试中达到与Group-DRO等先进方法相当的结果，尤其在高偏置数据集中表现更优。</p>
<p><strong>Insight:</strong> 论文揭示了扩散模型通过适当微调和聚类可以生成更平衡的训练数据，从而有效减少图像分类中的群体偏见，尤其适用于高偏置数据集。</p>
<p><strong>Abstract:</strong> Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.</p>
  </div>
</details>

<hr>
<h3 id="10-WiCV-at-CVPR-2025-The-Women-in-Computer-Vision-Workshop-cs-CVPDF"><a href="#10-WiCV-at-CVPR-2025-The-Women-in-Computer-Vision-Workshop-cs-CVPDF" class="headerlink" title="[10] WiCV at CVPR 2025: The Women in Computer Vision Workshop cs.CVPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08748">WiCV at CVPR 2025: The Women in Computer Vision Workshop</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08748" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Estefania Talavera, Deblina Bhattacharjee, Himangi Mittal, Mengwei Ren, Karen Sanchez</span></p>
<p><strong>TL;DR:</strong> 本文档是对2025年CVPR会议上举办的‘计算机视觉领域女性研讨会’（WiCV）的总结报告，内容包括研讨会概况、参与统计数据、导师计划成果及历史趋势分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探讨WiCV在过去16年中如何推动计算机视觉领域女性和少数群体的可见性、包容性和职业发展，并为未来的类似倡议提供参考。</p>
<p><strong>Result:</strong> 2025年WiCV共接受14篇长论文和36篇摘要海报，导师计划配对80名学员与37名导师，吸引了100多名现场参与者，并获得10家赞助商支持。</p>
<p><strong>Insight:</strong> WiCV的成功经验表明，持续的导师计划、赞助支持和社区参与是推动多样性和包容性的关键因素。</p>
<p><strong>Abstract:</strong> The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE&#x2F;CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year’s workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.</p>
  </div>
</details>

<hr>
<h3 id="11-SIFT-Graph-Benchmarking-Multimodal-Defense-Against-Image-Adversarial-Attacks-With-Robust-Feature-Graph-cs-CVPDF"><a href="#11-SIFT-Graph-Benchmarking-Multimodal-Defense-Against-Image-Adversarial-Attacks-With-Robust-Feature-Graph-cs-CVPDF" class="headerlink" title="[11] SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph cs.CVPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08810">SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08810" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jingjie He, Weijie Liang, Zihan Shan, Matthew Caesar</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为SIFT-Graph的多模态防御框架，通过结合手工提取和学习到的特征，增强了传统视觉模型对对抗攻击的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代深度视觉模型依赖于密集的像素级表示，这些表示对不可察觉的扰动高度敏感，而传统的防御策略缺乏整合鲁棒视觉特征的机制。因此，需要一种新的方法来解决这一脆弱性。</p>
<p><strong>Result:</strong> 实验表明，SIFT-Graph显著提高了视觉模型对梯度白盒对抗攻击的鲁棒性，同时仅略微降低了干净数据的准确率。</p>
<p><strong>Insight:</strong> 通过结合手工特征和学习特征，可以有效增强模型对对抗攻击的鲁棒性，同时保持对干净数据的性能。多模态融合可能是未来防御策略的重要方向。</p>
<p><strong>Abstract:</strong> Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.</p>
  </div>
</details>

<hr>
<h3 id="12-DT-NVS-Diffusion-Transformers-for-Novel-View-Synthesis-cs-CV-cs-AIPDF"><a href="#12-DT-NVS-Diffusion-Transformers-for-Novel-View-Synthesis-cs-CV-cs-AIPDF" class="headerlink" title="[12] DT-NVS: Diffusion Transformers for Novel View Synthesis cs.CV | cs.AIPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08823">DT-NVS: Diffusion Transformers for Novel View Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08823" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wonbong Jang, Jonathan Tremblay, Lourdes Agapito</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于扩散变换器（DT-NVS）的新视角合成方法，突破了现有方法在小幅度相机运动或对象中心场景下的限制，能够从单张图片生成多样化的3D场景视图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于扩散的方法仅适用于小幅相机运动或不自然的对象中心场景，限制了其在实际应用中的潜力。本文旨在解决这一问题，提出一种更通用的新视角合成方法。</p>
<p><strong>Result:</strong> 在单张输入图像的3D新视角合成任务中，DT-NVS优于现有基于扩散的3D感知模型和确定性方法，并能生成多样化输出。</p>
<p><strong>Insight:</strong> 变换器架构在3D视觉任务中表现出强大潜力，尤其是在处理非对齐数据集时；训练范式的创新可以提升模型性能。</p>
<p><strong>Abstract:</strong> Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="13-GMTRouter-Personalized-LLM-Router-over-Multi-turn-User-Interactions-cs-CL-cs-LGPDF"><a href="#13-GMTRouter-Personalized-LLM-Router-over-Multi-turn-User-Interactions-cs-CL-cs-LGPDF" class="headerlink" title="[13] GMTRouter: Personalized LLM Router over Multi-turn User Interactions cs.CL | cs.LGPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08590">GMTRouter: Personalized LLM Router over Multi-turn User Interactions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08590" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Encheng Xie, Yihang Sun, Tao Feng, Jiaxuan You</span></p>
<p><strong>TL;DR:</strong> GMTRouter是一个个性化LLM路由方法，通过异构图表示多轮用户交互，利用轻量级归纳图学习捕获用户偏好，显著优于基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LLM路由方法缺乏个性化和对用户偏好动态变化的适应能力，且用户数据稀缺、噪声大、格式不一致，限制了效果提升。</p>
<p><strong>Result:</strong> 在多个数据集上表现优异，准确率和AUC分别提升0.9-21.6%和0.006-0.309，且能适应新用户和偏好变化。</p>
<p><strong>Insight:</strong> 异构图和消息传递机制的有效性为LLM个性化路由提供了新思路，尤其在数据稀缺场景下优势明显。</p>
<p><strong>Abstract:</strong> Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GMTRouter">https://github.com/ulab-uiuc/GMTRouter</a>.</p>
  </div>
</details>

<hr>
<h3 id="14-Knowledge-Graph-Analysis-of-Legal-Understanding-and-Violations-in-LLMs-cs-CLPDF"><a href="#14-Knowledge-Graph-Analysis-of-Legal-Understanding-and-Violations-in-LLMs-cs-CLPDF" class="headerlink" title="[14] Knowledge Graph Analysis of Legal Understanding and Violations in LLMs cs.CLPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08593">Knowledge Graph Analysis of Legal Understanding and Violations in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08593" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abha Jha, Abel Salinas, Fred Morstatter</span></p>
<p><strong>TL;DR:</strong> 该论文研究了大型语言模型（LLMs）在法律理解和漏洞方面的表现，重点关注其对美国法典第18章第175条（生物武器相关法律）的解读能力。尽管LLMs在法律分析中有潜力，但也存在生成不安全输出的风险。作者提出了一种结合知识图谱与检索增强生成（RAG）的方法，评估LLMs的法律理解能力和潜在不安全行为。实验揭示了LLMs在法律推理和安全机制上的显著局限性，并提出了改进方向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLMs在敏感法律领域的应用增加，其在理解和遵守法律方面的能力及其潜在风险成为了重要问题。论文动机在于揭示LLMs在法律解读中的漏洞，并提出改进方法以避免潜在的法律违规风险。</p>
<p><strong>Result:</strong> 实验结果表明，LLMs在法律推理和安全机制上存在显著不足，例如可能生成违反法律的指导性内容。但这些发现也为未来改进提供了方向。</p>
<p><strong>Insight:</strong> 论文的关键见解在于LLMs虽然在法律分析中表现出潜力，但其安全性和法律合规性仍需通过技术手段（如增强安全协议和改进推理框架）进一步提升，以确保其在敏感领域的伦理应用。</p>
<p><strong>Abstract:</strong> The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs’ understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs’ reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.</p>
  </div>
</details>

<hr>
<h3 id="15-Chopping-Trees-Semantic-Similarity-Based-Dynamic-Pruning-for-Tree-of-Thought-Reasoning-cs-CL-cs-AIPDF"><a href="#15-Chopping-Trees-Semantic-Similarity-Based-Dynamic-Pruning-for-Tree-of-Thought-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[15] Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning cs.CL | cs.AIPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08595">Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08595" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joongho Kim, Xirui Huang, Zarreen Reza, Gabriel Grand, Kevin Zhu</span></p>
<p><strong>TL;DR:</strong> SSDP是一种基于语义相似性的动态剪枝方法，用于优化树状思维推理（ToT），通过实时聚类和剪枝冗余推理路径，显著减少计算开销并保持高准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 树状思维推理（ToT）虽然提升了LLMs的问题解决能力，但其语义冗余导致计算开销巨大，需要一种高效的方法来优化推理过程。</p>
<p><strong>Result:</strong> 在GSM8K和MATH500等基准测试中，SSDP实现了2.3倍的速度提升，同时准确性保持在最强基线的5%以内。</p>
<p><strong>Insight:</strong> SSDP展示了在LLM推理中通过动态优化可以减少计算开销而不显著影响性能，为高效推理提供了可扩展的方案。</p>
<p><strong>Abstract:</strong> Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at <a target="_blank" rel="noopener" href="https://github.com/kimjoonghokim/SSDP">https://github.com/kimjoonghokim/SSDP</a>.</p>
  </div>
</details>

<hr>
<h3 id="16-What-About-the-Scene-with-the-Hitler-Reference-HAUNT-A-Framework-to-Probe-LLMs’-Self-consistency-Via-Adversarial-Nudge-cs-CL-cs-AI-cs-CYPDF"><a href="#16-What-About-the-Scene-with-the-Hitler-Reference-HAUNT-A-Framework-to-Probe-LLMs’-Self-consistency-Via-Adversarial-Nudge-cs-CL-cs-AI-cs-CYPDF" class="headerlink" title="[16] What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs’ Self-consistency Via Adversarial Nudge cs.CL | cs.AI | cs.CYPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08596">What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs’ Self-consistency Via Adversarial Nudge</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08596" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arka Dutta, Sujan Dutta, Rijul Magu, Soumyajit Datta, Munmun De Choudhury</span></p>
<p><strong>TL;DR:</strong> 该论文提出了HAUNT框架，通过对抗性提示测试大语言模型（LLM）在封闭领域内的自一致性，揭示了不同LLM对对抗性提示的抵御能力差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 幻觉问题是LLM在高风险领域实际部署中的关键挑战，需要一种系统方法来评估其对对抗性提示的鲁棒性。</p>
<p><strong>Result:</strong> 评估发现Claude抵御能力最强，GPT和Grok中等，Gemini和DeepSeek较弱。</p>
<p><strong>Insight:</strong> LLM在信息检索中的广泛使用使其对对抗性提示的脆弱性成为重要问题，需要进一步改进。</p>
<p><strong>Abstract:</strong> Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \texttt{Claude} exhibits strong resilience, \texttt{GPT} and \texttt{Grok} demonstrate moderate resilience, while \texttt{Gemini} and \texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.</p>
  </div>
</details>

<hr>
<h3 id="17-Self-HarmLLM-Can-Large-Language-Model-Harm-Itself-cs-CL-cs-AIPDF"><a href="#17-Self-HarmLLM-Can-Large-Language-Model-Harm-Itself-cs-CL-cs-AIPDF" class="headerlink" title="[17] Self-HarmLLM: Can Large Language Model Harm Itself? cs.CL | cs.AIPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08597">Self-HarmLLM: Can Large Language Model Harm Itself?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08597" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Heehwan Kim, Sungjune Park, Daeseon Choi</span></p>
<p><strong>TL;DR:</strong> 该论文探讨了大型语言模型（LLM）可能通过自身生成的模糊有害查询（MHQ）进行自我攻击（Self-HarmLLM），实验表明在零样本和少样本条件下攻击成功率较高，同时揭示了自动评估对有害性判断的不准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有防御措施假设有害查询来自外部攻击者，但忽视了模型自身输出可能成为攻击载体的可能性。论文旨在验证LLM是否可能通过自身生成的MHQ实现自我攻击。</p>
<p><strong>Result:</strong> 实验表明MHQ攻击在零样本条件下的转化成功率达52%，越狱成功率达33%；少样本条件下分别为65%和41%。自动评估显著高估了有害性，平均差异达52%。</p>
<p><strong>Insight:</strong> LLM的防御机制需重新设计，以应对自我攻击；仅依赖自动评估可能导致误判，需结合人工评估建立更健壮的评测方法。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model’s own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.</p>
  </div>
</details>

<hr>
<h3 id="18-Retrieval-Augmented-Generation-of-Pediatric-Speech-Language-Pathology-vignettes-A-Proof-of-Concept-Study-cs-CL-cs-HCPDF"><a href="#18-Retrieval-Augmented-Generation-of-Pediatric-Speech-Language-Pathology-vignettes-A-Proof-of-Concept-Study-cs-CL-cs-HCPDF" class="headerlink" title="[18] Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study cs.CL | cs.HCPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08600">Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08600" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yilan Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种结合检索增强生成（RAG）与领域知识库的系统，用于生成儿科言语病理学（SLP）临床案例，展示了其技术可行性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 临床案例是SLP教育的重要工具，但手动创建耗时耗力。通用大语言模型（LLM）由于缺乏领域知识，生成的文本不准确且需要专家大量修改。</p>
<p><strong>Result:</strong> 结果表明商用模型质量略高，但开源模型表现也可接受，适合隐私敏感的机构部署。知识库的整合确保了生成内容符合专业指南。</p>
<p><strong>Insight:</strong> 未来应用可扩展至临床决策支持、自动化IEP目标生成和临床反思训练，但需进一步专家验证和学生测试。</p>
<p><strong>Abstract:</strong> Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal&#x2F;session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.</p>
  </div>
</details>

<hr>
<h3 id="19-Mina-A-Multilingual-LLM-Powered-Legal-Assistant-Agent-for-Bangladesh-for-Empowering-Access-to-Justice-cs-CL-cs-CY-cs-HC-cs-MA-cs-MMPDF"><a href="#19-Mina-A-Multilingual-LLM-Powered-Legal-Assistant-Agent-for-Bangladesh-for-Empowering-Access-to-Justice-cs-CL-cs-CY-cs-HC-cs-MA-cs-MMPDF" class="headerlink" title="[19] Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice cs.CL | cs.CY | cs.HC | cs.MA | cs.MMPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08605">Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CY | cs.HC | cs.MA | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08605" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam</span></p>
<p><strong>TL;DR:</strong> Mina是一个基于多语言LLM的法律助手，专为孟加拉国的低收入群体设计，旨在解决法律语言复杂、程序不透明和高成本的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 孟加拉国的低收入群体因法律语言复杂、程序不透明和高成本而难以获得负担得起的法律咨询。现有AI法律助手缺乏孟加拉语支持和针对本地化的适配。</p>
<p><strong>Result:</strong> 在孟加拉国律师理事会考试中的MCQ、笔试和模拟口试中，Mina得分75-80%，达到或超过人类平均表现，展示了强大的情境理解和法律推理能力。</p>
<p><strong>Insight:</strong> Mina展示了在多语言、低资源环境下开发领域专用AI系统的潜力，并为可持续的公共服务AI部署提供了案例。</p>
<p><strong>Abstract:</strong> Bangladesh’s low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.</p>
  </div>
</details>

<hr>
<h3 id="20-Structured-Uncertainty-guided-Clarification-for-LLM-Agents-cs-CL-cs-AIPDF"><a href="#20-Structured-Uncertainty-guided-Clarification-for-LLM-Agents-cs-CL-cs-AIPDF" class="headerlink" title="[20] Structured Uncertainty guided Clarification for LLM Agents cs.CL | cs.AIPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08798">Structured Uncertainty guided Clarification for LLM Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08798" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于结构化不确定性的POMDP方法（SAGE-Agent），用于优化LLM代理在工具调用中的澄清问题选择，显著提高了任务覆盖率和交互效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> LLM代理在处理模糊用户指令时容易导致工具调用错误和任务失败，现有的不确定性方法缺乏结构化建模和成本效率优化。</p>
<p><strong>Result:</strong> 在ClarifyBench上，SAGE-Agent将模糊任务的覆盖率提升了7-39%，同时减少澄清问题1.5-2.7倍。强化学习实验中，When2Call准确率显著提升（3B模型从36.5%到65.2%）。</p>
<p><strong>Insight:</strong> 结构化不确定性不仅优化了LLM代理的交互效率，还为强化学习提供了高质量的训练信号。</p>
<p><strong>Abstract:</strong> LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39% while reducing clarification questions by 1.5-2.7$\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5% to 65.2% (3B model) and 36.7% to 62.9% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.</p>
  </div>
</details>

<hr>
<h3 id="21-Toward-Automated-Cognitive-Assessment-in-Parkinson’s-Disease-Using-Pretrained-Language-Models-cs-CLPDF"><a href="#21-Toward-Automated-Cognitive-Assessment-in-Parkinson’s-Disease-Using-Pretrained-Language-Models-cs-CLPDF" class="headerlink" title="[21] Toward Automated Cognitive Assessment in Parkinson’s Disease Using Pretrained Language Models cs.CLPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.08806">Toward Automated Cognitive Assessment in Parkinson’s Disease Using Pretrained Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2511.08806" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Varada Khanna, Nilay Bhatt, Ikgyu Shin, Sule Tinaz, Yang Ren</span></p>
<p><strong>TL;DR:</strong> 该论文研究了如何利用预训练语言模型自动化评估帕金森病患者的认知功能，通过不同模型家族的比较，发现微调的Meta-Llama-3-8B-Instruct模型在提取复杂认知类别方面表现最佳。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 帕金森病患者的认知和情感变化可以通过其日常叙述理解，但传统的非结构化数据提取方法难以捕捉这些细微且重叠的认知概念。</p>
<p><strong>Result:</strong> 微调的Meta-Llama-3-8B-Instruct模型在整体F1-score上表现最佳（微平均0.74，宏平均0.59），尤其在上下文依赖类别（如思维和社交互动）中表现出色。</p>
<p><strong>Insight:</strong> 尽管任务因认知叙述的抽象性而更具挑战性，但NLP模型的持续优化有望成为帕金森病认知功能监测的有力工具，补充传统神经心理学评估。</p>
<p><strong>Abstract:</strong> Understanding how individuals with Parkinson’s disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-11-15/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-11-13/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
