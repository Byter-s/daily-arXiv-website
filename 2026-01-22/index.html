<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2026-01-22 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2026-01-22
            
            
        </div>
        <span class="post-date">
            Jan 22, 2026
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 66]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 139]</li>
<li><a href="#q-bio.GN">q-bio.GN</a> [Total: 1]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 11]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 3]</li>
<li><a href="#cs.MA">cs.MA</a> [Total: 1]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 4]</li>
<li><a href="#cs.CY">cs.CY</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 12]</li>
<li><a href="#cs.MM">cs.MM</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 8]</li>
<li><a href="#cs.GR">cs.GR</a> [Total: 1]</li>
<li><a href="#q-bio.QM">q-bio.QM</a> [Total: 1]</li>
<li><a href="#q-bio.NC">q-bio.NC</a> [Total: 1]</li>
<li><a href="#cs.SE">cs.SE</a> [Total: 2]</li>
<li><a href="#cs.SD">cs.SD</a> [Total: 1]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-Context-Discipline-and-Performance-Correlation-Analyzing-LLM-Performance-and-Quality-Degradation-Under-Varying-Context-Lengths-cs-CL-cs-AIPDF"><a href="#1-Context-Discipline-and-Performance-Correlation-Analyzing-LLM-Performance-and-Quality-Degradation-Under-Varying-Context-Lengths-cs-CL-cs-AIPDF" class="headerlink" title="[1] Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths cs.CL | cs.AIPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11564">Context Discipline and Performance Correlation: Analyzing LLM Performance and Quality Degradation Under Varying Context Lengths</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11564" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahilan Ayyachamy Nadar Ponnusamy, Karthic Chandran, M Maruf Hossain</span></p>
<p><strong>TL;DR:</strong> 本文研究了大型语言模型（LLMs）在扩展上下文窗口时面临的系统性能与模型质量之间的权衡问题，重点关注了Llama-3.1-70B和Qwen1.5-14B等密集Transformer架构在大量无关或干扰性上下文下的表现，并分析了KV缓存增长导致的非线性性能退化以及MoE架构在不同上下文规模下的独特行为异常。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLMs上下文窗口的不断扩展以支持复杂长文本推理，管理扩展上下文带来了严重的计算开销，本文旨在探究在存在大量无关上下文时，系统性能与模型质量之间的关键权衡。</p>
<p><strong>Result:</strong> 研究发现，随着Key-Value（KV）缓存的增长，模型性能出现非线性退化；对MoE架构的进一步分析揭示了在不同上下文规模下存在独特的行为异常，表明在高令牌量下，架构优势可能被基础设施瓶颈所掩盖。</p>
<p><strong>Insight:</strong> 论文的创新点在于系统性地分析了扩展上下文窗口对LLM性能的实际影响，特别是揭示了KV缓存增长与性能退化的非线性关系，以及MoE架构在高负载下可能出现的异常行为，这对优化LLM的上下文管理和系统设计具有重要启示。</p>
<p><strong>Abstract:</strong> The scaling trend in Large Language Models (LLMs) has prioritized increasing the maximum context window to facilitate complex, long-form reasoning and document analysis. However, managing this expanded context introduces severe computational overhead. This paper investigates the critical trade-off between system performance and model quality when dense transformer architectures–specifically Llama-3.1-70B and Qwen1.5-14B–are exposed to large volumes of irrelevant and distracting context. The research identifies a non-linear performance degradation tied to the growth of the Key-Value (KV) cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE) architecture reveals unique behavioral anomalies at varying context scales, suggesting that architectural benefits may be masked by infrastructure bottlenecks at high token volumes.</p>
  </div>
</details>

<hr>
<h3 id="2-Compass-Embedding-v4-Robust-Contrastive-Learning-for-Multilingual-E-commerce-Embeddings-cs-CL-cs-AIPDF"><a href="#2-Compass-Embedding-v4-Robust-Contrastive-Learning-for-Multilingual-E-commerce-Embeddings-cs-CL-cs-AIPDF" class="headerlink" title="[2] Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings cs.CL | cs.AIPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11565">Compass-Embedding v4: Robust Contrastive Learning for Multilingual E-commerce Embeddings</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11565" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pakorn Ueareeworakul, Shuman Liu, Jinghao Feng, Ling Hu, Zhantang Shi</span></p>
<p><strong>TL;DR:</strong> 本文提出了Compass-Embedding v4，一个针对东南亚电商场景优化的高效多语言嵌入框架。该框架通过类感知掩码（CAM）改进对比学习目标、构建多样化的训练语料库以及结合鲁棒性训练与模型合并优化部署，解决了数据稀缺、噪声监督和严格生产约束下的表示学习挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着全球电商向新兴市场扩张，低资源语言缺乏高质量的语义表示已成为检索、推荐和搜索系统的关键瓶颈。本文旨在为东南亚电商场景开发一个鲁棒的多语言嵌入模型。</p>
<p><strong>Result:</strong> 在多语言基准测试和专有电商任务上的广泛评估表明，Compass-Embedding v4在主要东南亚语言上达到了最先进的性能，在特定领域的检索和分类任务上显著优于通用嵌入模型，同时在高资源语言上保持有竞争力的表现。</p>
<p><strong>Insight:</strong> 创新点包括：1) 提出类感知掩码（CAM）来抑制对比学习中的无效负样本，改进语义判别；2) 通过基于上下文的合成数据生成、跨语言翻译和结构化电商数据构建来增强低资源语言的训练数据；3) 结合鲁棒性驱动的大批量训练与球面模型合并来缓解灾难性遗忘，并通过vLLM和FP8量化优化推理效率。</p>
<p><strong>Abstract:</strong> As global e-commerce rapidly expands into emerging markets, the lack of high-quality semantic representations for low-resource languages has become a decisive bottleneck for retrieval, recommendation, and search systems. In this work, we present Compass-Embedding v4, a high-efficiency multilingual embedding framework specifically optimized for Southeast Asian (SEA) e-commerce scenarios, where data scarcity, noisy supervision, and strict production constraints jointly challenge representation learning. Compass-Embedding v4 addresses three core challenges. First, large-batch contrastive training under mixed task supervision introduces systematic false negatives that degrade semantic alignment. We propose Class-Aware Masking (CAM), a lightweight modification to the InfoNCE objective that suppresses invalid in-batch negatives and improves semantic discrimination without altering training efficiency. Second, low-resource SEA languages suffer from limited and uneven data coverage. We construct a diversified training corpus through context-grounded synthetic data generation, cross-lingual translation, and structured e-commerce data construction, enabling robust multilingual and domain-specific learning. Third, production deployment requires high-throughput inference while preserving embedding quality. We combine robustness-driven large-batch training with spherical model merging to mitigate catastrophic forgetting, and optimize inference via vLLM and FP8 quantization. Extensive evaluations across multilingual benchmarks and proprietary e-commerce tasks show that Compass-Embedding v4 achieves state-of-the-art performance on major SEA languages, significantly outperforming general-purpose embedding models in domain-specific retrieval and classification, while maintaining competitive performance on high-resource languages.</p>
  </div>
</details>

<hr>
<h3 id="3-Measuring-Stability-Beyond-Accuracy-in-Small-Open-Source-Medical-Large-Language-Models-for-Pediatric-Endocrinology-cs-CL-cs-AIPDF"><a href="#3-Measuring-Stability-Beyond-Accuracy-in-Small-Open-Source-Medical-Large-Language-Models-for-Pediatric-Endocrinology-cs-CL-cs-AIPDF" class="headerlink" title="[3] Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology cs.CL | cs.AIPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11567">Measuring Stability Beyond Accuracy in Small Open-Source Medical Large Language Models for Pediatric Endocrinology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11567" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vanessa D’Amario, Randy Daniel, Alessandro Zanetti, Dhruv Edamadaka, Nitya Alaparthy</span></p>
<p><strong>TL;DR:</strong> 该研究评估了六款小型开源医学大语言模型（HuatuoGPT-o1、Diabetica-7B、Diabetica-o1、Meditron3-8B、MedFound-7B、ClinicaGPT-base-zh）在儿科内分泌学领域的表现，超越了传统的准确性评估，重点关注了模型在确定性（提示词变化、自我评估偏见）和随机性（输出变异性、一致性与正确性关系）设置下的稳定性、一致性和推理行为。研究发现，高一致性并不等同于高正确性，模型存在自我评估偏见和对解释顺序的依赖，且微小的系统级扰动（如CUDA构建差异）会导致模型输出发生统计显著变化，这凸显了在真实临床决策支持场景中需要更广泛的诊断框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前对小规模开源医学大语言模型的评估通常仅限于医学多选题基准的准确性，缺乏对其一致性、鲁棒性或推理行为的评估，这限制了对其在低资源部署和临床决策支持中潜在风险的理解。</p>
<p><strong>Result:</strong> 在儿科内分泌学评估中，HuatuoGPT-o1-8B取得了最高的性能表现和最高的一致性率。然而，高一致性并非正确性的可靠指标。模型在自我评估和解释顺序选择上表现出偏见。专家评审发现错误推理中混合了临床可接受的回答和临床疏忽。此外，即使准确性稳定，微小的系统级扰动（如CUDA构建差异）也会导致模型输出产生统计显著偏移。</p>
<p><strong>Insight:</strong> 论文的创新点在于将评估重点从单纯的准确性扩展到模型的稳定性、一致性和推理行为，揭示了在确定性（提示工程）和随机性（采样变异性）设置下模型输出的脆弱性。客观来看，其核心洞察是：对于临床应用的LLM评估，必须建立一个超越准确性的更广泛诊断框架，以捕捉模型在真实场景中可能存在的可重复性问题和潜在风险，这对确保AI辅助医疗的可靠性至关重要。</p>
<p><strong>Abstract:</strong> Small open-source medical large language models (LLMs) offer promising opportunities for low-resource deployment and broader accessibility. However, their evaluation is often limited to accuracy on medical multiple choice question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or reasoning behavior. We use MCQ coupled to human evaluation and clinical review to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024), Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B (Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In deterministic settings, we examine the effect of prompt variation on models’ output and self-assessment bias. In stochastic settings, we evaluate output variability and investigate the relationship between consistency and correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show that high consistency across the model response is not an indicator of correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1 exhibit self-assessment bias and dependency on the order of the candidate explanations. Expert review of incorrect reasoning rationales identified a mix of clinically acceptable responses and clinical oversight. We further show that system-level perturbations, such as differences in CUDA builds, can yield statistically significant shifts in model output despite stable accuracy. This work demonstrates that small, semantically negligible prompt perturbations lead to divergent outputs, raising concerns about reproducibility of LLM-based evaluations and highlights the output variability under different stochastic regimes, emphasizing the need of a broader diagnostic framework to understand potential pitfalls in real-world clinical decision support scenarios.</p>
  </div>
</details>

<hr>
<h3 id="4-Bielik-11B-v3-Multilingual-Large-Language-Model-for-European-Languages-cs-CL-cs-AIPDF"><a href="#4-Bielik-11B-v3-Multilingual-Large-Language-Model-for-European-Languages-cs-CL-cs-AIPDF" class="headerlink" title="[4] Bielik 11B v3: Multilingual Large Language Model for European Languages cs.CL | cs.AIPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11579">Bielik 11B v3: Multilingual Large Language Model for European Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11579" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Krzysztof Ociepa, Łukasz Flis, Remigiusz Kinas, Krzysztof Wróbel, Adrian Gwoździej</span></p>
<p><strong>TL;DR:</strong> 本文介绍了Bielik 11B v3，这是一个针对波兰语高度优化的多语言大语言模型，同时在欧洲其他语言上也保持强大能力。该模型基于Mistral 7B v0.2架构，通过深度扩展达到110亿参数，并采用包含持续预训练、监督微调、直接偏好优化和强化学习的四阶段训练流程。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 开发一个专门针对波兰语等资源相对较少的欧洲语言的高性能、资源高效的大语言模型，以提升这些语言的AI能力。</p>
<p><strong>Result:</strong> 综合评估表明，Bielik 11B v3在从基础语言理解到复杂推理的广泛任务上，显著超越了其他专门的波兰语模型，并且性能优于许多参数量是其2-6倍的更大模型，达到了SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于通过深度扩展（而非宽度扩展）Mistral架构并结合四阶段训练流程，实现了在特定语言（波兰语）上的卓越性能与参数效率。这为开发资源较少语言的高性能模型提供了新范式，其量化选项也增强了部署灵活性。</p>
<p><strong>Abstract:</strong> We present Bielik 11B v3, a state-of-the-art language model highly optimized for the Polish language, while also maintaining strong capabilities in other European languages. This model extends the Mistral 7B v0.2 architecture, scaled to 11B parameters via depth up-scaling. Its development involved a comprehensive four-stage training pipeline: continuous pre-training, supervised fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement learning.   Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional performance. It significantly surpasses other specialized Polish language models and outperforms many larger models (with 2-6 times more parameters) on a wide range of tasks, from basic linguistic understanding to complex reasoning.   The model’s parameter efficiency, combined with extensive quantization options, allows for effective deployment across diverse hardware configurations. Bielik 11B v3 not only advances AI capabilities for the Polish language but also establishes a new benchmark for developing resource-efficient, high-performance models for less-represented languages.</p>
  </div>
</details>

<hr>
<h3 id="5-Enhancing-the-QA-Model-through-a-Multi-domain-Debiasing-Framework-cs-CL-cs-AIPDF"><a href="#5-Enhancing-the-QA-Model-through-a-Multi-domain-Debiasing-Framework-cs-CL-cs-AIPDF" class="headerlink" title="[5] Enhancing the QA Model through a Multi-domain Debiasing Framework cs.CL | cs.AIPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11581">Enhancing the QA Model through a Multi-domain Debiasing Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11581" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuefeng Wang, ChangJae Lee</span></p>
<p><strong>TL;DR:</strong> 本研究针对问答模型在对抗性条件下表现出的偏见问题，提出了一个多领域去偏框架。通过评估ELECTRA-small模型在SQuAD v1.1及对抗数据集AddSent和AddOneSent上的表现，识别了词汇偏见、数值推理和实体识别等错误，并整合了知识蒸馏、去偏技术和领域扩展方法。实验结果显示，该框架在所有测试集上使精确匹配和F1分数提升了最高2.6个百分点，特别是在对抗性环境中取得了显著改进。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决问答模型在复杂查询和对抗性条件下因偏见（如词汇偏见、数值推理和实体识别错误）而导致的性能下降问题，以增强自然语言理解系统的鲁棒性和可靠性。</p>
<p><strong>Result:</strong> 在SQuAD v1.1、AddSent和AddOneSent数据集上，提出的多领域去偏框架使ELECTRA-small模型的Exact Match和F1分数提升了最高2.6个百分点，在对抗性环境中表现尤为突出，达到了性能改进的SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于整合了知识蒸馏、去偏技术和领域扩展的多领域去偏框架，针对性地缓解了模型偏见；从客观角度看，该方法通过系统性地识别和纠正多种偏见类型，为提升问答模型在复杂场景下的鲁棒性提供了可借鉴的通用策略。</p>
<p><strong>Abstract:</strong> Question-answering (QA) models have advanced significantly in machine reading comprehension but often exhibit biases that hinder their performance, particularly with complex queries in adversarial conditions. This study evaluates the ELECTRA-small model on the Stanford Question Answering Dataset (SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying errors related to lexical bias, numerical reasoning, and entity recognition, we develop a multi-domain debiasing framework incorporating knowledge distillation, debiasing techniques, and domain expansion. Our results demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1 scores across all test sets, with gains in adversarial contexts. These findings highlight the potential of targeted bias mitigation strategies to enhance the robustness and reliability of natural language understanding systems.</p>
  </div>
</details>

<hr>
<h3 id="6-Towards-AGI-A-Pragmatic-Approach-Towards-Self-Evolving-Agent-cs-CL-cs-AIPDF"><a href="#6-Towards-AGI-A-Pragmatic-Approach-Towards-Self-Evolving-Agent-cs-CL-cs-AIPDF" class="headerlink" title="[6] Towards AGI A Pragmatic Approach Towards Self Evolving Agent cs.CL | cs.AIPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11658">Towards AGI A Pragmatic Approach Towards Self Evolving Agent</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11658" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Indrajit Kar, Sammy Zonunpuia, Zonunfeli Ralte</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种分层自进化多智能体框架，旨在解决现有基于大语言模型的智能体在部署后静态、无法自主扩展能力或生成新工具的问题。该框架集成了基础LLM、操作SLM智能体、代码生成LLM和教师LLM，通过任务尝试、工具合成和进化阶段实现持续适应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是解决当前基于大语言模型的智能体在部署后静态、缺乏自主扩展能力、生成新工具或进化推理的局限性。</p>
<p><strong>Result:</strong> 在包含分层任务、工具使用轨迹和难度缩放的TaskCraft数据集上评估，进化后的智能体在所有设置中均优于原始版本，其中课程学习实现快速恢复和强泛化，基于奖励的学习在高难度任务上表现优异，遗传算法提供高行为多样性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个集成多种LLM的分层自进化框架，通过结合课程学习、奖励学习和遗传算法等进化范式，实现了智能体的自主、鲁棒和自我改进的进化能力。</p>
<p><strong>Abstract:</strong> Large Language Model (LLM) based agents are powerful yet fundamentally static after deployment, lacking the ability to autonomously expand capabilities, generate new tools, or evolve their reasoning. This work introduces a hierarchical self-evolving multi-agent framework that integrates a Base LLM, an operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable continuous adaptation. The workflow begins with the agent attempting a task using reasoning and existing tools; if unsuccessful, it escalates to tool synthesis through the Code-Gen LLM, and when failures persist, it triggers an evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these paradigms. CL delivers fast recovery and strong generalization, RL excels on high-difficulty tasks, and GA offers high behavioral diversity. Across all settings, evolved agents outperform their originals, demonstrating robust, autonomous, self-improving agentic evolution.</p>
  </div>
</details>

<hr>
<h3 id="7-Beyond-Tokens-Concept-Level-Training-Objectives-for-LLMs-cs-CLPDF"><a href="#7-Beyond-Tokens-Concept-Level-Training-Objectives-for-LLMs-cs-CLPDF" class="headerlink" title="[7] Beyond Tokens: Concept-Level Training Objectives for LLMs cs.CLPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11791">Beyond Tokens: Concept-Level Training Objectives for LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11791" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Laya Iyer, Pranav Somani, Alice Guo, Dan Jurafsky, Chen Shani</span></p>
<p><strong>TL;DR:</strong> 这篇论文指出，当前大语言模型（LLM）广泛使用的下一个词元预测（NTP）目标存在局限性，因为它只在词元层面进行优化，可能会惩罚语义正确但表达不同的输出。为此，作者提出从词元级预测转向概念级预测，将表达同一概念的不同表面形式（如“mom”、“mother”）归为一组，并引入了多种将概念监督整合到LLM训练中的方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> NTP目标在词元层面运作，即使替代的续写在语义上等价或同样合理，也会将其视为错误进行惩罚。这导致模型偏向于表面形式而非底层含义，训练信号与语义正确性之间存在不匹配。因此，需要学习在更高层次表示上运作的目标。</p>
<p><strong>Result:</strong> 实验表明，具备概念感知的模型在困惑度上更低，在领域转移下具有更强的鲁棒性，并且在多种NLP基准测试上比基于NTP的模型表现更强。</p>
<p><strong>Insight:</strong> 核心创新点是提出了“概念级监督”作为改进的训练信号，它通过将多个语义等价的表面形式映射到同一概念，使LLM的训练目标更好地与人类的语义抽象对齐，从而可能提升模型对含义而非形式的理解能力。</p>
<p><strong>Abstract:</strong> The next-token prediction (NTP) objective has been foundational in the development of modern large language models (LLMs), driving advances in fluency and generalization. However, NTP operates at the \textit{token} level, treating deviations from a single reference continuation as errors even when alternative continuations are equally plausible or semantically equivalent (e.g., <code>mom&#39;&#39; vs. </code>mother’’). As a result, token-level loss can penalize valid abstractions, paraphrases, or conceptually correct reasoning paths, biasing models toward surface form rather than underlying meaning. This mismatch between the training signal and semantic correctness motivates learning objectives that operate over higher-level representations. We propose a shift from token-level to concept-level prediction, where concepts group multiple surface forms of the same idea (e.g., <code>mom,&#39;&#39; </code>mommy,’’ &#96;&#96;mother’’ $\rightarrow$ \textit{MOTHER}). We introduce various methods for integrating conceptual supervision into LLM training and show that concept-aware models achieve lower perplexity, improved robustness under domain shift, and stronger performance than NTP-based models on diverse NLP benchmarks. This suggests \textit{concept-level supervision} as an improved training signal that better aligns LLMs with human semantic abstractions.</p>
  </div>
</details>

<hr>
<h3 id="8-ATOD-An-Evaluation-Framework-and-Benchmark-for-Agentic-Task-Oriented-Dialogue-System-cs-CL-cs-AI-cs-MAPDF"><a href="#8-ATOD-An-Evaluation-Framework-and-Benchmark-for-Agentic-Task-Oriented-Dialogue-System-cs-CL-cs-AI-cs-MAPDF" class="headerlink" title="[8] ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System cs.CL | cs.AI | cs.MAPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11854">ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11854" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifei Zhang, Hooshang Nayyeri, Rinat Khaziev, Emine Yilmaz, Gokhan Tur</span></p>
<p><strong>TL;DR:</strong> 本文提出了ATOD基准测试和评估框架，用于评估基于大语言模型的任务导向对话系统中代理行为的性能。ATOD通过合成对话生成管道创建了包含多目标协调、依赖管理、记忆、适应性和主动性等关键特征的标注对话。在此基础上，ATOD-Eval评估框架将这些维度转化为细粒度指标，支持可复现的离线和在线评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准测试缺乏对任务导向对话系统中代理行为（如交错目标协调、长时程上下文维护和异步主动执行）的系统性评估支持，因此需要一个新的评估框架来填补这一空白。</p>
<p><strong>Result:</strong> 实验表明，ATOD-Eval能够在任务完成度、代理能力和响应质量方面进行全面评估，并且提出的基于记忆的评估器在此评估设置下，相比现有的基于记忆和LLM的方法，提供了更好的准确性与效率权衡。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了首个专门针对代理式任务导向对话系统的综合评估基准和框架，通过合成数据生成和细粒度指标设计，解决了现有评估方法的不足，并引入了高效的基于记忆的评估器来平衡评估的准确性和效率。</p>
<p><strong>Abstract:</strong> Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting.</p>
  </div>
</details>

<hr>
<h3 id="9-CTPD-Cross-Tokenizer-Preference-Distillation-cs-CLPDF"><a href="#9-CTPD-Cross-Tokenizer-Preference-Distillation-cs-CLPDF" class="headerlink" title="[9] CTPD: Cross Tokenizer Preference Distillation cs.CLPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11865">CTPD: Cross Tokenizer Preference Distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11865" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Truong Nguyen, Phi Van Dat, Ngan Nguyen, Linh Ngo Van, Trung Le</span></p>
<p><strong>TL;DR:</strong> 本文提出了跨分词器偏好蒸馏（CTPD）框架，首次实现了在不同分词方案的语言模型之间迁移人类偏好对齐行为。该框架通过字符级对齐跨度投影、跨分词器令牌重要性采样和教师锚定参考等创新技术，解决了异构分词器下细粒度偏好信息蒸馏的难题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有知识蒸馏方法在预训练和指令微调中广泛应用，但在语言模型与人类偏好对齐方面，尤其是在更现实的跨分词器场景下，其应用仍未被充分探索。教师模型与学生模型分词方案的不兼容性阻碍了细粒度的白盒偏好信息蒸馏。</p>
<p><strong>Result:</strong> 在多个基准测试上的实验证实了CTPD的有效性，相比现有方法取得了显著的性能提升，表明其是跨不同分词方案进行偏好蒸馏的实用且通用的解决方案。</p>
<p><strong>Insight:</strong> 摘要宣称的创新点包括：1）对齐跨度投影，将教师和学生令牌映射到共享的字符级跨度以实现精确监督迁移；2）跨分词器适应的令牌级重要性采样（TIS-DPO），以改进信用分配；3）教师锚定参考，允许学生在DPO风格的目标中直接利用教师的偏好。从客观角度看，该研究将重要性采样理论分析应用于跨分词器偏好对齐，为异构模型间的行为迁移提供了系统框架。</p>
<p><strong>Abstract:</strong> While knowledge distillation has seen widespread use in pre-training and instruction tuning, its application to aligning language models with human preferences remains underexplored, particularly in the more realistic cross-tokenizer setting. The incompatibility of tokenization schemes between teacher and student models has largely prevented fine-grained, white-box distillation of preference information. To address this gap, we propose Cross-Tokenizer Preference Distillation (CTPD), the first unified framework for transferring human-aligned behavior between models with heterogeneous tokenizers. CTPD introduces three key innovations: (1) Aligned Span Projection, which maps teacher and student tokens to shared character-level spans for precise supervision transfer; (2) a cross-tokenizer adaptation of Token-level Importance Sampling (TIS-DPO) for improved credit assignment; and (3) a Teacher-Anchored Reference, allowing the student to directly leverage the teacher’s preferences in a DPO-style objective. Our theoretical analysis grounds CTPD in importance sampling, and experiments across multiple benchmarks confirm its effectiveness, with significant performance gains over existing methods. These results establish CTPD as a practical and general solution for preference distillation across diverse tokenization schemes, opening the door to more accessible and efficient alignment of language models.</p>
  </div>
</details>

<hr>
<h3 id="10-Advances-in-LLM-Reasoning-Enable-Flexibility-in-Clinical-Problem-Solving-cs-CLPDF"><a href="#10-Advances-in-LLM-Reasoning-Enable-Flexibility-in-Clinical-Problem-Solving-cs-CLPDF" class="headerlink" title="[10] Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving cs.CLPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11866">Advances in LLM Reasoning Enable Flexibility in Clinical Problem-Solving</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11866" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kie Shidara, Preethi Prem, Jonathan Kim, Anna Podlasek, Feng Liu</span></p>
<p><strong>TL;DR:</strong> 本研究评估了OpenAI、Grok、Gemini、Claude和DeepSeek等系列的大型语言模型在医学抽象与推理语料库上的表现，发现先进的推理模型能够有效避免因Einstellung效应导致的思维僵化陷阱，在对抗性医学问答基准上达到了人类水平。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLMs在医学QA基准上取得了高准确率，但其临床推理的灵活性仍受质疑，本研究旨在探究推理能力的进步是否提升了LLMs在临床推理中的认知灵活性。</p>
<p><strong>Result:</strong> 在mARC基准测试中，强推理模型比弱推理模型更频繁地避免了Einstellung陷阱，达到了人类水平性能；在医生最常出错的问题上，前5名模型以高置信度正确回答了55%至70%的问题。</p>
<p><strong>Insight:</strong> 论文的创新点在于利用mARC这一专门设计来诱发Einstellung效应的对抗性基准，系统评估了LLMs的临床推理灵活性，结果表明先进推理模型可能比人类更不易受思维定势影响，这为LLMs在医疗决策支持中的应用提供了新见解。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have achieved high accuracy on medical question-answer (QA) benchmarks, yet their capacity for flexible clinical reasoning has been debated. Here, we asked whether advances in reasoning LLMs improve their cognitive flexibility in clinical reasoning. We assessed reasoning models from the OpenAI, Grok, Gemini, Claude, and DeepSeek families on the medicine abstraction and reasoning corpus (mARC), an adversarial medical QA benchmark which utilizes the Einstellung effect to induce inflexible overreliance on learned heuristic patterns in contexts where they become suboptimal. We found that strong reasoning models avoided Einstellung-based traps more often than weaker reasoning models, achieving human-level performance on mARC. On questions most commonly missed by physicians, the top 5 performing models answered 55% to 70% correctly with high confidence, indicating that these models may be less susceptible than humans to Einstellung effects. Our results indicate that strong reasoning models demonstrate improved flexibility in medical reasoning, achieving performance on par with humans on mARC.</p>
  </div>
</details>

<hr>
<h3 id="11-PPA-Plan-Proactive-Pitfall-Avoidance-for-Reliable-Planning-in-Long-Context-LLM-Reasoning-cs-CLPDF"><a href="#11-PPA-Plan-Proactive-Pitfall-Avoidance-for-Reliable-Planning-in-Long-Context-LLM-Reasoning-cs-CLPDF" class="headerlink" title="[11] PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning cs.CLPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11908">PPA-Plan: Proactive Pitfall Avoidance for Reliable Planning in Long-Context LLM Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11908" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Byeongjin Kim, Gyuwan Kim, Seo Yeon Park</span></p>
<p><strong>TL;DR:</strong> 本文提出PPA-Plan，一种用于长上下文推理的主动规划策略，旨在通过预先识别并规避潜在的逻辑陷阱和错误假设来提升规划可靠性，从而解决现有规划-执行框架因依赖表层线索而导致规划不可靠的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大型语言模型在长上下文推理中因相关信息稀疏分布而表现不佳的问题，特别是现有规划-执行框架因规划生成不可靠且难以事后修正而效果有限。</p>
<p><strong>Result:</strong> 在长上下文问答基准测试上的实验表明，执行PPA-Plan生成的规划持续优于现有的规划-执行方法和直接提示方法。</p>
<p><strong>Insight:</strong> 创新点在于将主动规避失败（而非事后反应性修正）的思想引入规划过程，通过将潜在陷阱形式化为负面约束来条件化规划生成，这为提升长上下文推理的鲁棒性提供了新思路。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) struggle with reasoning over long contexts where relevant information is sparsely distributed. Although plan-and-execute frameworks mitigate this by decomposing tasks into planning and execution, their effectiveness is often limited by unreliable plan generation due to dependence on surface-level cues. Consequently, plans may be based on incorrect assumptions, and once a plan is formed, identifying what went wrong and revising it reliably becomes difficult, limiting the effectiveness of reactive refinement. To address this limitation, we propose PPA-Plan, a proactive planning strategy for long-context reasoning that focuses on preventing such failures before plan generation. PPA-Plan identifies potential logical pitfalls and false assumptions, formulates them as negative constraints, and conditions plan generation on explicitly avoiding these constraints. Experiments on long-context QA benchmarks show that executing plans generated by PPA-Plan consistently outperforms existing plan-and-execute methods and direct prompting.</p>
  </div>
</details>

<hr>
<h3 id="12-Double-Calibration-Towards-Trustworthy-LLMs-via-Calibrating-Knowledge-and-Reasoning-Confidence-cs-CL-cs-AIPDF"><a href="#12-Double-Calibration-Towards-Trustworthy-LLMs-via-Calibrating-Knowledge-and-Reasoning-Confidence-cs-CL-cs-AIPDF" class="headerlink" title="[12] Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence cs.CL | cs.AIPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11956">Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11956" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出Double-Cal框架，通过双重校准原则提升大型语言模型的可信度，利用轻量级代理模型生成知识图谱证据并校准证据置信度，进而指导黑盒LLM生成更准确且置信度可追溯的预测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有基于知识图谱增强的LLM方法无法量化检索证据和模型推理中认知不确定性的问题，以应对LLM的幻觉挑战。</p>
<p><strong>Result:</strong> 在知识密集型基准测试中，Double-Cal以较低的token成本显著提升了黑盒LLM的准确性和置信度校准性能。</p>
<p><strong>Insight:</strong> 创新性地提出双重校准原则，将证据置信度校准与LLM推理置信度校准相结合，实现预测置信度与证据不确定性的可追溯关联，为可信LLM提供新思路。</p>
<p><strong>Abstract:</strong> Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs’ reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost.</p>
  </div>
</details>

<hr>
<h3 id="13-PEARL-Self-Evolving-Assistant-for-Time-Management-with-Reinforcement-Learning-cs-CLPDF"><a href="#13-PEARL-Self-Evolving-Assistant-for-Time-Management-with-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[13] PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning cs.CLPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11957">PEARL: Self-Evolving Assistant for Time Management with Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11957" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bingxuan Li, Jeonghwan Kim, Cheng Qian, Xiusi Chen, Eitan Anzenberg</span></p>
<p><strong>TL;DR:</strong> 本文提出PEARL，一个基于强化学习的自我进化时间管理助手，用于解决日历冲突问题。作者引入了CalConflictBench基准测试来评估长视野日历冲突解决能力，并展示了现有LLM代理在此任务上的不足。PEARL通过外部记忆模块和优化的回合奖励设计，使代理能够动态推断和适应用户偏好，显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 繁忙的专业人士经常面临重叠的日历邀请，需要反复决定参加、重新安排或拒绝哪些会议，这一偏好驱动的决策过程称为日历冲突解决。自动化此过程至关重要但具有挑战性，因为调度工作耗时且人工委托难以扩展，因此研究是否可信赖大型语言模型（LLM）或语言代理来管理时间。</p>
<p><strong>Result:</strong> 在CalConflictBench基准测试上，当前LLM代理表现不佳，例如Qwen-3-30B-Think的平均错误率为35%。相比之下，PEARL实现了0.76的错误减少率，与最强基线相比平均错误率提高了55%，达到了最先进水平（SOTA）。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个强化学习框架PEARL，通过外部记忆模块和优化的回合奖励设计来增强语言代理，使其能够逐步推断和动态适应用户偏好。从客观角度看，这为解决长视野、偏好驱动的决策任务提供了一种可扩展的方法，结合了LLM的推理能力和强化学习的适应性学习。</p>
<p><strong>Abstract:</strong> Overlapping calendar invitations force busy professionals to repeatedly decide which meetings to attend, reschedule, or decline. We refer to this preference-driven decision process as calendar conflict resolution. Automating such process is crucial yet challenging. Scheduling logistics drain hours, and human delegation often fails at scale, which motivate we to ask: Can we trust large language model (LLM) or language agent to manager time? To enable systematic study of this question, we introduce CalConflictBench, a benchmark for long-horizon calendar conflict resolution. Conflicts are presented sequentially and agents receive feedback after each round, requiring them to infer and adapt to user preferences progressively. Our experiments show that current LLM agents perform poorly with high error rates, e.g., Qwen-3-30B-Think has 35% average error rate. To address this gap, we propose PEARL, a reinforcement-learning framework that augments language agent with an external memory module and optimized round-wise reward design, enabling agent to progressively infer and adapt to user preferences on-the-fly. Experiments on CalConflictBench shows that PEARL achieves 0.76 error reduction rate, and 55% improvement in average error rate compared to the strongest baseline.</p>
  </div>
</details>

<hr>
<h3 id="14-Acting-Flatterers-via-LLMs-Sycophancy-Combating-Clickbait-with-LLMs-Opposing-Stance-Reasoning-cs-CL-cs-AIPDF"><a href="#14-Acting-Flatterers-via-LLMs-Sycophancy-Combating-Clickbait-with-LLMs-Opposing-Stance-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[14] Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning cs.CL | cs.AIPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12019">Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12019" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的对抗立场推理框架（SORG）来检测点击诱饵新闻标题，通过利用大语言模型（LLM）的奉承倾向生成支持与反对的对比推理对，并构建一个基于BERT的本地模型（ORCD）进行整合与检测，在三个基准数据集上超越了现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在线点击诱饵内容泛滥，而利用LLM进行检测时，其固有的’奉承’倾向（倾向于迎合用户既有信念而非遵循指令追求真实）会阻碍效果。本文旨在利用而非消除这种倾向，从对立视角生成对比推理以提升检测效果。</p>
<p><strong>Result:</strong> 在三个基准数据集上的实验表明，该方法在点击诱饵检测任务上，持续超越了直接LLM提示、微调的小型语言模型以及现有的最先进基线模型。</p>
<p><strong>Insight:</strong> 核心创新在于将LLM的’奉承’缺陷转化为生成高质量对立推理对的资源，无需真实标签；并设计了结合三个BERT编码器的本地模型，利用对比学习和基于LLM生成可信度分数的软标签进行鲁棒性增强。</p>
<p><strong>Abstract:</strong> The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users’ beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines.</p>
  </div>
</details>

<hr>
<h3 id="15-Multimodal-Generative-Engine-Optimization-Rank-Manipulation-for-Vision-Language-Model-Rankers-cs-CL-cs-AI-cs-LGPDF"><a href="#15-Multimodal-Generative-Engine-Optimization-Rank-Manipulation-for-Vision-Language-Model-Rankers-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[15] Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers cs.CL | cs.AI | cs.LGPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12263">Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12263" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为多模态生成引擎优化（MGEO）的新型对抗性攻击框架，旨在揭示基于视觉语言模型（VLM）的产品搜索系统的关键漏洞。该框架通过联合优化难以察觉的图像扰动和流畅的文本后缀，使恶意行为者能够不公平地提升目标产品的排名。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于探索视觉语言模型在竞争性排名场景下对抗对抗性操纵的鲁棒性，这一问题此前尚未得到充分研究。</p>
<p><strong>Result:</strong> 在真实世界数据集上使用最先进模型进行的广泛实验表明，这种协同攻击显著优于仅文本或仅图像的基线方法。</p>
<p><strong>Insight:</strong> 创新点在于揭示了多模态协同（通常是VLM的优势）可能被武器化，从而在不触发传统内容过滤器的情况下损害搜索排名的完整性，并提出了利用交替梯度优化策略来利用VLM内部深度跨模态耦合的联合攻击方法。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.</p>
  </div>
</details>

<hr>
<h3 id="16-Simulated-Annealing-Enhances-Theory-of-Mind-Reasoning-in-Autoregressive-Language-Models-cs-CL-cs-AIPDF"><a href="#16-Simulated-Annealing-Enhances-Theory-of-Mind-Reasoning-in-Autoregressive-Language-Models-cs-CL-cs-AIPDF" class="headerlink" title="[16] Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models cs.CL | cs.AIPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12269">Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12269" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xucong Hu, Jian-Qiao Zhu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于模拟退火增强的马尔可夫链蒙特卡洛（MCMC）采样方法，用于从自回归语言模型中提取其潜在的心智理论（ToM）推理能力，而无需进行额外的权重更新或验证。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自回归语言模型通常被批评为仅优化表面合理性（局部连贯性），而未能保持正确的潜在状态表示（全局连贯性），因此在依赖潜在心理状态推理的心智理论任务上表现不佳。</p>
<p><strong>Result:</strong> 实验表明，该方法在ToM任务上显著提升了性能，优于固定温度的power sampling方法，证明了基于采样的优化能够有效提取语言模型的潜在能力。</p>
<p><strong>Insight:</strong> 创新点在于将模拟退火过程融入MCMC采样，通过逐步从高温到低温调整回火分布，从而更有效地从序列级概率分布中采样，以恢复模型固有的ToM推理能力，这为不依赖重训练而挖掘模型潜在能力提供了新思路。</p>
<p><strong>Abstract:</strong> Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan &amp; Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.</p>
  </div>
</details>

<hr>
<h3 id="17-System-Mediated-Attention-Imbalances-Make-Vision-Language-Models-Say-Yes-cs-CLPDF"><a href="#17-System-Mediated-Attention-Imbalances-Make-Vision-Language-Models-Say-Yes-cs-CLPDF" class="headerlink" title="[17] System-Mediated Attention Imbalances Make Vision-Language Models Say Yes cs.CLPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12430">System-Mediated Attention Imbalances Make Vision-Language Models Say Yes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12430" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tsan Tsai Chan, Varsha Suresh, Anisha Saha, Michael Hahn, Vera Demberg</span></p>
<p><strong>TL;DR:</strong> 该论文研究了视觉语言模型（VLM）中普遍存在的‘是’偏见幻觉问题，提出其根源在于模态间注意力分配不平衡，特别是系统模态权重冗余导致对图像和文本输入的关注不足。通过因果性地将注意力从系统模态重新分配到图像和文本输入，可以有效抑制这种偏见，且效果优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有缓解VLM幻觉的策略通常偏向以图像为中心的解释，优先增加图像注意力而较少考虑系统模态和文本模态的作用。本文旨在从更全面的系统介导视角出发，探究注意力不平衡的根本原因。</p>
<p><strong>Result:</strong> 通过因果干预重新分配注意力，显著抑制了‘是’偏见，在相关基准测试中通常优于现有方法。</p>
<p><strong>Insight:</strong> 创新点在于将VLM幻觉（特别是‘是’偏见）归因于系统权重冗余导致的对细粒度输入表征的依赖不足，并验证了调整系统模态注意力作为缓解杠杆的有效性，为理解与改善VLM可靠性提供了新视角。</p>
<p><strong>Abstract:</strong> Vision-language model (VLM) hallucination is commonly linked to imbalanced allocation of attention across input modalities: system, image and text. However, existing mitigation strategies tend towards an image-centric interpretation of these imbalances, often prioritising increased image attention while giving less consideration to the roles of the other modalities. In this study, we evaluate a more holistic, system-mediated account, which attributes these imbalances to functionally redundant system weights that reduce attention to image and textual inputs. We show that this framework offers a useful empirical perspective on the yes-bias, a common form of hallucination in which VLMs indiscriminately respond ‘yes’. Causally redistributing attention from the system modality to image and textual inputs substantially suppresses this bias, often outperforming existing approaches. We further present evidence suggesting that system-mediated attention imbalances contribute to the yes-bias by encouraging a default reliance on coarse input representations, which are effective for some tasks but ill-suited to others. Taken together, these findings firmly establish system attention as a key factor in VLM hallucination and highlight its potential as a lever for mitigation.</p>
  </div>
</details>

<hr>
<h3 id="18-Incentivizing-In-depth-Reasoning-over-Long-Contexts-with-Process-Advantage-Shaping-cs-CL-cs-AIPDF"><a href="#18-Incentivizing-In-depth-Reasoning-over-Long-Contexts-with-Process-Advantage-Shaping-cs-CL-cs-AIPDF" class="headerlink" title="[18] Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping cs.CL | cs.AIPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12465">Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12465" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan</span></p>
<p><strong>TL;DR:</strong> 本文针对大语言模型在长上下文推理中存在的’几乎正确’现象，提出了一种名为DeepReasonQA的知识图谱驱动合成框架，用于生成高难度、多跳的长上下文问答对，并在此基础上引入了Long-context Process Advantage Shaping方法，通过对推理步骤进行细粒度信用分配来捕捉关键学习信号，从而显著提升了长上下文推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决强化学习与可验证奖励方法在长上下文推理场景下性能下降的问题，特别是针对’几乎正确’轨迹（即大部分步骤正确但最终答案错误）因被整体惩罚而丢失有价值学习信号，以及现有数据缺乏推动模型进行复杂多跳推理的高推理密度这两个瓶颈。</p>
<p><strong>Result:</strong> 在三个长上下文推理基准测试上的实验表明，该方法大幅优于RLVR基线模型，并与前沿大语言模型性能相当，同时使用的参数量要少得多。进一步分析证实了该方法在强化长上下文推理的同时保持了稳定的强化学习训练。</p>
<p><strong>Insight:</strong> 主要创新点在于：1) 通过可控的知识图谱驱动框架合成具有内在推理链的高难度长上下文数据，以提升推理密度；2) 提出LongPAS方法，从有效性和相关性两个维度评估推理步骤，对’几乎正确’的轨迹进行细粒度的信用分配，从而有效利用部分正确的学习信号。这为解决长序列任务中的奖励稀疏和信用分配问题提供了新思路。</p>
<p><strong>Abstract:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the “almost-there” phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from “almost-there” trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.</p>
  </div>
</details>

<hr>
<h3 id="19-DoPE-Decoy-Oriented-Perturbation-Encapsulation-Human-Readable-AI-Hostile-Documents-for-Academic-Integrity-cs-CLPDF"><a href="#19-DoPE-Decoy-Oriented-Perturbation-Encapsulation-Human-Readable-AI-Hostile-Documents-for-Academic-Integrity-cs-CLPDF" class="headerlink" title="[19] DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity cs.CLPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12505">DoPE: Decoy Oriented Perturbation Encapsulation Human-Readable, AI-Hostile Documents for Academic Integrity</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12505" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ashish Raj Shekhar, Shiven Agarwal, Priyanuj Bordoloi, Yash Shah, Tejas Anvekar</span></p>
<p><strong>TL;DR:</strong> 本文提出DoPE（诱饵导向扰动封装）框架，通过在PDF&#x2F;HTML考试文档中嵌入语义诱饵，利用多模态大语言模型（MLLM）渲染-解析流程中的不一致性，实现模型无关的预防（阻止或混淆自动解题）与检测（标记盲目依赖AI的行为），以保护学术诚信。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型（MLLMs）能直接处理考试文档，威胁传统评估和学术诚信，需要一种文档层防御机制来防止自动化作弊并检测AI依赖。</p>
<p><strong>Result:</strong> 在Integrity-Bench基准（包含1826个PDF+HTML考试）上评估，针对OpenAI和Anthropic的黑盒MLLMs，DoPE实现了91.4%的检测率（误报率8.7%），并在96.3%的尝试中阻止成功完成或诱导诱饵对齐的失败。</p>
<p><strong>Insight:</strong> 创新点包括利用渲染-解析差异嵌入语义诱饵的文档层防御框架，以及LLM引导的FewSoRT-Q&#x2F;D管道生成和封装诱饵，提供模型无关的预防与检测，无需依赖传统单次分类器。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) can directly consume exam documents, threatening conventional assessments and academic integrity. We present DoPE (Decoy-Oriented Perturbation Encapsulation), a document-layer defense framework that embeds semantic decoys into PDF&#x2F;HTML assessments to exploit render-parse discrepancies in MLLM pipelines. By instrumenting exams at authoring time, DoPE provides model-agnostic prevention (stop or confound automated solving) and detection (flag blind AI reliance) without relying on conventional one-shot classifiers. We formalize prevention and detection tasks, and introduce FewSoRT-Q, an LLM-guided pipeline that generates question-level semantic decoys and FewSoRT-D to encapsulate them into watermarked documents. We evaluate on Integrity-Bench, a novel benchmark of 1826 exams (PDF+HTML) derived from public QA datasets and OpenCourseWare. Against black-box MLLMs from OpenAI and Anthropic, DoPE yields strong empirical gains: a 91.4% detection rate at an 8.7% false-positive rate using an LLM-as-Judge verifier, and prevents successful completion or induces decoy-aligned failures in 96.3% of attempts. We release Integrity-Bench, our toolkit, and evaluation code to enable reproducible study of document-layer defenses for academic integrity.</p>
  </div>
</details>

<hr>
<h3 id="20-Improving-Low-Resource-Machine-Translation-via-Round-Trip-Reinforcement-Learning-cs-CL-cs-AIPDF"><a href="#20-Improving-Low-Resource-Machine-Translation-via-Round-Trip-Reinforcement-Learning-cs-CL-cs-AIPDF" class="headerlink" title="[20] Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning cs.CL | cs.AIPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12535">Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12535" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmed Attia, Alham Fikri</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于往返强化学习的自监督微调方法，用于提升低资源机器翻译的性能。该方法利用NLLB模型系列，将英语翻译为目标低资源语言后再回译成英语，并结合chrF++和BLEU作为奖励函数来优化模型。在NLLB-MD数据集上对多个低资源语言（如Central Aymara、Friulian、Wolof和俄语）的实验表明，该方法能持续提升翻译质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 低资源机器翻译在数据稀缺的情况下性能有限，现有方法尚未充分探索自监督强化学习在微调中的应用潜力，本文旨在通过往返回译和奖励机制来改进低资源语言的翻译效果。</p>
<p><strong>Result:</strong> 在NLLB-MD数据集上评估了600M和1.3B参数的NLLB模型，对Central Aymara、Friulian、Wolof和俄语等语言均观察到一致的改进，定性分析显示翻译输出的流畅性和语义保真度有所提升。</p>
<p><strong>Insight:</strong> 创新点在于结合往返回译与强化学习进行自监督微调，利用chrF++和BLEU构建奖励函数，无需额外平行数据即可提升低资源翻译性能；客观分析认为该方法可扩展性强，能利用预训练知识实现持续自我改进。</p>
<p><strong>Abstract:</strong> Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.</p>
  </div>
</details>

<hr>
<h3 id="21-Disagreement-as-Data-Reasoning-Trace-Analytics-in-Multi-Agent-Systems-cs-CLPDF"><a href="#21-Disagreement-as-Data-Reasoning-Trace-Analytics-in-Multi-Agent-Systems-cs-CLPDF" class="headerlink" title="[21] Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems cs.CLPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12618">Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12618" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Elham Tajik, Conrad Borchers, Bahar Shahrokhian, Sebastian Simon, Ali Keramati</span></p>
<p><strong>TL;DR:</strong> 本研究提出将大型语言模型（LLM）智能体在多智能体系统中生成的推理轨迹作为一种新型的过程数据，用于增强定性编码中的解释实践。通过应用余弦相似度分析智能体间的推理轨迹，系统性地检测、量化和解释分歧，将分歧重新定义为有意义的分析信号。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着生成式AI的兴起，全自动和人机协作工作流成为有前景的分析方法，但指导此类工作流的方法学标准仍然有限。本研究旨在利用LLM智能体的推理轨迹来改进定性编码的解释过程，特别是在多智能体系统中。</p>
<p><strong>Result:</strong> 在分析近10,000个人类辅导对话片段编码的智能体对实例中，LLM智能体的语义推理相似度能够稳健地区分共识与分歧，并与人类编码可靠性相关。定性分析揭示了代码内细微的教学子功能以及概念代码本细化的机会。</p>
<p><strong>Insight:</strong> 创新点在于将智能体间的推理轨迹分歧视为一种新的分析信号，通过整合定量相似度度量与定性审查，该方法有潜力通过揭示解释歧义来改进和加速编码过程中评分者间可靠性的建立，特别是在LLM与人类协作时。</p>
<p><strong>Abstract:</strong> Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents’ semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.</p>
  </div>
</details>

<hr>
<h3 id="22-Objective-Matters-Fine-Tuning-Objectives-Shape-Safety-Robustness-and-Persona-Drift-cs-CL-cs-LGPDF"><a href="#22-Objective-Matters-Fine-Tuning-Objectives-Shape-Safety-Robustness-and-Persona-Drift-cs-CL-cs-LGPDF" class="headerlink" title="[22] Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift cs.CL | cs.LGPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12639">Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12639" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Daniel Vennemeyer, Punya Syon Pandey, Phan Anh Duong, Michael Umeokoli, Samuel Ratnam</span></p>
<p><strong>TL;DR:</strong> 本文通过控制变量实验比较了六种微调目标（SFT、DPO、CFT、IP、ORPO和KL正则化）对LLM安全性、鲁棒性和角色漂移的影响，发现微调目标在小规模训练时对安全性影响不大，但在大规模训练时成为对抗鲁棒性和角色稳定性的主要驱动因素。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是探究微调目标如何影响LLM的对齐安全性和对抗鲁棒性，解决现有研究中对微调目标角色分析不足的问题。</p>
<p><strong>Result:</strong> 实验在封闭式推理和开放式生成任务上进行，结果显示：小训练量时各目标鲁棒性相似但能力不同；大训练量时目标差异显著，监督和基于偏好的微调会同时提升能力和对抗脆弱性&#x2F;角色漂移，而ORPO和KL正则化等约束学习信号的目标能显著缓解这些问题。</p>
<p><strong>Insight:</strong> 创新点在于首次系统控制数据、领域等变量，量化比较不同微调目标对安全-能力边界的影响；客观分析表明，约束性目标（如ORPO、KL正则化）在大规模微调中能有效解耦能力提升与安全风险，为安全对齐提供了重要设计启示。</p>
<p><strong>Abstract:</strong> Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives – Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning – holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals – especially ORPO and KL-regularization – substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.</p>
  </div>
</details>

<hr>
<h3 id="23-Towards-Robust-Process-Reward-Modeling-via-Noise-aware-Learning-cs-CLPDF"><a href="#23-Towards-Robust-Process-Reward-Modeling-via-Noise-aware-Learning-cs-CLPDF" class="headerlink" title="[23] Towards Robust Process Reward Modeling via Noise-aware Learning cs.CLPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12748">Towards Robust Process Reward Modeling via Noise-aware Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12748" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bin Xie, Bingbing Xu, Xueyun Tian, Yilin Chen, Huawei Shen</span></p>
<p><strong>TL;DR:</strong> 本文针对过程奖励模型（PRM）中广泛使用的蒙特卡洛估计（MCE）方法会产生策略依赖的噪声奖励（包括错误奖励错误步骤和错误惩罚正确步骤）的问题，提出了一个两阶段框架来缓解噪声监督。该框架在标注阶段引入了反思感知的标签校正机制，利用大语言模型（LLM）作为裁判来检测与当前推理步骤相关的反思和自我纠正行为，从而抑制被高估的奖励；在训练阶段进一步提出了噪声感知迭代训练框架，使PRM能够基于自身置信度逐步细化噪声标签。实验表明，该方法显著提高了步骤级正确性判别能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 过程奖励模型（PRMs）在复杂推理中取得了强劲结果，但受限于昂贵的步骤级监督。广泛使用的替代方案蒙特卡洛估计（MCE）会产生策略依赖的奖励，从而引入标签噪声（包括错误奖励错误步骤和错误惩罚正确步骤），这损害了PRM的训练。本文旨在解决MCE带来的噪声监督问题，以提升PRM的鲁棒性。</p>
<p><strong>Result:</strong> 大量实验表明，该方法显著提高了步骤级正确性判别能力，在平均F1分数上比使用噪声监督训练的PRM实现了高达27%的绝对增益。</p>
<p><strong>Insight:</strong> 主要创新点在于：1）提出了一个两阶段框架来系统性地处理PRM训练中的噪声监督问题；2）在标注阶段，创新性地利用LLM作为裁判来检测推理轨迹中的反思和自我纠正行为，以校正被MCE高估的奖励标签；3）在训练阶段，设计了噪声感知迭代训练框架，使模型能够基于自身置信度迭代地细化噪声标签，实现自我净化。这为在弱监督或噪声监督下训练可靠的奖励模型提供了新思路。</p>
<p><strong>Abstract:</strong> Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \underline{\textbf{N}}oise-\underline{\textbf{A}}ware \underline{\textbf{I}}terative \underline{\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27% absolute gain in average F1 over PRMs trained with noisy supervision.</p>
  </div>
</details>

<hr>
<h3 id="24-Who-Does-This-Name-Remind-You-of-Nationality-Prediction-via-Large-Language-Model-Associative-Memory-cs-CLPDF"><a href="#24-Who-Does-This-Name-Remind-You-of-Nationality-Prediction-via-Large-Language-Model-Associative-Memory-cs-CLPDF" class="headerlink" title="[24] Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory cs.CLPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12771">Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12771" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Keito Inoshita</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为LLM Associative Memory Agents (LAMA)的新框架，用于通过大语言模型（LLM）的联想记忆能力来预测名字对应的国籍或地区。该方法不直接推理，而是通过回忆同名知名人物并聚合其国籍信息进行间接推理，利用包含人物代理和媒体代理的双代理架构并行处理不同知识领域，在99个国家的国籍预测任务上取得了0.817的准确率，显著优于传统LLM提示方法和神经网络模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于探索如何更有效地激发LLM中蕴含的世界知识。国籍&#x2F;地区预测任务不仅需要语言特征，还需要文化历史背景知识，传统LLM直接推理方法在应用抽象语言规则方面存在局限。</p>
<p><strong>Result:</strong> 在99个国家的国籍预测任务上，LAMA达到了0.817的准确率，大幅超越了传统的LLM提示方法和神经网络模型，实现了SOTA性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了基于联想记忆而非直接推理的多代理系统（LAMA），其核心洞察是LLM在回忆具体实例上比进行抽象推理更可靠，且基于回忆的方法对低频国籍具有鲁棒性，双代理架构能产生互补的协同效应。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.</p>
  </div>
</details>

<hr>
<h3 id="25-Do-Clinical-Question-Answering-Systems-Really-Need-Specialised-Medical-Fine-Tuning-cs-CLPDF"><a href="#25-Do-Clinical-Question-Answering-Systems-Really-Need-Specialised-Medical-Fine-Tuning-cs-CLPDF" class="headerlink" title="[25] Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning? cs.CLPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12812">Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12812" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sushant Kumar Ray, Gautam Siddharth Kashyap, Sahil Tripathi, Nipun Joshi, Vijay Govindarajan</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一个名为MEDASSESS-X的临床问答框架，它通过在推理时进行对齐而非监督微调，来挑战领域特定微调对于临床问答系统是必要的这一假设。该框架使用轻量级的引导向量来引导模型激活，使其进行医学上一致的推理，从而在通用和专用医学大语言模型上都能稳定提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是挑战临床问答系统部署中普遍存在的‘专业化谬误’，即认为必须对医学领域进行专门的微调。现有专用医学LLM存在覆盖范围窄、重新训练成本高和适应性有限等实际限制。</p>
<p><strong>Result:</strong> 实验表明，MEDASSESS-X在所有LLM家族上都带来了稳定的性能提升，将准确率提高了最多6%，事实一致性提高了7%，并将安全错误率降低了多达50%。</p>
<p><strong>Insight:</strong> 主要创新点是提出了一个推理时对齐的框架，无需更新模型权重或进行领域特定再训练，即可稳定提升临床问答性能。这挑战了领域特定微调是必要的这一固有观念，为高效部署提供了新思路。</p>
<p><strong>Abstract:</strong> Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.</p>
  </div>
</details>

<hr>
<h3 id="26-Multimodal-Multi-Agent-Empowered-Legal-Judgment-Prediction-cs-CLPDF"><a href="#26-Multimodal-Multi-Agent-Empowered-Legal-Judgment-Prediction-cs-CLPDF" class="headerlink" title="[26] Multimodal Multi-Agent Empowered Legal Judgment Prediction cs.CLPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12815">Multimodal Multi-Agent Empowered Legal Judgment Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12815" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhaolu Kang, Junhao Gong, Qingxi Chen, Hao Zhang, Jiaxin Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了JurisMMA框架，用于法律判决预测任务，通过多智能体协作分解审判流程，并构建了包含10万条中文司法记录的多模态数据集JurisMM进行验证。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统法律判决预测方法在处理多指控、多样化证据时缺乏适应性，需要更有效的框架来标准化和分解审判任务。</p>
<p><strong>Result:</strong> 在JurisMM数据集和基准LawBench上的实验验证了框架的有效性，表明其不仅适用于法律判决预测，还能扩展到更广泛的法律应用。</p>
<p><strong>Insight:</strong> 创新点在于引入多智能体协作机制来结构化处理法律审判流程，并构建大规模多模态司法数据集，为法律AI方法提供了新视角。</p>
<p><strong>Abstract:</strong> Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework’s effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.</p>
  </div>
</details>

<hr>
<h3 id="27-Rapport-du-Projet-de-Recherche-TRAIMA-cs-CLPDF"><a href="#27-Rapport-du-Projet-de-Recherche-TRAIMA-cs-CLPDF" class="headerlink" title="[27] Rapport du Projet de Recherche TRAIMA cs.CLPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12844">Rapport du Projet de Recherche TRAIMA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12844" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Julie Rançon, Jean-François Cerisier, Emilie Remond, Aurélien Nguyen, Andrew Peterson</span></p>
<p><strong>TL;DR:</strong> TRAIMA项目（2019-2020）研究如何利用机器学习自动处理教育环境中的多模态互动（如言语、副语言和非语言行为），以解决传统手动分析耗时且难以规模化的问题。项目聚焦于法语课堂中的解释性和协作性互动序列，通过定义理论框架、比较转录方法并利用多模态语料库，为自动化分析建立了方法论基础。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决教育互动研究中手动分析多模态数据（言语、副语言、非语言）效率低下、难以规模化的问题，探索机器学习在此领域的应用潜力。</p>
<p><strong>Result:</strong> 项目未提供具体的定量结果或基准测试，但通过手动转录序列的比较分析，展示了转录实践的变异性和解释性维度，并建立了可用于未来自动化研究的参考数据集（如INTER-EXPLIC语料库）。</p>
<p><strong>Insight:</strong> 创新点包括将解释性话语精确定义为三元序列（开场、解释核心、收尾），系统评估多模态转录规范的适用性，并强调理论明确性和研究者反思性作为自动化分析的前提，为教育多模态分析与人工智能的跨学科研究奠定了基础。</p>
<p><strong>Abstract:</strong> The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.</p>
  </div>
</details>

<hr>
<h3 id="28-Bridging-the-Knowledge-Action-Gap-by-Evaluating-LLMs-in-Dynamic-Dental-Clinical-Scenarios-cs-CLPDF"><a href="#28-Bridging-the-Knowledge-Action-Gap-by-Evaluating-LLMs-in-Dynamic-Dental-Clinical-Scenarios-cs-CLPDF" class="headerlink" title="[28] Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios cs.CLPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12974">Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12974" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongyang Ma, Tiantian Gu, Huaiyuan Sun, Huilin Zhu, Yongxin Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了标准化临床管理与性能评估（SCMPE）基准，用于全面评估大型语言模型（LLMs）在牙科临床场景中的表现，从静态知识任务到动态多轮患者交互模拟。研究发现，模型在静态任务中表现良好，但在动态临床对话中性能显著下降，主要瓶颈在于主动信息收集和动态状态跟踪能力不足，而非知识保留。研究还量化了检索增强生成（RAG）的作用，发现其在静态任务中能减少幻觉，但在动态工作流中效果有限且不稳定。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于推动LLMs从被动的知识检索器向自主临床代理转变，这需要评估标准从静态准确性转向动态行为可靠性，特别是在牙科这一需要高质量AI建议以支持患者参与决策的领域。</p>
<p><strong>Result:</strong> 在SCMPE基准上的分析结果显示，模型在静态客观任务中表现出高熟练度，但在动态临床对话中性能急剧下降。研究揭示了普遍存在的“高效能、低安全”风险，并量化了RAG的效果：其在静态任务中减轻了幻觉，但在动态工作流中效果有限且有时导致性能退化。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个综合评估临床LLMs从知识到工作流行为的基准（SCMPE），并实证揭示了动态临床推理能力（如信息收集和状态跟踪）是当前LLMs应用于自主临床实践的关键瓶颈，而非单纯的知识不足；同时指出，仅依靠外部知识（如RAG）无法弥补推理差距，需要领域自适应的预训练。从客观角度看，该研究为临床AI的安全性评估和后续发展提供了重要的实证路线图。</p>
<p><strong>Abstract:</strong> The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management &amp; Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping “Guideline Adherence” versus “Decision Quality” reveals a prevalent “High Efficacy, Low Safety” risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.</p>
  </div>
</details>

<hr>
<h3 id="29-The-Bitter-Lesson-of-Diffusion-Language-Models-for-Agentic-Workflows-A-Comprehensive-Reality-Check-cs-CLPDF"><a href="#29-The-Bitter-Lesson-of-Diffusion-Language-Models-for-Agentic-Workflows-A-Comprehensive-Reality-Check-cs-CLPDF" class="headerlink" title="[29] The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check cs.CLPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12979">The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12979" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qingyu Lu, Liang Ding, Kanjian Zhang, Jinxia Zhang, Dacheng Tao</span></p>
<p><strong>TL;DR:</strong> 本文对基于扩散的大语言模型在智能体工作流中的应用进行了全面评估，发现尽管其旨在解决自回归模型的顺序延迟瓶颈，但当前dLLMs在需要长程规划的具身智能体和需要精确格式的工具调用智能体任务中表现不佳，经常导致系统性失败。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探究基于扩散的大语言模型是否能够替代自回归模型，成为实时智能体交互的有效骨干，以解决效率瓶颈问题。</p>
<p><strong>Result:</strong> 在Agentboard和BFCL基准测试上的结果表明，当前dLLMs无法作为可靠的智能体骨干，在具身设置中难以处理时间反馈进行分支，在工具调用设置中无法在扩散噪声下保持符号精度（如严格的JSON模式）。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于引入了DiffuAgent多智能体评估框架来评估dLLMs的潜力。客观分析认为，其核心洞察是dLLMs在非因果角色（如记忆总结和工具选择）中有效，但要在智能体任务中可行，需要将因果、精确和逻辑基础的推理机制整合到去噪过程中。</p>
<p><strong>Abstract:</strong> The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a “bitter lesson”: current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.</p>
  </div>
</details>

<hr>
<h3 id="30-ChartAttack-Testing-the-Vulnerability-of-LLMs-to-Malicious-Prompting-in-Chart-Generation-cs-CLPDF"><a href="#30-ChartAttack-Testing-the-Vulnerability-of-LLMs-to-Malicious-Prompting-in-Chart-Generation-cs-CLPDF" class="headerlink" title="[30] ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation cs.CLPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12983">ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12983" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jesus-German Ortiz-Barajas, Jonathan Tonglet, Vivek Gupta, Iryna Gurevych</span></p>
<p><strong>TL;DR:</strong> 本文提出了ChartAttack框架，用于评估多模态大语言模型在图表生成任务中因恶意提示而产生误导性图表的风险。该框架通过注入误导性设计元素，诱导对底层数据的错误解读，并构建了AttackViz数据集进行测试。实验表明，ChartAttack显著降低了MLLM在图表问答任务上的性能，突显了基于MLLM的图表生成系统在鲁棒性和安全性方面的迫切需求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着多模态大语言模型被越来越多地用于从数据表自动生成图表，虽然提高了数据分析与报告效率，但也引入了新的滥用风险。本文旨在系统评估MLLM如何被恶意利用以大规模生成误导性图表，从而揭示其安全漏洞。</p>
<p><strong>Result:</strong> 在领域内和跨领域设置下的实验表明，ChartAttack显著降低了MLLM阅读器的问答性能，分别使准确率平均下降19.6分和14.9分。一项人类研究进一步显示，参与者面对由ChartAttack生成的误导性图表时，准确率平均下降20.2分。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了首个针对MLLM图表生成系统的对抗性评估框架ChartAttack，并构建了配套的AttackViz数据集。从客观角度看，这项工作将对抗性测试从传统的文本或图像领域扩展到了结构化数据可视化生成场景，为评估和提升MLLM在数据解释任务中的鲁棒性提供了新的方法论和基准。</p>
<p><strong>Abstract:</strong> Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.</p>
  </div>
</details>

<hr>
<h3 id="31-Graph-Reasoning-Paradigm-Structured-and-Symbolic-Reasoning-with-Topology-Aware-Reinforcement-Learning-for-Large-Language-Models-cs-CLPDF"><a href="#31-Graph-Reasoning-Paradigm-Structured-and-Symbolic-Reasoning-with-Topology-Aware-Reinforcement-Learning-for-Large-Language-Models-cs-CLPDF" class="headerlink" title="[31] Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models cs.CLPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12995">Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12995" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Runxuan Liu, Xianhao Ou, Xinyan Ma, Jiyuan Wang, Jiafeng Liang</span></p>
<p><strong>TL;DR:</strong> 本文提出图推理范式（GRP），通过图结构表示和步骤级认知标签实现结构化与符号化推理，以解决当前大语言模型推理中存在的非结构化文本语义评估瓶颈、粗粒度监督、奖励攻击、高训练成本和泛化能力差等问题。基于GRP进一步设计了过程感知分层裁剪组相对策略优化（PASC-GRPO），利用结构化评估替代语义评估，通过图结构结果奖励实现过程感知验证，并借助分层裁剪优势估计缓解奖励攻击。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对现有基于可验证奖励强化学习（RLVR）的长思维链方法在推理时生成非结构化文本导致的语义评估计算瓶颈、粗粒度监督、奖励攻击、高训练成本及泛化能力差等问题，提出结构化与符号化的推理框架。</p>
<p><strong>Result:</strong> 实验在数学推理和代码生成任务上展示了显著性能提升，具体定量结果未在摘要中给出，但暗示了方法在相关基准上的有效性。</p>
<p><strong>Insight:</strong> 创新点在于将推理过程结构化表示为带认知标签的图，从而支持细粒度、过程感知的评估与优化；PASC-GRPO通过图结构奖励和分层裁剪机制，提升了训练效率并缓解了奖励攻击问题，为LLM推理优化提供了可借鉴的结构化范式。</p>
<p><strong>Abstract:</strong> Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.</p>
  </div>
</details>

<hr>
<h3 id="32-Tears-or-Cheers-Benchmarking-LLMs-via-Culturally-Elicited-Distinct-Affective-Responses-cs-CLPDF"><a href="#32-Tears-or-Cheers-Benchmarking-LLMs-via-Culturally-Elicited-Distinct-Affective-Responses-cs-CLPDF" class="headerlink" title="[32] Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses cs.CLPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13024">Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13024" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chongyuan Dai, Yaling Shen, Jinpeng Hu, Zihan Gao, Jia Li</span></p>
<p><strong>TL;DR:</strong> 本文介绍了CEDAR，一个用于评估大语言模型文化对齐性的多模态基准测试，专注于捕捉文化引发的不同情感反应。该基准包含10,962个实例，涵盖7种语言和14种细粒度情感类别，通过新颖的流程构建，结合LLM生成的临时标签和严格的人工评估。对17个代表性多语言模型的评估揭示了语言一致性与文化对齐之间的脱节。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型文化对齐评估主要关注声明性知识（如地理事实或社会习俗），未能捕捉不同社会文化视角下的主观解释差异，特别是情感处理方面的文化差异。</p>
<p><strong>Result:</strong> 对17个代表性多语言模型的综合评估表明，当前模型在文化基础的情感理解方面仍面临重大挑战，揭示了语言一致性与文化对齐之间的脱节。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个专注于文化引发情感反应差异的多模态基准CEDAR，并提出了一种利用LLM生成临时标签来筛选跨文化情感区分实例，再通过人工评估获得可靠标注的新流程，从而更有效地评估模型的文化对齐性，而非仅依赖语言能力。</p>
<p><strong>Abstract:</strong> Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \underline{\textsc{E}}licited \underline{\textsc{D}}istinct \underline{\textsc{A}}ffective \underline{\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.</p>
  </div>
</details>

<hr>
<h3 id="33-Agentic-Conversational-Search-with-Contextualized-Reasoning-via-Reinforcement-Learning-cs-CL-cs-IRPDF"><a href="#33-Agentic-Conversational-Search-with-Contextualized-Reasoning-via-Reinforcement-Learning-cs-CL-cs-IRPDF" class="headerlink" title="[33] Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning cs.CL | cs.IRPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13115">Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13115" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fengran Mo, Yifan Gao, Sha Li, Hansi Zeng, Xin Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于强化学习的对话式搜索代理，该代理能够在多轮对话中交替进行搜索和推理，以动态适应用户不断变化的意图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有对话系统通常采用静态的查询改写、检索和生成流程，分别优化不同环节，且多针对单轮交互，难以处理多轮对话中用户意图的动态演变和检索与生成的协同优化问题。</p>
<p><strong>Result:</strong> 在四个广泛使用的对话基准测试上的实验结果表明，该方法超越了多个现有强基线模型，证明了其有效性。</p>
<p><strong>Insight:</strong> 核心创新在于通过强化学习训练，使代理能够学习在多轮对话中交替进行搜索和推理的探索性和适应性行为，以协同优化检索和生成，从而更好地服务于动态演变的用户目标。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.</p>
  </div>
</details>

<hr>
<h3 id="34-Probe-and-Skip-Self-Predictive-Token-Skipping-for-Efficient-Long-Context-LLM-Inference-cs-CL-cs-LGPDF"><a href="#34-Probe-and-Skip-Self-Predictive-Token-Skipping-for-Efficient-Long-Context-LLM-Inference-cs-CL-cs-LGPDF" class="headerlink" title="[34] Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference cs.CL | cs.LGPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13155">Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13155" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zimeng Wu, Donghao Wang, Chaozhe Jin, Jiaxin Chen, Yunhong Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为SPTS（自预测令牌跳过）的无训练框架，旨在提升长上下文LLM推理的效率。该方法通过部分注意力探测（PAP）和低秩变换探测（LTP）策略，在多头注意力和前馈网络中选择性地跳过信息量较少的令牌，并结合多阶段延迟剪枝（MSDP）策略重新分配跳过预算，从而在保持模型性能的同时显著降低推理延迟。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长上下文推理增强了LLM的推理能力，但带来了巨大的计算开销。现有的令牌导向方法（如剪枝和跳过）在减少推理延迟方面有潜力，但仍受限于固有的加速潜力有限、代理信号过时和冗余干扰等问题，导致速度与准确性的权衡不理想。</p>
<p><strong>Result:</strong> 大量实验表明，该方法在保持最先进模型性能的同时，在预填充和端到端生成阶段分别实现了高达2.46倍和2.29倍的加速。</p>
<p><strong>Insight:</strong> 创新点包括：1）提出无训练框架SPTS，通过PAP和LTP组件特定策略实现选择性令牌跳过；2）引入MSDP策略跨层渐进剪枝冗余令牌；3）利用部分前向注意力计算和低秩代理网络预测令牌变换，以更准确地识别信息量少的令牌，从而优化速度-准确性权衡。</p>
<p><strong>Abstract:</strong> Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.</p>
  </div>
</details>

<hr>
<h3 id="35-OpenExempt-A-Diagnostic-Benchmark-for-Legal-Reasoning-and-a-Framework-for-Creating-Custom-Benchmarks-on-Demand-cs-CLPDF"><a href="#35-OpenExempt-A-Diagnostic-Benchmark-for-Legal-Reasoning-and-a-Framework-for-Creating-Custom-Benchmarks-on-Demand-cs-CLPDF" class="headerlink" title="[35] OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand cs.CLPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13183">OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13183" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sergio Servantez, Sarah B. Lawsky, Rajiv Jain, Daniel W. Linna, Kristian Hammond</span></p>
<p><strong>TL;DR:</strong> 本文介绍了OpenExempt，一个用于法律推理诊断评估的框架和基准测试。该框架利用美国破产法典的专家符号表示，动态生成大量自然语言推理任务及其机器可计算解决方案，允许用户精细控制任务复杂度和范围。基于此构建的OpenExempt基准包含9,765个样本，涵盖九个评估套件，旨在深入探究模型能力。对13个不同语言模型的实验揭示了在较长推理路径和存在混淆语句时出现的性能陡降。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有推理基准（尤其是在法律等复杂规则领域）的局限性，即静态问答对仅提供性能快照，将复杂行为压缩为单一准确率指标，且构建成本高，难以隔离特定失败模式。</p>
<p><strong>Result:</strong> 在OpenExempt基准上对13个语言模型的实验表明，在较长推理路径和存在混淆语句的情况下，模型性能出现显著下降（性能陡降）。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个动态生成诊断基准的框架，使用专家符号表示来按需创建定制化任务，实现了对特定推理技能的隔离测试和精细评估，为理解和改进推理系统提供了新工具。</p>
<p><strong>Abstract:</strong> Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.</p>
  </div>
</details>

<hr>
<h3 id="36-Beyond-Single-shot-Writing-Deep-Research-Agents-are-Unreliable-at-Multi-turn-Report-Revision-cs-CL-cs-AIPDF"><a href="#36-Beyond-Single-shot-Writing-Deep-Research-Agents-are-Unreliable-at-Multi-turn-Report-Revision-cs-CL-cs-AIPDF" class="headerlink" title="[36] Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision cs.CL | cs.AIPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13217">Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13217" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bingsen Chen, Boyan Li, Ping Nie, Yuyu Zhang, Xi Ye</span></p>
<p><strong>TL;DR:</strong> 本文提出了Mr Dre评估套件，首次将多轮报告修订作为深度研究代理（DRAs）的新评估维度，揭示了现有DRAs在迭代修订中存在内容倒退和引用质量下降等关键缺陷。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有DRA基准将报告生成视为单次写作任务，与人类研究者通过自我反思或同行反馈进行迭代起草和修订的方式存在根本差异，论文旨在探索DRAs能否可靠地根据用户反馈修订报告。</p>
<p><strong>Result:</strong> 对五种不同DRAs的分析表明，虽然代理能处理大部分用户反馈，但会在16-27%的先前覆盖内容和引用质量上出现倒退；即使最佳性能代理在多轮修订后仍存在显著改进空间，且无法通过提示工程或专用修订子代理等推理时修复轻松解决。</p>
<p><strong>Insight:</strong> 创新点在于建立了多轮报告修订的评估框架（Mr Dre），包含统一的长篇报告评估协议和人工验证的反馈模拟流程；客观分析揭示了DRA在迭代任务中的系统性不可靠性，挑战了当前单次评估范式的有效性。</p>
<p><strong>Abstract:</strong> Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback’s scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.</p>
  </div>
</details>

<hr>
<h3 id="37-Autoregressive-Models-Rival-Diffusion-Models-at-ANY-ORDER-Generation-cs-CL-cs-AIPDF"><a href="#37-Autoregressive-Models-Rival-Diffusion-Models-at-ANY-ORDER-Generation-cs-CL-cs-AIPDF" class="headerlink" title="[37] Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation cs.CL | cs.AIPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13228">Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13228" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Zeming Wei</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为A3（Any-order Any-subset Autoregressive modeling）的通用框架，它将标准自回归模型扩展为支持任意词元组和生成顺序的建模方式，从而在保持自回归模型概率严谨性和多层依赖建模优势的同时，继承了扩散模型在并行和双向生成方面的灵活性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散语言模型虽然支持任意顺序生成和双向条件生成，但其单步依赖的建模方式限制了模型深度，导致样本质量和稳定性通常低于自回归模型。本文旨在解决这一问题，探索如何让自回归模型也能具备扩散模型的灵活性。</p>
<p><strong>Result:</strong> 在问答、常识推理和故事填充任务上的实验表明，A3框架在保持灵活解码能力的同时，性能优于基于扩散的模型。</p>
<p><strong>Insight:</strong> 核心创新点在于将扩散式训练重新表述为结构化的多组预测过程，并通过双流注意力架构和渐进适应策略，使预训练的自回归模型能够过渡到任意顺序预测。这为语言建模提供了一个统一、灵活且高效的新范式。</p>
<p><strong>Abstract:</strong> Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models’ flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.</p>
  </div>
</details>

<hr>
<h3 id="38-Unlearning-in-LLMs-Methods-Evaluation-and-Open-Challenges-cs-CLPDF"><a href="#38-Unlearning-in-LLMs-Methods-Evaluation-and-Open-Challenges-cs-CLPDF" class="headerlink" title="[38] Unlearning in LLMs: Methods, Evaluation, and Open Challenges cs.CLPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13264">Unlearning in LLMs: Methods, Evaluation, and Open Challenges</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13264" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tyler Lizzo, Larry Heck</span></p>
<p><strong>TL;DR:</strong> 这篇综述论文系统性地梳理了大语言模型（LLM）中的机器遗忘（Unlearning）领域，涵盖了方法分类、评估体系以及未来挑战。它旨在为开发可靠且负责任的LLM遗忘技术提供路线图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大语言模型的广泛应用引发了隐私、版权、安全和偏见等严重关切，机器遗忘作为一种无需完全重新训练即可选择性移除模型知识或数据的范式，为解决这些问题提供了可能。</p>
<p><strong>Result:</strong> 本文是一篇综述，未提出新方法或报告具体实验结果，但系统回顾了现有评估生态系统，包括用于衡量遗忘效果、知识保留和鲁棒性的基准、指标和数据集。</p>
<p><strong>Insight:</strong> 论文的创新点在于对LLM遗忘方法进行了结构化分类（数据为中心、参数为中心、架构为中心、混合等），并全面梳理了评估框架。从客观角度看，其核心贡献是为这一新兴领域提供了一个清晰的概览和未来研究方向（如可扩展效率、形式化保证、跨语言&#x2F;多模态遗忘、对抗性再学习的鲁棒性），对后续研究和实践具有指导意义。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.</p>
  </div>
</details>

<hr>
<h3 id="39-OI-Bench-An-Option-Injection-Benchmark-for-Evaluating-LLM-Susceptibility-to-Directive-Interference-cs-CLPDF"><a href="#39-OI-Bench-An-Option-Injection-Benchmark-for-Evaluating-LLM-Susceptibility-to-Directive-Interference-cs-CLPDF" class="headerlink" title="[39] OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference cs.CLPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13300">OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13300" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yow-Fu Liou, Yu-Chien Tang, Yu-Hsiang Liu, An-Zi Yen</span></p>
<p><strong>TL;DR:</strong> 本文提出了OI-Bench基准测试，通过在多选题（MCQA）中注入包含误导性指令的额外选项来评估大语言模型（LLM）对指令干扰的易感性。该基准包含3000个涵盖知识、推理和常识任务的问题，并设计了16种指令类型。作者评估了12个LLM，揭示了显著的脆弱性，并探讨了缓解策略。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究表明LLM的决策会受到社会线索、框架和指令等指令信号的干扰，但缺乏系统性的评估基准。本文旨在构建一个结合选项界面操纵和指令干扰的基准，以系统评估模型对此类干扰的鲁棒性。</p>
<p><strong>Result:</strong> 在OI-Bench上对12个LLM的实验结果显示，模型存在显著的脆弱性，且鲁棒性表现各异。攻击成功率揭示了模型易受指令干扰的影响，但论文未明确提及是否达到SOTA或与特定模型比较。</p>
<p><strong>Insight:</strong> 创新点在于提出了‘选项注入’这一基准测试方法，将误导性指令嵌入标准化选择题结构，实现了可扩展的评估。这为系统研究LLM在基于选择的界面中对指令干扰的鲁棒性提供了新工具和视角。</p>
<p><strong>Abstract:</strong> Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.</p>
  </div>
</details>

<hr>
<h3 id="40-Recurrent-Confidence-Chain-Temporal-Aware-Uncertainty-Quantification-in-Large-Language-Models-cs-CL-cs-LGPDF"><a href="#40-Recurrent-Confidence-Chain-Temporal-Aware-Uncertainty-Quantification-in-Large-Language-Models-cs-CL-cs-LGPDF" class="headerlink" title="[40] Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models cs.CL | cs.LGPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13368">Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13368" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhenjiang Mao, Anirudhh Venkat</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Recurrent Confidence Chain的新方法，用于量化大型语言模型在长序列推理任务中的不确定性，通过引入跨步骤注意力分析语义相关性，并采用隐藏置信机制保留历史置信信息，从而更准确地评估整体置信度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大型语言模型在应用链式思维等推理机制时，虽然性能强大，但难以评估答案的不确定性，可能导致误导性幻觉；现有方法常忽略置信度在时间维度上的传播，使得早期低置信步骤被掩盖，导致整体置信度高估。</p>
<p><strong>Result:</strong> 在GAOKAO数学基准和CLadder因果推理数据集上，使用主流开源大模型进行评估，该方法在负对数似然和期望校准误差指标上均优于现有最先进方法，实现了预测质量与校准之间的更优平衡。</p>
<p><strong>Insight:</strong> 创新点在于首次将时间感知的置信传播机制引入不确定性量化，通过跨步骤注意力捕捉语义依赖，并设计隐藏置信层来整合历史信息，这为长序列推理中的置信校准提供了新思路，可提升模型输出的可靠性。</p>
<p><strong>Abstract:</strong> As reasoning modules, such as the chain-of-thought mechanism, are applied to large language models, they achieve strong performance on various tasks such as answering common-sense questions and solving math problems. The main challenge now is to assess the uncertainty of answers, which can help prevent misleading or serious hallucinations for users. Although current methods analyze long reasoning sequences by filtering unrelated tokens and examining potential connections between nearby tokens or sentences, the temporal spread of confidence is often overlooked. This oversight can lead to inflated overall confidence, even when earlier steps exhibit very low confidence. To address this issue, we propose a novel method that incorporates inter-step attention to analyze semantic correlations across steps. For handling long-horizon responses, we introduce a hidden confidence mechanism to retain historical confidence information, which is then combined with stepwise confidence to produce a more accurate overall estimate. We evaluate our method on the GAOKAO math benchmark and the CLadder causal reasoning dataset using mainstream open-source large language models. Our approach is shown to outperform state-of-the-art methods by achieving a superior balance between predictive quality and calibration, demonstrated by strong performance on both Negative Log-Likelihood and Expected Calibration Error.</p>
  </div>
</details>

<hr>
<h3 id="41-Confidence-over-Time-Confidence-Calibration-with-Temporal-Logic-for-Large-Language-Model-Reasoning-cs-CL-cs-LGPDF"><a href="#41-Confidence-over-Time-Confidence-Calibration-with-Temporal-Logic-for-Large-Language-Model-Reasoning-cs-CL-cs-LGPDF" class="headerlink" title="[41] Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning cs.CL | cs.LGPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13387">Confidence over Time: Confidence Calibration with Temporal Logic for Large Language Model Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13387" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhenjiang Mao, Anirudhh Venkat, Artem Bisliouk, Akshat Kothiyal, Sindhura Kumbakonam Subramanian</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于信号时序逻辑（STL）的新方法，用于校准大型语言模型在长链、多步推理任务（如数学解题和科学问答）中的置信度。传统方法将整个推理过程简化为单一标量分数，而本文通过挖掘区分正确与错误回答的置信度信号时序模式，并利用超网络参数化STL模块，从而生成更准确的置信度估计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有置信度估计方法通常将整个推理过程压缩为单一标量，忽略了置信度在生成过程中的动态演变，导致其对回答长度或冗余等表面因素敏感，且难以区分正确推理与自信陈述的错误。</p>
<p><strong>Result:</strong> 在多个推理任务上的实验表明，该方法生成的置信度分数比基线方法（如简单标量估计）具有更好的校准性。</p>
<p><strong>Insight:</strong> 创新点在于首次将信号时序逻辑（STL）应用于LLM推理置信度的时序模式分析，发现跨任务通用的STL模式，并引入超网络对STL参数进行个性化调整，从而更精细地捕捉和利用置信度随时间的演变信号。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) increasingly rely on long-form, multi-step reasoning to solve complex tasks such as mathematical problem solving and scientific question answering. Despite strong performance, existing confidence estimation methods typically reduce an entire reasoning process to a single scalar score, ignoring how confidence evolves throughout the generation. As a result, these methods are often sensitive to superficial factors such as response length or verbosity, and struggle to distinguish correct reasoning from confidently stated errors. We propose to characterize the stepwise confidence signal using Signal Temporal Logic (STL). Using a discriminative STL mining procedure, we discover temporal formulas that distinguish confidence signals of correct and incorrect responses. Our analysis found that the STL patterns generalize across tasks, and numeric parameters exhibit sensitivity to individual questions. Based on these insights, we develop a confidence estimation approach that informs STL blocks with parameter hypernetworks. Experiments on multiple reasoning tasks show our confidence scores are more calibrated than the baselines.</p>
  </div>
</details>

<hr>
<h3 id="42-Beyond-Memorization-Testing-LLM-Reasoning-on-Unseen-Theory-of-Computation-Tasks-cs-CL-cs-AI-cs-FLPDF"><a href="#42-Beyond-Memorization-Testing-LLM-Reasoning-on-Unseen-Theory-of-Computation-Tasks-cs-CL-cs-AI-cs-FLPDF" class="headerlink" title="[42] Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks cs.CL | cs.AI | cs.FLPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13392">Beyond Memorization: Testing LLM Reasoning on Unseen Theory of Computation Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.FL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13392" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur</span></p>
<p><strong>TL;DR:</strong> 这篇论文评估了大语言模型（LLM）在确定性有限自动机（DFA）构造任务上的推理能力，特别是处理未见问题的表现。研究发现，LLM在事实性问题和见过的构造问题上表现良好，但在涉及多重交互约束或通过阿登定理系统生成的未见问题上，准确率大幅下降，揭示了其生成语法上合理的DFA与进行语义正确的形式推理能力之间存在根本差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于探究LLM在形式语言任务上的强性能是源于真正的符号推理，还是仅仅是对熟悉结构的模式匹配，通过引入一个包含未见问题的DFA构造基准来测试其泛化与推理能力。</p>
<p><strong>Result:</strong> 在事实性问题上达到完美准确率，在见过的任务上达到84-90%的准确率，但在未见问题上准确率下降了30-64%。评估了多阶段提示协议，但未能可靠解决全局不一致或结构有缺陷的自动机。</p>
<p><strong>Insight:</strong> 论文的创新点在于设计了一个包含手工制作和系统生成的未见问题的基准，以严格测试LLM的推理泛化能力。客观分析表明，LLM在形式推理中存在系统性误解（如语言约束、克林星号语义处理）和全局一致性保持的缺陷，且错误在不同提示策略下持续存在，这暴露了其语义推理能力的根本局限性。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have demonstrated strong performance on formal language tasks, yet whether this reflects genuine symbolic reasoning or pattern matching on familiar constructions remains unclear. We introduce a benchmark for deterministic finite automata (DFA) construction from regular languages, comprising factual knowledge questions, seen construction problems from public sources, and two types of unseen problems: hand-crafted instances with multiple interacting constraints and systematically generated problems via Arden’s theorem. Models achieve perfect accuracy on factual questions and 84-90% on seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%), with failures stemming from systematic misinterpretation of language constraints, incorrect handling of Kleene-star semantics, and a failure to preserve global consistency. We evaluate a three-stage hint protocol that enables correction of shallow errors but does not reliably resolve globally inconsistent or structurally flawed automata. Our analysis across multiple prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that errors persist regardless of prompting approach, exposing a fundamental gap between LLMs’ ability to generate syntactically plausible DFAs and their capacity for semantically correct formal reasoning.</p>
  </div>
</details>

<hr>
<h3 id="43-Trust-Me-I’m-an-Expert-Decoding-and-Steering-Authority-Bias-in-Large-Language-Models-cs-CL-cs-LGPDF"><a href="#43-Trust-Me-I’m-an-Expert-Decoding-and-Steering-Authority-Bias-in-Large-Language-Models-cs-CL-cs-LGPDF" class="headerlink" title="[43] Trust Me, I’m an Expert: Decoding and Steering Authority Bias in Large Language Models cs.CL | cs.LGPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13433">Trust Me, I’m an Expert: Decoding and Steering Authority Bias in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13433" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Priyanka Mary Mammen, Emil Joswin, Shankar Venkitachalam</span></p>
<p><strong>TL;DR:</strong> 本文研究了大型语言模型在推理任务中是否对权威来源的认可存在系统性偏见，通过数学、法律和医学领域的四个数据集评估了11个模型，发现模型随着来源专业度的增加更容易受到错误或误导性认可的影响，导致准确性下降和错误答案置信度上升，同时揭示了这种权威偏见在模型中的机制性编码，并展示了如何引导模型远离偏见以提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索语言模型在推理任务中是否基于认可来源的专业度表现出系统性偏见，以弥补先前研究中来源可信度影响未被充分探索的空白。</p>
<p><strong>Result:</strong> 在数学、法律和医学推理的四个数据集上，评估了11个模型，发现模型随着来源专业度增加更易受错误认可影响，导致准确性下降和错误答案置信度上升；通过机制性干预可引导模型远离偏见，提升性能。</p>
<p><strong>Insight:</strong> 创新点在于揭示了语言模型中权威偏见的机制性编码，并提出了可引导模型减少偏见的方法，为理解和改进模型在受认可影响时的鲁棒性提供了新视角。</p>
<p><strong>Abstract:</strong> Prior research demonstrates that performance of language models on reasoning tasks can be influenced by suggestions, hints and endorsements. However, the influence of endorsement source credibility remains underexplored. We investigate whether language models exhibit systematic bias based on the perceived expertise of the provider of the endorsement. Across 4 datasets spanning mathematical, legal, and medical reasoning, we evaluate 11 models using personas representing four expertise levels per domain. Our results reveal that models are increasingly susceptible to incorrect&#x2F;misleading endorsements as source expertise increases, with higher-authority sources inducing not only accuracy degradation but also increased confidence in wrong answers. We also show that this authority bias is mechanistically encoded within the model and a model can be steered away from the bias, thereby improving its performance even when an expert gives a misleading endorsement.</p>
  </div>
</details>

<hr>
<h3 id="44-PhysicsSolutionAgent-Towards-Multimodal-Explanations-for-Numerical-Physics-Problem-Solving-cs-CL-cs-HCPDF"><a href="#44-PhysicsSolutionAgent-Towards-Multimodal-Explanations-for-Numerical-Physics-Problem-Solving-cs-CL-cs-HCPDF" class="headerlink" title="[44] PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving cs.CL | cs.HCPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13453">PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13453" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aditya Thole, Anmol Agrawal, Arnav Ramamoorthy, Dhruv Kumar</span></p>
<p><strong>TL;DR:</strong> 本文提出了PhysicsSolutionAgent（PSA），一个能够生成长达六分钟Manim动画视频以解释数值物理问题的自主代理。研究通过包含15个定量参数的自动化评估流程和视觉语言模型的反馈来迭代提升视频质量，并在32个数值与理论物理问题视频上进行了评估。结果显示，PSA使用GPT-5-mini实现了100%的视频完成率和平均3.8&#x2F;5的自动化评分，但也揭示了视觉布局不一致和反馈中视觉内容解释错误等关键问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决数值物理问题通常需要超越纯文本的解决方案，清晰的视觉推理能显著提升概念理解。尽管大语言模型在文本形式的物理问题上表现强劲，但其生成长篇高质量视觉解释的能力尚未得到充分探索。</p>
<p><strong>Result:</strong> 在32个数值与理论物理问题视频上的评估表明，PSA使用GPT-5-mini实现了100%的视频完成率，平均自动化评分为3.8&#x2F;5。视频质量在问题难度以及任务是数值型还是理论型上存在系统性差异。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了一个结合Manim动画生成与自动化评估（含VLM反馈）的自主代理PSA，用于生成物理问题解释视频。客观分析认为，其提出的多模态评估框架（结合定量参数与VLM迭代反馈）以及对生成视频中暴露的可靠Manim代码生成、多模态推理与评估挑战的揭示，为未来教育系统的视觉理解、验证与评估框架的改进指明了方向。</p>
<p><strong>Abstract:</strong> Explaining numerical physics problems often requires more than text-based solutions; clear visual reasoning can substantially improve conceptual understanding. While large language models (LLMs) demonstrate strong performance on many physics questions in textual form, their ability to generate long, high-quality visual explanations remains insufficiently explored. In this work, we introduce PhysicsSolutionAgent (PSA), an autonomous agent that generates physics-problem explanation videos of up to six minutes using Manim animations. To evaluate the generated videos, we design an assessment pipeline that performs automated checks across 15 quantitative parameters and incorporates feedback from a vision-language model (VLM) to iteratively improve video quality. We evaluate PSA on 32 videos spanning numerical and theoretical physics problems. Our results reveal systematic differences in video quality depending on problem difficulty and whether the task is numerical or theoretical. Using GPT-5-mini, PSA achieves a 100% video-completion rate with an average automated score of 3.8&#x2F;5. However, qualitative analysis and human inspection uncover both minor and major issues, including visual layout inconsistencies and errors in how visual content is interpreted during feedback. These findings expose key limitations in reliable Manim code generation and highlight broader challenges in multimodal reasoning and evaluation for visual explanations of numerical physics problems. Our work underscores the need for improved visual understanding, verification, and evaluation frameworks in future multimodal educational systems</p>
  </div>
</details>

<hr>
<h3 id="45-HateXScore-A-Metric-Suite-for-Evaluating-Reasoning-Quality-in-Hate-Speech-Explanations-cs-CL-cs-AIPDF"><a href="#45-HateXScore-A-Metric-Suite-for-Evaluating-Reasoning-Quality-in-Hate-Speech-Explanations-cs-CL-cs-AIPDF" class="headerlink" title="[45] HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations cs.CL | cs.AIPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13547">HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate Speech Explanations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13547" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujia Hu, Roy Ka-Wei Lee</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为HateXScore的评估指标套件，用于评估仇恨言论检测模型中解释的推理质量。该套件包含四个组成部分，旨在诊断标准指标（如准确率或F1分数）无法揭示的可解释性失败和标注不一致问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前仇恨言论检测的评估框架很少评估模型为何将文本判定为仇恨言论，缺乏对解释推理质量的系统评估，因此需要一个新的指标来弥补这一空白。</p>
<p><strong>Result:</strong> 在六个不同的仇恨言论数据集上进行了评估，人类评估结果与HateXScore显示出高度一致性，验证了其作为可信赖和透明内容审核实用工具的有效性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个多维度、可配置的指标套件，专门用于评估解释的推理质量，包括结论明确性、引用文本的忠实性与因果依据、受保护群体识别以及逻辑一致性，这为模型可解释性提供了更精细的诊断工具。</p>
<p><strong>Abstract:</strong> Hateful speech detection is a key component of content moderation, yet current evaluation frameworks rarely assess why a text is deemed hateful. We introduce \textsf{HateXScore}, a four-component metric suite designed to evaluate the reasoning quality of model explanations. It assesses (i) conclusion explicitness, (ii) faithfulness and causal grounding of quoted spans, (iii) protected group identification (policy-configurable), and (iv) logical consistency among these elements. Evaluated on six diverse hate speech datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal interpretability failures and annotation inconsistencies that are invisible to standard metrics like Accuracy or F1. Moreover, human evaluation shows strong agreement with \textsf{HateXScore}, validating it as a practical tool for trustworthy and transparent moderation.   \textcolor{red}{Disclaimer: This paper contains sensitive content that may be disturbing to some readers.}</p>
  </div>
</details>

<hr>
<h3 id="46-Activation-Space-Anchored-Access-Control-for-Multi-Class-Permission-Reasoning-in-Large-Language-Models-cs-CLPDF"><a href="#46-Activation-Space-Anchored-Access-Control-for-Multi-Class-Permission-Reasoning-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[46] Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models cs.CLPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13630">Activation-Space Anchored Access Control for Multi-Class Permission Reasoning in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13630" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhaopeng Zhang, Pengcheng Sun, Lan Zhang, Chen Tang, Jiewei Lai</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为激活空间锚定访问控制（AAAC）的无训练框架，用于解决大型语言模型在知识库问答中可能超出用户权限范围泄露敏感信息的问题。该方法基于中间激活的几何规律性，通过构建权限锚点库和多锚点引导机制，在推理时重定向查询激活至授权区域，从而抑制越权生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型在知识库问答部署中，可能无意中回答超出用户权限范围的问题，导致敏感内容泄露，这使得在细粒度访问控制要求下部署知识库问答变得困难。</p>
<p><strong>Result:</strong> 在三个大型语言模型系列上的广泛实验表明，AAAC将权限违反率降低了高达86.5%，将基于提示的攻击成功率降低了90.7%，同时与基线相比，以较小的推理开销提高了响应可用性。</p>
<p><strong>Insight:</strong> 创新点在于发现了同一查询在不同权限范围下诱导的中间激活表示具有明显的聚类和可分离性这一几何规律，并基于此构建了无需训练、仅需少量离线样本的锚点库和引导机制，实现了设计上抑制越权生成的多类别权限控制。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly deployed over knowledge bases for efficient knowledge retrieval and question answering. However, LLMs can inadvertently answer beyond a user’s permission scope, leaking sensitive content, thus making it difficult to deploy knowledge-base QA under fine-grained access control requirements. In this work, we identify a geometric regularity in intermediate activations: for the same query, representations induced by different permission scopes cluster distinctly and are readily separable. Building on this separability, we propose Activation-space Anchored Access Control (AAAC), a training-free framework for multi-class permission control. AAAC constructs an anchor bank, with one permission anchor per class, from a small offline sample set and requires no fine-tuning. At inference time, a multi-anchor steering mechanism redirects each query’s activations toward the anchor-defined authorized region associated with the current user, thereby suppressing over-privileged generations by design. Finally, extensive experiments across three LLM families demonstrate that AAAC reduces permission violation rates by up to 86.5% and prompt-based attack success rates by 90.7%, while improving response usability with minor inference overhead compared to baselines.</p>
  </div>
</details>

<hr>
<h3 id="47-Temporal-Spatial-Decouple-before-Act-Disentangled-Representation-Learning-for-Multimodal-Sentiment-Analysis-cs-CL-cs-AI-cs-MMPDF"><a href="#47-Temporal-Spatial-Decouple-before-Act-Disentangled-Representation-Learning-for-Multimodal-Sentiment-Analysis-cs-CL-cs-AI-cs-MMPDF" class="headerlink" title="[47] Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis cs.CL | cs.AI | cs.MMPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13659">Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13659" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为TSDA（Temporal-Spatial Decouple before Act）的新方法，用于多模态情感分析。该方法在模态交互前，将每个模态（语言、视觉、声学）的信号显式解耦为时间动态和空间结构上下文，分别进行编码和对齐，最后通过门控机制重新耦合用于任务。实验表明该方法优于基线模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有主流方法基于模态不变&#x2F;特定因子分解或复杂融合，但仍依赖于时空混合建模，忽略了时空异质性，导致时空信息不对称和性能受限。</p>
<p><strong>Result:</strong> 广泛的实验表明，TSDA在基准测试中超越了基线模型。消融分析研究证实了其设计的必要性和可解释性。</p>
<p><strong>Insight:</strong> 核心创新点在于在交互前显式解耦每个模态的时空信息，并进行因子一致的对齐与解相关正则化，以减少跨因子信息泄漏，同时保留互补性。这为解决多模态中的时空异质性问题提供了新思路。</p>
<p><strong>Abstract:</strong> Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.</p>
  </div>
</details>

<hr>
<h3 id="48-Dr-Assistant-Enhancing-Clinical-Diagnostic-Inquiry-via-Structured-Diagnostic-Reasoning-Data-and-Reinforcement-Learning-cs-CLPDF"><a href="#48-Dr-Assistant-Enhancing-Clinical-Diagnostic-Inquiry-via-Structured-Diagnostic-Reasoning-Data-and-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[48] Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning cs.CLPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13690">Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13690" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Guo, Fanfu Wang, Jianwei Lv, Xincheng Shi, Yuchen Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Dr. Assistant的临床诊断辅助模型，通过引入临床诊断推理数据结构和两阶段训练方法，旨在提升大型语言模型在临床诊断推理和问询方面的能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有临床决策支持系统维护成本高、泛化能力差，以及大型语言模型在诊断推理和问询技能方面受限的问题。</p>
<p><strong>Result:</strong> 实验表明，Dr. Assistant在开源模型中表现最优，并与闭源模型性能相当，在作者提出的诊断推理与问询评估基准上提供了有效的临床诊断问询指导方案。</p>
<p><strong>Insight:</strong> 创新点在于设计了结构化临床诊断推理数据来捕捉抽象临床逻辑，并采用监督微调结合定制化奖励函数的强化学习两阶段训练策略，以系统性地提升模型的诊断推理链和主动问询能力。</p>
<p><strong>Abstract:</strong> Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.</p>
  </div>
</details>

<hr>
<h3 id="49-OptiSQL-Executable-SQL-Generation-from-Optical-TokensOptiSQL-Executable-SQL-Generation-from-Optical-Tokens-cs-CLPDF"><a href="#49-OptiSQL-Executable-SQL-Generation-from-Optical-TokensOptiSQL-Executable-SQL-Generation-from-Optical-Tokens-cs-CLPDF" class="headerlink" title="[49] OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens cs.CLPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13695">OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13695" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sifan Li, Hongkai Chen, Yujun Cai, Liyang Chen, Qingwen Ye</span></p>
<p><strong>TL;DR:</strong> OptiSQL是一个视觉驱动的框架，它直接从表格图像和自然语言问题生成可执行的SQL查询，使用紧凑的光学令牌作为高效接口。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统文本到SQL方法需要将表格完全线性化为文本模式所带来的巨大令牌开销问题，并适应表格以视觉形式出现在文档或网页中的现实场景。</p>
<p><strong>Result:</strong> 在可视化的Spider 2.0-Snow基准测试中，OptiSQL在保持强大执行准确性的同时，将表格输入令牌减少了一个数量级，并且鲁棒性分析表明光学令牌在视觉扰动下能保留基本结构信息。</p>
<p><strong>Insight:</strong> 创新点在于利用面向OCR的视觉编码器将表格结构和内容压缩成少量光学令牌，并冻结编码器以隔离表示充分性，从而实现了从光学表示直接生成SQL的高效方法。</p>
<p><strong>Abstract:</strong> Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.</p>
  </div>
</details>

<hr>
<h3 id="50-Simulated-Ignorance-Fails-A-Systematic-Study-of-LLM-Behaviors-on-Forecasting-Problems-Before-Model-Knowledge-Cutoff-cs-CL-cs-AIPDF"><a href="#50-Simulated-Ignorance-Fails-A-Systematic-Study-of-LLM-Behaviors-on-Forecasting-Problems-Before-Model-Knowledge-Cutoff-cs-CL-cs-AIPDF" class="headerlink" title="[50] Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff cs.CL | cs.AIPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13717">Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13717" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi</span></p>
<p><strong>TL;DR:</strong> 本文系统评估了大型语言模型在预测问题上的表现，发现模拟无知方法无法有效近似真实无知状态，提示指令无法可靠地’回滚’模型知识，因此基于模拟无知的回顾性预测评估方法存在根本缺陷。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决评估LLM预测能力时面临的方法论困境：前瞻性评估延迟过高，而回顾性预测因模型知识截止日期不断更新导致清洁评估数据迅速减少，模拟无知方法被提出作为潜在解决方案但缺乏系统性验证。</p>
<p><strong>Result:</strong> 在477个竞赛级问题和9个模型上的实验表明：模拟无知与真实无知存在52%的性能差距；思维链推理无法有效抑制先验知识；推理优化模型虽然推理轨迹质量更高，但模拟无知保真度更差。</p>
<p><strong>Insight:</strong> 揭示了提示工程在控制模型知识激活方面的根本局限性，证明基于模拟无知的回顾性预测评估方法论存在缺陷，为未来预测能力评估提供了重要的方法论警示。</p>
<p><strong>Abstract:</strong> Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) – evaluating on already-resolved events – faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably “rewind” model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.</p>
  </div>
</details>

<hr>
<h3 id="51-On-Temperature-Constrained-Non-Deterministic-Machine-Translation-Potential-and-Evaluation-cs-CLPDF"><a href="#51-On-Temperature-Constrained-Non-Deterministic-Machine-Translation-Potential-and-Evaluation-cs-CLPDF" class="headerlink" title="[51] On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation cs.CLPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13729">On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13729" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weichuan Wang, Mingyang Liu, Linqi Song, Chen Ma</span></p>
<p><strong>TL;DR:</strong> 本文系统性地评估了现代机器翻译系统中的温度约束非确定性机器翻译现象，发现其在解决多模态问题方面具有潜力，但同时也带来了评估挑战。研究揭示了ND-MT系统中存在的’水桶效应’，并提出了ExpectoSample策略以自动评估指标可靠性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 机器翻译作为复杂的非确定性NLP任务，其非确定性特性尚未被充分探索，且现有评估框架无法有效处理ND-MT带来的性能评估不一致问题。</p>
<p><strong>Result:</strong> 在三个开放数据集上使用词法和语义指标评估了五个SOTA ND-MT系统，发现ND-MT生成的最低质量候选译文在不同采样规模下决定了系统整体排名，即存在’水桶效应’。</p>
<p><strong>Insight:</strong> 创新点在于首次系统界定并实证了温度约束ND-MT现象及其评估困境，提出的’水桶效应’揭示了ND-MT评估的本质矛盾，ExpectoSample策略为选择鲁棒ND-MT系统提供了自动化评估新思路。</p>
<p><strong>Abstract:</strong> In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.</p>
  </div>
</details>

<hr>
<h3 id="52-Dimension-First-Evaluation-of-Speech-to-Speech-Models-with-Structured-Acoustic-Cues-cs-CLPDF"><a href="#52-Dimension-First-Evaluation-of-Speech-to-Speech-Models-with-Structured-Acoustic-Cues-cs-CLPDF" class="headerlink" title="[52] Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues cs.CLPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13742">Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13742" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arjun Chandra, Kevin Miller, Venkatesh Ravichandran, Constantinos Papayiannis, Venkatesh Saligrama</span></p>
<p><strong>TL;DR:</strong> 本文提出了TRACE框架，通过让大型语言模型（LLM）对音频线索进行文本化推理，实现高效、低成本且与人类评价对齐的语音到语音（S2S）模型评估。该方法首先引入人类思维链（HCoT）标注协议，将评估分解为内容、音质和副语言三个维度，并构建音频信号的文本蓝图供LLM进行维度判断，最后通过确定性策略融合为总体评分。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前自动S2S评估方法依赖不透明且昂贵的音频语言模型（ALMs），而具备强大推理能力的LLM仅限于处理文本内容。本文旨在解决这一问题，使LLM能够利用音频线索进行推理，从而实现成本效益高且与人类评价一致的S2S评估。</p>
<p><strong>Result:</strong> TRACE框架在S2S评估任务中，与人类评分者的一致性高于ALMs和仅基于转录文本的LLM评估方法，同时显著降低了成本。</p>
<p><strong>Insight:</strong> 创新点在于将音频信号转化为结构化文本蓝图（包含内容、音质、副语言等维度），使LLM能够进行多维度推理评估，并结合人类思维链标注提升诊断能力。该方法为S2S评估提供了一种可扩展、低成本且与人类对齐的新范式。</p>
<p><strong>Abstract:</strong> Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.</p>
  </div>
</details>

<hr>
<h3 id="53-Knowledge-Graph-Assisted-LLM-Post-Training-for-Enhanced-Legal-Reasoning-cs-CL-cs-LGPDF"><a href="#53-Knowledge-Graph-Assisted-LLM-Post-Training-for-Enhanced-Legal-Reasoning-cs-CL-cs-LGPDF" class="headerlink" title="[53] Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning cs.CL | cs.LGPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13806">Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13806" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dezhao Song, Guglielmo Bonifazi, Frank Schilder, Jonathan Richard Schwarz</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种知识图谱辅助的大语言模型后训练方法，旨在提升模型在法律领域的推理能力。该方法基于IRAC框架构建了一个包含12K法律案例的知识图谱，并利用该图谱生成训练数据，对三种不同架构和规模的SOTA大语言模型进行了监督微调和直接偏好优化。实验表明，后训练模型在多个法律基准测试上优于基线模型，特别是在推理任务上表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大语言模型的后训练主要依赖大规模文本语料和人类反馈，缺乏对领域知识结构的捕捉，导致模型在处理复杂推理任务（尤其是法律等高风险专业领域）时存在困难。法律推理需要深刻理解法律概念间的关系，而现有方法未能有效整合此类结构化知识。</p>
<p><strong>Result:</strong> 在4&#x2F;5个多样化的法律基准测试（涵盖14个任务）上，后训练模型取得了优于基线模型的平均性能。具体而言，70B参数的DPO模型在4&#x2F;6个推理任务上取得了最佳成绩，其表现优于基线模型和一个141B参数的SOTA法律大语言模型。</p>
<p><strong>Insight:</strong> 创新点在于将领域知识图谱（基于IRAC框架构建）系统地整合到大语言模型的后训练中，以增强其结构化推理能力。该方法通过图谱生成训练数据并结合SFT和DPO进行优化，为高风险专业领域（如法律）的模型能力提升提供了一种可推广的范式。</p>
<p><strong>Abstract:</strong> LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs’ reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4&#x2F;5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4&#x2F;6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs’ legal reasoning capability.</p>
  </div>
</details>

<hr>
<h3 id="54-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs-cs-CL-cs-CV-cs-MMPDF"><a href="#54-FutureOmni-Evaluating-Future-Forecasting-from-Omni-Modal-Context-for-Multimodal-LLMs-cs-CL-cs-CV-cs-MMPDF" class="headerlink" title="[54] FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs cs.CL | cs.CV | cs.MMPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13836">FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13836" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qian Chen, Jinlan Fu, Changsong Li, See-Kiong Ng, Xipeng Qiu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了首个用于评估多模态大语言模型（MLLMs）从视听环境中进行全模态未来事件预测能力的基准测试FutureOmni，并构建了一个指令微调数据集和Omni-Modal Future Forecasting（OFF）训练策略以提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准主要关注回顾性理解，而MLLMs从视听线索预测未来事件的能力尚未被充分探索，因此需要建立一个专门的评估基准来填补这一空白。</p>
<p><strong>Result:</strong> 在FutureOmni基准上评估了13个全模态和7个纯视频模型，结果显示当前系统在视听未来预测（尤其是语音密集场景）上表现不佳，最佳模型Gemini 3 Flash的准确率仅为64.8%；提出的OFF训练策略在FutureOmni及其他流行基准上均提升了未来预测和泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个专注于全模态未来预测的基准测试FutureOmni，并提出了一个可扩展的LLM辅助、人类在环的数据构建流程以及OFF指令微调策略，以增强模型的跨模态因果时序推理和知识利用能力。</p>
<p><strong>Abstract:</strong> Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (<a target="_blank" rel="noopener" href="https://github.com/OpenMOSS/FutureOmni">https://github.com/OpenMOSS/FutureOmni</a>) and datasets (<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni">https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni</a>).</p>
  </div>
</details>

<hr>
<h3 id="55-Pedagogical-Alignment-for-Vision-Language-Action-Models-A-Comprehensive-Framework-for-Data-Architecture-and-Evaluation-in-Education-cs-CLPDF"><a href="#55-Pedagogical-Alignment-for-Vision-Language-Action-Models-A-Comprehensive-Framework-for-Data-Architecture-and-Evaluation-in-Education-cs-CLPDF" class="headerlink" title="[55] Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education cs.CLPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13876">Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13876" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Unggi Lee, Jahyun Jeong, Sunyoung Shin, Haeun Park, Jeongsu Moon</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为’教学对齐VLA框架’的综合框架，旨在使轻量级视觉-语言-动作模型适用于资源受限的教育环境。该框架通过文本修复、LLM知识蒸馏、安全训练和教学评估四个组件，使模型在保持任务性能的同时，能够生成适合科学教育的解释性文本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前VLA模型需要大量计算资源且牺牲了语言生成能力以追求效率，不适合需要可解释、能生成解释的教育环境。为了解决教师在安全、一致地进行科学演示时面临的挑战，并利用机器人辅助STEM教育，本文旨在开发一个适用于教育场景的轻量级、具备教学能力的VLA框架。</p>
<p><strong>Result:</strong> 在涵盖物理、化学、生物和地球科学的五个科学演示任务上进行了评估。实验结果表明，该框架在任务性能（成功率、协议合规性、效率、安全性）上与基线模型相当，同时能生成上下文恰当的教育解释。评估结合了教师调查和LLM-as-Judge方法。</p>
<p><strong>Insight:</strong> 创新点在于将’教学对齐’概念系统性地引入VLA模型设计，通过一个包含数据、架构和评估的完整框架来同时优化任务执行和教学输出。具体技术贡献包括文本修复以恢复语言能力、LLM蒸馏传递教学知识、针对教育环境的安全训练，以及专门为科学教育调整的评估框架。</p>
<p><strong>Abstract:</strong> Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.</p>
  </div>
</details>

<hr>
<h3 id="56-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization-cs-CLPDF"><a href="#56-AgentEHR-Advancing-Autonomous-Clinical-Decision-Making-via-Retrospective-Summarization-cs-CLPDF" class="headerlink" title="[56] AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization cs.CLPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13918">AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13918" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yusheng Liao, Chuan Xuan, Yutong Cai, Lina Yang, Zhe Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了AgentEHR基准和RetroSum框架，旨在解决LLM在原始、高噪声电子健康记录中自主导航和进行复杂临床决策（如诊断和治疗规划）的挑战。通过引入回顾性总结机制和动态经验策略，该方法有效防止了长上下文信息丢失和推理连续性断裂，显著提升了任务性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LLM在医疗领域的应用多依赖精心处理的输入和简化的检索任务，与真实、原始、高噪声的临床环境存在差距。本文旨在弥合这一差距，推动LLM在原始EHR数据中进行自主、长程、交互式推理的复杂决策任务。</p>
<p><strong>Result:</strong> 在提出的AgentEHR基准上进行广泛评估，RetroSum框架相比竞争基线实现了高达29.16%的性能提升，同时将总交互错误显著降低了高达92.3%。</p>
<p><strong>Insight:</strong> 核心创新点在于将回顾性总结机制与动态演化的经验策略相结合。回顾性机制通过动态重估交互历史来防止长上下文信息丢失并确保逻辑连贯性；演化策略则通过从记忆库中检索累积经验来弥合领域差距，这对于处理原始、复杂、时序性的EHR数据具有借鉴意义。</p>
<p><strong>Abstract:</strong> Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.</p>
  </div>
</details>

<hr>
<h3 id="57-HyperWalker-Dynamic-Hypergraph-Based-Deep-Diagnosis-for-Multi-Hop-Clinical-Modeling-across-EHR-and-X-Ray-in-Medical-VLMs-cs-CL-cs-CVPDF"><a href="#57-HyperWalker-Dynamic-Hypergraph-Based-Deep-Diagnosis-for-Multi-Hop-Clinical-Modeling-across-EHR-and-X-Ray-in-Medical-VLMs-cs-CL-cs-CVPDF" class="headerlink" title="[57] HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs cs.CL | cs.CVPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13919">HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13919" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuezhe Yang, Hao Wang, Yige Peng, Jinman Kim, Lei Bi</span></p>
<p><strong>TL;DR:</strong> 本文提出HyperWalker，一种基于动态超图的深度诊断框架，用于整合电子健康记录（EHR）和X射线图像的多模态临床数据，通过强化学习智能体在超图中导航以识别最优诊断路径，并在测试时训练中采用多跳正交检索策略来增强临床推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有医学视觉语言模型（VLMs）通常采用样本隔离的推理范式，独立处理病例，无法利用纵向EHR数据或相关患者案例的结构化信息，限制了仅基于图像的推理，忽略了外部补充医学证据。</p>
<p><strong>Result:</strong> 在MIMIC数据集上的医学报告生成（MRG）和EHRXQA数据集上的医学视觉问答（VQA）实验中，HyperWalker实现了最先进的性能。</p>
<p><strong>Insight:</strong> 创新点包括构建动态超图（iBrochure）来建模EHR数据的结构异质性和多模态临床信息的高阶关联，以及引入强化学习导航器（Walker）和多跳正交检索的linger机制，以在测试时迭代选择临床互补的邻域案例，从而提升诊断的全面性和准确性。</p>
<p><strong>Abstract:</strong> Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/Bean-Young/HyperWalker">https://github.com/Bean-Young/HyperWalker</a></p>
  </div>
</details>

<hr>
<h3 id="58-Automatic-Prompt-Optimization-for-Dataset-Level-Feature-Discovery-cs-CLPDF"><a href="#58-Automatic-Prompt-Optimization-for-Dataset-Level-Feature-Discovery-cs-CLPDF" class="headerlink" title="[58] Automatic Prompt Optimization for Dataset-Level Feature Discovery cs.CLPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13922">Automatic Prompt Optimization for Dataset-Level Feature Discovery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13922" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Adrian Cosma, Oleg Szehr, David Kletz, Alessandro Antonucci, Olivier Pelletier</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种自动提示优化框架，用于从无结构文本中进行数据集级特征发现。该方法将特征发现定义为数据集级提示优化问题，通过多智能体协作迭代优化特征定义，以生成可解释且具有判别性的全局特征集，从而优化下游监督学习目标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前从无结构文本中提取特征的方法主要依赖手工设计的提示或固定特征模式，缺乏自动化且难以扩展到数据集级别。本文旨在解决这一问题，通过自动优化提示来发现全局可解释特征，以提升下游分类任务的性能。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体的定量实验结果或基准测试，但声称该方法提供了一种原则性机制，用于从无结构文本中自动发现特征，与依赖逐样本监督的先前提示优化方法不同。</p>
<p><strong>Insight:</strong> 创新点在于将特征发现重新定义为数据集级提示优化问题，并引入多智能体框架进行迭代优化，强调全局特征集的生成而非逐样本预测，这为自动化特征工程提供了新思路。</p>
<p><strong>Abstract:</strong> Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.</p>
  </div>
</details>

<hr>
<h3 id="59-“The-Whole-Is-Greater-Than-the-Sum-of-Its-Parts”-A-Compatibility-Aware-Multi-Teacher-CoT-Distillation-Framework-cs-CL-cs-AIPDF"><a href="#59-“The-Whole-Is-Greater-Than-the-Sum-of-Its-Parts”-A-Compatibility-Aware-Multi-Teacher-CoT-Distillation-Framework-cs-CL-cs-AIPDF" class="headerlink" title="[59] “The Whole Is Greater Than the Sum of Its Parts”: A Compatibility-Aware Multi-Teacher CoT Distillation Framework cs.CL | cs.AIPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13992">“The Whole Is Greater Than the Sum of Its Parts”: A Compatibility-Aware Multi-Teacher CoT Distillation Framework</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13992" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为COMPACT的兼容性感知多教师思维链（CoT）蒸馏框架，旨在解决现有单教师CoT蒸馏方法因教师模型能力偏差和灾难性遗忘而限制学生模型潜力的问题。该框架通过动态加权教师梯度，自适应地融合不同教师的监督，其核心是引入一个多维兼容性度量，包括基于图的共识、基于互信息的适应性和基于损失的难度评估，以过滤误导性推理路径、检测学生真正理解推理过程的“顿悟时刻”，并评估学生对教师指导的接受度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有CoT蒸馏方法通常依赖单一教师模型，但由于单个大语言模型（LLM）往往存在特定的能力偏差并可能遭受灾难性遗忘，这限制了学生模型（SLM）的潜力。虽然利用多样化的教师模型具有吸引力，但有效融合它们的监督仍然具有挑战性，因为教师-学生的不兼容性可能放大幻觉，而被动监督无法确保学生真正内化逻辑。</p>
<p><strong>Result:</strong> 广泛的实验和潜在空间分析表明，COMPACT能够在不损害模型原有知识结构的情况下有效整合多样化的推理能力，在多个基准测试中取得了最先进的性能，同时缓解了灾难性遗忘。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个多维兼容性度量来动态评估和加权教师梯度，从而自适应地融合多教师监督。这包括：1）基于图的共识来识别主流推理路径以过滤误导性理由；2）基于互信息的适应性来检测学生真正理解推理过程的“顿悟时刻”，而非仅仅模仿；3）基于损失的难度来评估学生对教师指导的接受度并防止负迁移。这种机制确保了学生模型能够从多个教师中有效学习，提升推理能力并保持知识稳定性。</p>
<p><strong>Abstract:</strong> Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student’s potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student’s real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect “epiphany moments” for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher’s guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model’s original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.</p>
  </div>
</details>

<hr>
<h3 id="60-BACH-V-Bridging-Abstract-and-Concrete-Human-Values-in-Large-Language-Models-cs-CLPDF"><a href="#60-BACH-V-Bridging-Abstract-and-Concrete-Human-Values-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[60] BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models cs.CLPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14007">BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14007" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junyu Zhang, Yipeng Kang, Jiong Guo, Jiayu Zhan, Junqi Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种抽象-具身框架，将概念理解分解为三种能力：抽象概念解释、抽象概念在具体事件中的具身化，以及抽象原则在具体决策中的应用。以人类价值观为测试平台，通过探测和干预技术，研究了多个开源大语言模型在十个价值维度上的表现。研究发现，模型内部存在结构化的价值表征，能够桥接抽象概念与具体行动，为构建价值驱动的自主AI系统提供了机制基础。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探究大语言模型是真正理解抽象概念，还是仅仅将其作为统计模式进行操作。以语义丰富且对齐核心的人类价值观为切入点，旨在理解模型如何桥接抽象概念与具体情境。</p>
<p><strong>Result:</strong> 在六个开源LLM和十个价值维度上，探测实验表明，仅基于抽象价值描述训练的探测模型能可靠地在具体事件叙述和决策推理中检测到相同价值，证明了跨层级的迁移能力。干预实验揭示了不对称性：干预价值表征能因果性地改变具体判断和决策，但不会改变抽象解释，表明编码的抽象价值充当了稳定的锚点。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个分解概念理解的抽象-具身框架，并综合运用探测和干预方法来研究模型内部的价值表征。客观分析认为，其揭示了LLM内部价值表征的结构化特性及其在抽象与具体之间的桥梁作用，为AI对齐提供了更透明、可泛化的机制性理解和操作基础。</p>
<p><strong>Abstract:</strong> Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.</p>
  </div>
</details>

<hr>
<h3 id="61-RM-Distiller-Exploiting-Generative-LLM-for-Reward-Model-Distillation-cs-CLPDF"><a href="#61-RM-Distiller-Exploiting-Generative-LLM-for-Reward-Model-Distillation-cs-CLPDF" class="headerlink" title="[61] RM-Distiller: Exploiting Generative LLM for Reward Model Distillation cs.CLPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14032">RM-Distiller: Exploiting Generative LLM for Reward Model Distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14032" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongli Zhou, Hui Huang, Wei Liu, Chenglong Wang, Xingyuan Bu</span></p>
<p><strong>TL;DR:</strong> 本文提出RM-Distiller框架，旨在系统性地利用生成式大语言模型的多方面能力进行奖励模型蒸馏，以解决现有方法仅将教师模型视为简单二元标注器、未能充分利用其丰富知识的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于高质量人类偏好标注获取困难，从生成式LLMs中蒸馏偏好已成为标准实践，但现有方法未能充分挖掘教师模型的知识与能力用于奖励模型蒸馏。</p>
<p><strong>Result:</strong> 大量实验表明，RM-Distiller在奖励模型基准测试和基于强化学习的对齐任务上均显著优于传统蒸馏方法。</p>
<p><strong>Insight:</strong> 创新点在于系统性地利用教师LLMs的三种能力：精炼能力（合成高相关响应对以创建细粒度对比信号）、评分能力（通过边界感知优化目标引导RM捕捉精确偏好强度）和生成能力（引入教师生成分布以正则化RM，保留其基础语言知识）。这是首次对生成式LLMs进行奖励模型蒸馏的系统性研究。</p>
<p><strong>Abstract:</strong> Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher’s generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.</p>
  </div>
</details>

<hr>
<h3 id="62-Top-10-Open-Challenges-Steering-the-Future-of-Diffusion-Language-Model-and-Its-Variants-cs-CL-cs-AIPDF"><a href="#62-Top-10-Open-Challenges-Steering-the-Future-of-Diffusion-Language-Model-and-Its-Variants-cs-CL-cs-AIPDF" class="headerlink" title="[62] Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants cs.CL | cs.AIPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14041">Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14041" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen</span></p>
<p><strong>TL;DR:</strong> 本文是一篇前瞻性论文，探讨了扩散语言模型（DLMs）作为自回归（AR）大语言模型替代方案的潜力与挑战。论文指出，尽管DLMs在文本生成上提供了更全局、可迭代的生成范式，但其发展仍受限于AR遗留框架。作者识别了阻碍DLMs突破的十大核心挑战，并提出了一个围绕基础架构、算法优化、认知推理和统一多模态智能四大支柱的战略路线图，旨在推动DLMs实现其“GPT-4时刻”。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大语言模型（LLMs）主要基于自回归（AR）架构，该架构存在因果瓶颈，限制了模型对文本全局结构的预见能力和迭代优化能力。扩散语言模型（DLMs）作为一种整体、双向的去噪过程，提供了变革性的替代方案，但其潜力因受限于AR遗留的基础设施和优化框架而未被充分挖掘。本文旨在识别并解决这些根本性挑战。</p>
<p><strong>Result:</strong> 本文是一篇观点性论文，未报告具体的定量实验结果或基准测试。其核心成果是系统性地提出了十大开放挑战和一个战略路线图，为DLMs的未来发展指明了方向。</p>
<p><strong>Insight:</strong> 论文的主要创新点在于系统性识别了DLMs发展的十大挑战（如架构惯性、梯度稀疏性、线性推理局限等），并提出了一个构建“扩散原生”生态系统的战略框架，其核心思想包括多尺度分词、主动重掩码和潜在思维等。从客观角度看，该研究为从AR主导范式向DLM范式的范式转变提供了清晰的理论分析和路线图，强调了超越因果视野、实现复杂结构推理和动态自校正的重要性，对下一代AI（特别是多模态智能）的发展具有指导意义。</p>
<p><strong>Abstract:</strong> The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential <code>brick-by-brick&#39;&#39; process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their </code>GPT-4 moment’’. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.</p>
  </div>
</details>

<hr>
<h3 id="63-XCR-Bench-A-Multi-Task-Benchmark-for-Evaluating-Cultural-Reasoning-in-LLMs-cs-CL-cs-AI-cs-CYPDF"><a href="#63-XCR-Bench-A-Multi-Task-Benchmark-for-Evaluating-Cultural-Reasoning-in-LLMs-cs-CL-cs-AI-cs-CYPDF" class="headerlink" title="[63] XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs cs.CL | cs.AI | cs.CYPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14063">XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14063" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Shaoxiong Ji, Hassan Alhuzali</span></p>
<p><strong>TL;DR:</strong> 本文介绍了XCR-Bench，一个用于评估大型语言模型跨文化推理能力的多任务基准，包含4900个平行句子和1098个独特的文化特定项，覆盖三个推理任务。研究发现，当前最先进的LLM在识别和适应社交礼仪与文化参照相关的文化特定项时存在一致弱点，并揭示了模型在文化适应中编码的区域和民族宗教偏见。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于缺乏高质量、带有文化特定项标注的平行跨文化句子对语料库，评估LLM跨文化能力（包括识别文化特定项并适应不同文化背景）的进展受限，因此构建了XCR-Bench基准来解决这一限制。</p>
<p><strong>Result:</strong> 在XCR-Bench基准上的评估表明，最先进的LLM在识别和适应社交礼仪与文化参照相关的文化特定项时表现出一致弱点，并显示出在单一语言设置中进行文化适应时编码的区域和民族宗教偏见。</p>
<p><strong>Insight:</strong> 创新点在于将Newmark的文化特定项框架与Hall的文化三元论结合，构建了一个系统分析文化推理的基准，超越了表层文化元素，深入半可见和不可见的文化层面（如社会规范、信仰和价值观），为跨文化NLP研究提供了新工具。</p>
<p><strong>Abstract:</strong> Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark’s CSI framework with Hall’s Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.</p>
  </div>
</details>

<hr>
<h3 id="64-NewsRECON-News-article-REtrieval-for-image-CONtextualization-cs-CLPDF"><a href="#64-NewsRECON-News-article-REtrieval-for-image-CONtextualization-cs-CLPDF" class="headerlink" title="[64] NewsRECON: News article REtrieval for image CONtextualization cs.CLPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14121">NewsRECON: News article REtrieval for image CONtextualization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14121" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jonathan Tonglet, Iryna Gurevych, Tinne Tuytelaars, Marie-Francine Moens</span></p>
<p><strong>TL;DR:</strong> 本文提出NewsRECON方法，用于在无法使用反向图像搜索（RIS）证据的情况下，通过将新闻图像链接到相关新闻文章，利用文章元数据推断图像的拍摄时间和地点。该方法整合了双编码器检索事件相关文章，以及两个交叉编码器根据地点和事件一致性对文章进行重排序。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决新闻图像时间和地点识别中，当反向图像搜索（RIS）工具无法返回结果时的挑战，为记者和取证专家提供更可靠的图像上下文推断方法。</p>
<p><strong>Result:</strong> 在TARA和5Pils-OOC数据集上的实验表明，NewsRECON优于先前工作，且与多模态大语言模型结合时，在无RIS证据情况下取得了新的SOTA结果。</p>
<p><strong>Insight:</strong> 创新点在于将图像上下文推断问题转化为新闻文章检索任务，通过结合双编码器检索和交叉编码器重排序的多阶段方法，有效利用文章元数据中的时空信息，提升了在缺乏直接视觉匹配证据时的推理能力。</p>
<p><strong>Abstract:</strong> Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.</p>
  </div>
</details>

<hr>
<h3 id="65-Domain-Adaptation-through-Synthetic-Data-Fine-Tuning-Large-Language-Models-for-German-Law-cs-CL-cs-AIPDF"><a href="#65-Domain-Adaptation-through-Synthetic-Data-Fine-Tuning-Large-Language-Models-for-German-Law-cs-CL-cs-AIPDF" class="headerlink" title="[65] Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law cs.CL | cs.AIPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14160">Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14160" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Jule Berghaus</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种通过合成数据生成方法，将大型语言模型（LLMs）适配到德国法律问答领域的有效途径。该方法直接从权威德国法规中系统性地生成高质量、多样化且法律准确的问答对，并利用自动化过滤和参数高效微调技术，显著提升了LLMs在德国法律问答任务上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决LLMs在法律推理等专业领域因专家知识有限而产生事实错误或幻觉的问题，特别是在德国法律领域缺乏高质量标注数据的情况下。</p>
<p><strong>Result:</strong> 使用本文合成数据微调的LLMs在德国法律问答任务上显著优于基线模型，证明了精心设计的合成数据可作为高风险、知识密集型领域中人工标注的可靠替代方案。</p>
<p><strong>Insight:</strong> 创新点在于直接从权威法规生成高质量合成数据的方法，以及结合自动化过滤与参数高效微调的技术路径，为领域适配提供了一种成本效益高且可靠的解决方案。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.</p>
  </div>
</details>

<hr>
<h3 id="66-Which-Reasoning-Trajectories-Teach-Students-to-Reason-Better-A-Simple-Metric-of-Informative-Alignment-cs-CLPDF"><a href="#66-Which-Reasoning-Trajectories-Teach-Students-to-Reason-Better-A-Simple-Metric-of-Informative-Alignment-cs-CLPDF" class="headerlink" title="[66] Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment cs.CLPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14249">Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14249" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuming Yang, Mingyoung Lai, Wanxu Zhao, Xiaoran Fan, Zhiheng Xi</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Rank-Surprisal Ratio (RSR)的简单度量指标，用于评估思维链轨迹在教师-学生大语言模型知识蒸馏中的适用性。该指标通过结合令牌的平均排名和负对数似然，同时捕捉轨迹的对齐性和信息量，从而比现有方法更有效地预测学生模型的训练后性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法主要依赖学生模型的似然性来评估思维链轨迹的适用性，这倾向于选择与模型当前行为高度对齐但可能信息量不足的轨迹，而忽略了更具信息量的轨迹。本文旨在解决如何更全面地评估轨迹的适用性问题，因为更强的教师模型生成的轨迹并不总是能产生更好的学生模型。</p>
<p><strong>Result:</strong> 在五个学生模型和来自11个不同教师的推理轨迹上的实验表明，RSR与训练后性能具有强相关性（平均斯皮尔曼相关系数为0.86），优于现有度量指标。该指标在轨迹选择和教师选择任务中都证明了其实用性。</p>
<p><strong>Insight:</strong> 核心创新点是提出了RSR这一简单、可解释的度量指标，其动机源于观察到有效的推理轨迹通常结合了低绝对概率和相对高排名的令牌，从而平衡了学习信号的强度和行为对齐。这为知识蒸馏中的数据选择提供了一个新的、更有效的标准。</p>
<p><strong>Abstract:</strong> Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model’s current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory’s average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.</p>
  </div>
</details>

<hr>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="67-Domain-Specific-Self-Supervised-Pre-training-for-Agricultural-Disease-Classification-A-Hierarchical-Vision-Transformer-Study-cs-CV-cs-LGPDF"><a href="#67-Domain-Specific-Self-Supervised-Pre-training-for-Agricultural-Disease-Classification-A-Hierarchical-Vision-Transformer-Study-cs-CV-cs-LGPDF" class="headerlink" title="[67] Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study cs.CV | cs.LGPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11612">Domain-Specific Self-Supervised Pre-training for Agricultural Disease Classification: A Hierarchical Vision Transformer Study</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11612" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arnav S. Sonavane</span></p>
<p><strong>TL;DR:</strong> 本文研究了领域特定的自监督预训练对农业病害分类的影响，使用分层视觉Transformer（HierarchicalViT, HVT）。研究发现，仅用3000张未标记的农业图像进行SimCLR预训练，就能带来+4.57%的准确率提升，超过了分层架构设计带来的+3.70%增益。这种自监督学习的优势是架构无关的，在Swin-Base和ViT-Base上也分别实现了+4.08%和+4.20%的提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决农业病害分类任务中，如何有效利用领域特定数据提升模型性能的问题，并探讨在有限标注数据下，领域自监督预训练与先进模型架构设计（如分层Transformer）相比的相对重要性。</p>
<p><strong>Result:</strong> 在三个数据集上评估：棉花叶病数据集（7类，90.24%准确率）、PlantVillage（38类，96.3%）、PlantDoc（27类，87.1%）。在参数量相近的情况下，HVT-Base（78M参数）达到88.91%准确率，优于Swin-Base（88M参数）的87.23%（提升+1.68%）。模型校准分析显示HVT的预期校准误差（ECE）为3.56%（经温度缩放后降至1.52%）。</p>
<p><strong>Insight:</strong> 论文宣称的核心创新点是揭示了在农业病害分类等特定领域任务中，收集领域数据并进行自监督预训练，其性能提升可能比选择更复杂的模型架构更为关键（即“数据优先于架构”）。从客观角度看，这为资源有限的实践者提供了明确的优先级指导：优先投资于领域无标签数据收集与自监督预训练，而非一味追求更复杂的模型设计。</p>
<p><strong>Abstract:</strong> We investigate the impact of domain-specific self-supervised pre-training on agricultural disease classification using hierarchical vision transformers. Our key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural images provides a +4.57% accuracy improvement–exceeding the +3.70% gain from hierarchical architecture design. Critically, we show this SSL benefit is architecture-agnostic: applying the same pre-training to Swin-Base yields +4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain data collection over architectural choices. Using HierarchicalViT (HVT), a Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27 classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91% vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment reliability, we report calibration analysis showing HVT achieves 3.56% ECE (1.52% after temperature scaling). Code: <a target="_blank" rel="noopener" href="https://github.com/w2sg-arnav/HierarchicalViT">https://github.com/w2sg-arnav/HierarchicalViT</a></p>
  </div>
</details>

<hr>
<h3 id="68-Multi-modal-MRI-Based-Alzheimer’s-Disease-Diagnosis-with-Transformer-based-Image-Synthesis-and-Transfer-Learning-cs-CV-cs-LG-q-bio-NCPDF"><a href="#68-Multi-modal-MRI-Based-Alzheimer’s-Disease-Diagnosis-with-Transformer-based-Image-Synthesis-and-Transfer-Learning-cs-CV-cs-LG-q-bio-NCPDF" class="headerlink" title="[68] Multi-modal MRI-Based Alzheimer’s Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning cs.CV | cs.LG | q-bio.NCPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11614">Multi-modal MRI-Based Alzheimer’s Disease Diagnosis with Transformer-based Image Synthesis and Transfer Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG | q-bio.NC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11614" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jason Qiu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于Transformer的3D TransUNet图像合成框架，用于从常规T1加权MRI预测扩散MRI（dMRI）的分数各向异性（FA）和平均扩散率（MD）图，以解决dMRI采集耗时且易受运动伪影影响的问题。通过将合成的高保真dMRI特征（SSIM&gt;0.93，皮尔逊相关性&gt;0.94）与T1w MRI结合，构建多模态诊断模型，显著提升了阿尔茨海默病（AD）分类准确率（从78.75%增至83.75%）和轻度认知障碍（MCI）检测性能（提升12.5%）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于早期检测阿尔茨海默病至关重要，但临床常规使用的T1w MRI仅能识别晚期宏观脑部变化，而能检测早期微观结构异常的dMRI却因采集耗时、易受运动伪影限制而难以常规应用。因此，研究旨在从易获取的T1w MRI合成dMRI特征，以弥补多模态成像在临床中的可及性差距。</p>
<p><strong>Result:</strong> 在图像合成方面，模型生成的FA和MD图与真实dMRI相比，结构相似性指数（SSIM）超过0.93，皮尔逊相关性大于0.94，表明高保真合成。在多模态诊断中，AD分类准确率提升5%（达到83.75%），MCI检测性能提升12.5%，展示了合成特征的有效性。</p>
<p><strong>Insight:</strong> 论文的创新点在于利用Transformer-based的3D TransUNet框架从单模态T1w MRI合成多模态dMRI特征，实现了高质量微观结构信息的推断；客观分析认为，该方法通过迁移学习将多模态成像优势扩展到缺乏扩散数据的临床场景，有望提升AD诊断的可及性、效率和准确性，为医学图像合成与疾病诊断结合提供了新思路。</p>
<p><strong>Abstract:</strong> Alzheimer’s disease (AD) is a progressive neurodegenerative disorder in which pathological changes begin many years before the onset of clinical symptoms, making early detection essential for timely intervention. T1-weighted (T1w) Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to identify macroscopic brain alterations, but these changes typically emerge relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is sensitive to earlier microstructural abnormalities by probing water diffusion in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean diffusivity (MD), provide complementary information about white matter integrity and neurodegeneration. However, dMRI acquisitions are time-consuming and susceptible to motion artifacts, limiting their routine use in clinical populations. To bridge this gap, I propose a 3D TransUNet image synthesis framework that predicts FA and MD maps directly from T1w MRI. My model generates high-fidelity maps, achieving a structural similarity index (SSIM) exceeding 0.93 and a strong Pearson correlation (&gt;0.94) with ground-truth dMRI. When integrated into a multi-modal diagnostic model, these synthetic features boost AD classification accuracy by 5% (78.75%-&gt;83.75%) and, most importantly, improve mild cognitive impairment (MCI) detection by 12.5%. This study demonstrates that high-quality diffusion microstructural information can be inferred from routinely acquired T1w MRI, effectively transferring the benefits of multi-modality imaging to settings where diffusion data are unavailable. By reducing scan time while preserving complementary structural and microstructural information, the proposed approach has the potential to improve the accessibility, efficiency, and accuracy of AD diagnosis in clinical practice.</p>
  </div>
</details>

<hr>
<h3 id="69-KG-ViP-Bridging-Knowledge-Grounding-and-Visual-Perception-in-Multi-modal-LLMs-for-Visual-Question-Answering-cs-CVPDF"><a href="#69-KG-ViP-Bridging-Knowledge-Grounding-and-Visual-Perception-in-Multi-modal-LLMs-for-Visual-Question-Answering-cs-CVPDF" class="headerlink" title="[69] KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering cs.CVPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11632">KG-ViP: Bridging Knowledge Grounding and Visual Perception in Multi-modal LLMs for Visual Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11632" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiyang Li, Ao Ke, Yukun Cao, Xike Xie</span></p>
<p><strong>TL;DR:</strong> 论文提出KG-ViP框架，通过融合场景图和常识图来增强多模态大语言模型在视觉问答任务中的性能，旨在同时解决知识幻觉和细粒度视觉感知不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有VQA多模态大语言模型存在知识幻觉和细粒度视觉感知不足的双重局限，而常识图和场景图分别能提供外部知识和捕捉视觉细节，但以往工作孤立处理二者，未能发挥协同潜力。</p>
<p><strong>Result:</strong> 在FVQA 2.0+和MVQA基准测试上的大量实验表明，KG-ViP显著优于现有VQA方法。</p>
<p><strong>Insight:</strong> 创新点在于提出一个统一的检索与融合流程，利用查询作为语义桥梁逐步整合场景图和常识图，生成统一的结构化上下文以促进可靠的多模态推理，实现了两种图结构的协同增强。</p>
<p><strong>Abstract:</strong> Multi-modal Large Language Models (MLLMs) for Visual Question Answering (VQA) often suffer from dual limitations: knowledge hallucination and insufficient fine-grained visual perception. Crucially, we identify that commonsense graphs and scene graphs provide precisely complementary solutions to these respective deficiencies by providing rich external knowledge and capturing fine-grained visual details. However, prior works typically treat them in isolation, overlooking their synergistic potential. To bridge this gap, we propose KG-ViP, a unified framework that empowers MLLMs by fusing scene graphs and commonsense graphs. The core of the KG-ViP framework is a novel retrieval-and-fusion pipeline that utilizes the query as a semantic bridge to progressively integrate both graphs, synthesizing a unified structured context that facilitates reliable multi-modal reasoning. Extensive experiments on FVQA 2.0+ and MVQA benchmarks demonstrate that KG-ViP significantly outperforms existing VQA methods.</p>
  </div>
</details>

<hr>
<h3 id="70-Beyond-Accuracy-Evaluating-Grounded-Visual-Evidence-in-Thinking-with-Images-cs-CVPDF"><a href="#70-Beyond-Accuracy-Evaluating-Grounded-Visual-Evidence-in-Thinking-with-Images-cs-CVPDF" class="headerlink" title="[70] Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images cs.CVPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11633">Beyond Accuracy: Evaluating Grounded Visual Evidence in Thinking with Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11633" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuchen Li, Xuzhao Li, Renjie Pi, Shiyu Hu, Jian Zhao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了ViEBench，一个用于评估视觉语言模型（VLMs）忠实视觉推理能力的、过程可验证的基准测试。它包含200张多场景高分辨率图像及专家标注的视觉证据，将任务按难度分为感知和推理维度，并引入一个双轴矩阵提供细粒度评估指标。实验揭示了VLMs在推理过程中可能存在的证据错位和利用失败等问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准测试主要依赖结果导向的准确率，缺乏评估模型是否能准确利用细粒度视觉线索进行多步推理的能力，因此需要一个新的基准来评估VLMs推理过程的真实性。</p>
<p><strong>Result:</strong> 实验发现：（1）VLMs有时能给出正确答案，但其依据的视觉区域却是不相关的；（2）它们可能成功定位到正确证据，但仍无法利用该证据得出准确结论。ViEBench被证明能作为一个更具可解释性和实用性的基准，用于全面评估智能体VLMs的有效性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个过程可验证的、细粒度的评估基准（ViEBench），通过双轴矩阵诊断模型在不同任务复杂度下的行为，超越了仅关注最终答案准确性的传统评估方式，有助于揭示模型推理过程的内部机制和潜在缺陷。</p>
<p><strong>Abstract:</strong> Despite the remarkable progress of Vision-Language Models (VLMs) in adopting “Thinking-with-Images” capabilities, accurately evaluating the authenticity of their reasoning process remains a critical challenge. Existing benchmarks mainly rely on outcome-oriented accuracy, lacking the capability to assess whether models can accurately leverage fine-grained visual cues for multi-step reasoning. To address these limitations, we propose ViEBench, a process-verifiable benchmark designed to evaluate faithful visual reasoning. Comprising 200 multi-scenario high-resolution images with expert-annotated visual evidence, ViEBench uniquely categorizes tasks by difficulty into perception and reasoning dimensions, where reasoning tasks require utilizing localized visual details with prior knowledge. To establish comprehensive evaluation criteria, we introduce a dual-axis matrix that provides fine-grained metrics through four diagnostic quadrants, enabling transparent diagnosis of model behavior across varying task complexities. Our experiments yield several interesting observations: (1) VLMs can sometimes produce correct final answers despite grounding on irrelevant regions, and (2) they may successfully locate the correct evidence but still fail to utilize it to reach accurate conclusions. Our findings demonstrate that ViEBench can serve as a more explainable and practical benchmark for comprehensively evaluating the effectiveness agentic VLMs. The codes will be released at: <a target="_blank" rel="noopener" href="https://github.com/Xuchen-Li/ViEBench">https://github.com/Xuchen-Li/ViEBench</a>.</p>
  </div>
</details>

<hr>
<h3 id="71-When-Rules-Fall-Short-Agent-Driven-Discovery-of-Emerging-Content-Issues-in-Short-Video-Platforms-cs-CVPDF"><a href="#71-When-Rules-Fall-Short-Agent-Driven-Discovery-of-Emerging-Content-Issues-in-Short-Video-Platforms-cs-CVPDF" class="headerlink" title="[71] When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms cs.CVPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11634">When Rules Fall Short: Agent-Driven Discovery of Emerging Content Issues in Short Video Platforms</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11634" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenghui Yu, Hongwei Wang, Junwen Chen, Zixuan Wang, Bingfeng Deng</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于多模态大语言模型（LLM）智能体的自动方法，用于发现短视频平台中现有标注策略未覆盖的新兴内容问题。该方法通过自动召回潜在问题视频、两阶段聚类分组，并由智能体生成更新的标注策略，从而有效扩展对新兴问题的覆盖。该方法已在实际系统中部署。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 短视频平台趋势快速演变，每天都会出现现有标注策略无法覆盖的新内容问题。传统的人工发现问题方式速度太慢，导致标注策略更新延迟，对有效内容治理构成重大挑战。</p>
<p><strong>Result:</strong> 离线和在线实验表明，该基于智能体的方法显著提高了新兴问题发现的有效性（F1分数提升超过20%），并增强了后续问题治理的性能（问题视频的观看量减少约15%）。与人工发现问题相比，它大大降低了时间成本并加速了标注策略的迭代。</p>
<p><strong>Insight:</strong> 创新点在于利用多模态LLM智能体实现自动化、高效的新兴内容问题发现与策略更新闭环。该方法将问题发现从依赖人工的缓慢过程转变为基于智能体分析的快速、可扩展流程，为平台内容治理提供了新的自动化解决方案。</p>
<p><strong>Abstract:</strong> Trends on short-video platforms evolve at a rapid pace, with new content issues emerging every day that fall outside the coverage of existing annotation policies. However, traditional human-driven discovery of emerging issues is too slow, which leads to delayed updates of annotation policies and poses a major challenge for effective content governance. In this work, we propose an automatic issue discovery method based on multimodal LLM agents. Our approach automatically recalls short videos containing potential new issues and applies a two-stage clustering strategy to group them, with each cluster corresponding to a newly discovered issue. The agent then generates updated annotation policies from these clusters, thereby extending coverage to these emerging issues. Our agent has been deployed in the real system. Both offline and online experiments demonstrate that this agent-based method significantly improves the effectiveness of emerging-issue discovery (with an F1 score improvement of over 20%) and enhances the performance of subsequent issue governance (reducing the view count of problematic videos by approximately 15%). More importantly, compared to manual issue discovery, it greatly reduces time costs and substantially accelerates the iteration of annotation policies.</p>
  </div>
</details>

<hr>
<h3 id="72-Now-You-See-Me-Now-You-Don’t-A-Unified-Framework-for-Expression-Consistent-Anonymization-in-Talking-Head-Videos-cs-CVPDF"><a href="#72-Now-You-See-Me-Now-You-Don’t-A-Unified-Framework-for-Expression-Consistent-Anonymization-in-Talking-Head-Videos-cs-CVPDF" class="headerlink" title="[72] Now You See Me, Now You Don’t: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos cs.CVPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11635">Now You See Me, Now You Don’t: A Unified Framework for Expression Consistent Anonymization in Talking Head Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11635" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anil Egin, Andrea Tangherloni, Antitza Dantcheva</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Anon-NET的统一框架，用于在说话头部视频中进行表情一致的去身份化。该框架通过基于扩散的生成模型进行人脸修复，并利用高级属性识别和运动感知的表情迁移进行引导，最后通过视频驱动动画对去身份化的人脸进行动画处理，以在保护隐私的同时保留原视频的年龄、性别、种族、姿态和表情等属性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决人脸视频匿名化任务中，如何在有效去身份化以保护隐私的同时，保持视频的视觉真实性和时间一致性，并保留如表情、姿态等关键下游任务所需的非身份属性。</p>
<p><strong>Result:</strong> 在包含多样化面部动态的VoxCeleb2、CelebV-HQ和HDTF数据集上进行了广泛实验，结果表明Anon-NET在混淆身份方面有效，同时保持了视觉真实性和时间一致性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个统一框架，将基于扩散模型的人脸修复与高级属性引导和运动感知的表情迁移相结合，实现了表情一致的去身份化；其将去身份化与动画生成分离的两阶段流程设计，有助于更好地控制属性保留和身份混淆的平衡。</p>
<p><strong>Abstract:</strong> Face video anonymization is aimed at privacy preservation while allowing for the analysis of videos in a number of computer vision downstream tasks such as expression recognition, people tracking, and action recognition. We propose here a novel unified framework referred to as Anon-NET, streamlined to de-identify facial videos, while preserving age, gender, race, pose, and expression of the original video. Specifically, we inpaint faces by a diffusion-based generative model guided by high-level attribute recognition and motion-aware expression transfer. We then animate deidentified faces by video-driven animation, which accepts the de-identified face and the original video as input. Extensive experiments on the datasets VoxCeleb2, CelebV-HQ, and HDTF, which include diverse facial dynamics, demonstrate the effectiveness of AnonNET in obfuscating identity while retaining visual realism and temporal consistency. The code of AnonNet will be publicly released.</p>
  </div>
</details>

<hr>
<h3 id="73-Evaluating-Self-Correcting-Vision-Agents-Through-Quantitative-and-Qualitative-Metrics-cs-CVPDF"><a href="#73-Evaluating-Self-Correcting-Vision-Agents-Through-Quantitative-and-Qualitative-Metrics-cs-CVPDF" class="headerlink" title="[73] Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics cs.CVPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11637">Evaluating Self-Correcting Vision Agents Through Quantitative and Qualitative Metrics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11637" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aradhya Dixit</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种诊断性微基准，用于评估视觉语言代理（VLA）的自我纠正能力。通过将任务成功率（TSR）与纠正成功率（CSR）解耦，揭示了初始任务能力并不能预测修复能力，并量化了纠正尝试的收益递减效应（在三次重试后饱和）。失败分类表明，语义漂移（约28%的失败）是主要的推理瓶颈，这为开发状态感知、可信赖的多模态代理提供了可复现的评估框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前多模态基础模型的进展使得视觉语言代理能够将复杂视觉任务分解为基于工具的可执行计划，但现有基准对迭代自我纠正的定量限制和主导推理瓶颈缺乏深入刻画。</p>
<p><strong>Result:</strong> 在提出的诊断微基准上，任务成功率（TSR）为62%，而纠正成功率（CSR）仅为25%至33%，表明初始能力与修复能力不相关；纠正收益在三次重试后达到饱和；失败分类显示语义漂移约占失败案例的28%。</p>
<p><strong>Insight:</strong> 创新点在于设计了一个解耦任务成功与纠正成功的诊断性微基准，明确量化了自我纠正的收益递减规律，并通过失败分类（如语义漂移）识别出关键的推理瓶颈，为构建状态感知的多模态代理提供了可复现的评估路径。</p>
<p><strong>Abstract:</strong> Recent progress in multimodal foundation models has enabled Vision-Language Agents (VLAs) to decompose complex visual tasks into executable tool-based plans. While recent benchmarks have begun to evaluate iterative self-correction, its quantitative limits and dominant reasoning bottlenecks remain poorly characterized. This work introduces a Diagnostic Micro-Benchmark. Our analysis decouples Task Success Rate (TSR &#x3D; 62 percent) from Correction Success Rate (CSR &#x3D; 25 to 33 percent), revealing that initial competence does not predict repair ability. We explicitly quantify the diminishing returns of correction, which saturates after three retries. Our Failure Taxonomy reveals a frequent factor is Semantic Drift (about 28 percent of failures), a loss of contextual state. By isolating this reasoning bottleneck, this benchmark defines a reproducible framework toward stateful, trustworthy multimodal agents.</p>
  </div>
</details>

<hr>
<h3 id="74-Mixture-of-Distributions-Matters-Dynamic-Sparse-Attention-for-Efficient-Video-Diffusion-Transformers-cs-CV-cs-LGPDF"><a href="#74-Mixture-of-Distributions-Matters-Dynamic-Sparse-Attention-for-Efficient-Video-Diffusion-Transformers-cs-CV-cs-LGPDF" class="headerlink" title="[74] Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers cs.CV | cs.LGPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11641">Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient Video Diffusion Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11641" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuxi Liu, Yipeng Hu, Zekun Zhang, Kunze Jiang, Kun Yuan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MOD-DiT的新型免采样动态注意力框架，旨在解决视频扩散变换器（DiT）中自注意力机制二次复杂度带来的计算瓶颈。该方法通过两阶段过程（基于先验信息建模线性近似模型以预测注意力掩码，并结合在线块掩码策略动态应用）来高效且准确地建模动态稀疏注意力模式，从而在保证生成质量的同时显著加速视频生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频扩散变换器的自注意力机制具有二次复杂度，限制了其实际部署。现有的稀疏注意力方法要么依赖过于简化的静态模式，要么需要计算昂贵的采样操作来实现动态稀疏性，导致模式预测不准确和生成质量下降。</p>
<p><strong>Result:</strong> 在多个基准测试和模型架构上的广泛评估表明，MOD-DiT能实现一致的加速和质量提升，验证了其在高效、高质量视频生成方面的有效性，并克服了传统稀疏注意力方法的计算限制。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种免采样的动态稀疏注意力框架，通过两阶段过程（混合分布建模线性近似预测掩码，结合在线块掩码策略）来高效捕捉动态注意力模式，避免了重复采样开销，在计算效率和生成质量之间取得了更好的平衡。</p>
<p><strong>Abstract:</strong> While Diffusion Transformers (DiTs) have achieved notable progress in video generation, this long-sequence generation task remains constrained by the quadratic complexity inherent to self-attention mechanisms, creating significant barriers to practical deployment. Although sparse attention methods attempt to address this challenge, existing approaches either rely on oversimplified static patterns or require computationally expensive sampling operations to achieve dynamic sparsity, resulting in inaccurate pattern predictions and degraded generation quality. To overcome these limitations, we propose a \underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution \textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention framework that accurately models evolving attention patterns through a two-stage process. First, MOD-DiT leverages prior information from early denoising steps and adopts a {distributed mixing approach} to model an efficient linear approximation model, which is then used to predict mask patterns for a specific denoising interval. Second, an online block masking strategy dynamically applies these predicted masks while maintaining historical sparsity information, eliminating the need for repetitive sampling operations. Extensive evaluations demonstrate consistent acceleration and quality improvements across multiple benchmarks and model architectures, validating MOD-DiT’s effectiveness for efficient, high-quality video generation while overcoming the computational limitations of traditional sparse attention approaches.</p>
  </div>
</details>

<hr>
<h3 id="75-Predicting-When-to-Trust-Vision-Language-Models-for-Spatial-Reasoning-cs-CV-cs-AIPDF"><a href="#75-Predicting-When-to-Trust-Vision-Language-Models-for-Spatial-Reasoning-cs-CV-cs-AIPDF" class="headerlink" title="[75] Predicting When to Trust Vision-Language Models for Spatial Reasoning cs.CV | cs.AIPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11644">Predicting When to Trust Vision-Language Models for Spatial Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11644" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhammad Imran, Yugyung Lee</span></p>
<p><strong>TL;DR:</strong> 本文针对视觉语言模型在空间推理任务中存在的系统性错误，提出了一种基于视觉的置信度估计框架，通过独立的几何验证来预测何时信任VLM的空间预测。该方法融合了几何对齐、空间模糊性、检测质量和VLM内部不确定性四种信号，显著提升了置信度估计的准确性，并展示了在选择性预测和场景图构建中的实际应用价值。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉语言模型在多模态任务中表现出色，但在空间推理方面存在系统性失败（如CLIP和BLIP-2在基本方向关系上准确率仅49%至54%），为确保在机器人和自主系统中的安全部署，需要预测何时信任VLM的空间预测而非盲目接受所有输出。</p>
<p><strong>Result:</strong> 在BLIP-2上AUROC达到0.674（比基于文本的基线提升34.0%），在CLIP上达到0.583（提升16.1%），并泛化于生成式和分类式架构；在60%目标准确率下，选择性预测的覆盖率在BLIP-2上达到61.9%（基线为27.6%，提升2.2倍）；在场景图构建中，基于置信度的剪枝将精度从52.1%提升至78.3%，同时保留了68.2%的边。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种不依赖VLM自评估的、基于独立几何验证的视觉置信度估计框架，通过融合多源信号（特别是视觉几何信号贡献了87.4%的重要性）有效提升了预测可靠性；客观来看，该方法将外部验证与模型内部不确定性结合，为VLM的可靠部署提供了可解释且实用的解决方案。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) demonstrate impressive capabilities across multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships. For safe deployment in robotics and autonomous systems, we need to predict when to trust VLM spatial predictions rather than accepting all outputs. We propose a vision-based confidence estimation framework that validates VLM predictions through independent geometric verification using object detection. Unlike text-based approaches relying on self-assessment, our method fuses four signals via gradient boosting: geometric alignment between VLM claims and coordinates, spatial ambiguity from overlap, detection quality, and VLM internal uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing across generative and classification architectures. Our framework enables selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus 27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals vision-based signals contribute 87.4% of model importance versus 12.7% from VLM confidence, validating that external geometric verification outperforms self-assessment. We demonstrate reliable scene graph construction where confidence-based pruning improves precision from 52.1% to 78.3% while retaining 68.2% of edges.</p>
  </div>
</details>

<hr>
<h3 id="76-Aesthetics-as-Structural-Harm-Algorithmic-Lookism-Across-Text-to-Image-Generation-and-Classification-cs-CV-cs-AI-cs-CYPDF"><a href="#76-Aesthetics-as-Structural-Harm-Algorithmic-Lookism-Across-Text-to-Image-Generation-and-Classification-cs-CV-cs-AI-cs-CYPDF" class="headerlink" title="[76] Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification cs.CV | cs.AI | cs.CYPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11651">Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image Generation and Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11651" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver</span></p>
<p><strong>TL;DR:</strong> 本文研究了文本到图像生成AI和下游性别分类任务中的算法外貌主义，即基于外貌的系统性偏好。通过分析Stable Diffusion 2.1和3.5 Medium生成的26400张合成人脸，揭示了生成模型如何系统地将面部吸引力与正面属性关联，并发现性别分类算法存在显著偏差，其中女性面孔（尤其是带有负面属性的）误分类率远高于男性。研究还指出新模型通过年龄同质化、性别化曝光模式和地理简化加剧了美学约束。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是调查生成式AI和下游视觉任务中存在的系统性外貌偏见（算法外貌主义），揭示其如何编码社会建构的偏见并加剧不平等。</p>
<p><strong>Result:</strong> 在Stable Diffusion生成的合成人脸数据集上进行了分析，发现生成模型系统性地关联吸引力与正面属性；在三个性别分类算法中，女性面孔（特别是带有负面属性的）误分类率显著高于男性；新模型（如Stable Diffusion 3.5 Medium）表现出更强的年龄同质化等美学约束。</p>
<p><strong>Insight:</strong> 创新点在于将算法外貌主义概念化为跨AI视觉系统（生成与分类）的系统性基础设施，揭示了偏见在生成和识别任务中的复合效应；方法上，通过大规模合成人脸分析量化了吸引力-属性关联和性别分类偏差，并追踪了模型迭代中偏见的演变模式。</p>
<p><strong>Abstract:</strong> This paper examines algorithmic lookism-the systematic preferential treatment based on physical appearance-in text-to-image (T2I) generative AI and a downstream gender classification task. Through the analysis of 26,400 synthetic faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how generative AI models systematically associate facial attractiveness with positive attributes and vice-versa, mirroring socially constructed biases rather than evidence-based correlations. Furthermore, we find significant gender bias in three gender classification algorithms depending on the attributes of the input faces. Our findings reveal three critical harms: (1) the systematic encoding of attractiveness-positive attribute associations in T2I models; (2) gender disparities in classification systems, where women’s faces, particularly those generated with negative attributes, suffer substantially higher misclassification rates than men’s; and (3) intensifying aesthetic constraints in newer models through age homogenization, gendered exposure patterns, and geographic reductionism. These convergent patterns reveal algorithmic lookism as systematic infrastructure operating across AI vision systems, compounding existing inequalities through both representation and recognition.   Disclaimer: This work includes visual and textual content that reflects stereotypical associations between physical appearance and socially constructed attributes, including gender, race, and traits associated with social desirability. Any such associations found in this study emerge from the biases embedded in generative AI systems-not from empirical truths or the authors’ views.</p>
  </div>
</details>

<hr>
<h3 id="77-LTV-YOLO-A-Lightweight-Thermal-Object-Detector-for-Young-Pedestrians-in-Adverse-Conditions-cs-CVPDF"><a href="#77-LTV-YOLO-A-Lightweight-Thermal-Object-Detector-for-Young-Pedestrians-in-Adverse-Conditions-cs-CVPDF" class="headerlink" title="[77] LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions cs.CVPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11662">LTV-YOLO: A Lightweight Thermal Object Detector for Young Pedestrians in Adverse Conditions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11662" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abdullah Jirjees, Ryan Myers, Muhammad Haris Ikram, Mohamed H. Zaki</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为LTV-YOLO的轻量级热成像目标检测模型，专门用于在低光照和恶劣天气条件下检测儿童和青少年等易受伤害的道路使用者。该模型基于YOLO11架构，通过集成深度可分离卷积和特征金字塔网络，在保持紧凑架构的同时，实现了对小尺度、部分遮挡和热特征明显的行人的高效检测，并优化了在边缘设备上的实时性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在低光照和恶劣天气条件下，传统可见光RGB相机难以可靠检测儿童和青少年等易受伤害的道路使用者，这构成了计算机视觉、监控和自动驾驶系统中的关键挑战。本文旨在利用长波红外热成像技术，为这些特定场景提供一个轻量、高效的专用检测解决方案。</p>
<p><strong>Result:</strong> 论文宣称LTV-YOLO在检测小尺度、部分遮挡和热特征明显的易受伤害道路使用者方面表现出色，同时保持了紧凑的架构和边缘设备上的实时性能。虽然没有提及具体的基准测试数据集或定量结果，但强调了其在恶劣条件下的强性能。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个任务专用的、仅使用热成像的、面向边缘设备的轻量级检测模型，专门针对恶劣条件下的小型或遮挡的易受伤害道路使用者（如儿童）。虽然深度可分离卷积和特征金字塔网络是标准组件，但将其集成到针对此特定场景优化的纯热成像流程中是新颖的贡献。</p>
<p><strong>Abstract:</strong> Detecting vulnerable road users (VRUs), particularly children and adolescents, in low light and adverse weather conditions remains a critical challenge in computer vision, surveillance, and autonomous vehicle systems. This paper presents a purpose-built lightweight object detection model designed to identify young pedestrians in various environmental scenarios. To address these challenges, our approach leverages thermal imaging from long-wave infrared (LWIR) cameras, which enhances detection reliability in conditions where traditional RGB cameras operating in the visible spectrum fail. Based on the YOLO11 architecture and customized for thermal detection, our model, termed LTV-YOLO (Lightweight Thermal Vision YOLO), is optimized for computational efficiency, accuracy and real-time performance on edge devices. By integrating separable convolutions in depth and a feature pyramid network (FPN), LTV-YOLO achieves strong performance in detecting small-scale, partially occluded, and thermally distinct VRUs while maintaining a compact architecture. This work contributes a practical and scalable solution to improve pedestrian safety in intelligent transportation systems, particularly in school zones, autonomous navigation, and smart city infrastructure. Unlike prior thermal detectors, our contribution is task-specific: a thermally only edge-capable design designed for young and small VRUs (children and distant adults). Although FPN and depthwise separable convolutions are standard components, their integration into a thermal-only pipeline optimized for short&#x2F;occluded VRUs under adverse conditions is, to the best of our knowledge, novel.</p>
  </div>
</details>

<hr>
<h3 id="78-UAV-Based-Infrastructure-Inspections-A-Literature-Review-and-Proposed-Framework-for-AEC-FM-cs-CV-cs-ROPDF"><a href="#78-UAV-Based-Infrastructure-Inspections-A-Literature-Review-and-Proposed-Framework-for-AEC-FM-cs-CV-cs-ROPDF" class="headerlink" title="[78] UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM cs.CV | cs.ROPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11665">UAV-Based Infrastructure Inspections: A Literature Review and Proposed Framework for AEC+FM</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11665" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amir Farzin Nikkhah, Dong Chen, Bradford Campbell, Somayeh Asadi, Arsalan Heydarian</span></p>
<p><strong>TL;DR:</strong> 这篇论文是一篇关于无人机在建筑、工程、施工和设施管理领域进行基础设施检测的文献综述，并提出了一个整合多模态数据与先进机器学习模型的框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 无人机正在变革AEC+FM领域的基础设施检测，但现有方法在实时处理、多模态数据融合和泛化性方面仍面临挑战，需要提出一个更系统、准确的框架来应对。</p>
<p><strong>Result:</strong> 论文通过综述150多项研究，总结了无人机在结构健康监测、灾害响应等多个应用中的价值，并提出了一个融合RGB、LiDAR和热成像数据与Transformer架构的框架，旨在提高缺陷检测的准确性和可靠性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个整合多传感器数据与基于Transformer的先进架构的系统性工作流框架，并强调了动态路径规划、多模态融合以及未来轻量化AI模型、自适应飞行规划和合成数据集等研究方向。</p>
<p><strong>Abstract:</strong> Unmanned Aerial Vehicles (UAVs) are transforming infrastructure inspections in the Architecture, Engineering, Construction, and Facility Management (AEC+FM) domain. By synthesizing insights from over 150 studies, this review paper highlights UAV-based methodologies for data acquisition, photogrammetric modeling, defect detection, and decision-making support. Key innovations include path optimization, thermal integration, and advanced machine learning (ML) models such as YOLO and Faster R-CNN for anomaly detection. UAVs have demonstrated value in structural health monitoring (SHM), disaster response, urban infrastructure management, energy efficiency evaluations, and cultural heritage preservation. Despite these advancements, challenges in real-time processing, multimodal data fusion, and generalizability remain. A proposed workflow framework, informed by literature and a case study, integrates RGB imagery, LiDAR, and thermal sensing with transformer-based architectures to improve accuracy and reliability in detecting structural defects, thermal anomalies, and geometric inconsistencies. The proposed framework ensures precise and actionable insights by fusing multimodal data and dynamically adapting path planning for complex environments, presented as a comprehensive step-by-step guide to address these challenges effectively. This paper concludes with future research directions emphasizing lightweight AI models, adaptive flight planning, synthetic datasets, and richer modality fusion to streamline modern infrastructure inspections.</p>
  </div>
</details>

<hr>
<h3 id="79-MATEX-Multi-scale-Attention-and-Text-guided-Explainability-of-Medical-Vision-Language-Models-cs-CV-cs-AIPDF"><a href="#79-MATEX-Multi-scale-Attention-and-Text-guided-Explainability-of-Medical-Vision-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[79] MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models cs.CV | cs.AIPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11666">MATEX: Multi-scale Attention and Text-guided Explainability of Medical Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11666" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhammad Imran, Chi Lee, Yugyung Lee</span></p>
<p><strong>TL;DR:</strong> MATEX是一种新颖的医学视觉-语言模型可解释性框架，它通过结合解剖学信息驱动的空间推理，协同利用多层注意力展开、文本引导的空间先验和层一致性分析，生成精确、稳定且具有临床意义的梯度归因图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有方法在空间精度不足、缺乏解剖学基础以及注意力粒度有限等方面的关键局限性，旨在为医学影像AI提供更忠实和可解释的模型解释。</p>
<p><strong>Result:</strong> 在MS-CXR数据集上的评估表明，MATEX在空间精度和与专家标注结果的匹配度方面均优于当前最先进的M2IB方法。</p>
<p><strong>Insight:</strong> 创新点在于将解剖学先验知识整合到可解释性框架中，通过多尺度注意力机制和文本引导来提升归因图的质量与临床相关性，这为增强放射学AI应用的信任度和透明度提供了新思路。</p>
<p><strong>Abstract:</strong> We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a novel framework that advances interpretability in medical vision-language models by incorporating anatomically informed spatial reasoning. MATEX synergistically combines multi-layer attention rollout, text-guided spatial priors, and layer consistency analysis to produce precise, stable, and clinically meaningful gradient attribution maps. By addressing key limitations of prior methods, such as spatial imprecision, lack of anatomical grounding, and limited attention granularity, MATEX enables more faithful and interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX outperforms the state-of-the-art M2IB approach in both spatial precision and alignment with expert-annotated findings. These results highlight MATEX’s potential to enhance trust and transparency in radiological AI applications.</p>
  </div>
</details>

<hr>
<h3 id="80-Generating-metamers-of-human-scene-understanding-cs-CV-cs-AIPDF"><a href="#80-Generating-metamers-of-human-scene-understanding-cs-CV-cs-AIPDF" class="headerlink" title="[80] Generating metamers of human scene understanding cs.CV | cs.AIPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11675">Generating metamers of human scene understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11675" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ritik Raina, Abe Leite, Alexandros Graikos, Seoyoung Ahn, Dimitris Samaras</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MetamerGen的生成模型，用于生成与人类潜在场景理解对齐的图像。该模型结合了从视觉外围获取的低分辨率场景要点信息和从注视点获取的高分辨率信息，通过双流潜在扩散模型生成图像，这些图像在感知上等同于人类观看场景后形成的内部表征。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类视觉系统通过结合外围低分辨率要点信息和注视点高分辨率信息来理解场景，但现有方法难以生成与这种潜在人类表征对齐的图像。本文旨在解决这一图像合成问题，以更好地理解人类场景理解机制。</p>
<p><strong>Result:</strong> 通过行为实验（相同-不同判断任务）评估，MetamerGen生成的图像在感知上与人类潜在场景表征对齐，尤其是在基于观看者自身注视区域生成时，高层语义对齐最能预测图像是否为感知等价物（metamer）。</p>
<p><strong>Insight:</strong> 创新点包括：1）将外围与注视信息结合的双流潜在扩散模型架构；2）使用DINOv2令牌融合细节特征与场景上下文；3）提出了一种基于人类行为实验的感知对齐评估方法，为研究视觉处理层次特征提供了新工具。</p>
<p><strong>Abstract:</strong> Human vision combines low-resolution “gist” information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. “foveated”) inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a “same” or “different” response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers’ own fixated regions.</p>
  </div>
</details>

<hr>
<h3 id="81-SemAlign-Language-Guided-Semi-supervised-Domain-Generalization-cs-CVPDF"><a href="#81-SemAlign-Language-Guided-Semi-supervised-Domain-Generalization-cs-CVPDF" class="headerlink" title="[81] SemAlign: Language Guided Semi-supervised Domain Generalization cs.CVPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11724">SemAlign: Language Guided Semi-supervised Domain Generalization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11724" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muditha Fernando, Kajhanan Kailainathan, Krishnakanth Nagaratnam, Isuranga Udaravi Bandara Senavirathne, Ranga Rodrigo</span></p>
<p><strong>TL;DR:</strong> 本文提出SemAlign方法，通过将模型中间特征与视觉语言模型（VLM）的语义丰富且泛化的特征空间对齐，以促进域不变性，并结合图像级增强和输出级正则化策略，在半监督域泛化（SSDG）任务中实现了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有SSDG方法过度关注伪标签精度而忽视训练中最大化数据利用的问题，旨在提升模型在未见目标域上的泛化能力。</p>
<p><strong>Result:</strong> 在四个基准测试上进行了广泛实验，与现有SSDG基线相比，该方法在定性和定量上均达到了最先进（SOTA）水平。</p>
<p><strong>Insight:</strong> 创新点在于利用VLM的广义特征空间进行特征对齐以增强域不变性，并结合数据增强和正则化策略来优化数据利用和防止过拟合，为SSDG提供了新的视角。</p>
<p><strong>Abstract:</strong> Semi-supervised Domain Generalization (SSDG) addresses the challenge of generalizing to unseen target domains with limited labeled data. Existing SSDG methods highlight the importance of achieving high pseudo-labeling (PL) accuracy and preventing model overfitting as the main challenges in SSDG. In this light, we show that the SSDG literature’s excessive focus on PL accuracy, without consideration for maximum data utilization during training, limits potential performance improvements. We propose a novel approach to the SSDG problem by aligning the intermediate features of our model with the semantically rich and generalized feature space of a Vision Language Model (VLM) in a way that promotes domain-invariance. The above approach is enhanced with effective image-level augmentation and output-level regularization strategies to improve data utilization and minimize overfitting. Extensive experimentation across four benchmarks against existing SSDG baselines suggests that our method achieves SOTA results both qualitatively and quantitatively. The code will be made publicly available.</p>
  </div>
</details>

<hr>
<h3 id="82-SpaRRTa-A-Synthetic-Benchmark-for-Evaluating-Spatial-Intelligence-in-Visual-Foundation-Models-cs-CV-cs-LGPDF"><a href="#82-SpaRRTa-A-Synthetic-Benchmark-for-Evaluating-Spatial-Intelligence-in-Visual-Foundation-Models-cs-CV-cs-LGPDF" class="headerlink" title="[82] SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models cs.CV | cs.LGPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11729">SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in Visual Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11729" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Turhan Can Kargin, Wojciech Jasiński, Adam Pardyl, Bartosz Zieliński, Marcin Przewięźlikowski</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为SpaRRTa的合成基准测试，用于评估视觉基础模型的空间智能，特别是识别图像中物体相对位置的能力。通过生成大量具有可控物体布局和空间标注的真实感图像，该基准揭示了现有先进模型在空间推理能力上的显著差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉基础模型在语义理解上表现出色，但空间推理能力有限，且其空间感知能力是否真实存在或仅针对特定3D任务过拟合尚不明确。</p>
<p><strong>Result:</strong> 在SpaRRTa基准上对一系列最先进的视觉基础模型进行评估，结果显示它们在空间推理能力上存在显著差异。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个专注于空间关系识别而非精确度量预测的基准，这有助于探究支撑高级空间理解的基础能力，并为开发具有空间感知能力的视觉模型提供指导。</p>
<p><strong>Abstract:</strong> Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic understanding of images but exhibit limited spatial reasoning capabilities, which limits their applicability to embodied systems. As a result, recent work incorporates some 3D tasks (such as depth estimation) into VFM training. However, VFM performance remains inconsistent across other spatial tasks, raising the question of whether these models truly have spatial awareness or overfit to specific 3D objectives. To address this question, we introduce the Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the ability of VFMs to identify relative positions of objects in the image. Unlike traditional 3D objectives that focus on precise metric prediction (e.g., surface normal estimation), SpaRRTa probes a fundamental capability underpinning more advanced forms of human-like spatial understanding. SpaRRTa generates an arbitrary number of photorealistic images with diverse scenes and fully controllable object arrangements, along with freely accessible spatial annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant disparities between their spatial reasoning abilities. Through our analysis, we provide insights into the mechanisms that support or hinder spatial awareness in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding the development of future spatially aware visual models.</p>
  </div>
</details>

<hr>
<h3 id="83-studentSplat-Your-Student-Model-Learns-Single-view-3D-Gaussian-Splatting-cs-CVPDF"><a href="#83-studentSplat-Your-Student-Model-Learns-Single-view-3D-Gaussian-Splatting-cs-CVPDF" class="headerlink" title="[83] studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting cs.CVPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11772">studentSplat: Your Student Model Learns Single-view 3D Gaussian Splatting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11772" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yimu Pan, Hongda Mao, Qingshuang Chen, Yelin Kim</span></p>
<p><strong>TL;DR:</strong> 论文提出studentSplat，一种用于单视图3D场景重建的3D高斯泼溅方法。它通过教师-学生架构解决单视图固有的尺度模糊问题，并引入外推网络补全缺失场景信息，实现了高质量的新视角合成和场景重建。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决单视图3D场景重建中因视角信息有限导致的尺度模糊和外推困难问题，现有前馈式3D高斯泼溅方法在多视图或单视图物体重建上表现良好，但单视图场景重建仍探索不足。</p>
<p><strong>Result:</strong> 在单视图新视角重建质量上达到SOTA，在场景级别性能与多视图方法相当；作为自监督单视图深度估计方法也表现出竞争力。</p>
<p><strong>Insight:</strong> 创新点包括：1) 使用多视图教师模型为单视图学生模型提供几何监督的教师-学生架构；2) 补全缺失场景上下文的外推网络。这为单视图3D理解任务提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advance in feed-forward 3D Gaussian splatting has enable remarkable multi-view 3D scene reconstruction or single-view 3D object reconstruction but single-view 3D scene reconstruction remain under-explored due to inherited ambiguity in single-view. We present \textbf{studentSplat}, a single-view 3D Gaussian splatting method for scene reconstruction. To overcome the scale ambiguity and extrapolation problems inherent in novel-view supervision from a single input, we introduce two techniques: 1) a teacher-student architecture where a multi-view teacher model provides geometric supervision to the single-view student during training, addressing scale ambiguity and encourage geometric validity; and 2) an extrapolation network that completes missing scene context, enabling high-quality extrapolation. Extensive experiments show studentSplat achieves state-of-the-art single-view novel-view reconstruction quality and comparable performance to multi-view methods at the scene level. Furthermore, studentSplat demonstrates competitive performance as a self-supervised single-view depth estimation method, highlighting its potential for general single-view 3D understanding tasks.</p>
  </div>
</details>

<hr>
<h3 id="84-Digital-FAST-An-AI-Driven-Multimodal-Framework-for-Rapid-and-Early-Stroke-Screening-cs-CVPDF"><a href="#84-Digital-FAST-An-AI-Driven-Multimodal-Framework-for-Rapid-and-Early-Stroke-Screening-cs-CVPDF" class="headerlink" title="[84] Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening cs.CVPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11896">Digital FAST: An AI-Driven Multimodal Framework for Rapid and Early Stroke Screening</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11896" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ngoc-Khai Hoang, Thi-Nhu-Mai Nguyen, Huy-Hieu Pham</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Digital FAST的快速、非侵入性多模态深度学习框架，用于基于F.A.S.T.评估数据的自动二分类中风筛查。该框架整合了面部表情、语音信号和上半身运动的互补信息，通过基于注意力的融合机制学习跨模态交互，在自收集数据集上实现了高准确率和F1分数。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 早期识别中风症状对于及时干预和改善患者预后至关重要，尤其是在院前环境中。本研究旨在开发一个快速、非侵入性的自动筛查框架，以解决传统方法在速度和可靠性上的不足。</p>
<p><strong>Result:</strong> 在包含222个视频（来自37名受试者）的自收集数据集上，所提出的多模态模型一致优于单模态基线，达到了95.83%的准确率和96.00%的F1分数，并在测试集中成功检测出所有中风病例，实现了敏感性和特异性的良好平衡。</p>
<p><strong>Insight:</strong> 创新点包括：1）整合面部、语音和运动三种互补模态，通过注意力融合增强诊断鲁棒性；2）分别采用Transformer、Audio Spectrogram Transformer和MLP-Mixer网络处理不同模态数据，有效捕捉时空依赖关系；3）展示了多模态学习和迁移学习在中风早期筛查中的潜力，但强调需要更大、更具临床代表性的数据集以支持实际部署。</p>
<p><strong>Abstract:</strong> Early identification of stroke symptoms is essential for enabling timely intervention and improving patient outcomes, particularly in prehospital settings. This study presents a fast, non-invasive multimodal deep learning framework for automatic binary stroke screening based on data collected during the F.A.S.T. assessment. The proposed approach integrates complementary information from facial expressions, speech signals, and upper-body movements to enhance diagnostic robustness. Facial dynamics are represented using landmark based features and modeled with a Transformer architecture to capture temporal dependencies. Speech signals are converted into mel spectrograms and processed using an Audio Spectrogram Transformer, while upper-body pose sequences are analyzed with an MLP-Mixer network to model spatiotemporal motion patterns. The extracted modality specific representations are combined through an attention-based fusion mechanism to effectively learn cross modal interactions. Experiments conducted on a self-collected dataset of 222 videos from 37 subjects demonstrate that the proposed multimodal model consistently outperforms unimodal baselines, achieving 95.83% accuracy and a 96.00% F1-score. The model attains a strong balance between sensitivity and specificity and successfully detects all stroke cases in the test set. These results highlight the potential of multimodal learning and transfer learning for early stroke screening, while emphasizing the need for larger, clinically representative datasets to support reliable real-world deployment.</p>
  </div>
</details>

<hr>
<h3 id="85-Effects-of-the-retina-inspired-light-intensity-encoding-on-color-discrimination-performance-cs-CVPDF"><a href="#85-Effects-of-the-retina-inspired-light-intensity-encoding-on-color-discrimination-performance-cs-CVPDF" class="headerlink" title="[85] Effects of the retina-inspired light intensity encoding on color discrimination performance cs.CVPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11909">Effects of the retina-inspired light intensity encoding on color discrimination performance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11909" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Io Yamada, Hirotsugu Okuno</span></p>
<p><strong>TL;DR:</strong> 本研究探讨了视网膜启发的光强编码函数（对数函数与Naka-Rushton函数）对中心&#x2F;环绕Retinex模型颜色恒常性性能的影响，通过使用可变颜色LED照明目标并评估HSV和对手色平面表示下的颜色区分能力，发现Naka-Rushton函数结合双对手色平面表示能提供更优的颜色区分性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 颜色是物体识别等视觉功能的重要信息源，但易受光照颜色影响；颜色恒常性（CC）是视觉系统独立于光照感知目标颜色的关键能力，研究旨在评估不同光强编码函数对经典Retinex模型CC性能的影响。</p>
<p><strong>Result:</strong> 实验结果表明，在多种光照颜色下，采用Naka-Rushton函数结合双对手色平面表示时，模型对视觉目标的颜色区分性能优于原始对数函数方法。</p>
<p><strong>Insight:</strong> 创新点在于将视网膜光感受器响应的Naka-Rushton函数引入Retinex模型，并结合对手色理论的双对手色平面表示，这为仿生视觉系统设计提供了更接近生物机制的编码策略，可能提升颜色恒常性的鲁棒性。</p>
<p><strong>Abstract:</strong> Color is an important source of information for visual functions such as object recognition, but it is greatly affected by the color of illumination. The ability to perceive the color of a visual target independent of illumination color is called color constancy (CC), and is an important feature for vision systems that use color information. In this study, we investigated the effects of the light intensity encoding function on the performance of CC of the center&#x2F;surround (C&#x2F;S) retinex model, which is a well-known model inspired by CC of the visual nervous system. The functions used to encode light intensity are the logarithmic function used in the original C&#x2F;S retinex model and the Naka-Rushton (N-R) function, which is a model of retinal photoreceptor response. Color-variable LEDs were used to illuminate visual targets with various lighting colors, and color information computed by each model was used to evaluate the degree to which the color of visual targets illuminated with different lighting colors could be discriminated. Color information was represented using the HSV color space and a color plane based on the classical opponent color theory. The results showed that the combination of the N-R function and the double opponent color plane representation provided superior discrimination performance.</p>
  </div>
</details>

<hr>
<h3 id="86-A-Training-Free-Guess-What-Vision-Language-Model-from-Snippets-to-Open-Vocabulary-Object-Detection-cs-CVPDF"><a href="#86-A-Training-Free-Guess-What-Vision-Language-Model-from-Snippets-to-Open-Vocabulary-Object-Detection-cs-CVPDF" class="headerlink" title="[86] A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection cs.CVPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11910">A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11910" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guiying Zhu, Bowen Yang, Yin Zhuang, Tong Zhang, Guanqun Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无需训练的开放词汇目标检测方法GW-VLM，它通过多尺度视觉语言搜索和上下文概念提示，利用预训练的视觉语言模型和大型语言模型进行’猜猜看’式推理，在自然和遥感数据集上实现了先进的检测性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有开放词汇目标检测方法通常依赖大规模预训练的基础模型，但忽略了基于这些模型建立对任意对象认知的通用理解范式，本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在COCO val、Pascal VOC、DIOR和NWPU-10数据集上的实验表明，GW-VLM无需任何训练步骤即可达到最先进的开放词汇目标检测性能。</p>
<p><strong>Insight:</strong> 创新点在于将预训练VLM和LLM以’猜猜看’游戏方式结合，通过多尺度视觉语言软对齐生成片段，并利用上下文概念提示流使LLM理解这些片段，形成了一种无需训练的通用理解范式。</p>
<p><strong>Abstract:</strong> Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of “guess what”. Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.</p>
  </div>
</details>

<hr>
<h3 id="87-Effects-of-Gabor-Filters-on-Classification-Performance-of-CNNs-Trained-on-a-Limited-Number-of-Conditions-cs-CV-cs-LGPDF"><a href="#87-Effects-of-Gabor-Filters-on-Classification-Performance-of-CNNs-Trained-on-a-Limited-Number-of-Conditions-cs-CV-cs-LGPDF" class="headerlink" title="[87] Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions cs.CV | cs.LGPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11918">Effects of Gabor Filters on Classification Performance of CNNs Trained on a Limited Number of Conditions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11918" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Akito Morita, Hirotsugu Okuno</span></p>
<p><strong>TL;DR:</strong> 本研究提出了一种利用Gabor滤波器作为卷积神经网络（CNN）预处理器的技术，旨在提升在有限条件下训练的CNN的准确性和泛化能力，并减小模型尺寸，以适用于边缘设备上的机器人视觉应用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决边缘设备上CNN模型需小型化且能在有限数据条件下高效训练以识别特定视觉目标的问题，受视觉神经系统（VNS）从少量视觉经验中学习的启发。</p>
<p><strong>Result:</strong> 在自建的包含不同相机位置图像的测试集上，使用Gabor滤波器预处理的CNN相比未使用的模型，在泛化性能上有所提升，并有助于减小CNN的规模。</p>
<p><strong>Insight:</strong> 创新点在于借鉴生物视觉机制，将Gabor滤波器作为固定特征提取器集成到CNN预处理中，以增强模型在数据有限时的泛化能力和压缩性；这为边缘设备上的轻量级视觉模型设计提供了新思路。</p>
<p><strong>Abstract:</strong> In this study, we propose a technique to improve the accuracy and reduce the size of convolutional neural networks (CNNs) running on edge devices for real-world robot vision applications. CNNs running on edge devices must have a small architecture, and CNNs for robot vision applications involving on-site object recognition must be able to be trained efficiently to identify specific visual targets from data obtained under a limited variation of conditions. The visual nervous system (VNS) is a good example that meets the above requirements because it learns from few visual experiences. Therefore, we used a Gabor filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs to investigate the accuracy of the CNNs trained with small amounts of data. To evaluate how well CNNs trained on image data acquired under a limited variation of conditions generalize to data acquired under other conditions, we created an image dataset consisting of images acquired from different camera positions, and investigated the accuracy of the CNNs that trained using images acquired at a certain distance. The results were compared after training on multiple CNN architectures with and without Gabor filters as preprocessing. The results showed that preprocessing with Gabor filters improves the generalization performance of CNNs and contributes to reducing the size of CNNs.</p>
  </div>
</details>

<hr>
<h3 id="88-SupScene-Learning-Overlap-Aware-Global-Descriptor-for-Unconstrained-SfM-cs-CVPDF"><a href="#88-SupScene-Learning-Overlap-Aware-Global-Descriptor-for-Unconstrained-SfM-cs-CVPDF" class="headerlink" title="[88] SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM cs.CVPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11930">SupScene: Learning Overlap-Aware Global Descriptor for Unconstrained SfM</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11930" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xulei Shi, Maoyu Wang, Yuning Peng, Guanbo Wang, Xin Wang</span></p>
<p><strong>TL;DR:</strong> SupScene提出了一种用于无约束运动恢复结构（SfM）的、学习感知重叠区域的全局描述符的新方法。该方法通过基于子图的训练策略和一种受DINO启发的VLAD聚合器（DiVLAD），来更好地捕捉图像对之间的几何可匹配性，而非仅仅语义相似性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在无约束SfM中，图像检索的关键是找到具有几何可匹配性的重叠图像对，而不仅仅是语义相似的图像。现有基于深度学习的批量二元分类方法（重叠&#x2F;非重叠）未能有效捕捉这一细微差别。</p>
<p><strong>Result:</strong> 在GL3D数据集上的大量实验表明，该方法达到了最先进的性能，显著优于NetVLAD，同时仅引入了可忽略不计的额外可训练参数。</p>
<p><strong>Insight:</strong> 创新点在于：1）提出基于子图的训练策略，利用不同权重的真实几何重叠关系进行细粒度监督；2）提出DiVLAD聚合器，利用ViT最后一层的多头注意力图，并通过可学习的门控机制自适应地结合语义线索与视觉特征，生成更具判别力的全局描述符。该训练策略在不同聚合技术上都能带来一致的性能提升。</p>
<p><strong>Abstract:</strong> Image retrieval is a critical step for alleviating the quadratic complexity of image matching in unconstrained Structure-from-Motion (SfM). However, in this context, image retrieval typically focuses more on the image pairs of geometric matchability than on those of semantic similarity, a nuance that most existing deep learning-based methods guided by batched binaries (overlapping vs. non-overlapping pairs) fail to capture. In this paper, we introduce SupScene, a novel solution that learns global descriptors tailored for finding overlapping image pairs of similar geometric nature for SfM. First, to better underline co-visible regions, we employ a subgraph-based training strategy that moves beyond equally important isolated pairs, leveraging ground-truth geometric overlapping relationships with various weights to provide fine-grained supervision via a soft supervised contrastive loss. Second, we introduce DiVLAD, a DINO-inspired VLAD aggregator that leverages the inherent multi-head attention maps from the last block of ViT. And then, a learnable gating mechanism is designed to adaptively utilize these semantically salient cues with visual features, enabling a more discriminative global descriptor. Extensive experiments on the GL3D dataset demonstrate that our method achieves state-of-the-art performance, significantly outperforming NetVLAD while introducing a negligible number of additional trainable parameters. Furthermore, we show that the proposed training strategy brings consistent gains across different aggregation techniques. Code and models are available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/SupScene-5B73">https://anonymous.4open.science/r/SupScene-5B73</a>.</p>
  </div>
</details>

<hr>
<h3 id="89-Language-Guided-and-Motion-Aware-Gait-Representation-for-Generalizable-Recognition-cs-CVPDF"><a href="#89-Language-Guided-and-Motion-Aware-Gait-Representation-for-Generalizable-Recognition-cs-CVPDF" class="headerlink" title="[89] Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition cs.CVPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11931">Language-Guided and Motion-Aware Gait Representation for Generalizable Recognition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11931" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhengxian Wu, Chuanrui Zhang, Shenao Jiang, Hangrui Xu, Zirui Liao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为LMGait的语言引导和运动感知步态识别框架，旨在解决现有方法因直接提取图像特征和池化操作而导致的静态噪声过拟合及动态运动区域捕获不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有步态识别方法通常依赖复杂架构直接从图像提取特征，并通过池化操作获得序列级表示，这容易导致对静态噪声（如衣物）过拟合，且无法有效捕捉动态运动区域。</p>
<p><strong>Result:</strong> 摘要中未提及具体的定量实验结果或基准测试对比。</p>
<p><strong>Insight:</strong> 创新点在于利用设计的步态相关语言提示来引导模型捕捉步态序列中的关键运动特征，这为步态识别提供了一种新的、结合语言先验知识来增强运动感知能力的思路。</p>
<p><strong>Abstract:</strong> Gait recognition is emerging as a promising technology and an innovative field within computer vision. However, existing methods typically rely on complex architectures to directly extract features from images and apply pooling operations to obtain sequence-level representations. Such designs often lead to overfitting on static noise (e.g., clothing), while failing to effectively capture dynamic motion regions.To address the above challenges, we present a Language guided and Motion-aware gait recognition framework, named LMGait.In particular, we utilize designed gait-related language cues to capture key motion features in gait sequences.</p>
  </div>
</details>

<hr>
<h3 id="90-Real-Time-Multi-Modal-Embedded-Vision-Framework-for-Object-Detection-Facial-Emotion-Recognition-and-Biometric-Identification-on-Low-Power-Edge-Platforms-cs-CVPDF"><a href="#90-Real-Time-Multi-Modal-Embedded-Vision-Framework-for-Object-Detection-Facial-Emotion-Recognition-and-Biometric-Identification-on-Low-Power-Edge-Platforms-cs-CVPDF" class="headerlink" title="[90] Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms cs.CVPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11970">Real-Time Multi-Modal Embedded Vision Framework for Object Detection Facial Emotion Recognition and Biometric Identification on Low-Power Edge Platforms</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11970" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">S. M. Khalid Bin Zahid, Md. Rakibul Hasan Nishat, Abdul Hasib, Md. Rakibul Hasan, Md. Ashiqussalehin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种面向低功耗边缘平台（如树莓派5）的实时多模态嵌入式视觉框架，该框架集成了目标检测、特定人脸识别和情绪识别任务，并引入了一种自适应调度机制，根据上下文触发动态激活不同模块，从而显著降低计算负载。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有智能监控系统通常独立处理目标检测、人脸识别和情绪分析等感知任务，缺乏一个统一的、能根据上下文动态分配计算资源的自适应运行时调度器，这限制了它们在低功耗边缘设备上的整体理解能力和效率。</p>
<p><strong>Result:</strong> 在树莓派5平台上，该系统将计算负载相比持续处理降低了65%。目标检测模块（YOLOv8n）的平均精度（AP）达到0.861，人脸识别准确率为88%，情绪检测（基于DeepFace的CNN）对特定情绪的判别能力很强（AUC最高达0.97），系统整体运行速度为5.6帧&#x2F;秒。</p>
<p><strong>Insight:</strong> 核心创新点是上下文感知的自适应调度机制，它通过选择性激活不同模块（如YOLOv8n、自定义FaceNet嵌入系统、DeepFace CNN）来优化资源分配。这证明了在低成本边缘硬件上，通过智能调度而非单纯依赖模型优化，是实现复杂多模态AI感知并兼顾性能与隐私保护的关键。</p>
<p><strong>Abstract:</strong> Intelligent surveillance systems often handle perceptual tasks such as object detection, facial recognition, and emotion analysis independently, but they lack a unified, adaptive runtime scheduler that dynamically allocates computational resources based on contextual triggers. This limits their holistic understanding and efficiency on low-power edge devices. To address this, we present a real-time multi-modal vision framework that integrates object detection, owner-specific face recognition, and emotion detection into a unified pipeline deployed on a Raspberry Pi 5 edge platform. The core of our system is an adaptive scheduling mechanism that reduces computational load by 65% compared to continuous processing by selectively activating modules such as, YOLOv8n for object detection, a custom FaceNet-based embedding system for facial recognition, and DeepFace’s CNN for emotion classification. Experimental results demonstrate the system’s efficacy, with the object detection module achieving an Average Precision (AP) of 0.861, facial recognition attaining 88% accuracy, and emotion detection showing strong discriminatory power (AUC up to 0.97 for specific emotions), while operating at 5.6 frames per second. Our work demonstrates that context-aware scheduling is the key to unlocking complex multi-modal AI on cost-effective edge hardware, making intelligent perception more accessible and privacy-preserving.</p>
  </div>
</details>

<hr>
<h3 id="91-AVIR-Adaptive-Visual-In-Document-Retrieval-for-Efficient-Multi-Page-Document-Question-Answering-cs-CVPDF"><a href="#91-AVIR-Adaptive-Visual-In-Document-Retrieval-for-Efficient-Multi-Page-Document-Question-Answering-cs-CVPDF" class="headerlink" title="[91] AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering cs.CVPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11976">AVIR: Adaptive Visual In-Document Retrieval for Efficient Multi-Page Document Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11976" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zongmin Li, Yachuan Li, Lei Kang, Dimosthenis Karatzas, Wenkang Ma</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种自适应视觉文档内检索（AVIR）框架，用于高效的多页文档视觉问答（MP-DocVQA）。该框架通过轻量级检索模型对页面进行相关性评分，然后根据分数分布自适应聚类和筛选相关页面，仅将选中的页面输入冻结的大型视觉语言模型生成答案，无需微调。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多页文档视觉问答中长文档带来的计算资源压力和注意力机制效率下降的问题。</p>
<p><strong>Result:</strong> 在MP-DocVQA数据集上，AVIR将问答所需的平均页面数减少了70%，并达到了84.58%的ANLS分数，超越了先前方法，且计算成本显著降低；在SlideVQA和DUDE基准测试上也验证了其有效性。</p>
<p><strong>Insight:</strong> 创新点在于通过自适应聚类和Top-K筛选机制动态选择相关页面，结合轻量检索与冻结大模型，在保持性能的同时大幅提升效率；客观分析其核心是文档内检索与内容自适应压缩的策略创新。</p>
<p><strong>Abstract:</strong> Multi-page Document Visual Question Answering (MP-DocVQA) remains challenging because long documents not only strain computational resources but also reduce the effectiveness of the attention mechanism in large vision-language models (LVLMs). We tackle these issues with an Adaptive Visual In-document Retrieval (AVIR) framework. A lightweight retrieval model first scores each page for question relevance. Pages are then clustered according to the score distribution to adaptively select relevant content. The clustered pages are screened again by Top-K to keep the context compact. However, for short documents, clustering reliability decreases, so we use a relevance probability threshold to select pages. The selected pages alone are fed to a frozen LVLM for answer generation, eliminating the need for model fine-tuning. The proposed AVIR framework reduces the average page count required for question answering by 70%, while achieving an ANLS of 84.58% on the MP-DocVQA dataset-surpassing previous methods with significantly lower computational cost. The effectiveness of the proposed AVIR is also verified on the SlideVQA and DUDE benchmarks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/Li-yachuan/AVIR">https://github.com/Li-yachuan/AVIR</a>.</p>
  </div>
</details>

<hr>
<h3 id="92-Nip-Rumors-in-the-Bud-Retrieval-Guided-Topic-Level-Adaptation-for-Test-Time-Fake-News-Video-Detection-cs-CVPDF"><a href="#92-Nip-Rumors-in-the-Bud-Retrieval-Guided-Topic-Level-Adaptation-for-Test-Time-Fake-News-Video-Detection-cs-CVPDF" class="headerlink" title="[92] Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection cs.CVPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11981">Nip Rumors in the Bud: Retrieval-Guided Topic-Level Adaptation for Test-Time Fake News Video Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11981" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jian Lang, Rongpei Hong, Ting Zhong, Yong Wang, Fan Zhou</span></p>
<p><strong>TL;DR:</strong> 本文提出RADAR框架，首次实现假新闻视频检测（FNVD）在测试时对未见新闻主题的适应。该框架通过检索引导的适应范式，利用目标域中稳定的视频作为参考，指导语义相关但不稳定实例的鲁棒适应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有假新闻视频检测方法通常假设训练和测试阶段新闻主题分布一致，无法检测与新兴事件和未见主题相关的假新闻视频，RADAR旨在填补这一空白。</p>
<p><strong>Result:</strong> 大量实验表明，RADAR在测试时假新闻视频检测上实现了优越性能，能够对未见假新闻视频主题进行强力的即时适应。</p>
<p><strong>Insight:</strong> 创新点包括：1）基于熵选择的检索机制，为适应提供稳定且相关的参考视频；2）稳定锚点引导的对齐模块，通过分布级匹配将不稳定实例表示与源域对齐；3）目标域感知的自训练范式，生成由稳定参考增强的信息性伪标签，以捕捉目标域中变化且不平衡的类别分布。</p>
<p><strong>Abstract:</strong> Fake News Video Detection (FNVD) is critical for social stability. Existing methods typically assume consistent news topic distribution between training and test phases, failing to detect fake news videos tied to emerging events and unseen topics. To bridge this gap, we introduce RADAR, the first framework that enables test-time adaptation to unseen news videos. RADAR pioneers a new retrieval-guided adaptation paradigm that leverages stable (source-close) videos from the target domain to guide robust adaptation of semantically related but unstable instances. Specifically, we propose an Entropy Selection-Based Retrieval mechanism that provides videos with stable (low-entropy), relevant references for adaptation. We also introduce a Stable Anchor-Guided Alignment module that explicitly aligns unstable instances’ representations to the source domain via distribution-level matching with their stable references, mitigating severe domain discrepancies. Finally, our novel Target-Domain Aware Self-Training paradigm can generate informative pseudo-labels augmented by stable references, capturing varying and imbalanced category distributions in the target domain and enabling RADAR to adapt to the fast-changing label distributions. Extensive experiments demonstrate that RADAR achieves superior performance for test-time FNVD, enabling strong on-the-fly adaptation to unseen fake news video topics.</p>
  </div>
</details>

<hr>
<h3 id="93-An-AI-IoT-Based-Smart-Wheelchair-with-Gesture-Controlled-Mobility-Deep-Learning-Based-Obstacle-Detection-Multi-Sensor-Health-Monitoring-and-Emergency-Alert-System-cs-CVPDF"><a href="#93-An-AI-IoT-Based-Smart-Wheelchair-with-Gesture-Controlled-Mobility-Deep-Learning-Based-Obstacle-Detection-Multi-Sensor-Health-Monitoring-and-Emergency-Alert-System-cs-CVPDF" class="headerlink" title="[93] An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System cs.CVPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11983">An AI-IoT Based Smart Wheelchair with Gesture-Controlled Mobility, Deep Learning-Based Obstacle Detection, Multi-Sensor Health Monitoring, and Emergency Alert System</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11983" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md. Asiful Islam, Abdul Hasib, Tousif Mahmud Emon, Khandaker Tabin Hasan, A. S. M. Ahsanul Sarkar Akib</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于AI-IoT的智能轮椅系统，集成了基于手套的手势控制以实现免提导航、使用YOLOv8进行实时物体检测并提供听觉反馈以避障、以及超声波传感器用于即时防撞。该系统还持续监测生命体征（心率、血氧、心电图、体温），并将数据上传至ThingSpeak云平台，在危急情况下触发邮件警报。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对日益增长的残障人士和老年人口对经济实惠、智能化的轮椅需求，现有传统轮椅缺乏动态功能，而许多智能替代方案仍存在成本高、功能单一（单模态）以及健康监测集成度有限的问题。本文旨在开发一种先进的、个性化的、可负担的辅助技术解决方案。</p>
<p><strong>Result:</strong> 在提出的集成系统中，手势控制实现了95.5%的成功率，超声波障碍物检测达到了94%的准确率，基于YOLOv8的物体检测则取得了91.5%的精确率、90.2%的召回率和90.8%的F1分数。</p>
<p><strong>Insight:</strong> 论文的主要创新点在于将多种AI与物联网技术（手势控制、基于深度学习的视觉避障、超声波避障、多传感器健康监测、云平台集成及紧急警报）整合到一个低成本、模块化的轮椅系统中，提供了一个多模态、实用、可扩展的解决方案，旨在弥合创新研究与实际部署之间的差距，提升用户自主性、安全性和独立性。</p>
<p><strong>Abstract:</strong> The growing number of differently-abled and elderly individuals demands affordable, intelligent wheelchairs that combine safe navigation with health monitoring. Traditional wheelchairs lack dynamic features, and many smart alternatives remain costly, single-modality, and limited in health integration. Motivated by the pressing demand for advanced, personalized, and affordable assistive technologies, we propose a comprehensive AI-IoT based smart wheelchair system that incorporates glove-based gesture control for hands-free navigation, real-time object detection using YOLOv8 with auditory feedback for obstacle avoidance, and ultrasonic for immediate collision avoidance. Vital signs (heart rate, SpO$_2$, ECG, temperature) are continuously monitored, uploaded to ThingSpeak, and trigger email alerts for critical conditions. Built on a modular and low-cost architecture, the gesture control achieved a 95.5% success rate, ultrasonic obstacle detection reached 94% accuracy, and YOLOv8-based object detection delivered 91.5% Precision, 90.2% Recall, and a 90.8% F1-score. This integrated, multi-modal approach offers a practical, scalable, and affordable solution, significantly enhancing user autonomy, safety, and independence by bridging the gap between innovative research and real-world deployment.</p>
  </div>
</details>

<hr>
<h3 id="94-Structural-Graph-Neural-Networks-with-Anatomical-Priors-for-Explainable-Chest-X-ray-Diagnosis-cs-CVPDF"><a href="#94-Structural-Graph-Neural-Networks-with-Anatomical-Priors-for-Explainable-Chest-X-ray-Diagnosis-cs-CVPDF" class="headerlink" title="[94] Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis cs.CVPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11987">Structural Graph Neural Networks with Anatomical Priors for Explainable Chest X-ray Diagnosis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11987" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Khaled Berkani</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种结合解剖学先验的结构化图推理框架，用于可解释的胸部X光诊断。该框架将卷积特征图重新解释为节点包含外观与空间坐标、边反映局部结构邻接关系的图结构，并引入定制化的结构传播机制显式建模空间关系，支持节点级病灶感知与图级诊断推理，通过节点重要性分数提供内在可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决传统视觉诊断方法缺乏结构化推理和可解释性的问题，通过引入显式的解剖学先验和结构化图表示，旨在提升医学影像诊断的可靠性与透明度。</p>
<p><strong>Result:</strong> 论文通过胸部X光案例研究验证了方法，表明结构先验能有效引导关系推理并提升可解释性，但摘要未提及具体定量结果或基准测试性能。</p>
<p><strong>Insight:</strong> 创新点在于将解剖先验编码为图结构并设计定制化传播机制，使图作为结构化推理的归纳偏置而非被动表示；其框架与领域无关，为结构感知和可解释学习提供了通用的图计算基础。</p>
<p><strong>Abstract:</strong> We present a structural graph reasoning framework that incorporates explicit anatomical priors for explainable vision-based diagnosis. Convolutional feature maps are reinterpreted as patch-level graphs, where nodes encode both appearance and spatial coordinates, and edges reflect local structural adjacency. Unlike conventional graph neural networks that rely on generic message passing, we introduce a custom structural propagation mechanism that explicitly models relative spatial relations as part of the reasoning process. This design enables the graph to act as an inductive bias for structured inference rather than a passive relational representation. The proposed model jointly supports node-level lesion-aware predictions and graph-level diagnostic reasoning, yielding intrinsic explainability through learned node importance scores without relying on post-hoc visualization techniques. We demonstrate the approach through a chest X-ray case study, illustrating how structural priors guide relational reasoning and improve interpretability. While evaluated in a medical imaging context, the framework is domain-agnostic and aligns with the broader vision of graph-based reasoning across artificial intelligence systems. This work contributes to the growing body of research exploring graphs as computational substrates for structure-aware and explainable learning.</p>
  </div>
</details>

<hr>
<h3 id="95-DAOS-A-Multimodal-In-cabin-Behavior-Monitoring-with-Driver-Action-Object-Synergy-Dataset-cs-CVPDF"><a href="#95-DAOS-A-Multimodal-In-cabin-Behavior-Monitoring-with-Driver-Action-Object-Synergy-Dataset-cs-CVPDF" class="headerlink" title="[95] DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset cs.CVPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11990">DAOS: A Multimodal In-cabin Behavior Monitoring with Driver Action-Object Synergy Dataset</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11990" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiming Li, Chen Cai, Tianyi Liu, Dan Lin, Wenqian Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为DAOS的多模态驾驶员行为监控数据集，包含9,787个视频片段，标注了36种细粒度驾驶员动作和15种物体类别，并提供了多视角的RGB、红外和深度数据。同时，作者提出了Action-Object-Relation Network (AOR-Net)模型，通过多层次推理和动作链提示机制来建模动作、物体及其关系的逻辑联系，以提升驾驶员动作识别的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有驾驶员监控数据集缺乏准确的物体位置标注或未将物体与相关动作关联，导致在驾驶员上半身动作相似时难以可靠区分，因此需要构建一个包含详细动作-物体协同关系的数据集和相应模型。</p>
<p><strong>Result:</strong> 在多个数据集上的广泛实验表明，AOR-Net模型优于其他最先进的方法，实现了SOTA性能。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个大规模、多模态、细粒度标注驾驶员动作与物体协同的数据集DAOS，并提出了AOR-Net模型，其核心是通过动作链提示机制和Mixture of Thoughts模块动态建模和选择关键的人-物关系，以应对驾驶舱内物体丰富或稀缺的场景。</p>
<p><strong>Abstract:</strong> In driver activity monitoring, movements are mostly limited to the upper body, which makes many actions look similar. To tell these actions apart, human often rely on the objects the driver is using, such as holding a phone compared with gripping the steering wheel. However, most existing driver-monitoring datasets lack accurate object-location annotations or do not link objects to their associated actions, leaving a critical gap for reliable action recognition. To address this, we introduce the Driver Action with Object Synergy (DAOS) dataset, comprising 9,787 video clips annotated with 36 fine-grained driver actions and 15 object classes, totaling more than 2.5 million corresponding object instances. DAOS offers multi-modal, multi-view data (RGB, IR, and depth) from front, face, left, and right perspectives. Although DAOS captures a wide range of cabin objects, only a few are directly relevant to each action for prediction, so focusing on task-specific human-object relations is essential. To tackle this challenge, we propose the Action-Object-Relation Network (AOR-Net). AOR-Net comprehends complex driver actions through multi-level reasoning and a chain-of-action prompting mechanism that models the logical relationships among actions, objects, and their relations. Additionally, the Mixture of Thoughts module is introduced to dynamically select essential knowledge at each stage, enhancing robustness in object-rich and object-scarce conditions. Extensive experiments demonstrate that our model outperforms other state-of-the-art methods on various datasets.</p>
  </div>
</details>

<hr>
<h3 id="96-SMc2f-Robust-Scenario-Mining-for-Robotic-Autonomy-from-Coarse-to-Fine-cs-CVPDF"><a href="#96-SMc2f-Robust-Scenario-Mining-for-Robotic-Autonomy-from-Coarse-to-Fine-cs-CVPDF" class="headerlink" title="[96] SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine cs.CVPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12010">SMc2f: Robust Scenario Mining for Robotic Autonomy from Coarse to Fine</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12010" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yifei Chen, Ross Greer</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种从粗到精的鲁棒性场景挖掘框架SMc2f，用于从海量真实世界驾驶日志中检索安全关键场景。该方法利用视觉语言模型进行粗粒度图像-文本过滤，结合RefAV框架构建成功案例库并进行少样本提示，最后通过文本-轨迹对比学习进行细粒度匹配，以提升检索质量和效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法（如RefAV）基于轨迹标签进行检索，忽略了自然语言与原始RGB图像的直接关联，且依赖于上游3D目标检测与跟踪的质量，导致下游时空定位不准确。本文旨在解决这些问题，实现更鲁棒的场景检索。</p>
<p><strong>Result:</strong> 在公开数据集上的实验表明，该方法在检索质量和效率上均取得了显著提升。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个从粗到精的流程，结合了视觉语言模型的粗过滤、基于成功案例的少样本提示增强大语言模型，以及通过文本-轨迹对比学习实现的细粒度匹配器，从而更直接地利用图像信息并减少对上游模块的依赖。</p>
<p><strong>Abstract:</strong> The safety validation of autonomous robotic vehicles hinges on systematically testing their planning and control stacks against rare, safety-critical scenarios. Mining these long-tail events from massive real-world driving logs is therefore a critical step in the robotic development lifecycle. The goal of the Scenario Mining task is to retrieve useful information to enable targeted re-simulation, regression testing, and failure analysis of the robot’s decision-making algorithms. RefAV, introduced by the Argoverse team, is an end-to-end framework that uses large language models (LLMs) to spatially and temporally localize scenarios described in natural language. However, this process performs retrieval on trajectory labels, ignoring the direct connection between natural language and raw RGB images, which runs counter to the intuition of video retrieval; it also depends on the quality of upstream 3D object detection and tracking. Further, inaccuracies in trajectory data lead to inaccuracies in downstream spatial and temporal localization. To address these issues, we propose Robust Scenario Mining for Robotic Autonomy from Coarse to Fine (SMc2f), a coarse-to-fine pipeline that employs vision-language models (VLMs) for coarse image-text filtering, builds a database of successful mining cases on top of RefAV and automatically retrieves exemplars to few-shot condition the LLM for more robust retrieval, and introduces text-trajectory contrastive learning to pull matched pairs together and push mismatched pairs apart in a shared embedding space, yielding a fine-grained matcher that refines the LLM’s candidate trajectories. Experiments on public datasets demonstrate substantial gains in both retrieval quality and efficiency.</p>
  </div>
</details>

<hr>
<h3 id="97-DIAMOND-SSS-Diffusion-Augmented-Multi-View-Optimization-for-Data-efficient-SubSurface-Scattering-cs-CVPDF"><a href="#97-DIAMOND-SSS-Diffusion-Augmented-Multi-View-Optimization-for-Data-efficient-SubSurface-Scattering-cs-CVPDF" class="headerlink" title="[97] DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering cs.CVPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12020">DIAMOND-SSS: Diffusion-Augmented Multi-View Optimization for Data-efficient SubSurface Scattering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12020" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guillermo Figueroa-Araneda, Iris Diana Jimenez, Florian Hofherr, Manny Ko, Hector Andrade-Loarca</span></p>
<p><strong>TL;DR:</strong> DIAMOND-SSS是一个数据高效的高保真半透明材质重建框架，通过扩散模型增强的稀疏多视角优化，仅需极少图像（如10张）即可实现高质量的可重光照神经渲染。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决神经渲染中次表面散射（SSS）建模需要密集多视角多光照数据（通常&gt;100视角+112 OLATs）的挑战，降低数据采集成本。</p>
<p><strong>Result:</strong> 在稀疏监督下实现SOTA质量的可重光照高斯渲染，相比SSS-3DGS减少90%真实数据需求，仅需7%数据集训练扩散模型即可生成替代95%缺失采集的光真实增强数据。</p>
<p><strong>Insight:</strong> 创新点包括：1）基于几何条件微调扩散模型实现数据增强；2）引入光照无关几何先验（多视角轮廓一致性损失和深度一致性损失）稳定稀疏&#x2F;合成监督下的重建。</p>
<p><strong>Abstract:</strong> Subsurface scattering (SSS) gives translucent materials – such as wax, jade, marble, and skin – their characteristic soft shadows, color bleeding, and diffuse glow. Modeling these effects in neural rendering remains challenging due to complex light transport and the need for densely captured multi-view, multi-light datasets (often more than 100 views and 112 OLATs).   We present DIAMOND-SSS, a data-efficient framework for high-fidelity translucent reconstruction from extremely sparse supervision – even as few as ten images. We fine-tune diffusion models for novel-view synthesis and relighting, conditioned on estimated geometry and trained on less than 7 percent of the dataset, producing photorealistic augmentations that can replace up to 95 percent of missing captures. To stabilize reconstruction under sparse or synthetic supervision, we introduce illumination-independent geometric priors: a multi-view silhouette consistency loss and a multi-view depth consistency loss.   Across all sparsity regimes, DIAMOND-SSS achieves state-of-the-art quality in relightable Gaussian rendering, reducing real capture requirements by up to 90 percent compared to SSS-3DGS.</p>
  </div>
</details>

<hr>
<h3 id="98-A-Unified-Masked-Jigsaw-Puzzle-Framework-for-Vision-and-Language-Models-cs-CVPDF"><a href="#98-A-Unified-Masked-Jigsaw-Puzzle-Framework-for-Vision-and-Language-Models-cs-CVPDF" class="headerlink" title="[98] A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models cs.CVPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12051">A Unified Masked Jigsaw Puzzle Framework for Vision and Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12051" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weixin Ye, Wei Wang, Yahui Liu, Yue Song, Bin Ren</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Masked Jigsaw Puzzle (MJP)的统一框架，旨在解决联邦学习中Transformer模型面临的两个关键挑战：抵御梯度攻击以及提升其在计算机视觉和自然语言处理任务中的性能。该框架通过随机打乱输入标记的顺序，并使用可学习的未知位置嵌入来掩盖这些打乱标记的位置信息，从而破坏位置嵌入中编码的局部空间信息，迫使模型学习更少依赖局部空间信息的特征表示。实验表明，MJP不仅能有效增强模型对梯度攻击的鲁棒性，还能在图像分类和文本情感分析等任务上提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在联邦学习中，Transformer架构面临梯度攻击的威胁，且其位置嵌入的梯度被发现包含足以重构输入数据的敏感信息。同时，模型在CV和NLP任务中的性能也有待提升。本文旨在通过破坏位置信息来同时解决这两个问题。</p>
<p><strong>Result:</strong> 实验在图像分类任务（如ImageNet-1K）和文本情感分析任务（如Yelp和Amazon数据集）上进行。结果表明，MJP框架不仅提高了模型抵御梯度攻击的鲁棒性，还提升了模型在这些基准任务上的性能，成为一个适用于不同基于Transformer的视觉和语言模型的统一框架。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于将随机打乱标记与使用可学习的未知位置嵌入进行掩码相结合，以一种统一的方式破坏了位置嵌入中的局部空间信息。这不仅是一种有效的隐私保护机制（对抗梯度攻击），还意外地成为一种提升模型表征能力的正则化或数据增强技术，使其在视觉和语言任务上均能受益，实现了安全与性能的协同提升。</p>
<p><strong>Abstract:</strong> In federated learning, Transformer, as a popular architecture, faces critical challenges in defending against gradient attacks and improving model performance in both Computer Vision (CV) and Natural Language Processing (NLP) tasks. It has been revealed that the gradient of Position Embeddings (PEs) in Transformer contains sufficient information, which can be used to reconstruct the input data. To mitigate this issue, we introduce a Masked Jigsaw Puzzle (MJP) framework. MJP starts with random token shuffling to break the token order, and then a learnable \textit{unknown (unk)} position embedding is used to mask out the PEs of the shuffled tokens. In this manner, the local spatial information which is encoded in the position embeddings is disrupted, and the models are forced to learn feature representations that are less reliant on the local spatial information. Notably, with the careful use of MJP, we can not only improve models’ robustness against gradient attacks, but also boost their performance in both vision and text application scenarios, such as classification for images (\textit{e.g.,} ImageNet-1K) and sentiment analysis for text (\textit{e.g.,} Yelp and Amazon). Experimental results suggest that MJP is a unified framework for different Transformer-based models in both vision and language tasks. Code is publicly available via <a target="_blank" rel="noopener" href="https://github.com/ywxsuperstar/transformerattack">https://github.com/ywxsuperstar/transformerattack</a></p>
  </div>
</details>

<hr>
<h3 id="99-Task-Driven-Prompt-Learning-A-Joint-Framework-for-Multi-modal-Cloud-Removal-and-Segmentation-cs-CVPDF"><a href="#99-Task-Driven-Prompt-Learning-A-Joint-Framework-for-Multi-modal-Cloud-Removal-and-Segmentation-cs-CVPDF" class="headerlink" title="[99] Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation cs.CVPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12052">Task-Driven Prompt Learning: A Joint Framework for Multi-modal Cloud Removal and Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12052" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zaiyan Zhang, Jie Li, Shaowei Shi, Qiangqiang Yuan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种任务驱动的多模态框架TDP-CR，用于联合执行遥感图像的去云和土地覆盖分割任务。该框架的核心是提示引导融合机制，利用可学习的退化提示编码云层厚度和空间不确定性，自适应地融合合成孔径雷达信息以修复被云层遮挡的光学数据。通过解耦重建和语义表示学习的参数高效两阶段训练策略，模型在保持高视觉保真度的同时，显著提升了分割性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有遥感图像去云方法通常侧重于低层级的视觉保真度优化，可能导致对分析和就绪数据至关重要的纹理和边界过度平滑，造成视觉上合理的修复结果与下游语义任务效用之间的不匹配。</p>
<p><strong>Result:</strong> 在LuojiaSET-OSFCR数据集上的实验表明，TDP-CR在PSNR指标上超越了当前最先进的基线方法0.18 dB，同时仅使用了15%的参数；在多任务对比中，其mIoU指标持续提升了1.4%，有效地提供了分析和就绪数据。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个任务驱动的联合学习框架，将去云与分割任务统一优化；其核心的提示引导融合机制通过可学习的退化提示，实现了对云层退化模式的自适应建模与信息融合；此外，参数高效的两阶段训练策略解耦了重建与语义学习，在提升性能的同时控制了模型复杂度。</p>
<p><strong>Abstract:</strong> Optical remote sensing imagery is indispensable for Earth observation, yet persistent cloud occlusion limits its downstream utility. Most cloud removal (CR) methods are optimized for low-level fidelity and can over-smooth textures and boundaries that are critical for analysis-ready data (ARD), leading to a mismatch between visually plausible restoration and semantic utility. To bridge this gap, we propose TDP-CR, a task-driven multimodal framework that jointly performs cloud removal and land-cover segmentation. Central to our approach is a Prompt-Guided Fusion (PGF) mechanism, which utilizes a learnable degradation prompt to encode cloud thickness and spatial uncertainty. By combining global channel context with local prompt-conditioned spatial bias, PGF adaptively integrates Synthetic Aperture Radar (SAR) information only where optical data is corrupted. We further introduce a parameter-efficient two-phase training strategy that decouples reconstruction and semantic representation learning. Experiments on the LuojiaSET-OSFCR dataset demonstrate the superiority of our framework: TDP-CR surpasses heavy state-of-the-art baselines by 0.18 dB in PSNR while using only 15% of the parameters, and achieves a 1.4% improvement in mIoU consistently against multi-task competitors, effectively delivering analysis-ready data.</p>
  </div>
</details>

<hr>
<h3 id="100-Learning-Language-Driven-Sequence-Level-Modal-Invariant-Representations-for-Video-Based-Visible-Infrared-Person-Re-Identification-cs-CVPDF"><a href="#100-Learning-Language-Driven-Sequence-Level-Modal-Invariant-Representations-for-Video-Based-Visible-Infrared-Person-Re-Identification-cs-CVPDF" class="headerlink" title="[100] Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification cs.CVPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12062">Learning Language-Driven Sequence-Level Modal-Invariant Representations for Video-Based Visible-Infrared Person Re-Identification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12062" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaomei Yang, Xizhan Gao, Antai Liu, Kang Wei, Fa Zhu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种语言驱动的序列级模态不变表示学习方法（LSMRL），用于解决视频可见光-红外行人重识别（VVI-ReID）任务。该方法通过空间-时序特征学习（STFL）、语义扩散（SD）和跨模态交互（CMI）三个模块，在CLIP模型基础上高效建模时空信息、促进跨模态交互并显式地学习模态不变特征。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于CLIP生成模态共享语言提示来指导模态不变表示学习的方法，在高效的时空建模、充分的跨模态交互以及显式的模态级损失指导方面仍存在不足，本文旨在解决这些问题。</p>
<p><strong>Result:</strong> 在大规模VVI-ReID数据集上的大量实验表明，LSMRL方法优于所有现有方法（AOTA）。</p>
<p><strong>Insight:</strong> 创新点在于：1）在CLIP基础上以最小改动构建了参数和计算高效的时空建模模块（STFL）；2）提出语义扩散（SD）模块，将语言提示扩散到视觉特征中以建立初步模态一致性；3）设计跨模态交互（CMI）模块，利用双向跨模态自注意力消除剩余模态差异；4）引入两种显式的模态级损失来增强特征的判别性和对未见类别的泛化能力。</p>
<p><strong>Abstract:</strong> The core of video-based visible-infrared person re-identification (VVI-ReID) lies in learning sequence-level modal-invariant representations across different modalities. Recent research tends to use modality-shared language prompts generated by CLIP to guide the learning of modal-invariant representations. Despite achieving optimal performance, such methods still face limitations in efficient spatial-temporal modeling, sufficient cross-modal interaction, and explicit modality-level loss guidance. To address these issues, we propose the language-driven sequence-level modal-invariant representation learning (LSMRL) method, which includes spatial-temporal feature learning (STFL) module, semantic diffusion (SD) module and cross-modal interaction (CMI) module. To enable parameter- and computation-efficient spatial-temporal modeling, the STFL module is built upon CLIP with minimal modifications. To achieve sufficient cross-modal interaction and enhance the learning of modal-invariant features, the SD module is proposed to diffuse modality-shared language prompts into visible and infrared features to establish preliminary modal consistency. The CMI module is further developed to leverage bidirectional cross-modal self-attention to eliminate residual modality gaps and refine modal-invariant representations. To explicitly enhance the learning of modal-invariant representations, two modality-level losses are introduced to improve the features’ discriminative ability and their generalization to unseen categories. Extensive experiments on large-scale VVI-ReID datasets demonstrate the superiority of LSMRL over AOTA methods.</p>
  </div>
</details>

<hr>
<h3 id="101-Learning-Stochastic-Bridges-for-Video-Object-Removal-via-Video-to-Video-Translation-cs-CVPDF"><a href="#101-Learning-Stochastic-Bridges-for-Video-Object-Removal-via-Video-to-Video-Translation-cs-CVPDF" class="headerlink" title="[101] Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation cs.CVPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12066">Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12066" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zijie Lou, Xiangwei Feng, Jiaxin Wang, Xiaochao Qu, Luoqi Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于随机桥模型的视频对象移除方法，将任务重新定义为视频到视频的翻译问题。该方法利用输入视频作为结构先验，通过自适应掩码调制策略平衡背景保真度和生成灵活性，在视觉质量和时间一致性上显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于扩散模型的视频对象移除方法从高斯噪声开始生成，丢弃了输入视频丰富的结构和上下文先验，导致对象擦除不完整或生成内容与场景物理逻辑冲突。</p>
<p><strong>Result:</strong> 大量实验表明，该方法在视觉质量和时间一致性上显著优于现有方法，达到了SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于将视频对象移除重新定义为通过随机桥模型进行的视频到视频翻译任务，并提出了自适应掩码调制策略来平衡先验强度与对象移除能力，有效利用了输入视频的结构信息。</p>
<p><strong>Abstract:</strong> Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene’s physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency.</p>
  </div>
</details>

<hr>
<h3 id="102-CroBIM-V-Memory-Quality-Controlled-Remote-Sensing-Referring-Video-Object-Segmentation-cs-CV-cs-CLPDF"><a href="#102-CroBIM-V-Memory-Quality-Controlled-Remote-Sensing-Referring-Video-Object-Segmentation-cs-CV-cs-CLPDF" class="headerlink" title="[102] CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation cs.CV | cs.CLPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12076">CroBIM-V: Memory-Quality Controlled Remote Sensing Referring Video Object Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12076" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">H. Jiang, Y. Sun, Z. Dong, T. Liu, Y. Gu</span></p>
<p><strong>TL;DR:</strong> 本文针对遥感视频指代目标分割任务，构建了首个大规模因果感知标注基准RS-RVOS Bench，并提出了一种基于记忆质量控制的在线分割框架MQC-SAM，通过时序运动一致性模块校准初始记忆，并利用解耦注意力机制动态评估和筛选记忆质量，有效抑制了误差传播。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决遥感视频指代目标分割中目标显著性弱、视觉信息截断严重、缺乏大规模专用基准，以及现有模型因初始记忆构建偏差和噪声积累导致误差传播的问题。</p>
<p><strong>Result:</strong> 在自建的RS-RVOS Bench基准上进行了广泛实验，MQC-SAM达到了最先进的性能水平。</p>
<p><strong>Insight:</strong> 创新点包括构建首个因果感知的大规模遥感视频指代分割数据集，以及提出结合时序运动先验的初始记忆校准和基于解耦注意力的动态质量评估记忆集成机制，可借鉴其通过数据构造和方法设计共同推动领域发展的思路，以及在线分割中控制记忆质量的策略。</p>
<p><strong>Abstract:</strong> Remote sensing video referring object segmentation (RS-RVOS) is challenged by weak target saliency and severe visual information truncation in dynamic scenes, making it extremely difficult to maintain discriminative target representations during segmentation. Moreover, progress in this field is hindered by the absence of large-scale dedicated benchmarks, while existing models are often affected by biased initial memory construction that impairs accurate instance localization in complex scenarios, as well as indiscriminate memory accumulation that encodes noise from occlusions or misclassifications, leading to persistent error propagation. This paper advances RS-RVOS research through dual contributions in data and methodology. First, we construct RS-RVOS Bench, the first large-scale benchmark comprising 111 video sequences, about 25,000 frames, and 213,000 temporal referring annotations. Unlike common RVOS benchmarks where many expressions are written with access to the full video context, our dataset adopts a strict causality-aware annotation strategy in which linguistic references are generated solely from the target state in the initial frame. Second, we propose a memory-quality-aware online referring segmentation framework, termed Memory Quality Control with Segment Anything Model (MQC-SAM). MQC-SAM introduces a temporal motion consistency module for initial memory calibration, leveraging short-term motion trajectory priors to correct structural deviations and establish accurate memory anchoring. Furthermore, it incorporates a decoupled attention-based memory integration mechanism with dynamic quality assessment, selectively updating high-confidence semantic features while filtering unreliable information, thereby effectively preventing error accumulation and propagation. Extensive experiments on RS-RVOS Bench demonstrate that MQC-SAM achieves state-of-the-art performance.</p>
  </div>
</details>

<hr>
<h3 id="103-Conditional-Random-Fields-for-Interactive-Refinement-of-Histopathological-Predictions-cs-CV-cs-AIPDF"><a href="#103-Conditional-Random-Fields-for-Interactive-Refinement-of-Histopathological-Predictions-cs-CV-cs-AIPDF" class="headerlink" title="[103] Conditional Random Fields for Interactive Refinement of Histopathological Predictions cs.CV | cs.AIPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12082">Conditional Random Fields for Interactive Refinement of Histopathological Predictions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12082" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tiffanie Godelaine, Maxime Zanella, Karim El Khoury, Saïd Mahmoudi, Benoît Macq</span></p>
<p><strong>TL;DR:</strong> 本文提出HistoCRF框架，利用条件随机场（CRF）优化组织病理学图像中视觉语言模型（VLM）的零样本预测，无需额外训练，通过设计促进标签多样性的成对势函数并结合专家标注，显著提升分类准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 组织病理学图像分析对癌症检测和分期具有重要临床价值，现有视觉语言模型虽能提供零样本预测但存在不完美，需通过后处理进行精细化改进。</p>
<p><strong>Result:</strong> 在五个涵盖不同器官和疾病的patch级分类数据集上，相比零样本预测，无标注时平均准确率提升16.0%，仅使用100个专家标注时提升27.5%，结合人机交互迭代标注可进一步提升32.6%。</p>
<p><strong>Insight:</strong> 创新点在于将CRF适配于组织病理学应用，设计新型成对势函数以增强标签多样性并有效利用专家标注；客观来看，该方法通过轻量级后处理框架结合人机交互，实现了对基础模型预测的高效细化，具有较低部署成本。</p>
<p><strong>Abstract:</strong> Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on <a target="_blank" rel="noopener" href="https://github.com/tgodelaine/HistoCRF">https://github.com/tgodelaine/HistoCRF</a>.</p>
  </div>
</details>

<hr>
<h3 id="104-Detecting-3D-Line-Segments-for-6DoF-Pose-Estimation-with-Limited-Data-cs-CVPDF"><a href="#104-Detecting-3D-Line-Segments-for-6DoF-Pose-Estimation-with-Limited-Data-cs-CVPDF" class="headerlink" title="[104] Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data cs.CVPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12090">Detecting 3D Line Segments for 6DoF Pose Estimation with Limited Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12090" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matej Mok, Lukáš Gajdošech, Michal Mesároš, Martin Madaras, Viktor Kocur</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对工业环境中箱体（bin）的6DoF姿态估计新方法。该方法利用箱体的长方体几何特性，首先从结构化点云数据中检测其顶边对应的3D线段，然后通过简单的几何处理鲁棒地确定箱体的6DoF姿态。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统深度学习方法在数据稀缺、物体实例多变的真实工业场景中，因需要大量训练数据或CAD模型而应用受限的问题。</p>
<p><strong>Result:</strong> 在扩展的公开数据集上，该方法在姿态精度（3厘米平移误差，8.2°旋转误差）上显著优于当前最先进的6DoF姿态估计方法，且推理时不需要实例特定的CAD模型。</p>
<p><strong>Insight:</strong> 创新点在于将2D线段检测网络LeTR扩展到结构化点云以检测3D线段，并利用简单的几何处理从这些线段中鲁棒地推导姿态，从而在有限数据下实现高精度，且对合成训练数据的有效利用提升了在真实扫描数据上的性能。</p>
<p><strong>Abstract:</strong> The task of 6DoF object pose estimation is one of the fundamental problems of 3D vision with many practical applications such as industrial automation. Traditional deep learning approaches for this task often require extensive training data or CAD models, limiting their application in real-world industrial settings where data is scarce and object instances vary. We propose a novel method for 6DoF pose estimation focused specifically on bins used in industrial settings. We exploit the cuboid geometry of bins by first detecting intermediate 3D line segments corresponding to their top edges. Our approach extends the 2D line segment detection network LeTR to operate on structured point cloud data. The detected 3D line segments are then processed using a simple geometric procedure to robustly determine the bin’s 6DoF pose. To evaluate our method, we extend an existing dataset with a newly collected and annotated dataset, which we make publicly available. We show that incorporating synthetic training data significantly improves pose estimation accuracy on real scans. Moreover, we show that our method significantly outperforms current state-of-the-art 6DoF pose estimation methods in terms of the pose accuracy (3 cm translation error, 8.2$^\circ$ rotation error) while not requiring instance-specific CAD models during inference.</p>
  </div>
</details>

<hr>
<h3 id="105-Energy-Aware-Ensemble-Learning-for-Coffee-Leaf-Disease-Classification-cs-CVPDF"><a href="#105-Energy-Aware-Ensemble-Learning-for-Coffee-Leaf-Disease-Classification-cs-CVPDF" class="headerlink" title="[105] Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification cs.CVPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12109">Energy-Aware Ensemble Learning for Coffee Leaf Disease Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12109" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Larissa Ferreira Rodrigues Moreira, Rodrigo Moreira, Leonardo Gabriel Ferreira Rodrigues</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种面向咖啡叶病害分类的节能集成学习方法，通过知识蒸馏将数据中心训练的高容量卷积神经网络（CNN）的知识迁移到紧凑CNN上，并结合简单优化的集成策略，在严格的计算和能源约束下提升准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决咖啡叶病害现场诊断中，人工智能视觉模型因受限设备和间歇性连接而难以部署的问题，旨在实现可持续的端侧诊断。</p>
<p><strong>Result:</strong> 在精心整理的咖啡叶数据集上，蒸馏后的轻量级集成模型取得了与先前工作相当的竞争性准确率，同时显著降低了能耗和碳足迹。</p>
<p><strong>Insight:</strong> 创新点在于将知识蒸馏与集成学习结合，通过优化集成策略在资源受限的物联网设备上实现高效诊断，表明轻量模型经过适当蒸馏和集成可提供实用解决方案。</p>
<p><strong>Abstract:</strong> Coffee yields are contingent on the timely and accurate diagnosis of diseases; however, assessing leaf diseases in the field presents significant challenges. Although Artificial Intelligence (AI) vision models achieve high accuracy, their adoption is hindered by the limitations of constrained devices and intermittent connectivity. This study aims to facilitate sustainable on-device diagnosis through knowledge distillation: high-capacity Convolutional Neural Networks (CNNs) trained in data centers transfer knowledge to compact CNNs through Ensemble Learning (EL). Furthermore, dense tiny pairs were integrated through simple and optimized ensembling to enhance accuracy while adhering to strict computational and energy constraints. On a curated coffee leaf dataset, distilled tiny ensembles achieved competitive with prior work with significantly reduced energy consumption and carbon footprint. This indicates that lightweight models, when properly distilled and ensembled, can provide practical diagnostic solutions for Internet of Things (IoT) applications.</p>
  </div>
</details>

<hr>
<h3 id="106-CARLA-Round-A-Multi-Factor-Simulation-Dataset-for-Roundabout-Trajectory-Prediction-cs-CVPDF"><a href="#106-CARLA-Round-A-Multi-Factor-Simulation-Dataset-for-Roundabout-Trajectory-Prediction-cs-CVPDF" class="headerlink" title="[106] CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction cs.CVPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12119">CARLA-Round: A Multi-Factor Simulation Dataset for Roundabout Trajectory Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12119" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaotong Zhou, Zhenhui Yuan, Yi Han, Tianhua Xu, Laurence T. Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了CARLA-Round，一个用于环岛场景车辆轨迹预测的多因素仿真数据集。该数据集通过结构化设计，系统性地控制了天气条件和交通密度水平，生成了25种可控场景，并提供了详细的驾驶行为注释，以弥补真实世界数据在因素隔离和观测完整性上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 环岛场景的车辆轨迹预测因圆形道路几何、连续汇入与让行交互以及缺乏交通信号而极具挑战，但现有数据集稀缺且真实数据存在观测不完整和因素混杂的问题，难以支撑准确预测算法的开发。</p>
<p><strong>Result:</strong> 使用标准基线模型（LSTM, GCN, GRU+GCN）的验证实验表明，交通密度对预测难度具有主导性的单调影响，而天气条件则呈现非线性影响。最佳模型在真实世界rounD数据集上达到了0.312米的平均位移误差（ADE），证明了有效的仿真到真实迁移能力。</p>
<p><strong>Insight:</strong> 论文的创新点在于通过结构化、可控的仿真环境，系统性地生成并量化了不同因素（天气、交通密度）对轨迹预测性能的影响，这种设计使得在混杂的真实数据中难以分离的因素影响得以精确分析，为算法开发和评估提供了新的数据范式。</p>
<p><strong>Abstract:</strong> Accurate trajectory prediction of vehicles at roundabouts is critical for reducing traffic accidents, yet it remains highly challenging due to their circular road geometry, continuous merging and yielding interactions, and absence of traffic signals. Developing accurate prediction algorithms relies on reliable, multimodal, and realistic datasets; however, such datasets for roundabout scenarios are scarce, as real-world data collection is often limited by incomplete observations and entangled factors that are difficult to isolate. We present CARLA-Round, a systematically designed simulation dataset for roundabout trajectory prediction. The dataset varies weather conditions (five types) and traffic density levels (spanning Level-of-Service A-E) in a structured manner, resulting in 25 controlled scenarios. Each scenario incorporates realistic mixtures of driving behaviors and provides explicit annotations that are largely absent from existing datasets. Unlike randomly sampled simulation data, this structured design enables precise analysis of how different conditions influence trajectory prediction performance. Validation experiments using standard baselines (LSTM, GCN, GRU+GCN) reveal traffic density dominates prediction difficulty with strong monotonic effects, while weather shows non-linear impacts. The best model achieves 0.312m ADE on real-world rounD dataset, demonstrating effective sim-to-real transfer. This systematic approach quantifies factor impacts impossible to isolate in confounded real-world datasets. Our CARLA-Round dataset is available at <a target="_blank" rel="noopener" href="https://github.com/Rebecca689/CARLA-Round">https://github.com/Rebecca689/CARLA-Round</a>.</p>
  </div>
</details>

<hr>
<h3 id="107-VIRTUE-Versatile-Video-Retrieval-Through-Unified-Embeddings-cs-CVPDF"><a href="#107-VIRTUE-Versatile-Video-Retrieval-Through-Unified-Embeddings-cs-CVPDF" class="headerlink" title="[107] VIRTUE: Versatile Video Retrieval Through Unified Embeddings cs.CVPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12193">VIRTUE: Versatile Video Retrieval Through Unified Embeddings</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12193" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shaunak Halbe, Bhagyashree Puranik, Jayakrishnan Unnikrishnan, Kushan Thakkar, Vimal Bhat</span></p>
<p><strong>TL;DR:</strong> VIRTUE是一个基于多模态大语言模型（MLLM）的通用视频检索框架，它在一个统一的架构中集成了语料库级检索、细粒度时刻定位以及组合多模态查询能力。该方法通过共享的MLLM主干生成视觉和文本嵌入，并利用对比学习进行对齐，从而实现高效的基于嵌入的候选搜索。模型在70万对视觉-文本数据上使用低秩适应（LoRA）高效训练，在零样本视频检索任务上超越了其他MLLM方法，并且在零样本组合视频检索上取得了SOTA结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代视频检索系统需要处理从语料库级检索、细粒度时刻定位到灵活多模态查询的多样化任务。专用架构通过在大规模数据集上训练模态特定编码器实现了强大的检索性能，但缺乏处理组合多模态查询的能力；而基于MLLM的方法虽然支持丰富的多模态搜索，但其检索性能远低于专用系统。本文旨在开发一个统一的MLLM框架，以弥补这一差距。</p>
<p><strong>Result:</strong> 在零样本视频检索任务上，VIRTUE超越了其他MLLM方法。同一模型无需额外训练即可在零样本时刻检索上取得有竞争力的结果，并在零样本组合视频检索上达到SOTA。通过额外训练对基于嵌入搜索得到的候选进行重排序后，模型性能大幅超越现有MLLM检索系统，并达到与在数量级更大数据上训练的SOTA专用模型相当的检索性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个统一的MLLM框架，将多种视频检索任务（语料库检索、时刻检索、组合查询）整合到单一架构中，通过共享主干和对比对齐实现高效嵌入。客观来看，其高效训练策略（使用LoRA在相对较小的70万对数据上训练）和将嵌入搜索与重排序阶段解耦的设计，是实现强大且通用检索性能的关键，为构建多功能且高效的视频检索系统提供了新思路。</p>
<p><strong>Abstract:</strong> Modern video retrieval systems are expected to handle diverse tasks ranging from corpus-level retrieval and fine-grained moment localization to flexible multimodal querying. Specialized architectures achieve strong retrieval performance by training modality-specific encoders on massive datasets, but they lack the ability to process composed multimodal queries. In contrast, multimodal LLM (MLLM)-based methods support rich multimodal search but their retrieval performance remains well below that of specialized systems. We present VIRTUE, an MLLM-based versatile video retrieval framework that integrates corpus and moment-level retrieval capabilities while accommodating composed multimodal queries within a single architecture. We use contrastive alignment of visual and textual embeddings generated using a shared MLLM backbone to facilitate efficient embedding-based candidate search. Our embedding model, trained efficiently using low-rank adaptation (LoRA) on 700K paired visual-text data samples, surpasses other MLLM-based methods on zero-shot video retrieval tasks. Additionally, we demonstrate that the same model can be adapted without further training to achieve competitive results on zero-shot moment retrieval, and state of the art results for zero-shot composed video retrieval. With additional training for reranking candidates identified in the embedding-based search, our model substantially outperforms existing MLLM-based retrieval systems and achieves retrieval performance comparable to state of the art specialized models which are trained on orders of magnitude larger data.</p>
  </div>
</details>

<hr>
<h3 id="108-Where-It-Moves-It-Matters-Referring-Surgical-Instrument-Segmentation-via-Motion-cs-CV-cs-AIPDF"><a href="#108-Where-It-Moves-It-Matters-Referring-Surgical-Instrument-Segmentation-via-Motion-cs-CV-cs-AIPDF" class="headerlink" title="[108] Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion cs.CV | cs.AIPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12224">Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12224" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai</span></p>
<p><strong>TL;DR:</strong> 本文提出SurgRef，一种基于运动引导的框架，用于手术视频中的参考分割任务，通过捕捉手术器械的运动模式而非静态外观，实现对自由形式语言描述的定位与分割。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法依赖静态视觉线索和预定义器械名称，难以泛化；本文旨在利用器械运动信息解决遮挡、歧义或陌生术语下的分割问题。</p>
<p><strong>Result:</strong> 在Ref-IMotion数据集上，SurgRef实现了最先进的准确性和泛化能力，为语言驱动的手术视频分割设立了新基准。</p>
<p><strong>Insight:</strong> 创新点在于将运动作为核心线索进行参考分割，并构建了包含密集时空掩码和以运动为中心表达的多机构数据集，提升了模型对动态场景的理解能力。</p>
<p><strong>Abstract:</strong> Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.</p>
  </div>
</details>

<hr>
<h3 id="109-Less-is-More-Label-Guided-Summarization-of-Procedural-and-Instructional-Videos-cs-CV-cs-AIPDF"><a href="#109-Less-is-More-Label-Guided-Summarization-of-Procedural-and-Instructional-Videos-cs-CV-cs-AIPDF" class="headerlink" title="[109] Less is More: Label-Guided Summarization of Procedural and Instructional Videos cs.CV | cs.AIPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12243">Less is More: Label-Guided Summarization of Procedural and Instructional Videos</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shreya Rajpal, Michal Golovanesky, Carsten Eickhoff</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PRISM的三阶段框架，用于生成基于语义的视频摘要，特别针对手术训练等高风险领域的程序性和教学视频。该方法结合自适应视觉采样、标签驱动的关键帧锚定以及大型语言模型的上下文验证，确保所选帧反映有意义的过程转换，同时过滤掉通用或幻觉内容。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有视频摘要方法在理解视频语义和捕捉时序流程方面的不足，特别是在程序性和教学视频中生成上下文连贯且语义基础扎实的摘要的需求。</p>
<p><strong>Result:</strong> 在指令性和活动数据集上评估，尽管采样少于原始帧的5%，但摘要保留了84%的语义内容，比基线方法提升高达33%，在语义对齐和精确度方面均表现出色。</p>
<p><strong>Insight:</strong> 创新点在于集成自适应采样、标签驱动锚定和LLM验证的三阶段框架，确保摘要的语义基础和上下文连贯性，可借鉴于其他需要高精度语义理解的视频处理任务。</p>
<p><strong>Abstract:</strong> Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what’s happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.</p>
  </div>
</details>

<hr>
<h3 id="110-AgenticPruner-MAC-Constrained-Neural-Network-Compression-via-LLM-Driven-Strategy-Search-cs-CVPDF"><a href="#110-AgenticPruner-MAC-Constrained-Neural-Network-Compression-via-LLM-Driven-Strategy-Search-cs-CVPDF" class="headerlink" title="[110] AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search cs.CVPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12272">AgenticPruner: MAC-Constrained Neural Network Compression via LLM-Driven Strategy Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12272" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shahrzad Esmat, Mahdi Banisharif, Ali Jannesari</span></p>
<p><strong>TL;DR:</strong> 本文提出AgenticPruner框架，利用大语言模型驱动的智能体协同搜索策略，实现MAC计算量约束下的神经网络压缩。该方法通过三个智能体（分析模型MAC分布的Profiling Agent、协调工作流的Master Agent、以及基于Claude 3.5 Sonnet从历史尝试中学习最优策略的Analysis Agent）进行迭代策略学习，在满足严格MAC预算的同时保持模型精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有神经网络剪枝方法主要关注参数量减少，无法直接控制计算成本（MAC操作数），导致在需要严格满足MAC预算的部署场景中推理延迟不可预测。本文旨在解决MAC约束下的模型压缩问题。</p>
<p><strong>Result:</strong> 在ImageNet-1K数据集上验证了ResNet、ConvNeXt和DeiT架构：ResNet-50达到1.77G MACs时精度77.04%（比基线+0.91%）；ResNet-101达到4.22G MACs时精度78.94%（+1.56%）；ConvNeXt-Small剪枝至8.17G MACs时实现1.41倍GPU加速和1.07倍CPU加速，参数量减少45%。Vision Transformer能在用户定义的容差带内（通常超支+1%至+5%，欠支-5%至-15%）满足MAC预算。</p>
<p><strong>Insight:</strong> 创新点包括：1）LLM驱动的多智能体协同框架实现MAC约束优化；2）基于上下文学习（in-context learning）的策略搜索，将收敛成功率从网格搜索的48%提升至71%；3）在同构剪枝的图结构分组基础上，通过分析剪枝迭代间的模式实现上下文感知自适应，自动收敛到目标MAC预算。</p>
<p><strong>Abstract:</strong> Neural network pruning remains essential for deploying deep learning models on resource-constrained devices, yet existing approaches primarily target parameter reduction without directly controlling computational cost. This yields unpredictable inference latency in deployment scenarios where strict Multiply-Accumulate (MAC) operation budgets must be met. We propose AgenticPruner, a framework utilizing large language models to achieve MAC-constrained optimization through iterative strategy learning. Our approach coordinates three specialized agents: a Profiling Agent that analyzes model architecture and MAC distributions, a Master Agent that orchestrates the workflow with divergence monitoring, and an Analysis Agent powered by Claude 3.5 Sonnet that learns optimal strategies from historical attempts. Through in-context learning, the Analysis Agent improves convergence success rate from 48% to 71% compared to grid search. Building upon isomorphic pruning’s graph-based structural grouping, our method adds context-aware adaptation by analyzing patterns across pruning iterations, enabling automatic convergence to target MAC budgets within user-defined tolerance bands.   We validate our framework on ImageNet-1K across ResNet, ConvNeXt, and DeiT architectures. On CNNs, our approach achieves MAC targeting while maintaining or improving accuracy: ResNet-50 reaches 1.77G MACs with 77.04% accuracy (+0.91% vs baseline); ResNet-101 achieves 4.22G MACs with 78.94% accuracy (+1.56% vs baseline). For ConvNeXt-Small, pruning to 8.17G MACs yields 1.41x GPU and 1.07x CPU speedup with 45% parameter reduction. On Vision Transformers, we demonstrate MAC-budget compliance within user-defined tolerance bands (typically +1% to +5% overshoot, -5% to -15% undershoot), establishing feasibility for deployment scenarios requiring strict computational guarantees.</p>
  </div>
</details>

<hr>
<h3 id="111-CytoCLIP-Learning-Cytoarchitectural-Characteristics-in-Developing-Human-Brain-Using-Contrastive-Language-Image-Pre-Training-cs-CV-cs-AIPDF"><a href="#111-CytoCLIP-Learning-Cytoarchitectural-Characteristics-in-Developing-Human-Brain-Using-Contrastive-Language-Image-Pre-Training-cs-CV-cs-AIPDF" class="headerlink" title="[111] CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training cs.CV | cs.AIPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12282">CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12282" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam</span></p>
<p><strong>TL;DR:</strong> 本文提出CytoCLIP，一种基于预训练CLIP框架的视觉-语言模型，用于学习人类胎儿大脑细胞构筑特征的联合视觉-文本表示。该模型包含两个变体：一个使用低分辨率全区域图像学习整体细胞构筑模式，另一个使用高分辨率图像块学习细胞级细节表示。模型在包含不同胎龄胎儿大脑NISSL染色组织切片的训练数据集上进行训练，并在区域分类和跨模态检索任务中评估其理解和泛化能力。实验结果表明，CytoCLIP在多种数据设置下均优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大脑不同区域的功能与其独特的细胞构筑（由细胞的空间排列和形态定义）密切相关。手动在脑组织切片中描绘这些区域耗时且需要专业知识，因此需要自动化方法来减少专家工作量。</p>
<p><strong>Result:</strong> CytoCLIP在区域分类任务中表现优异：低分辨率全区域图像分类的F1分数达到0.87，高分辨率图像块分类的F1分数达到0.91。在包含不同年龄和切片平面的多种数据设置下，其性能均超越现有方法。</p>
<p><strong>Insight:</strong> 论文的创新点在于将CLIP框架应用于大脑细胞构筑分析，通过视觉-语言对比学习联合表示，并设计了针对整体模式和细胞细节的双分辨率模型变体。从客观角度看，该方法将自然图像预训练模型成功迁移到高度专业化的神经解剖学领域，实现了对细胞构筑特征的有效编码和跨模态理解。</p>
<p><strong>Abstract:</strong> The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model’s understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.</p>
  </div>
</details>

<hr>
<h3 id="112-Concepts-from-Representations-Post-hoc-Concept-Bottleneck-Models-via-Sparse-Decomposition-of-Visual-Representations-cs-CVPDF"><a href="#112-Concepts-from-Representations-Post-hoc-Concept-Bottleneck-Models-via-Sparse-Decomposition-of-Visual-Representations-cs-CVPDF" class="headerlink" title="[112] Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations cs.CVPDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12303">Concepts from Representations: Post-hoc Concept Bottleneck Models via Sparse Decomposition of Visual Representations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12303" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shizhan Gong, Xiaofan Zhang, Qi Dou</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PCBM-ReD的后置概念瓶颈模型，通过稀疏分解视觉表示来增强预训练模型的可解释性。该方法自动从预训练编码器中提取视觉概念，利用多模态大语言模型（MLLMs）基于视觉可识别性和任务相关性进行概念标注与筛选，并通过重构引导的优化选择独立概念子集，最终将图像表示分解为概念嵌入的线性组合以适配概念瓶颈模型框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决深度学习模型在图像识别中因不透明性而难以部署于关键领域的问题，同时克服现有后置解释方法和前置概念瓶颈模型（CBMs）在概念相关性不可靠、概念定义非视觉化或劳动密集、以及模型或数据无关假设等方面的局限性。</p>
<p><strong>Result:</strong> 在11个图像分类任务上的广泛实验表明，PCBM-ReD实现了最先进的准确率，缩小了与端到端模型的性能差距，并展现出更好的可解释性。</p>
<p><strong>Insight:</strong> 创新点在于利用CLIP的视觉-文本对齐特性，通过稀疏分解将预训练模型的表示线性化为人类可理解的概念嵌入，从而在保持高性能的同时实现后置可解释性；客观分析认为，该方法结合了自动概念提取、多模态标注和优化选择，为黑盒模型的可解释性提供了一种高效且通用的解决方案。</p>
<p><strong>Abstract:</strong> Deep learning has achieved remarkable success in image recognition, yet their inherent opacity poses challenges for deployment in critical domains. Concept-based interpretations aim to address this by explaining model reasoning through human-understandable concepts. However, existing post-hoc methods and ante-hoc concept bottleneck models (CBMs), suffer from limitations such as unreliable concept relevance, non-visual or labor-intensive concept definitions, and model or data-agnostic assumptions. This paper introduces Post-hoc Concept Bottleneck Model via Representation Decomposition (PCBM-ReD), a novel pipeline that retrofits interpretability onto pretrained opaque models. PCBM-ReD automatically extracts visual concepts from a pre-trained encoder, employs multimodal large language models (MLLMs) to label and filter concepts based on visual identifiability and task relevance, and selects an independent subset via reconstruction-guided optimization. Leveraging CLIP’s visual-text alignment, it decomposes image representations into linear combination of concept embeddings to fit into the CBMs abstraction. Extensive experiments across 11 image classification tasks show PCBM-ReD achieves state-of-the-art accuracy, narrows the performance gap with end-to-end models, and exhibits better interpretability.</p>
  </div>
</details>

<hr>
<h3 id="113-A-Two-Stage-Globally-Diverse-Adversarial-Attack-for-Vision-Language-Pre-training-Models-cs-CV-cs-AIPDF"><a href="#113-A-Two-Stage-Globally-Diverse-Adversarial-Attack-for-Vision-Language-Pre-training-Models-cs-CV-cs-AIPDF" class="headerlink" title="[113] A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models cs.CV | cs.AIPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12304">A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12304" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wutao Chen, Huaqin Zou, Chen Wan, Lifeng Huang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为2S-GDA的两阶段全局多样性对抗攻击框架，用于攻击视觉-语言预训练模型。该方法通过全局多样性策略引入文本扰动，并结合多尺度调整和块状洗牌旋转增强视觉多样性，以解决现有多模态攻击中扰动多样性有限和多阶段流程不稳定的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对视觉-语言预训练模型在黑盒场景下易受对抗样本攻击，且现有攻击方法存在扰动多样性有限和多阶段流程不稳定的挑战，本文旨在设计一种更有效的对抗攻击框架。</p>
<p><strong>Result:</strong> 在VLP模型上的大量实验表明，2S-GDA在黑盒设置下的攻击成功率比现有最先进方法提升了高达11.17%，并展现出稳定的性能提升。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个模块化的两阶段攻击框架，通过全局多样性策略（候选文本扩展与全局感知替换）生成文本扰动，并利用多尺度调整和块状洗牌旋转增强视觉多样性，从而提高了对抗样本的可迁移性和攻击效果。</p>
<p><strong>Abstract:</strong> Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.</p>
  </div>
</details>

<hr>
<h3 id="114-Multi-Sensor-Matching-with-HyperNetworks-cs-CVPDF"><a href="#114-Multi-Sensor-Matching-with-HyperNetworks-cs-CVPDF" class="headerlink" title="[114] Multi-Sensor Matching with HyperNetworks cs.CVPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12325">Multi-Sensor Matching with HyperNetworks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12325" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eli Passov, Nathan S. Netanyahu, Yosi Keller</span></p>
<p><strong>TL;DR:</strong> 本文提出一种基于超网络的轻量级描述子学习架构，用于提升多模态图像块匹配的鲁棒性。该架构通过超网络模块实现通道级的自适应缩放与平移，并结合条件实例归一化在浅层进行模态特定适应（如可见光与红外图像），在保持推理效率的同时增强了对外观变化的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态图像匹配（如可见光与红外图像）中因外观差异导致的描述子泛化能力不足的问题，旨在设计一个高效且适应性强的方法。</p>
<p><strong>Result:</strong> 在VIS-NIR及其他VIS-IR基准测试中取得了最先进（SOTA）的结果，并在其他数据集上达到或超越了先前方法（尽管先前方法推理成本更高）。同时发布了包含50万对图像块的跨平台（地面&#x2F;空中）VIS-IR数据集GAP-VIR，以促进领域偏移研究。</p>
<p><strong>Insight:</strong> 创新点在于将超网络与条件实例归一化结合，以轻量级方式实现模态自适应的描述子学习；客观来看，该方法在保持推理效率的前提下，通过自适应调制提升了跨模态匹配的鲁棒性，并贡献了新的评估数据集。</p>
<p><strong>Abstract:</strong> Hypernetworks are models that generate or modulate the weights of another network. They provide a flexible mechanism for injecting context and task conditioning and have proven broadly useful across diverse applications without significant increases in model size. We leverage hypernetworks to improve multimodal patch matching by introducing a lightweight descriptor-learning architecture that augments a Siamese CNN with (i) hypernetwork modules that compute adaptive, per-channel scaling and shifting and (ii) conditional instance normalization that provides modality-specific adaptation (e.g., visible vs. infrared, VIS-IR) in shallow layers. This combination preserves the efficiency of descriptor-based methods during inference while increasing robustness to appearance shifts. Trained with a triplet loss and hard-negative mining, our approach achieves state-of-the-art results on VIS-NIR and other VIS-IR benchmarks and matches or surpasses prior methods on additional datasets, despite their higher inference cost. To spur progress on domain shift, we also release GAP-VIR, a cross-platform (ground&#x2F;aerial) VIS-IR patch dataset with 500K pairs, enabling rigorous evaluation of cross-domain generalization and adaptation.</p>
  </div>
</details>

<hr>
<h3 id="115-EmoKGEdit-Training-free-Affective-Injection-via-Visual-Cue-Transformation-cs-CVPDF"><a href="#115-EmoKGEdit-Training-free-Affective-Injection-via-Visual-Cue-Transformation-cs-CVPDF" class="headerlink" title="[115] EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation cs.CVPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12326">EmoKGEdit: Training-free Affective Injection via Visual Cue Transformation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12326" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jing Zhang, Bingjie Fan</span></p>
<p><strong>TL;DR:</strong> 本文提出EmoKGEdit，一种无需训练的图像情感编辑框架，通过构建多模态情感关联知识图谱（MSA-KG）来解耦对象、场景、属性和情感之间的复杂关系，并利用该图谱指导多模态大模型推理情感相关的视觉线索，从而在保持图像结构的同时精确注入目标情感。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有图像情感编辑方法难以从潜在内容表示中解耦情感线索，常导致情感表达弱且视觉结构扭曲，本文旨在解决这一问题，实现精确且结构保持的情感编辑。</p>
<p><strong>Result:</strong> 大量实验表明，EmoKGEdit在情感保真度和内容保持方面均表现出色，并超越了现有最先进方法。</p>
<p><strong>Insight:</strong> 创新点在于构建了显式编码对象-属性-情感因果链的MSA-KG作为外部知识，支持思维链推理，并设计了潜在空间中解耦结构与情感的编辑模块，确保情感有效注入且严格维持视觉空间一致性。</p>
<p><strong>Abstract:</strong> Existing image emotion editing methods struggle to disentangle emotional cues from latent content representations, often yielding weak emotional expression and distorted visual structures. To bridge this gap, we propose EmoKGEdit, a novel training-free framework for precise and structure-preserving image emotion editing. Specifically, we construct a Multimodal Sentiment Association Knowledge Graph (MSA-KG) to disentangle the intricate relationships among objects, scenes, attributes, visual clues and emotion. MSA-KG explicitly encode the causal chain among object-attribute-emotion, and as external knowledge to support chain of thought reasoning, guiding the multimodal large model to infer plausible emotion-related visual cues and generate coherent instructions. In addition, based on MSA-KG, we design a disentangled structure-emotion editing module that explicitly separates emotional attributes from layout features within the latent space, which ensures that the target emotion is effectively injected while strictly maintaining visual spatial coherence. Extensive experiments demonstrate that EmoKGEdit achieves excellent performance in both emotion fidelity and content preservation, and outperforms the state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="116-FlowIID-Single-Step-Intrinsic-Image-Decomposition-via-Latent-Flow-Matching-cs-CVPDF"><a href="#116-FlowIID-Single-Step-Intrinsic-Image-Decomposition-via-Latent-Flow-Matching-cs-CVPDF" class="headerlink" title="[116] FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching cs.CVPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12329">FlowIID: Single-Step Intrinsic Image Decomposition via Latent Flow Matching</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12329" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mithlesh Singla, Seema Kumari, Shanmuganathan Raman</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为FlowIID的单步内在图像分解方法，该方法基于潜在流匹配技术，将图像分解为反照率和着色分量。该方法设计紧凑、参数高效，适合资源受限和实时视觉应用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有内在图像分解模型虽然效果好，但参数量大，导致在实际应用中与其他模型结合时成本高昂。论文旨在解决这一问题，提出一种参数高效且能单步推理的解决方案。</p>
<p><strong>Result:</strong> FlowIID在多个基准测试中取得了具有竞争力且优于现有模型的结果，尽管设计紧凑，但性能表现优异。</p>
<p><strong>Insight:</strong> 创新点在于将VAE引导的潜在空间与流匹配模块结合，实现了稳定且高效的单步分解；从客观角度看，该方法通过流匹配简化了生成过程，降低了计算复杂度，为轻量级实时应用提供了新思路。</p>
<p><strong>Abstract:</strong> Intrinsic Image Decomposition (IID) separates an image into albedo and shading components. It is a core step in many real-world applications, such as relighting and material editing. Existing IID models achieve good results, but often use a large number of parameters. This makes them costly to combine with other models in real-world settings. To address this problem, we propose a flow matching-based solution. For this, we design a novel architecture, FlowIID, based on latent flow matching. FlowIID combines a VAE-guided latent space with a flow matching module, enabling a stable decomposition of albedo and shading. FlowIID is not only parameter-efficient, but also produces results in a single inference step. Despite its compact design, FlowIID delivers competitive and superior results compared to existing models across various benchmarks. This makes it well-suited for deployment in resource-constrained and real-time vision applications.</p>
  </div>
</details>

<hr>
<h3 id="117-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents-cs-CVPDF"><a href="#117-MMDeepResearch-Bench-A-Benchmark-for-Multimodal-Deep-Research-Agents-cs-CVPDF" class="headerlink" title="[117] MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents cs.CVPDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12346">MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12346" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peizhou Huang, Zixuan Zhong, Zhongwei Wan, Donghao Zhou, Samiul Alam</span></p>
<p><strong>TL;DR:</strong> 本文介绍了MMDeepResearch-Bench（MMDR-Bench），这是一个用于评估多模态深度研究代理（DRAs）的基准测试，包含140个跨21个领域的专家构建任务，要求模型基于图像-文本包生成引用丰富的报告。论文还提出了一套统一的、可解释的评估流程（FLAE、TRACE、MOSAIC），用于细粒度评估报告质量、引用证据对齐以及文本-视觉完整性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的基准测试主要针对纯文本设置或短格式多模态问答，缺乏对端到端多模态证据使用的评估，因此需要一个新的基准来评估深度研究代理在多模态理解与引用报告生成方面的能力。</p>
<p><strong>Result:</strong> 在25个最先进模型上的实验揭示了生成质量、引用规范性和多模态基础之间的系统性权衡，表明仅凭流畅的文本生成并不能保证忠实使用证据，且多模态完整性仍是深度研究代理的关键瓶颈。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个强调报告式合成与显式证据使用的多模态基准，并提出了一个支持细粒度错误诊断的统一评估框架，这有助于推动多模态深度研究代理在证据忠实性和视觉-文本一致性方面的进步。</p>
<p><strong>Abstract:</strong> Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.</p>
  </div>
</details>

<hr>
<h3 id="118-From-Prompts-to-Pavement-LMMs-based-Agentic-Behavior-Tree-Generation-Framework-for-Autonomous-Vehicles-cs-CV-cs-AI-cs-ROPDF"><a href="#118-From-Prompts-to-Pavement-LMMs-based-Agentic-Behavior-Tree-Generation-Framework-for-Autonomous-Vehicles-cs-CV-cs-AI-cs-ROPDF" class="headerlink" title="[118] From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles cs.CV | cs.AI | cs.ROPDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12358">From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12358" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Omar Y. Goba, Ahmed Y. Gado, Catherine M. Elias, Ahmed Hussein</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于大语言模型（LLMs）和多模态视觉模型（LVMs）的智能体框架，用于为自动驾驶车辆动态生成和调整行为树（BTs）。该框架通过描述器、规划器和生成器三个智能体协作，在基线行为树失效时，自动评估场景关键性、规划高层子目标并生成可执行的XML格式行为子树，从而在模拟环境中成功应对意外障碍（如道路堵塞），无需人工干预。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统行为树虽然提供了结构化的决策逻辑，但本质上是静态的，需要大量人工调优，这限制了其在SAE L5级自动驾驶中的适用性。本文旨在解决自动驾驶车辆在不可预测的真实环境中需要自适应行为规划器的问题。</p>
<p><strong>Result:</strong> 在CARLA+Nav2仿真环境中，系统仅在基线行为树失效时触发，成功实现了在无人工干预下绕行意外障碍（如街道堵塞）。与静态行为树基线相比，该方法是一个概念验证，可扩展到多种驾驶场景。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个由LLMs&#x2F;LVMs驱动的多智能体框架，将链式符号提示、上下文学习与行为树生成相结合，实现了从自然语言提示到可执行行为树的动态、按需生成，为自动驾驶的适应性规划提供了新思路。</p>
<p><strong>Abstract:</strong> Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.</p>
  </div>
</details>

<hr>
<h3 id="119-DepthCropSeg-Scaling-a-Crop-Segmentation-Foundation-Model-With-Depth-Labeled-Data-cs-CVPDF"><a href="#119-DepthCropSeg-Scaling-a-Crop-Segmentation-Foundation-Model-With-Depth-Labeled-Data-cs-CVPDF" class="headerlink" title="[119] DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data cs.CVPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12366">DepthCropSeg++: Scaling a Crop Segmentation Foundation Model With Depth-Labeled Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12366" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiafei Zhang, Songliang Cao, Binghui Xu, Yanan Li, Weiwei Jia</span></p>
<p><strong>TL;DR:</strong> DepthCropSeg++是一个用于作物分割的基础模型，能够在开放田间环境下分割不同作物物种。该模型通过构建大规模、多物种、多场景的深度标注数据集，并基于ViT-Adapter架构进行增强，采用两阶段自训练流程，显著提升了作物分割的泛化能力和精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前作物分割模型因像素级标注成本高昂，通常依赖有限数据，仅在特定作物类型或受控环境下表现良好，缺乏泛化能力。本文旨在构建一个能够跨物种、跨场景泛化的作物分割基础模型，以支持植物表型分析、密度估计和杂草控制等下游农业任务。</p>
<p><strong>Result:</strong> 在综合测试集上，DepthCropSeg++达到了93.11%的mIoU，显著优于有监督基线（+0.36%）和通用视觉基础模型如SAM（+48.57%）。在夜间环境（86.90% mIoU）、高密度冠层（90.09% mIoU）和未见作物品种（90.09% mIoU）等挑战性场景下表现优异，确立了作物分割的新SOTA。</p>
<p><strong>Insight:</strong> 创新点包括：1) 构建了大规模、多物种（30+）、多环境（15种条件）的深度标注作物分割数据集（28,406张图像），解决了数据稀缺问题；2) 在SOTA的ViT-Adapter架构中引入动态上采样以增强细节感知；3) 采用两阶段自训练流程，有效利用未标注或弱标注数据。从客观角度看，将基础模型范式与领域特定（农业）的大规模数据工程和架构优化相结合，是提升专业视觉任务性能的有效路径。</p>
<p><strong>Abstract:</strong> DepthCropSeg++: a foundation model for crop segmentation, capable of segmenting different crop species under open in-field environment. Crop segmentation is a fundamental task for modern agriculture, which closely relates to many downstream tasks such as plant phenotyping, density estimation, and weed control. In the era of foundation models, a number of generic large language and vision models have been developed. These models have demonstrated remarkable real world generalization due to significant model capacity and largescale datasets. However, current crop segmentation models mostly learn from limited data due to expensive pixel-level labelling cost, often performing well only under specific crop types or controlled environment. In this work, we follow the vein of our previous work DepthCropSeg, an almost unsupervised approach to crop segmentation, to scale up a cross-species and crossscene crop segmentation dataset, with 28,406 images across 30+ species and 15 environmental conditions. We also build upon a state-of-the-art semantic segmentation architecture ViT-Adapter architecture, enhance it with dynamic upsampling for improved detail awareness, and train the model with a two-stage selftraining pipeline. To systematically validate model performance, we conduct comprehensive experiments to justify the effectiveness and generalization capabilities across multiple crop datasets. Results demonstrate that DepthCropSeg++ achieves 93.11% mIoU on a comprehensive testing set, outperforming both supervised baselines and general-purpose vision foundation models like Segmentation Anything Model (SAM) by significant margins (+0.36% and +48.57% respectively). The model particularly excels in challenging scenarios including night-time environment (86.90% mIoU), high-density canopies (90.09% mIoU), and unseen crop varieties (90.09% mIoU), indicating a new state of the art for crop segmentation.</p>
  </div>
</details>

<hr>
<h3 id="120-A-Hierarchical-Benchmark-of-Foundation-Models-for-Dermatology-cs-CVPDF"><a href="#120-A-Hierarchical-Benchmark-of-Foundation-Models-for-Dermatology-cs-CVPDF" class="headerlink" title="[120] A Hierarchical Benchmark of Foundation Models for Dermatology cs.CVPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12382">A Hierarchical Benchmark of Foundation Models for Dermatology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12382" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Furkan Yuceyalcin, Abdurrahim Yilmaz, Burak Temelkuran</span></p>
<p><strong>TL;DR:</strong> 本研究评估了十种基础模型在皮肤病变分层分类中的嵌入表示效用，揭示了模型在诊断粒度上的能力差异，即通用医学影像模型在高级别筛查中表现优异，而皮肤病学专用模型在细粒度亚型分类上更具优势。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前皮肤病学基准测试常将复杂的诊断分类简化为扁平的二分类任务，这掩盖了模型执行细粒度鉴别诊断的能力，而这对临床工作流整合至关重要。</p>
<p><strong>Result:</strong> 在DERM12345数据集上，通用医学影像模型MedImageInsights在二元恶性检测上获得最高加权F1分数（97.52%），但在40类亚型分类上降至65.50%；皮肤病学专用模型（如MedSigLip，69.79%）在细粒度分类上表现更佳。</p>
<p><strong>Insight:</strong> 论文创新点在于引入了分层评估框架，以评估模型在四个临床粒度级别上的性能，并揭示了模型能力中的’粒度鸿沟’，表明诊断支持系统需要结合通用筛查模型和针对细粒度区分的专用建模策略。</p>
<p><strong>Abstract:</strong> Foundation models have transformed medical image analysis by providing robust feature representations that reduce the need for large-scale task-specific training. However, current benchmarks in dermatology often reduce the complex diagnostic taxonomy to flat, binary classification tasks, such as distinguishing melanoma from benign nevi. This oversimplification obscures a model’s ability to perform fine-grained differential diagnoses, which is critical for clinical workflow integration. This study evaluates the utility of embeddings derived from ten foundation models, spanning general computer vision, general medical imaging, and dermatology-specific domains, for hierarchical skin lesion classification. Using the DERM12345 dataset, which comprises 40 lesion subclasses, we calculated frozen embeddings and trained lightweight adapter models using a five-fold cross-validation. We introduce a hierarchical evaluation framework that assesses performance across four levels of clinical granularity: 40 Subclasses, 15 Main Classes, 2 and 4 Superclasses, and Binary Malignancy. Our results reveal a “granularity gap” in model capabilities: MedImageInsights achieved the strongest overall performance (97.52% weighted F1-Score on Binary Malignancy detection) but declined to 65.50% on fine-grained 40-class subtype classification. Conversely, MedSigLip (69.79%) and dermatology-specific models (Derm Foundation and MONET) excelled at fine-grained 40-class subtype discrimination while achieving lower overall performance than MedImageInsights on broader classification tasks. Our findings suggest that while general medical foundation models are highly effective for high-level screening, specialized modeling strategies are necessary for the granular distinctions required in diagnostic support systems.</p>
  </div>
</details>

<hr>
<h3 id="121-HOT-POT-Optimal-Transport-for-Sparse-Stereo-Matching-cs-CV-math-OCPDF"><a href="#121-HOT-POT-Optimal-Transport-for-Sparse-Stereo-Matching-cs-CV-math-OCPDF" class="headerlink" title="[121] HOT-POT: Optimal Transport for Sparse Stereo Matching cs.CV | math.OCPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12423">HOT-POT: Optimal Transport for Sparse Stereo Matching</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | math.OC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12423" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Antonin Clerc, Michael Quellmalz, Moritz Piening, Philipp Flotho, Gregor Kornhardt</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于最优传输（OT）的稀疏立体匹配方法HOT-POT，用于解决存在遮挡、运动和相机畸变等挑战的立体视觉问题。该方法将相机投影点建模为（半）线，利用极线距离和3D射线距离作为匹配质量的度量，并将其构建为可高效求解的（部分）最优传输问题。此外，该方法还通过分层OT扩展至无监督物体匹配，并在面部分析等应用中验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决在自动驾驶、机器人和面部分析等应用中，由于遮挡、运动和相机畸变导致的立体视觉匹配困难，特别是针对面部关键点等稀疏特征的匹配，因其参数敏感性而更具挑战性。</p>
<p><strong>Result:</strong> 数值实验表明，所提出的算法能够实现高效的特征和物体匹配，特别是在面部分析中匹配不同的关键点标注规范方面得到了验证。</p>
<p><strong>Insight:</strong> 创新点在于从最优传输的视角重新构建了稀疏立体匹配问题，引入了极线距离和3D射线距离作为OT成本函数，并将问题形式化为可高效求解的分配问题；同时，通过分层OT扩展至无监督物体匹配，为稀疏特征匹配提供了一种新的、稳健的框架。</p>
<p><strong>Abstract:</strong> Stereo vision between images faces a range of challenges, including occlusions, motion, and camera distortions, across applications in autonomous driving, robotics, and face analysis. Due to parameter sensitivity, further complications arise for stereo matching with sparse features, such as facial landmarks. To overcome this ill-posedness and enable unsupervised sparse matching, we consider line constraints of the camera geometry from an optimal transport (OT) viewpoint. Formulating camera-projected points as (half)lines, we propose the use of the classical epipolar distance as well as a 3D ray distance to quantify matching quality. Employing these distances as a cost function of a (partial) OT problem, we arrive at efficiently solvable assignment problems. Moreover, we extend our approach to unsupervised object matching by formulating it as a hierarchical OT problem. The resulting algorithms allow for efficient feature and object matching, as demonstrated in our numerical experiments. Here, we focus on applications in facial analysis, where we aim to match distinct landmarking conventions.</p>
  </div>
</details>

<hr>
<h3 id="122-Adversarial-Defense-in-Vision-Language-Models-An-Overview-cs-CV-cs-AIPDF"><a href="#122-Adversarial-Defense-in-Vision-Language-Models-An-Overview-cs-CV-cs-AIPDF" class="headerlink" title="[122] Adversarial Defense in Vision-Language Models: An Overview cs.CV | cs.AIPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12443">Adversarial Defense in Vision-Language Models: An Overview</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12443" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaowei Fu, Lei Zhang</span></p>
<p><strong>TL;DR:</strong> 本文综述了视觉语言模型（VLMs）对抗性防御的最新进展，系统梳理了三种主要防御范式：训练时防御、测试时自适应防御和无训练防御，并分析了各类方法的优势、局限及当前挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着视觉语言模型（如CLIP）的广泛应用，其易受复杂且难以察觉的对抗性攻击的脆弱性引发了安全担忧，这些攻击可能损害跨模态任务中的模型性能与系统安全，因此需要系统研究防御策略。</p>
<p><strong>Result:</strong> 综述未提供具体定量结果，但总结了各类防御方法在提升VLM鲁棒性方面的有效性、资源消耗和泛化能力等定性比较。</p>
<p><strong>Insight:</strong> 创新点在于系统性地将VLM对抗防御归纳为三大范式，并指出无训练防御通过直接处理对抗输入或特征嵌入来避免模型修改，为资源受限场景提供了灵活解决方案；客观来看，该分类框架为理解和比较不同防御方法提供了清晰视角。</p>
<p><strong>Abstract:</strong> The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.</p>
  </div>
</details>

<hr>
<h3 id="123-DCAC-Dynamic-Class-Aware-Cache-Creates-Stronger-Out-of-Distribution-Detectors-cs-CVPDF"><a href="#123-DCAC-Dynamic-Class-Aware-Cache-Creates-Stronger-Out-of-Distribution-Detectors-cs-CVPDF" class="headerlink" title="[123] DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors cs.CVPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12468">DCAC: Dynamic Class-Aware Cache Creates Stronger Out-of-Distribution Detectors</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12468" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yanqi Wu, Qichao Chen, Runhe Lai, Xinhua Lu, Jia-Xin Zhuang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DCAC（动态类感知缓存）的训练无关、测试时校准模块，用于解决深度神经网络在分布外检测中的过度自信预测问题。DCAC为每个分布内类别维护独立的缓存，收集高熵样本并校准输入样本的原始预测，通过轻量级两层模块利用缓存的视觉特征和预测概率来缓解对OOD样本的过度自信。该模块可无缝集成到多种现有OOD检测方法中，包括单模态和视觉语言模型，且计算开销极小。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决深度神经网络在测试时对未见分布外样本的过度自信预测问题，基于观察到被预测为同一类别或赋予高概率的OOD样本在视觉上比真实分布内样本更相似。</p>
<p><strong>Result:</strong> 在多个OOD基准测试上的广泛实验表明，DCAC显著增强了现有方法，例如在ImageNet OOD基准测试中与ASH-S集成时，将FPR95降低了6.55%，实现了实质性改进。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种基于类特定观察的动态类感知缓存机制，这是一种训练无关的测试时校准方法，可灵活集成到现有OOD检测框架中，通过缓存高熵样本进行预测校准，有效提升检测性能，同时保持低计算开销。</p>
<p><strong>Abstract:</strong> Out-of-distribution (OOD) detection remains a fundamental challenge for deep neural networks, particularly due to overconfident predictions on unseen OOD samples during testing. We reveal a key insight: OOD samples predicted as the same class, or given high probabilities for it, are visually more similar to each other than to the true in-distribution (ID) samples. Motivated by this class-specific observation, we propose DCAC (Dynamic Class-Aware Cache), a training-free, test-time calibration module that maintains separate caches for each ID class to collect high-entropy samples and calibrate the raw predictions of input samples. DCAC leverages cached visual features and predicted probabilities through a lightweight two-layer module to mitigate overconfident predictions on OOD samples. This module can be seamlessly integrated with various existing OOD detection methods across both unimodal and vision-language models while introducing minimal computational overhead. Extensive experiments on multiple OOD benchmarks demonstrate that DCAC significantly enhances existing methods, achieving substantial improvements, i.e., reducing FPR95 by 6.55% when integrated with ASH-S on ImageNet OOD benchmark.</p>
  </div>
</details>

<hr>
<h3 id="124-NeuralFur-Animal-Fur-Reconstruction-From-Multi-View-Images-cs-CV-cs-GRPDF"><a href="#124-NeuralFur-Animal-Fur-Reconstruction-From-Multi-View-Images-cs-CV-cs-GRPDF" class="headerlink" title="[124] NeuralFur: Animal Fur Reconstruction From Multi-View Images cs.CV | cs.GRPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12481">NeuralFur: Animal Fur Reconstruction From Multi-View Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12481" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vanessa Sklyarova, Berna Kabadayi, Anastasios Yiannakidis, Giorgio Becherini, Michael J. Black</span></p>
<p><strong>TL;DR:</strong> 本文提出NeuralFur，一种从多视角图像重建高保真动物毛发3D模型的方法。该方法利用视觉语言模型（VLM）获取动物身体各部位毛发的真实长度和结构先验知识，结合基于发丝的表征，在粗略表面几何上生长毛发，并通过几何与光度损失进行监督。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 从图像重建逼真的动物毛发几何极具挑战，因为毛发细节精细、存在自遮挡和视角依赖外观，且缺乏可用于学习不同动物毛发先验的数据集。</p>
<p><strong>Result:</strong> 该方法在多种不同毛发类型的动物上展示了泛化能力，实现了高保真3D毛发建模。</p>
<p><strong>Insight:</strong> 创新点在于首次提出利用视觉语言模型（VLM）的通用知识来引导多视角3D重建，特别是获取毛发长度先验并指导发丝生长方向，以解决缺乏特定数据集和方向模糊性问题。</p>
<p><strong>Abstract:</strong> Reconstructing realistic animal fur geometry from images is a challenging task due to the fine-scale details, self-occlusion, and view-dependent appearance of fur. In contrast to human hairstyle reconstruction, there are also no datasets that can be leveraged to learn a fur prior for different animals. In this work, we present a first multi-view-based method for high-fidelity 3D fur modeling of animals using a strand-based representation, leveraging the general knowledge of a vision language model. Given multi-view RGB images, we first reconstruct a coarse surface geometry using traditional multi-view stereo techniques. We then use a vision language model (VLM) system to retrieve information about the realistic length structure of the fur for each part of the body. We use this knowledge to construct the animal’s furless geometry and grow strands atop it. The fur reconstruction is supervised with both geometric and photometric losses computed from multi-view images. To mitigate orientation ambiguities stemming from the Gabor filters that are applied to the input images, we additionally utilize the VLM to guide the strands’ growth direction and their relation to the gravity vector that we incorporate as a loss. With this new schema of using a VLM to guide 3D reconstruction from multi-view inputs, we show generalization across a variety of animals with different fur types. For additional results and code, please refer to <a target="_blank" rel="noopener" href="https://neuralfur.is.tue.mpg.de/">https://neuralfur.is.tue.mpg.de</a>.</p>
  </div>
</details>

<hr>
<h3 id="125-Histopath-C-Towards-Realistic-Domain-Shifts-for-Histopathology-Vision-Language-Adaptation-cs-CVPDF"><a href="#125-Histopath-C-Towards-Realistic-Domain-Shifts-for-Histopathology-Vision-Language-Adaptation-cs-CVPDF" class="headerlink" title="[125] Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation cs.CVPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12493">Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12493" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mehrdad Noori, Gustavo Adolfo Vargas Hakim, David Osowiechi, Fereshteh Shakeri, Ali Bahri</span></p>
<p><strong>TL;DR:</strong> 本文提出了Histopath-C基准测试，用于模拟组织病理学图像中真实的域偏移（如染色、污染、模糊和噪声），并开发了LATTE方法，一种利用多文本模板的低秩适应策略，以增强视觉语言模型在组织病理学图像中的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决组织病理学视觉语言模型在面对真实世界域偏移（如染色变化和图像退化）时性能下降的问题。</p>
<p><strong>Result:</strong> 在多个组织病理学数据集上，LATTE方法优于为自然图像设计的最先进的测试时适应方法，实现了SOTA性能。</p>
<p><strong>Insight:</strong> 创新点包括引入合成腐蚀的基准测试Histopath-C以模拟真实域偏移，以及提出LATTE策略通过多文本模板和低秩适应来提升模型对文本输入的鲁棒性。</p>
<p><strong>Abstract:</strong> Medical Vision-language models (VLMs) have shown remarkable performances in various medical imaging domains such as histo-pathology by leveraging pre-trained, contrastive models that exploit visual and textual information. However, histopathology images may exhibit severe domain shifts, such as staining, contamination, blurring, and noise, which may severely degrade the VLM’s downstream performance. In this work, we introduce Histopath-C, a new benchmark with realistic synthetic corruptions designed to mimic real-world distribution shifts observed in digital histopathology. Our framework dynamically applies corruptions to any available dataset and evaluates Test-Time Adaptation (TTA) mechanisms on the fly. We then propose LATTE, a transductive, low-rank adaptation strategy that exploits multiple text templates, mitigating the sensitivity of histopathology VLMs to diverse text inputs. Our approach outperforms state-of-the-art TTA methods originally designed for natural images across a breadth of histopathology datasets, demonstrating the effectiveness of our proposed design for robust adaptation in histopathology images. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/Mehrdad-Noori/Histopath-C">https://github.com/Mehrdad-Noori/Histopath-C</a>.</p>
  </div>
</details>

<hr>
<h3 id="126-Video-Individual-Counting-and-Tracking-from-Moving-Drones-A-Benchmark-and-Methods-cs-CVPDF"><a href="#126-Video-Individual-Counting-and-Tracking-from-Moving-Drones-A-Benchmark-and-Methods-cs-CVPDF" class="headerlink" title="[126] Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12500">Video Individual Counting and Tracking from Moving Drones: A Benchmark and Methods</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12500" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yaowu Fan, Jia Wan, Tao Han, Andy J. Ma, Antoni B. Chan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个基于移动无人机视频的密集人群个体计数与跟踪新方法，并发布了大规模数据集MovingDroneCrowd++。针对现有方法在移动视角下性能不足的问题，作者提出了基于密度图分解的计数方法GD3A和基于描述子投票的跟踪方法DVTrack，在复杂场景下显著提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有密集人群计数与跟踪方法主要依赖固定摄像头数据集，其空间覆盖有限，难以应对大规模场景。为了克服这一限制，本文旨在利用移动无人机视频进行全场景的行人计数与跟踪。</p>
<p><strong>Result:</strong> 在提出的MovingDroneCrowd++数据集上，所提方法显著优于现有方法：GD3A将计数误差降低了47.4%，DVTrack将跟踪性能提升了39.2%，在密集人群和复杂运动条件下达到了新的SOTA水平。</p>
<p><strong>Insight:</strong> 主要创新点包括：1) 发布了首个大规模、由移动无人机拍摄的视频级密集人群计数与跟踪数据集；2) 提出了GD3A方法，通过带有自适应“垃圾桶”分数的最优传输建立跨帧描述子关联，将全局密度图分解为共享、流入和流出分量，避免了显式定位；3) 在GD3A基础上，通过描述子投票机制将描述子级匹配转化为实例级关联，实现了DVTrack跟踪方法。从客观角度看，将最优传输与密度图分解结合用于视频计数，以及描述子投票用于跟踪，是新颖且有效的技术路径。</p>
<p><strong>Abstract:</strong> Counting and tracking dense crowds in large-scale scenes is highly challenging, yet existing methods mainly rely on datasets captured by fixed cameras, which provide limited spatial coverage and are inadequate for large-scale dense crowd analysis. To address this limitation, we propose a flexible solution using moving drones to capture videos and perform video-level crowd counting and tracking of unique pedestrians across entire scenes. We introduce MovingDroneCrowd++, the largest video-level dataset for dense crowd counting and tracking captured by moving drones, covering diverse and complex conditions with varying flight altitudes, camera angles, and illumination. Existing methods fail to achieve satisfactory performance on this dataset. To this end, we propose GD3A (Global Density Map Decomposition via Descriptor Association), a density map-based video individual counting method that avoids explicit localization. GD3A establishes pixel-level correspondences between pedestrian descriptors across consecutive frames via optimal transport with an adaptive dustbin score, enabling the decomposition of global density maps into shared, inflow, and outflow components. Building on this framework, we further introduce DVTrack, which converts descriptor-level matching into instance-level associations through a descriptor voting mechanism for pedestrian tracking. Experimental results show that our methods significantly outperform existing approaches under dense crowds and complex motion, reducing counting error by 47.4 percent and improving tracking performance by 39.2 percent.</p>
  </div>
</details>

<hr>
<h3 id="127-XRefine-Attention-Guided-Keypoint-Match-Refinement-cs-CVPDF"><a href="#127-XRefine-Attention-Guided-Keypoint-Match-Refinement-cs-CVPDF" class="headerlink" title="[127] XRefine: Attention-Guided Keypoint Match Refinement cs.CVPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12530">XRefine: Attention-Guided Keypoint Match Refinement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12530" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jan Fabian Schmid, Annika Hagemann</span></p>
<p><strong>TL;DR:</strong> 本文提出XRefine，一种与关键点检测器无关的亚像素级关键点匹配优化方法，仅利用匹配关键点周围的图像块，通过基于交叉注意力的架构预测优化后的坐标，无需依赖检测器内部表示，并可扩展至多视图特征跟踪，在MegaDepth、KITTI和ScanNet数据集上验证了其提升几何估计精度的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有稀疏关键点匹配方法常因检测器空间不准确导致匹配误差，且现有优化方法通常与特定检测器绑定，需针对不同检测器重新训练，缺乏通用性。</p>
<p><strong>Result:</strong> 在MegaDepth、KITTI和ScanNet基准测试中，XRefine显著提高了几何估计精度，相比现有优化方法达到更优性能，同时保持运行时效率。</p>
<p><strong>Insight:</strong> 创新点在于提出一种检测器无关的优化架构，通过交叉注意力机制仅基于图像块学习坐标优化，无需检测器特定表示，实现了跨检测器的泛化能力，并可扩展支持多视图跟踪。</p>
<p><strong>Abstract:</strong> Sparse keypoint matching is crucial for 3D vision tasks, yet current keypoint detectors often produce spatially inaccurate matches. Existing refinement methods mitigate this issue through alignment of matched keypoint locations, but they are typically detector-specific, requiring retraining for each keypoint detector. We introduce XRefine, a novel, detector-agnostic approach for sub-pixel keypoint refinement that operates solely on image patches centered at matched keypoints. Our cross-attention-based architecture learns to predict refined keypoint coordinates without relying on internal detector representations, enabling generalization across detectors. Furthermore, XRefine can be extended to handle multi-view feature tracks. Experiments on MegaDepth, KITTI, and ScanNet demonstrate that the approach consistently improves geometric estimation accuracy, achieving superior performance compared to existing refinement methods while maintaining runtime efficiency. Our code and trained models can be found at <a target="_blank" rel="noopener" href="https://github.com/boschresearch/xrefine">https://github.com/boschresearch/xrefine</a>.</p>
  </div>
</details>

<hr>
<h3 id="128-Encoding-Emotion-Through-Self-Supervised-Eye-Movement-Reconstruction-cs-CV-cs-AIPDF"><a href="#128-Encoding-Emotion-Through-Self-Supervised-Eye-Movement-Reconstruction-cs-CV-cs-AIPDF" class="headerlink" title="[128] Encoding Emotion Through Self-Supervised Eye Movement Reconstruction cs.CV | cs.AIPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12534">Encoding Emotion Through Self-Supervised Eye Movement Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12534" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis Avramidis</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新颖的自监督眼动重建方法，用于从低分辨率自然视频中预测情感表达的多模态标记。研究利用大屠杀幸存者访谈视频数据，通过自监督预训练的眼动检测模型提取编码器嵌入，并微调于两个下游任务：眼动与语音情感方向估计的对齐，以及眼动对笑、哭&#x2F;抽泣、叹气等瞬时情感行为的预测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有研究多依赖高分辨率眼动追踪设备，限制了其应用范围。本文旨在探索如何从自然、低分辨率视频中利用眼动来预测情感表达，以扩大相关发现的潜在影响力。</p>
<p><strong>Result:</strong> 研究发现新模型能有效预测情感结果，并且在两项实验中均观察到预训练性能与情感处理性能呈正相关，表明自监督眼动重建是编码情感信号的有效方法。</p>
<p><strong>Insight:</strong> 创新点在于将语言模型的预训练方法（自监督学习）应用于眼动检测，使其能有效利用未标记视频数据，从而在低分辨率自然视频中实现情感预测，这为情感计算和眼动分析提供了新的数据高效学习范式。</p>
<p><strong>Abstract:</strong> The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation’s Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model’s encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying&#x2F;sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.</p>
  </div>
</details>

<hr>
<h3 id="129-Camera-Pose-Revisited-cs-CVPDF"><a href="#129-Camera-Pose-Revisited-cs-CVPDF" class="headerlink" title="[129] Camera Pose Revisited cs.CVPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12567">Camera Pose Revisited</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12567" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Władysław Skarbek, Michał Salomonowicz, Michał Król</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PnP-ProCay78的新算法，用于解决平面透视n点（PnP）问题，特别是相机标定中初始姿态估计。该方法结合了经典二次重建误差公式、旋转的Cayley参数化和最小二乘优化，并通过确定性选择起始点来避免耗时的搜索过程。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决计算机视觉中相机姿态估计的核心问题，特别是在相机标定和多传感器系统背景下，专注于平面PnP问题的初始姿态估计，旨在开发一种既准确又算法结构简单的方法。</p>
<p><strong>Result:</strong> 在集成RGB-IR设置中，使用高分辨率RGB相机和极低分辨率热像仪数据进行实验验证。结果表明，所提算法在投影精度上几乎与最优的SQPnP相当，略高于IPPE（两者都是OpenCV中著名的PnP过程），同时保持了显著更简单的算法结构。</p>
<p><strong>Insight:</strong> 创新点在于将投影误差最小化与通过解析消除平移的重建误差替代项相结合，形成了一种几何透明且计算高效的混合成本公式；同时，基于两个规范向量重建误差分析的确定性起始点选择避免了复杂搜索，并且Cayley空间中的优化轨迹分析为收敛过程提供了直观见解，具有教学吸引力。</p>
<p><strong>Abstract:</strong> Estimating the position and orientation of a camera with respect to an observed scene is one of the central problems in computer vision, particularly in the context of camera calibration and multi-sensor systems. This paper addresses the planar Perspective–$n$–Point problem, with special emphasis on the initial estimation of the pose of a calibration object. As a solution, we propose the \texttt{PnP-ProCay78} algorithm, which combines the classical quadratic formulation of the reconstruction error with a Cayley parameterization of rotations and least-squares optimization. The key component of the method is a deterministic selection of starting points based on an analysis of the reconstruction error for two canonical vectors, allowing costly solution-space search procedures to be avoided. Experimental validation is performed using data acquired also from high-resolution RGB cameras and very low-resolution thermal cameras in an integrated RGB–IR setup. The results demonstrate that the proposed algorithm achieves practically the same projection accuracy as optimal \texttt{SQPnP} and slightly higher than \texttt{IPPE}, both prominent \texttt{PnP-OpenCV} procedures. However, \texttt{PnP-ProCay78} maintains a significantly simpler algorithmic structure. Moreover, the analysis of optimization trajectories in Cayley space provides an intuitive insight into the convergence process, making the method attractive also from a didactic perspective. Unlike existing PnP solvers, the proposed \texttt{PnP-ProCay78} algorithm combines projection error minimization with an analytically eliminated reconstruction-error surrogate for translation, yielding a hybrid cost formulation that is both geometrically transparent and computationally efficient.</p>
  </div>
</details>

<hr>
<h3 id="130-Linear-Mechanisms-for-Spatiotemporal-Reasoning-in-Vision-Language-Models-cs-CVPDF"><a href="#130-Linear-Mechanisms-for-Spatiotemporal-Reasoning-in-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[130] Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models cs.CVPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12626">Linear Mechanisms for Spatiotemporal Reasoning in Vision Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12626" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Raphi Kang, Hongqiao Chen, Georgia Gkioxari, Pietro Perona</span></p>
<p><strong>TL;DR:</strong> 本文研究了视觉语言模型（VLMs）中时空推理的线性机制，发现模型通过将空间ID与文本激活线性绑定来编码物体位置，并通过语言标记进行推理。研究还扩展至视频VLMs，识别出类似的线性时间ID机制，旨在提升模型的可解释性和设计能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于探索VLMs中时空推理能力的底层机制，这些机制目前仍不透明，研究假设视觉&#x2F;几何和文本表示在VLM计算中必须结合，以因果解释模型行为。</p>
<p><strong>Result:</strong> 通过严格的因果干预实验，证明空间ID在VLM中间层能系统性地调节模型信念，并作为诊断工具识别现有VLMs的局限性，以及作为有价值的学习信号。</p>
<p><strong>Insight:</strong> 创新点在于揭示了VLMs中一种先前未被充分探索的内部线性时空ID机制，这有助于改进模型的可解释性，并为设计更对齐和强大的模型提供原则性指导。</p>
<p><strong>Abstract:</strong> Spatio-temporal reasoning is a remarkable capability of Vision Language Models (VLMs), but the underlying mechanisms of such abilities remain largely opaque. We postulate that visual&#x2F;geometrical and textual representations of spatial structure must be combined at some point in VLM computations. We search for such confluence, and ask whether the identified representation can causally explain aspects of input-output model behavior through a linear model. We show empirically that VLMs encode object locations by linearly binding \textit{spatial IDs} to textual activations, then perform reasoning via language tokens. Through rigorous causal interventions we demonstrate that these IDs, which are ubiquitous across the model, can systematically mediate model beliefs at intermediate VLM layers. Additionally, we find that spatial IDs serve as a diagnostic tool for identifying limitations in existing VLMs, and as a valuable learning signal. We extend our analysis to video VLMs and identify an analogous linear temporal ID mechanism. By characterizing our proposed spatiotemporal ID mechanism, we elucidate a previously underexplored internal reasoning process in VLMs, toward improved interpretability and the principled design of more aligned and capable models. We release our code for reproducibility: <a target="_blank" rel="noopener" href="https://github.com/Raphoo/linear-mech-vlms">https://github.com/Raphoo/linear-mech-vlms</a>.</p>
  </div>
</details>

<hr>
<h3 id="131-VILTA-A-VLM-in-the-Loop-Adversary-for-Enhancing-Driving-Policy-Robustness-cs-CVPDF"><a href="#131-VILTA-A-VLM-in-the-Loop-Adversary-for-Enhancing-Driving-Policy-Robustness-cs-CVPDF" class="headerlink" title="[131] VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness cs.CVPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12672">VILTA: A VLM-in-the-Loop Adversary for Enhancing Driving Policy Robustness</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12672" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qimao Chen, Fang Li, Shaoqing Xu, Zhiyi Lai, Zixun Xie</span></p>
<p><strong>TL;DR:</strong> 本文提出VILTA框架，通过将视觉语言模型（VLM）集成到自动驾驶（AD）智能体的闭环训练中，直接编辑周围智能体的未来轨迹，以生成多样且新颖的挑战性场景，从而增强AD策略在长尾关键事件中的安全性和鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决自动驾驶系统因真实世界数据中罕见但关键的驾驶场景（长尾问题）严重不足而面临的安全部署难题，克服现有基于规则启发式、重采样或离线生成模型的方法在生成多样新颖挑战方面的局限性。</p>
<p><strong>Result:</strong> 实验表明，该方法显著提升了最终自动驾驶策略的安全性和鲁棒性，特别是在处理关键长尾事件方面。</p>
<p><strong>Insight:</strong> 创新点在于将VLM直接嵌入训练循环，通过理解动态驾驶环境并直接精细编辑周围智能体轨迹来生成挑战，充分利用了VLM的强大泛化能力，超越了传统两阶段框架对下游算法泛化天花板的依赖。</p>
<p><strong>Abstract:</strong> The safe deployment of autonomous driving (AD) systems is fundamentally hindered by the long-tail problem, where rare yet critical driving scenarios are severely underrepresented in real-world data. Existing solutions including safety-critical scenario generation and closed-loop learning often rely on rule-based heuristics, resampling methods and generative models learned from offline datasets, limiting their ability to produce diverse and novel challenges. While recent works leverage Vision Language Models (VLMs) to produce scene descriptions that guide a separate, downstream model in generating hazardous trajectories for agents, such two-stage framework constrains the generative potential of VLMs, as the diversity of the final trajectories is ultimately limited by the generalization ceiling of the downstream algorithm. To overcome these limitations, we introduce VILTA (VLM-In-the-Loop Trajectory Adversary), a novel framework that integrates a VLM into the closed-loop training of AD agents. Unlike prior works, VILTA actively participates in the training loop by comprehending the dynamic driving environment and strategically generating challenging scenarios through direct, fine-grained editing of surrounding agents’ future trajectories. This direct-editing approach fully leverages the VLM’s powerful generalization capabilities to create a diverse curriculum of plausible yet challenging scenarios that extend beyond the scope of traditional methods. We demonstrate that our approach substantially enhances the safety and robustness of the resulting AD policy, particularly in its ability to navigate critical long-tail events.</p>
  </div>
</details>

<hr>
<h3 id="132-Fusing-in-3D-Free-Viewpoint-Fusion-Rendering-with-a-3D-Infrared-Visible-Scene-Representation-cs-CV-cs-CGPDF"><a href="#132-Fusing-in-3D-Free-Viewpoint-Fusion-Rendering-with-a-3D-Infrared-Visible-Scene-Representation-cs-CV-cs-CGPDF" class="headerlink" title="[132] Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation cs.CV | cs.CGPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12697">Fusing in 3D: Free-Viewpoint Fusion Rendering with a 3D Infrared-Visible Scene Representation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12697" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chao Yang, Deshui Miao, Chao Tian, Guoqing Zhu, Yameng Gu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的红外-可见光高斯融合（IVGF）框架，用于解决现有2D融合方法因固定视角而导致的场景信息丢失问题。该框架通过从多模态2D输入重建场景几何，实现了融合图像的直接渲染。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有红外-可见光图像融合方法局限于固定相机视角，缺乏对复杂场景的全面理解，导致关键信息丢失。本文旨在通过构建3D场景表示来克服这一限制。</p>
<p><strong>Result:</strong> 全面的定性和定量实验证明了所提方法的有效性，但摘要中未明确提及具体的基准测试或与现有SOTA模型的对比结果。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了一个基于3D高斯表示的融合渲染框架，并引入了跨模态调整（CMA）模块和融合损失函数，以解决跨模态冲突并保留各自模态的关键特征，实现了自由视点的融合图像生成。</p>
<p><strong>Abstract:</strong> Infrared-visible image fusion aims to integrate infrared and visible information into a single fused image. Existing 2D fusion methods focus on fusing images from fixed camera viewpoints, neglecting a comprehensive understanding of complex scenarios, which results in the loss of critical information about the scene. To address this limitation, we propose a novel Infrared-Visible Gaussian Fusion (IVGF) framework, which reconstructs scene geometry from multimodal 2D inputs and enables direct rendering of fused images. Specifically, we propose a cross-modal adjustment (CMA) module that modulates the opacity of Gaussians to solve the problem of cross-modal conflicts. Moreover, to preserve the distinctive features from both modalities, we introduce a fusion loss that guides the optimization of CMA, thus ensuring that the fused image retains the critical characteristics of each modality. Comprehensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed method.</p>
  </div>
</details>

<hr>
<h3 id="133-S2DiT-Sandwich-Diffusion-Transformer-for-Mobile-Streaming-Video-Generation-cs-CVPDF"><a href="#133-S2DiT-Sandwich-Diffusion-Transformer-for-Mobile-Streaming-Video-Generation-cs-CVPDF" class="headerlink" title="[133] S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation cs.CVPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12719">S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12719" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lin Zhao, Yushu Wu, Aleksei Lebedev, Dishani Lahiri, Meng Dong</span></p>
<p><strong>TL;DR:</strong> 本文提出了S2DiT，一种专为移动硬件设计的流式三明治扩散Transformer模型，旨在实现高效、高保真和流式的视频生成。它通过混合线性卷积混合注意力（LCHA）和步幅自注意力（SSA）等新型高效注意力机制，在生成更多token的同时保持效率，并利用预算感知的动态规划搜索确定三明治结构，再通过2合1蒸馏框架将大教师模型的能力迁移到紧凑的少步模型中，最终在iPhone上实现了超过10 FPS的流式生成，且质量与最先进的服务器视频模型相当。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有扩散Transformer（DiTs）视频生成模型计算成本过高，无法在移动设备上实现实时或设备端生成的问题。</p>
<p><strong>Result:</strong> 在iPhone上实现了超过10 FPS的流式视频生成，其生成质量与最先进的服务器视频模型（state-of-the-art）相当。</p>
<p><strong>Insight:</strong> 创新点包括：1）混合LCHA和SSA的高效注意力机制，在生成更多token时保持效率；2）通过预算感知动态规划搜索确定的三明治结构设计；3）将大教师模型（如Wan 2.2-14B）能力迁移到紧凑少步模型的2合1蒸馏框架。这些技术共同实现了移动端高效高质量的视频生成。</p>
<p><strong>Abstract:</strong> Diffusion Transformers (DiTs) have recently improved video generation quality. However, their heavy computational cost makes real-time or on-device generation infeasible. In this work, we introduce S2DiT, a Streaming Sandwich Diffusion Transformer designed for efficient, high-fidelity, and streaming video generation on mobile hardware. S2DiT generates more tokens but maintains efficiency with novel efficient attentions: a mixture of LinConv Hybrid Attention (LCHA) and Stride Self-Attention (SSA). Based on this, we uncover the sandwich design via a budget-aware dynamic programming search, achieving superior quality and efficiency. We further propose a 2-in-1 distillation framework that transfers the capacity of large teacher models (e.g., Wan 2.2-14B) to the compact few-step sandwich model. Together, S2DiT achieves quality on par with state-of-the-art server video models, while streaming at over 10 FPS on an iPhone.</p>
  </div>
</details>

<hr>
<h3 id="134-Moaw-Unleashing-Motion-Awareness-for-Video-Diffusion-Models-cs-CVPDF"><a href="#134-Moaw-Unleashing-Motion-Awareness-for-Video-Diffusion-Models-cs-CVPDF" class="headerlink" title="[134] Moaw: Unleashing Motion Awareness for Video Diffusion Models cs.CVPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12761">Moaw: Unleashing Motion Awareness for Video Diffusion Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12761" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianqi Zhang, Ziyi Wang, Wenzhao Zheng, Weiliang Chen, Yuanhui Huang</span></p>
<p><strong>TL;DR:</strong> Moaw是一个将视频扩散模型从图像到视频生成模态转换为视频到密集跟踪模态的框架，通过训练一个运动感知的扩散模型，识别并注入编码最强运动信息的特征，从而实现无需额外适配器的零样本运动迁移。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 受视频扩散模型在零样本设置下能自然捕捉帧间对应关系的启发，研究旨在通过监督训练更充分地利用其跟踪能力，以促进运动迁移。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体的定量结果或基准测试，但声称Moaw能够实现零样本运动迁移，为生成建模与运动理解之间的桥梁提供了新范式。</p>
<p><strong>Insight:</strong> 创新点在于通过模态转换和特征注入，利用视频扩散模型的同质性实现零样本运动迁移，为统一可控的视频学习框架铺平道路。</p>
<p><strong>Abstract:</strong> Video diffusion models, trained on large-scale datasets, naturally capture correspondences of shared features across frames. Recent works have exploited this property for tasks such as optical flow prediction and tracking in a zero-shot setting. Motivated by these findings, we investigate whether supervised training can more fully harness the tracking capability of video diffusion models. To this end, we propose Moaw, a framework that unleashes motion awareness for video diffusion models and leverages it to facilitate motion transfer. Specifically, we train a diffusion model for motion perception, shifting its modality from image-to-video generation to video-to-dense-tracking. We then construct a motion-labeled dataset to identify features that encode the strongest motion information, and inject them into a structurally identical video generation model. Owing to the homogeneity between the two networks, these features can be naturally adapted in a zero-shot manner, enabling motion transfer without additional adapters. Our work provides a new paradigm for bridging generative modeling and motion understanding, paving the way for more unified and controllable video learning frameworks.</p>
  </div>
</details>

<hr>
<h3 id="135-Towards-Unbiased-Source-Free-Object-Detection-via-Vision-Foundation-Models-cs-CVPDF"><a href="#135-Towards-Unbiased-Source-Free-Object-Detection-via-Vision-Foundation-Models-cs-CVPDF" class="headerlink" title="[135] Towards Unbiased Source-Free Object Detection via Vision Foundation Models cs.CVPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12765">Towards Unbiased Source-Free Object Detection via Vision Foundation Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12765" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhi Cai, Yingjie Gao, Yanan Zhang, Xinzhu Ma, Di Huang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DSOD的去偏置无源目标检测框架，旨在解决现有无源目标检测方法中存在的源域偏差问题。该框架利用视觉基础模型（VFM）的强大能力，通过统一特征注入模块和语义感知特征正则化来减轻模型对源域特征的依赖，并提供了一个无需VFM的蒸馏变体DSOD-distill以适应计算受限的场景。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有无源目标检测方法在跨域任务中虽无需源域数据，但存在源域偏差问题，导致模型泛化能力差且自训练过程中误差累积。</p>
<p><strong>Result:</strong> 在多个基准测试中，DSOD在正常到雾天天气适应任务上达到48.1% AP，跨场景适应任务上达到39.3% AP，合成到真实适应任务上达到61.4% AP，性能优于最先进的无源目标检测方法。</p>
<p><strong>Insight:</strong> 创新点包括利用VFM辅助减轻源域偏差的统一特征注入模块和语义感知特征正则化，以及通过双教师蒸馏方案实现无需VFM的轻量级变体，为跨域目标检测提供了有效的去偏置解决方案。</p>
<p><strong>Abstract:</strong> Source-Free Object Detection (SFOD) has garnered much attention in recent years by eliminating the need of source-domain data in cross-domain tasks, but existing SFOD methods suffer from the Source Bias problem, i.e. the adapted model remains skewed towards the source domain, leading to poor generalization and error accumulation during self-training. To overcome this challenge, we propose Debiased Source-free Object Detection (DSOD), a novel VFM-assisted SFOD framework that can effectively mitigate source bias with the help of powerful VFMs. Specifically, we propose Unified Feature Injection (UFI) module that integrates VFM features into the CNN backbone through Simple-Scale Extension (SSE) and Domain-aware Adaptive Weighting (DAAW). Then, we propose Semantic-aware Feature Regularization (SAFR) that constrains feature learning to prevent overfitting to source domain characteristics. Furthermore, we propose a VFM-free variant, termed DSOD-distill for computation-restricted scenarios through a novel Dual-Teacher distillation scheme. Extensive experiments on multiple benchmarks demonstrate that DSOD outperforms state-of-the-art SFOD methods, achieving 48.1% AP on Normal-to-Foggy weather adaptation, 39.3% AP on Cross-scene adaptation, and 61.4% AP on Synthetic-to-Real adaptation.</p>
  </div>
</details>

<hr>
<h3 id="136-Spatial-VLN-Zero-Shot-Vision-and-Language-Navigation-With-Explicit-Spatial-Perception-and-Exploration-cs-CV-eess-SYPDF"><a href="#136-Spatial-VLN-Zero-Shot-Vision-and-Language-Navigation-With-Explicit-Spatial-Perception-and-Exploration-cs-CV-eess-SYPDF" class="headerlink" title="[136] Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration cs.CV | eess.SYPDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12766">Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12766" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lu Yue, Yue Fan, Shiwei Lian, Yu Zhao, Jiaxin Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出Spatial-VLN框架，旨在解决零样本视觉语言导航（VLN）中大型语言模型（LLM）空间感知不足的问题，通过增强空间感知和主动探索机制，在复杂连续环境中提升导航性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有零样本VLN方法虽泛化能力强，但在复杂连续环境中面临门交互、多房间导航和模糊指令执行等空间感知瓶颈，导致高失败率。</p>
<p><strong>Result:</strong> 在VLN-CE基准测试中，Spatial-VLN仅使用低成本LLM即实现了最先进的性能；通过基于价值的路点采样策略缩小Sim2Real差距，并在真实世界评估中展现出优异的泛化性和鲁棒性。</p>
<p><strong>Insight:</strong> 创新点包括：空间感知增强模块整合全景过滤与门&#x2F;区域专家以生成空间一致的表征；探索式多专家推理模块利用并行LLM专家处理语义与空间转换，并通过查询-探索机制主动解决感知歧义；提出的价值采样策略有效连接仿真与真实世界。</p>
<p><strong>Abstract:</strong> Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at <a target="_blank" rel="noopener" href="https://yueluhhxx.github.io/Spatial-VLN-web/">https://yueluhhxx.github.io/Spatial-VLN-web/</a>.</p>
  </div>
</details>

<hr>
<h3 id="137-Delving-Deeper-Hierarchical-Visual-Perception-for-Robust-Video-Text-Retrieval-cs-CV-cs-MMPDF"><a href="#137-Delving-Deeper-Hierarchical-Visual-Perception-for-Robust-Video-Text-Retrieval-cs-CV-cs-MMPDF" class="headerlink" title="[137] Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval cs.CV | cs.MMPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12768">Delving Deeper: Hierarchical Visual Perception for Robust Video-Text Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12768" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zequn Xie, Boyun Zhang, Yuxiao Lin, Tao Jin</span></p>
<p><strong>TL;DR:</strong> 本文提出HVP-Net（分层视觉感知网络），通过从视觉编码器的多个中间层提取和精炼特征来挖掘更丰富的视频语义，以解决视频文本检索中因视频固有冗余和依赖粗糙的最终层特征而导致的匹配精度受限问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于CLIP等预训练模型的视频文本检索方法受限于视频的固有冗余和对粗糙最终层特征的依赖，导致匹配精度不高。</p>
<p><strong>Result:</strong> 在MSRVTT、DiDeMo和ActivityNet等具有挑战性的基准测试中取得了新的最先进（SOTA）性能。</p>
<p><strong>Insight:</strong> 创新点在于利用分层特征，从不同语义层次的原始补丁标记中逐步提炼出显著的视觉概念，在减少冗余的同时保留了对齐所需的关键细节，从而构建了更鲁棒的视频表示。</p>
<p><strong>Abstract:</strong> Video-text retrieval (VTR) aims to locate relevant videos using natural language queries. Current methods, often based on pre-trained models like CLIP, are hindered by video’s inherent redundancy and their reliance on coarse, final-layer features, limiting matching accuracy. To address this, we introduce the HVP-Net (Hierarchical Visual Perception Network), a framework that mines richer video semantics by extracting and refining features from multiple intermediate layers of a vision encoder. Our approach progressively distills salient visual concepts from raw patch-tokens at different semantic levels, mitigating redundancy while preserving crucial details for alignment. This results in a more robust video representation, leading to new state-of-the-art performance on challenging benchmarks including MSRVTT, DiDeMo, and ActivityNet. Our work validates the effectiveness of exploiting hierarchical features for advancing video-text retrieval. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/boyun-zhang/HVP-Net">https://github.com/boyun-zhang/HVP-Net</a>.</p>
  </div>
</details>

<hr>
<h3 id="138-Generalizable-and-Animatable-3D-Full-Head-Gaussian-Avatar-from-a-Single-Image-cs-CVPDF"><a href="#138-Generalizable-and-Animatable-3D-Full-Head-Gaussian-Avatar-from-a-Single-Image-cs-CVPDF" class="headerlink" title="[138] Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image cs.CVPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12770">Generalizable and Animatable 3D Full-Head Gaussian Avatar from a Single Image</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12770" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuling Zhao, Dan Xu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种从单张图像构建可泛化、可动画化的3D全头高斯化身的新框架。该方法通过将高斯图元嵌入参数化人脸模型的UV空间表面来实现高效动画控制，并利用预训练的3D生成对抗网络（GAN）中的全头先验知识进行全局特征提取和多视角监督，同时结合UV空间和人脸的对称性融合局部细粒度输入图像特征以提升重建保真度，实现了单次前向传播即可完成重建，支持实时动画和360度渲染。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决从单张图像构建3D可动画化头部化身时，现有方法在大视角变化下容易崩溃、影响真实感的问题，旨在实现单次前向传播即可重建、支持实时动画和全方位渲染的高质量3D全头化身。</p>
<p><strong>Result:</strong> 大量实验表明，该方法在3D全头建模质量和实时动画方面表现优异，提升了3D说话化身的真实感，但摘要未明确提及具体基准测试或与SOTA的定量比较。</p>
<p><strong>Insight:</strong> 创新点包括：1) 在参数化人脸模型UV空间表面嵌入高斯图元以实现高效动画控制；2) 利用预训练3D GAN的全头先验进行全局特征提取和多视角监督；3) 结合UV对称性和人脸对称性融合局部与全局特征以增强重建保真度。从客观角度看，该方法将3D高斯溅射与参数化模型、生成式先验相结合，为单图像3D头像重建提供了可动画化且高效的解决方案。</p>
<p><strong>Abstract:</strong> Building 3D animatable head avatars from a single image is an important yet challenging problem. Existing methods generally collapse under large camera pose variations, compromising the realism of 3D avatars. In this work, we propose a new framework to tackle the novel setting of one-shot 3D full-head animatable avatar reconstruction in a single feed-forward pass, enabling real-time animation and simultaneous 360$^\circ$ rendering views. To facilitate efficient animation control, we model 3D head avatars with Gaussian primitives embedded on the surface of a parametric face model within the UV space. To obtain knowledge of full-head geometry and textures, we leverage rich 3D full-head priors within a pretrained 3D generative adversarial network (GAN) for global full-head feature extraction and multi-view supervision. To increase the fidelity of the 3D reconstruction of the input image, we take advantage of the symmetric nature of the UV space and human faces to fuse local fine-grained input image features with the global full-head textures. Extensive experiments demonstrate the effectiveness of our method, achieving high-quality 3D full-head modeling as well as real-time animation, thereby improving the realism of 3D talking avatars.</p>
  </div>
</details>

<hr>
<h3 id="139-Combating-Noisy-Labels-through-Fostering-Self-and-Neighbor-Consistency-cs-CVPDF"><a href="#139-Combating-Noisy-Labels-through-Fostering-Self-and-Neighbor-Consistency-cs-CVPDF" class="headerlink" title="[139] Combating Noisy Labels through Fostering Self- and Neighbor-Consistency cs.CVPDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12795">Combating Noisy Labels through Fostering Self- and Neighbor-Consistency</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12795" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zeren Sun, Yazhou Yao, Tongliang Liu, Zechao Li, Fumin Shen</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Jo-SNC的噪声鲁棒方法，通过联合样本选择和基于自一致性及邻域一致性的模型正则化来应对监督深度学习中的标签噪声问题。该方法利用Jensen-Shannon散度衡量样本为干净样本或分布外噪声样本的“可能性”，结合自适应阈值方案，并对不同类型样本采用不同训练策略，同时引入三元组一致性正则化提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现实场景中普遍存在标签噪声，深度网络因记忆效应易受其影响。现有方法多集中于识别干净数据，但常忽略不同小批次间标签噪声的不平衡性，且对分布外噪声数据关注不足。</p>
<p><strong>Result:</strong> 在多个基准数据集上的大量实验和消融研究表明，该方法在性能上优于现有的最先进方法，展示了其有效性和优越性。</p>
<p><strong>Insight:</strong> 创新点包括：1) 利用Jensen-Shannon散度结合样本邻域信息增强干净样本识别的可靠性；2) 设计自适应、数据驱动的每类阈值调整方案；3) 对干净样本、分布内噪声样本和分布外噪声样本分别采用常规训练、部分标签学习和负学习；4) 提出促进自预测一致性、邻域预测一致性和特征一致性的三元组一致性正则化。</p>
<p><strong>Abstract:</strong> Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (\textbf{Jo}int sample selection and model regularization based on \textbf{S}elf- and \textbf{N}eighbor-\textbf{C}onsistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the &#96;&#96;likelihood’’ of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="140-Left-Right-Symmetry-Breaking-in-CLIP-style-Vision-Language-Models-Trained-on-Synthetic-Spatial-Relation-Data-cs-CV-cs-AI-cs-LGPDF"><a href="#140-Left-Right-Symmetry-Breaking-in-CLIP-style-Vision-Language-Models-Trained-on-Synthetic-Spatial-Relation-Data-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[140] Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data cs.CV | cs.AI | cs.LGPDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12809">Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12809" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</span></p>
<p><strong>TL;DR:</strong> 本文通过可控的一维图像-文本测试平台，探究了基于Transformer的视觉和文本编码器在CLIP风格对比目标训练下如何习得左右空间关系。研究发现，对比训练能够学习左右关系，且标签多样性比布局多样性更能驱动泛化能力。通过注意力分解，揭示了位置嵌入与词元嵌入的交互诱导出水平注意力梯度，从而打破编码器的左右对称性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 空间理解是视觉-语言模型的关键挑战，但模型是否真正习得空间关系及其机制尚不明确。本文旨在探究CLIP风格模型中左右关系理解的涌现机制。</p>
<p><strong>Result:</strong> 在合成空间关系数据上训练的轻量级Transformer编码器能够泛化到未见过的物体对，标签多样性是泛化的主要驱动力。注意力分解表明，位置与词元嵌入的交互诱导的水平注意力梯度是左右区分的关键，消除该贡献会显著降低左右辨别能力。</p>
<p><strong>Insight:</strong> 创新点在于使用可控一维测试平台进行机制性分析，揭示了对比训练中标签多样性对空间关系泛化的重要性，以及注意力机制中位置与词元嵌入交互在打破左右对称性中的作用，为CLIP风格模型获得关系能力提供了机制性见解。</p>
<p><strong>Abstract:</strong> Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.</p>
  </div>
</details>

<hr>
<h3 id="141-A-Generalist-Foundation-Model-for-Total-body-PET-CT-Enables-Diagnostic-Reporting-and-System-wide-Metabolic-Profiling-cs-CVPDF"><a href="#141-A-Generalist-Foundation-Model-for-Total-body-PET-CT-Enables-Diagnostic-Reporting-and-System-wide-Metabolic-Profiling-cs-CVPDF" class="headerlink" title="[141] A Generalist Foundation Model for Total-body PET&#x2F;CT Enables Diagnostic Reporting and System-wide Metabolic Profiling cs.CVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12820">A Generalist Foundation Model for Total-body PET&#x2F;CT Enables Diagnostic Reporting and System-wide Metabolic Profiling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12820" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wei Chen, Liang Wu, Shuyi Lu, Yuanyuan Sun, Wenkai Bi</span></p>
<p><strong>TL;DR:</strong> 本文提出了SDF-HOLO，一个用于全身PET&#x2F;CT成像的多模态基础模型。该模型通过双流编码器解耦CT和PET的表征学习，并通过跨模态交互模块耦合它们，以利用解剖学上下文优化PET信息聚合，同时利用代谢显著性指导细微的形态学推理。模型采用分层上下文建模来捕捉全身的长距离依赖关系，并使用解剖分割掩码作为显式语义锚点，在预训练中执行体素-掩码-文本对齐。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 全身PET&#x2F;CT成像具有系统范围的覆盖、异质的解剖与代谢信号以及结构化的放射学语义，这挑战了现有假设单模态输入、局部视野和粗粒度图文对齐的医疗AI模型。因此，需要开发一个能够处理这些挑战的、用于全身PET&#x2F;CT的通用基础模型。</p>
<p><strong>Result:</strong> 在肿瘤分割、低剂量病灶检测和多语言诊断报告生成等任务上，SDF-HOLO超越了强大的任务专用模型和临床参考基线，同时减少了定位错误和幻觉发现。</p>
<p><strong>Insight:</strong> 创新点包括：1）用于全身PET&#x2F;CT的双流融合架构，实现解剖与代谢信号的解耦与交互；2）结合局部窗口与全局注意力的分层上下文建模，以高效处理长距离依赖；3）使用解剖分割掩码作为显式语义锚点，实现体素、掩码和临床文本的细粒度对齐，以更好地桥接图像与语言。这为全身PET&#x2F;CT诊断和系统级精准肿瘤学提供了一个可扩展的计算基础。</p>
<p><strong>Abstract:</strong> Total-body PET&#x2F;CT enables system-wide molecular imaging, but heterogeneous anatomical and metabolic signals, approximately 2 m axial coverage, and structured radiology semantics challenge existing medical AI models that assume single-modality inputs, localized fields of view, and coarse image-text alignment. We introduce SDF-HOLO (Systemic Dual-stream Fusion Holo Model), a multimodal foundation model for holistic total-body PET&#x2F;CT, pre-trained on more than 10,000 patients. SDF-HOLO decouples CT and PET representation learning with dual-stream encoders and couples them through a cross-modal interaction module, allowing anatomical context to refine PET aggregation while metabolic saliency guides subtle morphological reasoning. To model long-range dependencies across the body, hierarchical context modeling combines efficient local windows with global attention. To bridge voxels and clinical language, we use anatomical segmentation masks as explicit semantic anchors and perform voxel-mask-text alignment during pre-training. Across tumor segmentation, low-dose lesion detection, and multilingual diagnostic report generation, SDF-HOLO outperforms strong task-specific and clinical-reference baselines while reducing localization errors and hallucinated findings. Beyond focal interpretation, the model enables system-wide metabolic profiling and reveals tumor-associated fingerprints of inter-organ metabolic network interactions, providing a scalable computational foundation for total-body PET&#x2F;CT diagnostics and system-level precision oncology.</p>
  </div>
</details>

<hr>
<h3 id="142-TreeDGS-Aerial-Gaussian-Splatting-for-Distant-DBH-Measurement-cs-CVPDF"><a href="#142-TreeDGS-Aerial-Gaussian-Splatting-for-Distant-DBH-Measurement-cs-CVPDF" class="headerlink" title="[142] TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement cs.CVPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12823">TreeDGS: Aerial Gaussian Splatting for Distant DBH Measurement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12823" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Belal Shaheen, Minh-Hieu Nguyen, Bach-Thuan Bui, Shubham, Tim Wu</span></p>
<p><strong>TL;DR:</strong> 本文提出了TreeDGS方法，利用3D高斯泼溅技术从航拍图像重建森林场景，并精确测量树木胸径。该方法通过高斯场提取密集点云，并结合不透明度可靠性评分进行加权圆拟合，在10个样地的实测数据上验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 航拍遥感虽然能高效进行大范围勘测，但在复杂自然场景中难以实现精确的物体级测量，特别是对树木胸径这种关键自然属性的空中直接测量。现有方法因树干在航拍图像中距离远、像素稀疏，导致重建几何约束弱。</p>
<p><strong>Result:</strong> 在10个具有实地测量胸径的样地上进行评估，TreeDGS达到了4.79厘米的均方根误差（约相当于该地面采样距离下的2.6个像素），优于最先进的激光雷达基线（7.91厘米均方根误差），实现了准确、低成本的空中胸径测量。</p>
<p><strong>Insight:</strong> 创新点在于将3D高斯泼溅作为连续、可致密化的场景表示用于树干测量，并引入了基于RaDe-GS的深度感知累积不透明度积分来提取密集点集，以及使用不透明度加权的实心圆拟合来估计胸径，从而在稀疏观测条件下提升了几何重建的约束和测量精度。</p>
<p><strong>Abstract:</strong> Aerial remote sensing enables efficient large-area surveying, but accurate direct object-level measurement remains difficult in complex natural scenes. Recent advancements in 3D vision, particularly learned radiance-field representations such as NeRF and 3D Gaussian Splatting, have begun to raise the ceiling on reconstruction fidelity and densifiable geometry from posed imagery. Nevertheless, direct aerial measurement of important natural attributes such as tree diameter at breast height (DBH) remains challenging. Trunks in aerial forest scans are distant and sparsely observed in image views: at typical operating altitudes, stems may span only a few pixels. With these constraints, conventional reconstruction methods leave breast-height trunk geometry weakly constrained. We present TreeDGS, an aerial image reconstruction method that leverages 3D Gaussian Splatting as a continuous, densifiable scene representation for trunk measurement. After SfM-MVS initialization and Gaussian optimization, we extract a dense point set from the Gaussian field using RaDe-GS’s depth-aware cumulative-opacity integration and associate each sample with a multi-view opacity reliability score. We then estimate DBH from trunk-isolated points using opacity-weighted solid-circle fitting. Evaluated on 10 plots with field-measured DBH, TreeDGS reaches 4.79,cm RMSE (about 2.6 pixels at this GSD) and outperforms a state-of-the-art LiDAR baseline (7.91,cm RMSE), demonstrating that densified splat-based geometry can enable accurate, low-cost aerial DBH measurement.</p>
  </div>
</details>

<hr>
<h3 id="143-Seeing-Isn’t-Always-Believing-Analysis-of-Grad-CAM-Faithfulness-and-Localization-Reliability-in-Lung-Cancer-CT-Classification-cs-CVPDF"><a href="#143-Seeing-Isn’t-Always-Believing-Analysis-of-Grad-CAM-Faithfulness-and-Localization-Reliability-in-Lung-Cancer-CT-Classification-cs-CVPDF" class="headerlink" title="[143] Seeing Isn’t Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification cs.CVPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12826">Seeing Isn’t Always Believing: Analysis of Grad-CAM Faithfulness and Localization Reliability in Lung Cancer CT Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12826" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Teerapong Panboonyuen</span></p>
<p><strong>TL;DR:</strong> 本研究对Grad-CAM在肺癌CT分类中的忠实性和定位可靠性进行了批判性分析，发现尽管Grad-CAM在大多数卷积网络中能有效突出肿瘤区域，但在视觉Transformer模型中其解释保真度显著下降，且不同模型间的显著性定位存在显著差异，揭示了当前基于显著性的可解释AI方法在医学影像中的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决Grad-CAM等基于热图的可解释AI技术在医学图像分析中是否真正代表深度模型内部决策过程的问题，评估其忠实性和可靠性。</p>
<p><strong>Result:</strong> 在公开的IQ-OTH&#x2F;NCCD数据集上评估了ResNet-50、ResNet-101、DenseNet-161、EfficientNet-B0和ViT-Base-Patch16-224五种架构，发现Grad-CAM在卷积网络中表现良好，但在ViT模型中因非局部注意力行为而解释保真度显著下降，且跨模型比较显示显著性定位存在实质性变异。</p>
<p><strong>Insight:</strong> 提出了结合定位准确性、基于扰动的忠实性和解释一致性的定量评估框架，强调了需要开发既计算可靠又具有临床意义的模型感知可解释性方法，并呼吁在医学AI中更谨慎地采用视觉解释工具。</p>
<p><strong>Abstract:</strong> Explainable Artificial Intelligence (XAI) techniques, such as Gradient-weighted Class Activation Mapping (Grad-CAM), have become indispensable for visualizing the reasoning process of deep neural networks in medical image analysis. Despite their popularity, the faithfulness and reliability of these heatmap-based explanations remain under scrutiny. This study critically investigates whether Grad-CAM truly represents the internal decision-making of deep models trained for lung cancer image classification. Using the publicly available IQ-OTH&#x2F;NCCD dataset, we evaluate five representative architectures: ResNet-50, ResNet-101, DenseNet-161, EfficientNet-B0, and ViT-Base-Patch16-224, to explore model-dependent variations in Grad-CAM interpretability. We introduce a quantitative evaluation framework that combines localization accuracy, perturbation-based faithfulness, and explanation consistency to assess Grad-CAM reliability across architectures. Experimental findings reveal that while Grad-CAM effectively highlights salient tumor regions in most convolutional networks, its interpretive fidelity significantly degrades for Vision Transformer models due to non-local attention behavior. Furthermore, cross-model comparisons indicate substantial variability in saliency localization, implying that Grad-CAM explanations may not always correspond to the true diagnostic evidence used by the networks. This work exposes critical limitations of current saliency-based XAI approaches in medical imaging and emphasizes the need for model-aware interpretability methods that are both computationally sound and clinically meaningful. Our findings aim to inspire a more cautious and rigorous adoption of visual explanation tools in medical AI, urging the community to rethink what it truly means to “trust” a model’s explanation.</p>
  </div>
</details>

<hr>
<h3 id="144-Proxy-Robustness-in-Vision-Language-Models-is-Effortlessly-Transferable-cs-CVPDF"><a href="#144-Proxy-Robustness-in-Vision-Language-Models-is-Effortlessly-Transferable-cs-CVPDF" class="headerlink" title="[144] Proxy Robustness in Vision Language Models is Effortlessly Transferable cs.CVPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12865">Proxy Robustness in Vision Language Models is Effortlessly Transferable</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12865" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaowei Fu, Fuxiang Huang, Lei Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为异构代理转移（HPT）的框架，用于在视觉语言模型（如CLIP）中实现对抗鲁棒性的高效迁移。该方法利用普通CLIP模型对不同架构CLIP生成的对抗样本的内在防御能力（即代理对抗鲁棒性），通过跨架构的鲁棒性蒸馏通道，将鲁棒性从代理模型转移到目标模型。为解决由此引发的过拟合和零样本泛化能力下降问题，进一步提出了泛化-枢轴解耦（GPD）策略，通过不同的学习率调度将转移过程解耦为保持泛化的预热阶段和提升鲁棒性的HPT阶段，从而在自然泛化与对抗鲁棒性之间取得平衡。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在视觉语言模型中，通过蒸馏进行对抗鲁棒性转移的传统方法面临巨大挑战：为大规模多模态模型（如CLIP）构建对抗鲁棒的教师模型需要极高的计算资源。本文旨在解决这一资源瓶颈，并探索如何高效地将鲁棒性迁移到VLMs中。</p>
<p><strong>Result:</strong> 在15个零样本数据集上的大量实验证明了所提HPT-GPD方法的有效性，该方法能够实现对抗鲁棒性的高效迁移，同时保持模型的零样本自然泛化能力。</p>
<p><strong>Insight:</strong> 核心创新点在于发现了普通CLIP模型对不同架构CLIP生成的对抗样本具有内在防御能力（代理对抗鲁棒性），并基于此构建了无需对抗训练教师模型的跨架构鲁棒性蒸馏框架（HPT）。同时，提出的泛化-枢轴解耦（GPD）策略巧妙地通过不同的学习率调度来解耦训练目标，有效缓解了鲁棒性迁移过程中的过拟合问题，平衡了鲁棒性与泛化能力。这为资源受限下的大规模多模态模型鲁棒性提升提供了新思路。</p>
<p><strong>Abstract:</strong> As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com&#x2F;fxw13&#x2F;HPT-GPD.</p>
  </div>
</details>

<hr>
<h3 id="145-Exploring-Talking-Head-Models-With-Adjacent-Frame-Prior-for-Speech-Preserving-Facial-Expression-Manipulation-cs-CVPDF"><a href="#145-Exploring-Talking-Head-Models-With-Adjacent-Frame-Prior-for-Speech-Preserving-Facial-Expression-Manipulation-cs-CVPDF" class="headerlink" title="[145] Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation cs.CVPDF"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12876">Exploring Talking Head Models With Adjacent Frame Prior for Speech-Preserving Facial Expression Manipulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12876" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhenxuan Lu, Zhihua Xu, Zhijing Yang, Feng Gao, Yongyi Lu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为THFEM的新框架，用于在保持原始口型的同时改变面部表情。该框架结合了音频驱动的说话头生成模型和面部表情保持技术，并通过相邻帧学习策略优化模型，以生成更真实、表情更准确的多帧图像序列。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的语音保持面部表情操纵技术在改变表情时难以准确同步口型，因为面部表情与嘴部形状之间存在复杂的相互作用。本文旨在利用音频驱动说话头生成模型在合成精确口型方面的优势，解决这一同步问题。</p>
<p><strong>Result:</strong> 广泛的实验评估表明，该框架在表情操纵过程中有效地保持了嘴部形状，显著提升了图像质量，验证了将音频驱动说话头生成模型与面部表情保持技术集成的巨大益处。</p>
<p><strong>Insight:</strong> 主要创新点在于将音频驱动说话头生成模型与面部表情操纵任务相结合，并提出了相邻帧学习策略来微调模型，使其能够预测连续帧序列，从而利用相邻帧信息提升生成图像的真实感和表情保真度。这是一种新颖的跨任务模型集成与序列生成优化方法。</p>
<p><strong>Abstract:</strong> Speech-Preserving Facial Expression Manipulation (SPFEM) is an innovative technique aimed at altering facial expressions in images and videos while retaining the original mouth movements. Despite advancements, SPFEM still struggles with accurate lip synchronization due to the complex interplay between facial expressions and mouth shapes. Capitalizing on the advanced capabilities of audio-driven talking head generation (AD-THG) models in synthesizing precise lip movements, our research introduces a novel integration of these models with SPFEM. We present a new framework, Talking Head Facial Expression Manipulation (THFEM), which utilizes AD-THG models to generate frames with accurately synchronized lip movements from audio inputs and SPFEM-altered images. However, increasing the number of frames generated by AD-THG models tends to compromise the realism and expression fidelity of the images. To counter this, we develop an adjacent frame learning strategy that finetunes AD-THG models to predict sequences of consecutive frames. This strategy enables the models to incorporate information from neighboring frames, significantly improving image quality during testing. Our extensive experimental evaluations demonstrate that this framework effectively preserves mouth shapes during expression manipulations, highlighting the substantial benefits of integrating AD-THG with SPFEM.</p>
  </div>
</details>

<hr>
<h3 id="146-YOLO26-An-Analysis-of-NMS-Free-End-to-End-Framework-for-Real-Time-Object-Detection-cs-CV-cs-AIPDF"><a href="#146-YOLO26-An-Analysis-of-NMS-Free-End-to-End-Framework-for-Real-Time-Object-Detection-cs-CV-cs-AIPDF" class="headerlink" title="[146] YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection cs.CV | cs.AIPDF"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12882">YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12882" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sudip Chakrabarty</span></p>
<p><strong>TL;DR:</strong> 本文分析了YOLO26，一种通过消除非极大值抑制（NMS）并采用原生端到端学习策略来重新定义实时目标检测范式的架构。研究探讨了其关键创新，包括用于稳定轻量级主干的MuSGD优化器、针对小目标感知分配的STAL以及用于动态监督的ProgLoss。结果表明，YOLO26在推理速度和检测精度上均超越了前代模型及SOTA竞争对手，建立了新的帕累托前沿。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统YOLO系列模型受限于NMS后处理带来的延迟和超参数敏感性，YOLO26旨在通过消除NMS，采用端到端学习策略来解决这一历史性的速度与精度权衡问题。</p>
<p><strong>Result:</strong> 在官方性能基准测试中，YOLO26在推理速度和检测精度上均超越了包括RTMDet和DAMO-YOLO在内的前代模型及SOTA竞争对手，建立了新的帕累托前沿。</p>
<p><strong>Insight:</strong> 主要创新点在于通过MuSGD优化器、STAL分配策略和ProgLoss动态监督，实现了无需NMS的端到端学习，从而将表征学习与启发式后处理解耦，解决了实时目标检测中延迟与精度的传统矛盾。</p>
<p><strong>Abstract:</strong> The “You Only Look Once” (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.</p>
  </div>
</details>

<hr>
<h3 id="147-Supervision-by-Hallucination-and-Transfer-A-Weakly-Supervised-Approach-for-Robust-and-Precise-Facial-Landmark-Detection-cs-CVPDF"><a href="#147-Supervision-by-Hallucination-and-Transfer-A-Weakly-Supervised-Approach-for-Robust-and-Precise-Facial-Landmark-Detection-cs-CVPDF" class="headerlink" title="[147] Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection cs.CVPDF"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12919">Supervision-by-Hallucination-and-Transfer: A Weakly-Supervised Approach for Robust and Precise Facial Landmark Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12919" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jun Wan, Yuanzhi Yao, Zhihui Lai, Jie Zhou, Xianxu Hou</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为监督-幻觉-迁移（SHT）的弱监督框架，用于解决低分辨率、数据不足和标注不精确导致的面部关键点检测（FLD）精度下降问题。该框架包含两个相互增强的新模块：双重幻觉学习网络（DHLN）和面部姿态迁移网络（FPTN）。DHLN通过结合FLD和面部超分辨率任务，从低分辨率输入学习高分辨率表征以恢复面部结构和细节；FPTN则通过面部姿态变换进一步优化关键点热图。实验表明，该方法在面部超分辨率和FLD任务上均超越了现有最先进技术。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决低分辨率人脸图像、训练数据不足以及标注不精确导致的面部关键点检测精度下降问题。</p>
<p><strong>Result:</strong> 在面部超分辨率和面部关键点检测任务上的实验结果表明，该方法超越了当前最先进（SOTA）的技术。</p>
<p><strong>Insight:</strong> 创新性地将面部超分辨率（幻觉）和面部姿态迁移任务集成到一个弱监督框架中，以相互增强的方式学习高分辨率表征并优化关键点检测，这是首次在此方向上的探索。</p>
<p><strong>Abstract:</strong> High-precision facial landmark detection (FLD) relies on high-resolution deep feature representations. However, low-resolution face images or the compression (via pooling or strided convolution) of originally high-resolution images hinder the learning of such features, thereby reducing FLD accuracy. Moreover, insufficient training data and imprecise annotations further degrade performance. To address these challenges, we propose a weakly-supervised framework called Supervision-by-Hallucination-and-Transfer (SHT) for more robust and precise FLD. SHT contains two novel mutually enhanced modules: Dual Hallucination Learning Network (DHLN) and Facial Pose Transfer Network (FPTN). By incorporating FLD and face hallucination tasks, DHLN is able to learn high-resolution representations with low-resolution inputs for recovering both facial structures and local details and generating more effective landmark heatmaps. Then, by transforming faces from one pose to another, FPTN can further improve landmark heatmaps and faces hallucinated by DHLN for detecting more accurate landmarks. To the best of our knowledge, this is the first study to explore weakly-supervised FLD by integrating face hallucination and facial pose transfer tasks. Experimental results of both face hallucination and FLD demonstrate that our method surpasses state-of-the-art techniques.</p>
  </div>
</details>

<hr>
<h3 id="148-Early-Prediction-of-Type-2-Diabetes-Using-Multimodal-data-and-Tabular-Transformers-cs-CV-cs-LGPDF"><a href="#148-Early-Prediction-of-Type-2-Diabetes-Using-Multimodal-data-and-Tabular-Transformers-cs-CV-cs-LGPDF" class="headerlink" title="[148] Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers cs.CV | cs.LGPDF"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12981">Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12981" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sulaiman Khan, Md. Rafiul Biswas, Zubair Shah</span></p>
<p><strong>TL;DR:</strong> 本研究提出了一种基于表格Transformer（TabTrans）架构的新方法，用于利用纵向患者数据进行2型糖尿病（T2DM）的早期风险预测。该方法整合了电子健康记录（EHR）和双能X射线吸收测定法（DXA）等多模态数据，旨在捕捉传统方法常忽略的疾病进展中的复杂、长程依赖关系。在卡塔尔生物银行（QBB）的回顾性队列（1,382名受试者）上验证，模型性能优于传统机器学习方法和生成式AI模型（如Claude 3.5 Sonnet、GPT-4、Gemini Pro）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统方法在分析纵向、多模态医疗表格数据时，难以有效捕捉复杂、长程依赖关系的问题，以实现更准确的2型糖尿病早期风险预测。</p>
<p><strong>Result:</strong> 在卡塔尔生物银行（QBB）队列上，所提出的TabTrans模型在T2DM预测任务中取得了ROC AUC ≥ 79.7%的性能，优于对比的生成式AI模型和传统机器学习方法。特征解释分析识别出内脏脂肪组织（VAT）质量&#x2F;体积、Ward区骨密度（BMD）和骨矿物质含量（BMC）等关键风险指标。</p>
<p><strong>Insight:</strong> 创新点在于将表格Transformer（TabTrans）架构应用于纵向、多模态医疗表格数据，以建模长程依赖；同时，将生成式AI模型（如Claude、GPT-4）作为基准进行比较是一个新颖的评估视角。从客观角度看，整合EHR和DXA等多模态数据并利用Transformer处理表格序列，为医疗风险预测提供了新的技术路径。</p>
<p><strong>Abstract:</strong> This study introduces a novel approach for early Type 2 Diabetes Mellitus (T2DM) risk prediction using a tabular transformer (TabTrans) architecture to analyze longitudinal patient data. By processing patients<code> longitudinal health records and bone-related tabular data, our model captures complex, long-range dependencies in disease progression that conventional methods often overlook. We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women (133 diabetic, 524 healthy). The study integrated electronic health records (EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed model</code>s performance is evaluated against conventional machine learning (ML) and generative AI models, including Claude 3.5 Sonnet (Anthropic<code>s constitutional AI), GPT-4 (OpenAI</code>s generative pre-trained transformer), and Gemini Pro (Google&#96;s multimodal language model). Our TabTrans model demonstrated superior predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction compared to both generative AI models and conventional ML approaches. Feature interpretation analysis identified key risk indicators, with visceral adipose tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important predictors associated with diabetes development in Qatari adults. These findings demonstrate the significant potential of TabTrans for analyzing complex tabular healthcare data, providing a powerful tool for proactive T2DM management and personalized clinical interventions in the Qatari population.   Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM, feature interpretation, tabular data</p>
  </div>
</details>

<hr>
<h3 id="149-AsyncBEV-Cross-modal-Flow-Alignment-in-Asynchronous-3D-Object-Detection-cs-CVPDF"><a href="#149-AsyncBEV-Cross-modal-Flow-Alignment-in-Asynchronous-3D-Object-Detection-cs-CVPDF" class="headerlink" title="[149] AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection cs.CVPDF"></a>[149] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12994">AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12994" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shiming Wang, Holger Caesar, Liangliang Nan, Julian F. P. Kooij</span></p>
<p><strong>TL;DR:</strong> 本文提出AsyncBEV，一种可训练的轻量级通用模块，旨在提升3D鸟瞰图（BEV）目标检测模型对传感器异步问题的鲁棒性。该方法通过估计不同传感器模态BEV特征间的2D流，并利用已知时间偏移对特征图进行扭曲和对齐，可集成到多种现有BEV检测器架构中。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动驾驶中多模态感知任务通常依赖严格同步的传感器，但实际中因传感器频率差异、网络延迟、硬件故障等因素常导致时间偏移，这种异步问题会降低感知性能，尤其对动态物体影响显著。</p>
<p><strong>Result:</strong> 在基于token的CMT和基于grid的UniBEV检测器上的大量实验表明，AsyncBEV显著提升了模型对LiDAR或相机间小到大异步偏移的鲁棒性。在最坏情况（0.5秒时间偏移）下，动态物体的NDS指标分别比经过自运动补偿的CMT和UniBEV基线高出16.6%和11.9%。</p>
<p><strong>Insight:</strong> 创新点在于受场景流估计启发，将已知时间偏移显式建模并用于跨模态特征流预测与对齐，这是一种轻量且通用的异步补偿方法，可即插即用地增强现有BEV检测器的实际部署鲁棒性。</p>
<p><strong>Abstract:</strong> In autonomous driving, multi-modal perception tasks like 3D object detection typically rely on well-synchronized sensors, both at training and inference. However, despite the use of hardware- or software-based synchronization algorithms, perfect synchrony is rarely guaranteed: Sensors may operate at different frequencies, and real-world factors such as network latency, hardware failures, or processing bottlenecks often introduce time offsets between sensors. Such asynchrony degrades perception performance, especially for dynamic objects. To address this challenge, we propose AsyncBEV, a trainable lightweight and generic module to improve the robustness of 3D Birds’ Eye View (BEV) object detection models against sensor asynchrony. Inspired by scene flow estimation, AsyncBEV first estimates the 2D flow from the BEV features of two different sensor modalities, taking into account the known time offset between these sensor measurements. The predicted feature flow is then used to warp and spatially align the feature maps, which we show can easily be integrated into different current BEV detector architectures (e.g., BEV grid-based and token-based). Extensive experiments demonstrate AsyncBEV improves robustness against both small and large asynchrony between LiDAR or camera sensors in both the token-based CMT and grid-based UniBEV, especially for dynamic objects. We significantly outperform the ego motion compensated CMT and UniBEV baselines, notably by $16.6$ % and $11.9$ % NDS on dynamic objects in the worst-case scenario of a $0.5 s$ time offset. Code will be released upon acceptance.</p>
  </div>
</details>

<hr>
<h3 id="150-Think3D-Thinking-with-Space-for-Spatial-Reasoning-cs-CVPDF"><a href="#150-Think3D-Thinking-with-Space-for-Spatial-Reasoning-cs-CVPDF" class="headerlink" title="[150] Think3D: Thinking with Space for Spatial Reasoning cs.CVPDF"></a>[150] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13029">Think3D: Thinking with Space for Spatial Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13029" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zaibin Zhang, Yuhan Wu, Lianjie Jia, Yifan Wang, Zhongbo Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了Think3D框架，通过整合3D重建模型（从图像&#x2F;视频恢复点云和相机位姿），使视觉大模型（VLM）能够进行主动的、交互式的3D空间推理，从而显著提升了其在多个空间推理基准上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉大模型本质上是2D感知器，缺乏对物理世界进行真正3D空间推理的能力，而空间智能是理解物理世界的关键。</p>
<p><strong>Result:</strong> 在无需额外训练的情况下，Think3D显著提升了GPT-4.1和Gemini 2.5 Pro等先进模型的性能，在BLINK Multi-view和MindCube基准上平均提升+7.8%，在VSI-Bench上提升+4.7%。通过强化学习策略优化视角选择后，小模型从工具使用中获得的收益从+0.7%提升至+6.8%。</p>
<p><strong>Insight:</strong> 核心创新在于将空间推理转化为一个交互式的3D思维链过程，通过主动的相机操作和视角切换来“思考”3D空间。这为无需训练、通过工具增强实现类人3D推理的多模态智能体提供了一条可行路径。</p>
<p><strong>Abstract:</strong> Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego&#x2F;global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at <a target="_blank" rel="noopener" href="https://github.com/zhangzaibin/spagent">https://github.com/zhangzaibin/spagent</a>.</p>
  </div>
</details>

<hr>
<h3 id="151-GridNet-HD-A-High-Resolution-Multi-Modal-Dataset-for-LiDAR-Image-Fusion-on-Power-Line-Infrastructure-cs-CVPDF"><a href="#151-GridNet-HD-A-High-Resolution-Multi-Modal-Dataset-for-LiDAR-Image-Fusion-on-Power-Line-Infrastructure-cs-CVPDF" class="headerlink" title="[151] GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure cs.CVPDF"></a>[151] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13052">GridNet-HD: A High-Resolution Multi-Modal Dataset for LiDAR-Image Fusion on Power Line Infrastructure</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13052" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Antoine Carreaud, Shanci Li, Malo De Lacour, Digre Frinde, Jan Skaloud</span></p>
<p><strong>TL;DR:</strong> 本文介绍了GridNet-HD，一个用于架空电力基础设施3D语义分割的多模态数据集，该数据集结合了高密度LiDAR和高分辨率倾斜影像，包含7,694张图像和25亿个点，标注为11个类别，并提供了预定义的数据划分和mIoU评估指标。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 目前缺乏同时提供高密度LiDAR、高分辨率倾斜影像以及电力线资产3D语义标签的公开数据集，本文旨在填补这一空白，以促进多模态融合方法在电力基础设施语义分割中的应用研究。</p>
<p><strong>Result:</strong> 在GridNet-HD数据集上，多模态融合模型比最佳单模态基线（仅LiDAR或仅图像）的mIoU高出5.55，证明了几何与外观信息的互补性。</p>
<p><strong>Insight:</strong> 该工作的核心创新在于构建了一个高质量、多模态的基准数据集，为电力线基础设施的3D理解提供了新的研究平台；其实验结果也定量验证了在多模态任务中融合不同传感器数据的有效性。</p>
<p><strong>Abstract:</strong> This paper presents GridNet-HD, a multi-modal dataset for 3D semantic segmentation of overhead electrical infrastructures, pairing high-density LiDAR with high-resolution oblique imagery. The dataset comprises 7,694 images and 2.5 billion points annotated into 11 classes, with predefined splits and mIoU metrics. Unimodal (LiDAR-only, image-only) and multi-modal fusion baselines are provided. On GridNet-HD, fusion models outperform the best unimodal baseline by +5.55 mIoU, highlighting the complementarity of geometry and appearance. As reviewed in Sec. 2, no public dataset jointly provides high-density LiDAR and high-resolution oblique imagery with 3D semantic labels for power-line assets. Dataset, baselines, and codes are available: <a target="_blank" rel="noopener" href="https://huggingface.co/collections/heig-vd-geo/gridnet-hd">https://huggingface.co/collections/heig-vd-geo/gridnet-hd</a>.</p>
  </div>
</details>

<hr>
<h3 id="152-Prototype-Learning-Based-Few-Shot-Segmentation-for-Low-Light-Crack-on-Concrete-Structures-cs-CVPDF"><a href="#152-Prototype-Learning-Based-Few-Shot-Segmentation-for-Low-Light-Crack-on-Concrete-Structures-cs-CVPDF" class="headerlink" title="[152] Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures cs.CVPDF"></a>[152] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13059">Prototype Learning-Based Few-Shot Segmentation for Low-Light Crack on Concrete Structures</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13059" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yulun Guo</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于原型学习的少样本分割方法，用于低光照环境下混凝土结构的裂缝检测。该方法结合Retinex理论与少样本学习，通过双分支网络学习光照不变的全局表示，并利用度量学习减少对大规模标注数据的依赖。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决低光照环境（如隧道、桥底）下裂缝检测因光照不足导致分割精度下降的问题，同时缓解像素级标注耗时且低光照标注数据稀缺的挑战。</p>
<p><strong>Result:</strong> 在多个基准测试上进行了广泛实验，结果表明该方法在低光照条件下 consistently 达到了 state-of-the-art 性能。</p>
<p><strong>Insight:</strong> 创新点包括：结合Retinex理论的光照不变表示学习、基于交叉相似性的先验掩码生成模块以捕捉裂缝位置和结构、以及多尺度特征增强模块以缓解空间不一致性。从客观角度看，该方法将低光照图像增强与少样本分割有效结合，为低资源场景下的缺陷检测提供了新思路。</p>
<p><strong>Abstract:</strong> Crack detection is critical for concrete infrastructure safety, but real-world cracks often appear in low-light environments like tunnels and bridge undersides, degrading computer vision segmentation accuracy. Pixel-level annotation of low-light crack images is extremely time-consuming, yet most deep learning methods require large, well-illuminated datasets. We propose a dual-branch prototype learning network integrating Retinex theory with few-shot learning for low-light crack segmentation. Retinex-based reflectance components guide illumination-invariant global representation learning, while metric learning reduces dependence on large annotated datasets. We introduce a cross-similarity prior mask generation module that computes high-dimensional similarities between query and support features to capture crack location and structure, and a multi-scale feature enhancement module that fuses multi-scale features with the prior mask to alleviate spatial inconsistency. Extensive experiments on multiple benchmarks demonstrate consistent state-of-the-art performance under low-light conditions. Code: <a target="_blank" rel="noopener" href="https://github.com/YulunGuo/CrackFSS">https://github.com/YulunGuo/CrackFSS</a>.</p>
  </div>
</details>

<hr>
<h3 id="153-GaussExplorer-3D-Gaussian-Splatting-for-Embodied-Exploration-and-Reasoning-cs-CVPDF"><a href="#153-GaussExplorer-3D-Gaussian-Splatting-for-Embodied-Exploration-and-Reasoning-cs-CVPDF" class="headerlink" title="[153] GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning cs.CVPDF"></a>[153] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13132">GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13132" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kim Yu-Ji, Dahye Lee, Kim Jun-Seong, GeonU Kim, Nam Hyeon-Woo</span></p>
<p><strong>TL;DR:</strong> GaussExplorer是一个基于3D高斯溅射（3DGS）的具身探索与推理框架，通过结合视觉语言模型（VLM）来处理复杂组合语言查询，并支持问题驱动的3D场景探索。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有语言嵌入3DGS方法难以处理复杂组合查询，而基于对象中心RGB-D结构化记忆的方法受限于固定视角，因此需要一种能支持灵活视角调整和复杂推理的具身探索方案。</p>
<p><strong>Result:</strong> 实验表明，该方法在多个基准测试上优于现有方法，验证了将VLM推理与3DGS结合用于具身任务的有效性。</p>
<p><strong>Insight:</strong> 创新点在于将VLM集成到3DGS中，通过查询关联图像识别和视角调整来增强视觉信息捕获，从而提升复杂语言查询下的3D场景推理能力。</p>
<p><strong>Abstract:</strong> We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.</p>
  </div>
</details>

<hr>
<h3 id="154-CLIP-Guided-Adaptable-Self-Supervised-Learning-for-Human-Centric-Visual-Tasks-cs-CVPDF"><a href="#154-CLIP-Guided-Adaptable-Self-Supervised-Learning-for-Human-Centric-Visual-Tasks-cs-CVPDF" class="headerlink" title="[154] CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks cs.CVPDF"></a>[154] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13133">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13133" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为CLASP的新型无监督预训练框架，专门针对以人为中心的视觉任务。该框架利用CLIP模型生成多层次语义伪标签，并通过提示控制的专家混合模块动态调整特征提取，以增强表示的表达能力和泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大规模未标记人体图像数据集的涌现，需要一种通用的无监督预训练模型来支持多样化的以人为中心的下游任务。</p>
<p><strong>Result:</strong> 在多个基准测试上的广泛实验表明，CLASP持续优于现有的无监督预训练方法，推动了以人为中心的视觉分析领域的发展。</p>
<p><strong>Insight:</strong> 创新点在于利用CLIP生成多层次语义伪标签来指导表示学习，并引入提示控制的专家混合模块实现任务自适应的特征提取，从而提升模型的泛化能力和可迁移性。</p>
<p><strong>Abstract:</strong> Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</p>
  </div>
</details>

<hr>
<h3 id="155-TVWorld-Foundations-for-Remote-Control-TV-Agents-cs-CV-cs-AI-cs-CLPDF"><a href="#155-TVWorld-Foundations-for-Remote-Control-TV-Agents-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[155] TVWorld: Foundations for Remote-Control TV Agents cs.CV | cs.AI | cs.CLPDF"></a>[155] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13142">TVWorld: Foundations for Remote-Control TV Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13142" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhantao Ma, Quanfeng Lu, Shuai Zhong, Dahai Yu, Ping Luo</span></p>
<p><strong>TL;DR:</strong> 本文提出了TVWorld，一个基于图结构的离线电视导航抽象环境，用于评估远程控制（RC）交互能力，并在此基础上构建了TVWorld-N（拓扑感知导航）和TVWorld-G（焦点感知定位）两个基准测试。研究发现现有智能体在长视野电视导航中存在拓扑意识不足的问题，因此提出了拓扑感知训练框架，并开发了TVTheseus基础模型，在TVWorld-N上取得了68.3%的成功率，超越了Gemini 3 Flash等强基线，达到了SOTA水平。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大视觉语言模型（LVLM）研究主要集中于点选（PnC）交互，而日常电视使用中常见的远程控制（RC）交互尚未得到充分探索，因此需要构建专门的评估基准和模型来解决这一问题。</p>
<p><strong>Result:</strong> 在TVWorld-N基准测试上，提出的TVTheseus模型取得了68.3%的成功率，超越了Gemini 3 Flash等强闭源基线，达到了最先进的（SOTA）性能。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了首个专注于远程控制电视导航的离线可复现评估环境TVWorld及其配套基准，并揭示了拓扑意识是长视野导航的关键瓶颈；提出的拓扑感知训练框架通过将导航结构知识注入LVLM，有效提升了模型在复杂电视界面中的导航能力，为设备控制智能体的开发提供了新思路。</p>
<p><strong>Abstract:</strong> Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.</p>
  </div>
</details>

<hr>
<h3 id="156-GTPred-Benchmarking-MLLMs-for-Interpretable-Geo-localization-and-Time-of-capture-Prediction-cs-CVPDF"><a href="#156-GTPred-Benchmarking-MLLMs-for-Interpretable-Geo-localization-and-Time-of-capture-Prediction-cs-CVPDF" class="headerlink" title="[156] GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction cs.CVPDF"></a>[156] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13207">GTPred: Benchmarking MLLMs for Interpretable Geo-localization and Time-of-capture Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13207" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinnao Li, Zijian Chen, Tingzhu Chen, Changbo Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了GTPred基准，用于评估多模态大语言模型在联合地理定位与拍摄时间预测任务上的性能，包含370张全球分布、时间跨度超过120年的图像，通过评估8个专有和7个开源MLLM，发现现有模型在世界知识和地理时间推理方面仍存在局限，但整合时间信息能显著提升定位性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有地理定位基准大多忽略图像中固有的时间信息，而时间信息能进一步约束位置推断，因此需要一个新的基准来评估MLLM在联合地理与时间预测任务上的能力。</p>
<p><strong>Result:</strong> 在GTPred基准上测试了15个MLLM，结果显示当前模型尽管视觉感知能力强，但在世界知识和地理时间推理方面有限；实验表明，整合时间信息能显著提升位置推断性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了首个联合地理定位与拍摄时间预测的基准GTPred，并引入了基于年份和分层位置序列匹配的评估方法，以及利用标注的真实推理过程来评估中间推理链，强调了时间信息对地理定位的重要性。</p>
<p><strong>Abstract:</strong> Geo-localization aims to infer the geographic location where an image was captured using observable visual evidence. Traditional methods achieve impressive results through large-scale training on massive image corpora. With the emergence of multi-modal large language models (MLLMs), recent studies have explored their applications in geo-localization, benefiting from improved accuracy and interpretability. However, existing benchmarks largely ignore the temporal information inherent in images, which can further constrain the location. To bridge this gap, we introduce GTPred, a novel benchmark for geo-temporal prediction. GTPred comprises 370 globally distributed images spanning over 120 years. We evaluate MLLM predictions by jointly considering year and hierarchical location sequence matching, and further assess intermediate reasoning chains using meticulously annotated ground-truth reasoning processes. Experiments on 8 proprietary and 7 open-source MLLMs show that, despite strong visual perception, current models remain limited in world knowledge and geo-temporal reasoning. Results also demonstrate that incorporating temporal information significantly enhances location inference performance.</p>
  </div>
</details>

<hr>
<h3 id="157-Not-all-Blends-are-Equal-The-BLEMORE-Dataset-of-Blended-Emotion-Expressions-with-Relative-Salience-Annotations-cs-CV-cs-HCPDF"><a href="#157-Not-all-Blends-are-Equal-The-BLEMORE-Dataset-of-Blended-Emotion-Expressions-with-Relative-Salience-Annotations-cs-CV-cs-HCPDF" class="headerlink" title="[157] Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations cs.CV | cs.HCPDF"></a>[157] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13225">Not all Blends are Equal: The BLEMORE Dataset of Blended Emotion Expressions with Relative Salience Annotations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.HC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13225" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tim Lachmann, Alexandra Israelsson, Christina Tornberg, Teimuraz Saghinadze, Michal Balazia</span></p>
<p><strong>TL;DR:</strong> 本文介绍了BLEMORE数据集，这是一个用于多模态（视频、音频）混合情感识别的新数据集，其特点是标注了混合情感中每种情感的相对显著性。该数据集包含来自58位演员的3000多个视频片段，涵盖了6种基本情感和10种不同的混合情感，每种混合情感有3种不同的显著性配置（50&#x2F;50、70&#x2F;30和30&#x2F;70）。作者使用该数据集对最先进的视频分类方法在两个混合情感预测任务上进行了广泛评估：预测样本中是否存在特定情感，以及预测混合情感中情感的相对显著性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类经常同时经历多种具有不同显著性的混合情感，而非单一基本情感。然而，大多数基于视频的情感识别方法仅能识别单一情感，少数尝试识别混合情感的方法通常无法评估混合情感中各情感的相对显著性。这一局限主要源于缺乏包含大量标注了相对显著性的混合情感样本的数据集。</p>
<p><strong>Result:</strong> 在验证集上，单模态分类器在情感存在预测任务上达到最高29%的准确率，在显著性预测任务上达到13%的准确率。多模态方法有明显提升，其中ImageBind + WavLM在情感存在预测上达到35%的准确率，HiCMAE在显著性预测上达到18%的准确率。在保留的测试集上，最佳模型（VideoMAEv2 + HuBERT）在情感存在预测上达到33%的准确率，最佳模型（HiCMAE）在显著性预测上达到18%的准确率。</p>
<p><strong>Insight:</strong> 论文的主要创新点是创建了首个大规模、标注了混合情感相对显著性的多模态数据集BLEMORE，填补了该研究领域的空白。从客观角度看，该数据集的结构化设计（明确的混合类型和显著性配置）为开发能够理解复杂混合情感的计算模型提供了关键资源，并推动了多模态方法在情感识别任务上的评估和进步。</p>
<p><strong>Abstract:</strong> Humans often experience not just a single basic emotion at a time, but rather a blend of several emotions with varying salience. Despite the importance of such blended emotions, most video-based emotion recognition approaches are designed to recognize single emotions only. The few approaches that have attempted to recognize blended emotions typically cannot assess the relative salience of the emotions within a blend. This limitation largely stems from the lack of datasets containing a substantial number of blended emotion samples annotated with relative salience. To address this shortcoming, we introduce BLEMORE, a novel dataset for multimodal (video, audio) blended emotion recognition that includes information on the relative salience of each emotion within a blend. BLEMORE comprises over 3,000 clips from 58 actors, performing 6 basic emotions and 10 distinct blends, where each blend has 3 different salience configurations (50&#x2F;50, 70&#x2F;30, and 30&#x2F;70). Using this dataset, we conduct extensive evaluations of state-of-the-art video classification approaches on two blended emotion prediction tasks: (1) predicting the presence of emotions in a given sample, and (2) predicting the relative salience of emotions in a blend. Our results show that unimodal classifiers achieve up to 29% presence accuracy and 13% salience accuracy on the validation set, while multimodal methods yield clear improvements, with ImageBind + WavLM reaching 35% presence accuracy and HiCMAE 18% salience accuracy. On the held-out test set, the best models achieve 33% presence accuracy (VideoMAEv2 + HuBERT) and 18% salience accuracy (HiCMAE). In sum, the BLEMORE dataset provides a valuable resource to advancing research on emotion recognition systems that account for the complexity and significance of blended emotion expressions.</p>
  </div>
</details>

<hr>
<h3 id="158-A-Semantic-Decoupling-Based-Two-Stage-Rainy-Day-Attack-for-Revealing-Weather-Robustness-Deficiencies-in-Vision-Language-Models-cs-CV-cs-AIPDF"><a href="#158-A-Semantic-Decoupling-Based-Two-Stage-Rainy-Day-Attack-for-Revealing-Weather-Robustness-Deficiencies-in-Vision-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[158] A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models cs.CV | cs.AIPDF"></a>[158] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13238">A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13238" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chengyin Hu, Xiang Chen, Zhe Jia, Weiwen Shi, Fengyu Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于语义解耦的两阶段雨天攻击框架，首次利用真实天气条件对视觉语言模型进行对抗攻击，通过建模降雨的全局效应和结构化雨滴变化，分析雨天扰动下模型决策的语义偏移。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 视觉语言模型在标准视觉条件下训练，但其在真实天气条件下的鲁棒性以及跨模态语义对齐的稳定性尚未得到充分研究，本文旨在揭示雨天场景下VLMs的脆弱性。</p>
<p><strong>Result:</strong> 实验表明，即使物理上合理且高度约束的天气扰动也能导致主流VLMs出现显著的语义错位，在多个任务上验证了攻击的有效性，突显了实际部署中的安全风险。</p>
<p><strong>Insight:</strong> 创新点在于将对抗攻击与真实天气建模结合，通过两阶段参数化扰动（全局调制与多尺度雨滴结构优化）实现物理可解释的攻击，揭示了光照建模和多尺度雨滴结构是引发语义偏移的关键因素。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.</p>
  </div>
</details>

<hr>
<h3 id="159-Enginuity-Building-an-Open-Multi-Domain-Dataset-of-Complex-Engineering-Diagrams-cs-CVPDF"><a href="#159-Enginuity-Building-an-Open-Multi-Domain-Dataset-of-Complex-Engineering-Diagrams-cs-CVPDF" class="headerlink" title="[159] Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams cs.CVPDF"></a>[159] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13299">Enginuity: Building an Open Multi-Domain Dataset of Complex Engineering Diagrams</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13299" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ethan Seefried, Prahitha Movva, Naga Harshita Marupaka, Tilak Kasturi, Tirthankar Ghosal</span></p>
<p><strong>TL;DR:</strong> 本文提出了Enginuity，这是首个开放、大规模、多领域的工程图数据集，包含全面的结构标注，旨在支持自动化图表解析。该数据集通过捕捉不同工程领域中的层次化组件关系、连接和语义元素，使多模态大语言模型能够处理结构化图表解析、跨模态信息检索和AI辅助工程仿真等关键下游任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决当前AI在科学发现工作流中因无法充分理解和操作工程图中嵌入的视觉-结构知识而面临的障碍，特别是在需要图表解释、技术图纸分析和视觉推理的假设生成、实验设计和发现环节。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体的定量实验结果或基准测试，但强调了数据集将支持多模态大语言模型处理关键下游任务。</p>
<p><strong>Insight:</strong> 创新点在于构建首个开放、大规模、多领域的工程图数据集，提供全面的结构标注，以促进AI在科学发现中对复杂工程图的理解和应用，从而突破现有AI参与科学工作流的限制。</p>
<p><strong>Abstract:</strong> We propose Enginuity - the first open, large-scale, multi-domain engineering diagram dataset with comprehensive structural annotations designed for automated diagram parsing. By capturing hierarchical component relationships, connections, and semantic elements across diverse engineering domains, our proposed dataset would enable multimodal large language models to address critical downstream tasks including structured diagram parsing, cross-modal information retrieval, and AI-assisted engineering simulation. Enginuity would be transformative for AI for Scientific Discovery by enabling artificial intelligence systems to comprehend and manipulate the visual-structural knowledge embedded in engineering diagrams, breaking down a fundamental barrier that currently prevents AI from fully participating in scientific workflows where diagram interpretation, technical drawing analysis, and visual reasoning are essential for hypothesis generation, experimental design, and discovery.</p>
  </div>
</details>

<hr>
<h3 id="160-CausalSpatial-A-Benchmark-for-Object-Centric-Causal-Spatial-Reasoning-cs-CVPDF"><a href="#160-CausalSpatial-A-Benchmark-for-Object-Centric-Causal-Spatial-Reasoning-cs-CVPDF" class="headerlink" title="[160] CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning cs.CVPDF"></a>[160] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13304">CausalSpatial: A Benchmark for Object-Centric Causal Spatial Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13304" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenxin Ma, Chenlong Wang, Ruisheng Yuan, Hao Chen, Nanru Dai</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为CausalSpatial的基准测试，用于评估多模态大语言模型在物体中心因果空间推理方面的能力，并揭示了模型在此任务上的严重不足。同时，论文提出了Causal Object World模型框架，通过生成假设动态的视频来外化模拟过程，以提升模型基于视觉证据的推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的多模态大语言模型主要局限于静态空间感知，无法像人类一样对静态场景进行因果空间推理（例如预测物体移动是否会引发碰撞），因此需要建立一个诊断性基准来评估和解决这一能力缺陷。</p>
<p><strong>Result:</strong> 在CausalSpatial基准的四个任务（碰撞、兼容性、遮挡和轨迹）上，人类得分84%，而GPT-5仅得54%，显示出巨大差距。提出的COW框架旨在通过生成视频提供明确的因果视觉线索来改善这一性能。</p>
<p><strong>Insight:</strong> 论文的创新点在于创建了一个专注于物体中心因果空间推理的基准测试，并揭示了模型过度依赖文本思维链而导致与视觉证据脱节的问题。提出的COW框架通过外化模拟过程，将推理过程锚定在物理现实而非语言先验上，为解决多模态模型的视觉基础问题提供了新思路。</p>
<p><strong>Abstract:</strong> Humans can look at a static scene and instantly predict what happens next – will moving this object cause a collision? We call this ability Causal Spatial Reasoning. However, current multimodal large language models (MLLMs) cannot do this, as they remain largely restricted to static spatial perception, struggling to answer “what-if” questions in a 3D scene. We introduce CausalSpatial, a diagnostic benchmark evaluating whether models can anticipate consequences of object motions across four tasks: Collision, Compatibility, Occlusion, and Trajectory. Results expose a severe gap: humans score 84% while GPT-5 achieves only 54%. Why do MLLMs fail? Our analysis uncovers a fundamental deficiency: models over-rely on textual chain-of-thought reasoning that drifts from visual evidence, producing fluent but spatially ungrounded hallucinations. To address this, we propose the Causal Object World model (COW), a framework that externalizes the simulation process by generating videos of hypothetical dynamics. With explicit visual cues of causality, COW enables models to ground their reasoning in physical reality rather than linguistic priors. We make the dataset and code publicly available here: <a target="_blank" rel="noopener" href="https://github.com/CausalSpatial/CausalSpatial">https://github.com/CausalSpatial/CausalSpatial</a></p>
  </div>
</details>

<hr>
<h3 id="161-MultiST-A-Cross-Attention-Based-Multimodal-Model-for-Spatial-Transcriptomic-cs-CV-cs-LGPDF"><a href="#161-MultiST-A-Cross-Attention-Based-Multimodal-Model-for-Spatial-Transcriptomic-cs-CV-cs-LGPDF" class="headerlink" title="[161] MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic cs.CV | cs.LGPDF"></a>[161] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13331">MultiST: A Cross-Attention-Based Multimodal Model for Spatial Transcriptomic</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13331" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wei Wang, Quoc-Toan Ly, Chong Yu, Jun Bai</span></p>
<p><strong>TL;DR:</strong> 本文提出了MultiST，一种基于交叉注意力的多模态模型，用于整合空间转录组学中的组织学形态和分子表达数据，以更清晰地解析空间域边界。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法在整合组织学形态和分子图谱方面不足，常采用浅层融合或忽略组织图像，导致空间域边界模糊，MultiST旨在解决这一挑战。</p>
<p><strong>Result:</strong> 在涵盖人类大脑皮层和乳腺癌组织的13个ST数据集上评估，MultiST相比现有方法能产生边界更清晰、更一致的空间域，并带来更稳定的伪时间轨迹和更可解释的细胞间相互作用模式。</p>
<p><strong>Insight:</strong> 创新点在于通过基于交叉注意力的融合统一建模空间拓扑、基因表达和组织形态，并采用基于图的基因编码器与对抗对齐学习鲁棒空间表示，同时整合颜色归一化的组织学特征以捕获分子-形态依赖性。</p>
<p><strong>Abstract:</strong> Spatial transcriptomics (ST) enables transcriptome-wide profiling while preserving the spatial context of tissues, offering unprecedented opportunities to study tissue organization and cell-cell interactions in situ. Despite recent advances, existing methods often lack effective integration of histological morphology with molecular profiles, relying on shallow fusion strategies or omitting tissue images altogether, which limits their ability to resolve ambiguous spatial domain boundaries. To address this challenge, we propose MultiST, a unified multimodal framework that jointly models spatial topology, gene expression, and tissue morphology through cross-attention-based fusion. MultiST employs graph-based gene encoders with adversarial alignment to learn robust spatial representations, while integrating color-normalized histological features to capture molecular-morphological dependencies and refine domain boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning two organs, including human brain cortex and breast cancer tissue. MultiST yields spatial domains with clearer and more coherent boundaries than existing methods, leading to more stable pseudotime trajectories and more biologically interpretable cell-cell interaction patterns. The MultiST framework and source code are available at <a target="_blank" rel="noopener" href="https://github.com/LabJunBMI/MultiST.git">https://github.com/LabJunBMI/MultiST.git</a>.</p>
  </div>
</details>

<hr>
<h3 id="162-Organ-Aware-Attention-Improves-CT-Triage-and-Classification-cs-CV-cs-AIPDF"><a href="#162-Organ-Aware-Attention-Improves-CT-Triage-and-Classification-cs-CV-cs-AIPDF" class="headerlink" title="[162] Organ-Aware Attention Improves CT Triage and Classification cs.CV | cs.AIPDF"></a>[162] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13385">Organ-Aware Attention Improves CT Triage and Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13385" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为ORACLE-CT的器官感知注意力方法，用于改进CT影像的快速分诊和分类。该方法通过器官掩码注意力和器官标量融合技术，在胸部CT-RATE数据集上达到AUROC 0.86，在腹部MERLIN数据集上达到AUROC 0.85，实现了在统一评估协议下最先进的监督分类性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决CT影像高通量分诊和分类的迫切需求，以改善患者护理并减轻放射科医生负担。现有视觉语言模型在处理3D解剖结构、协议偏移和噪声报告监督方面存在困难。</p>
<p><strong>Result:</strong> 在胸部CT-RATE数据集上AUROC达到0.86，在腹部MERLIN数据集（30个发现）上AUROC达到0.85，超越了所有已报告的线性探测视觉语言模型，实现了最先进的监督分类性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了器官感知注意力头（ORACLE-CT），结合器官掩码注意力（提供空间证据）和器官标量融合（轻量级融合归一化体积和平均HU线索），该方法与编码器无关，能有效处理3D医学影像的特定挑战。</p>
<p><strong>Abstract:</strong> There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at <a target="_blank" rel="noopener" href="https://github.com/lavsendahal/oracle-ct">https://github.com/lavsendahal/oracle-ct</a>.</p>
  </div>
</details>

<hr>
<h3 id="163-Reasoning-with-Pixel-level-Precision-QVLM-Architecture-and-SQuID-Dataset-for-Quantitative-Geospatial-Analytics-cs-CV-cs-AIPDF"><a href="#163-Reasoning-with-Pixel-level-Precision-QVLM-Architecture-and-SQuID-Dataset-for-Quantitative-Geospatial-Analytics-cs-CV-cs-AIPDF" class="headerlink" title="[163] Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics cs.CV | cs.AIPDF"></a>[163] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13401">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13401" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peter A. Massih, Eric Cosatto</span></p>
<p><strong>TL;DR:</strong> 该论文针对现有视觉语言模型在定量空间推理任务中因图像编码过程丢失像素级信息而导致计数和测量不准确的问题，提出了QVLM架构和SQuID数据集。QVLM通过解耦语言理解和视觉分析，生成可执行代码来调用分割模型获取像素级掩码并进行直接操作，从而保持空间索引的精确性。SQuID是一个包含2000个卫星图像问答对的基准数据集，用于评估定量空间推理能力。实验表明，QVLM在SQuID上的准确率显著优于传统VLM方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉语言模型在压缩图像时通过补丁嵌入破坏了像素级信息，导致其无法进行精确的计数和测量等定量空间推理。</p>
<p><strong>Result:</strong> 在提出的SQuID基准测试上，使用GPT-5作为编码器的QVLM模型达到了42.0%的准确率，而使用图像-问题对提示的传统VLM方法仅获得28.1%的准确率。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于通过架构解耦（将语言理解与视觉分析分离）和代码生成范式来保持像素级精度，这为需要精确空间索引的视觉推理任务（如地理空间分析）提供了一种新思路。从客观角度看，将视觉问题转化为可执行代码操作像素掩码，是一种有效绕过传统嵌入瓶颈、保留原始数据精度的创新方法。</p>
<p><strong>Abstract:</strong> Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</p>
  </div>
</details>

<hr>
<h3 id="164-Local-to-Global-Logical-Explanations-for-Deep-Vision-Models-cs-CV-cs-AIPDF"><a href="#164-Local-to-Global-Logical-Explanations-for-Deep-Vision-Models-cs-CV-cs-AIPDF" class="headerlink" title="[164] Local-to-Global Logical Explanations for Deep Vision Models cs.CV | cs.AIPDF"></a>[164] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13404">Local-to-Global Logical Explanations for Deep Vision Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13404" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对深度视觉模型的黑盒解释方法，通过可识别的基本概念生成局部和全局的逻辑解释。局部解释针对单张图像，全局解释针对图像集合，均以单调析取范式（MDNF）逻辑公式表示，确保模型对给定类别给出高分。同时，算法还支持以单调解释列表形式解释多类别分类。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决深度神经网络在图像分类中不透明、难以解释的问题，旨在提供人类可理解的解释方法，以增强模型的可信度和可解释性。</p>
<p><strong>Result:</strong> 在具有挑战性的视觉数据集上，所提出的解释方法在保真度和覆盖率方面表现出色，能够有效反映黑盒模型的行为。</p>
<p><strong>Insight:</strong> 创新点在于将局部和全局解释统一为MDNF逻辑公式，并引入单调解释列表处理多类别分类，兼顾了简单性、可解释性与高保真度，为黑盒模型解释提供了结构化、逻辑化的新思路。</p>
<p><strong>Abstract:</strong> While deep neural networks are extremely effective at classifying images, they remain opaque and hard to interpret. We introduce local and global explanation methods for black-box models that generate explanations in terms of human-recognizable primitive concepts. Both the local explanations for a single image and the global explanations for a set of images are cast as logical formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction guarantees that the model yields a high score on a given class. We also present an algorithm for explaining the classification of examples into multiple classes in the form of a monotone explanation list over primitive concepts. Despite their simplicity and interpretability we show that the explanations maintain high fidelity and coverage with respect to the blackbox models they seek to explain in challenging vision datasets.</p>
  </div>
</details>

<hr>
<h3 id="165-Analyzing-VLM-Based-Approaches-for-Anomaly-Classification-and-Segmentation-cs-CVPDF"><a href="#165-Analyzing-VLM-Based-Approaches-for-Anomaly-Classification-and-Segmentation-cs-CVPDF" class="headerlink" title="[165] Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation cs.CVPDF"></a>[165] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13440">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13440" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</span></p>
<p><strong>TL;DR:</strong> 本文对基于视觉语言模型（VLM）的异常分类与分割方法进行了系统性综述与分析，重点探讨了WinCLIP、AprilLab框架和组合提示集成等关键架构范式，并在MVTec AD和VisA等基准数据集上评估了其分类精度、分割性能和计算效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 利用VLM（特别是CLIP）的零样本&#x2F;少样本能力，解决工业质检等场景中异常检测对大量标注数据和特定任务训练的依赖问题。</p>
<p><strong>Result:</strong> 在MVTec AD和VisA等基准上评估了不同方法的分类准确率、分割精度和推理效率，为方法选择提供了实证依据。</p>
<p><strong>Insight:</strong> 创新点在于系统分析了VLM在异常检测中成功的关键因素（如特征提取、图文对齐、提示工程），并综合了方法选择的实用见解，指出了当前局限性与未来研究方向。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</p>
  </div>
</details>

<hr>
<h3 id="166-Optical-Linear-Systems-Framework-for-Event-Sensing-and-Computational-Neuromorphic-Imaging-cs-CVPDF"><a href="#166-Optical-Linear-Systems-Framework-for-Event-Sensing-and-Computational-Neuromorphic-Imaging-cs-CVPDF" class="headerlink" title="[166] Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging cs.CVPDF"></a>[166] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13498">Optical Linear Systems Framework for Event Sensing and Computational Neuromorphic Imaging</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13498" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nimrod Kruger, Nicholas Owen Ralph, Gregory Cohen, Paul Hurley</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于物理原理的处理框架，将事件流映射为像素级的对数强度和强度导数估计，并将其嵌入到一个具有时变点扩散函数的动态线性系统模型中。这使得能够直接从事件数据出发，利用已知（或参数化）的动态传递函数进行频域维纳反卷积，从而为动态光学系统的事件传感与基于模型的计算成像之间搭建了一座实用的桥梁。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 事件视觉传感器（神经形态相机）输出稀疏、异步的ON&#x2F;OFF事件，具有高动态范围和低数据带宽，但其非线性的事件表示难以与支撑大多数计算成像和光学系统设计的线性前向模型相集成。</p>
<p><strong>Result:</strong> 该方法在模拟（调制离焦下的单点源和重叠点源）和真实事件数据（可调焦望远镜观测星场）上进行了验证，展示了光源定位和可分离性。</p>
<p><strong>Insight:</strong> 核心创新点在于建立了一个将非线性事件流嵌入动态线性系统模型的框架，从而允许直接应用基于线性模型的逆滤波技术（如维纳反卷积）来处理事件数据，为事件传感与计算成像的结合提供了新途径。</p>
<p><strong>Abstract:</strong> Event vision sensors (neuromorphic cameras) output sparse, asynchronous ON&#x2F;OFF events triggered by log-intensity threshold crossings, enabling microsecond-scale sensing with high dynamic range and low data bandwidth. As a nonlinear system, this event representation does not readily integrate with the linear forward models that underpin most computational imaging and optical system design. We present a physics-grounded processing pipeline that maps event streams to estimates of per-pixel log-intensity and intensity derivatives, and embeds these measurements in a dynamic linear systems model with a time-varying point spread function. This enables inverse filtering directly from event data, using frequency-domain Wiener deconvolution with a known (or parameterised) dynamic transfer function. We validate the approach in simulation for single and overlapping point sources under modulated defocus, and on real event data from a tunable-focus telescope imaging a star field, demonstrating source localisation and separability. The proposed framework provides a practical bridge between event sensing and model-based computational imaging for dynamic optical systems.</p>
  </div>
</details>

<hr>
<h3 id="167-DIS2-Disentanglement-Meets-Distillation-with-Classwise-Attention-for-Robust-Remote-Sensing-Segmentation-under-Missing-Modalities-cs-CVPDF"><a href="#167-DIS2-Disentanglement-Meets-Distillation-with-Classwise-Attention-for-Robust-Remote-Sensing-Segmentation-under-Missing-Modalities-cs-CVPDF" class="headerlink" title="[167] DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities cs.CVPDF"></a>[167] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13502">DIS2: Disentanglement Meets Distillation with Classwise Attention for Robust Remote Sensing Segmentation under Missing Modalities</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13502" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nhi Kieu, Kien Nguyen, Arnold Wiliem, Clinton Fookes, Sridha Sridharan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DIS2的新方法，用于解决遥感图像分割中因模态缺失导致的多模态学习性能下降问题。该方法通过重新设计解耦学习与知识蒸馏的协同机制（DLKD），并结合类别特征学习模块（CFLM）和分层混合融合结构，实现了对缺失信息的主动补偿和类别自适应的模态贡献学习，从而在多个基准测试中显著超越了现有最先进方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 遥感多模态学习常因数据模态缺失而性能受损，且遥感数据具有高度异构性和尺度变化大的特点，导致其他领域有效的范式（如依赖模态不变特征的解耦学习或知识蒸馏）在此失效，因此需要专门设计的方法来补偿缺失信息并处理类别特定的模态贡献。</p>
<p><strong>Result:</strong> 在多个遥感分割基准测试上进行的大量实验表明，DIS2方法显著优于现有最先进（SOTA）方法。</p>
<p><strong>Insight:</strong> 创新点在于将解耦学习与知识蒸馏重新协同（DLKD），从依赖模态共享特征和无目标模仿转向主动引导的缺失特征补偿；同时引入类别特征学习模块（CFLM）自适应学习每个类别的判别性证据，并结合多分辨率特征的分层混合融合结构来增强预测。这些设计针对遥感数据的异构性和尺度变化进行了专门优化。</p>
<p><strong>Abstract:</strong> The efficacy of multimodal learning in remote sensing (RS) is severely undermined by missing modalities. The challenge is exacerbated by the RS highly heterogeneous data and huge scale variation. Consequently, paradigms proven effective in other domains often fail when confronted with these unique data characteristics. Conventional disentanglement learning, which relies on significant feature overlap between modalities (modality-invariant), is insufficient for this heterogeneity. Similarly, knowledge distillation becomes an ill-posed mimicry task where a student fails to focus on the necessary compensatory knowledge, leaving the semantic gap unaddressed. Our work is therefore built upon three pillars uniquely designed for RS: (1) principled missing information compensation, (2) class-specific modality contribution, and (3) multi-resolution feature importance. We propose a novel method DIS2, a new paradigm shifting from modality-shared feature dependence and untargeted imitation to active, guided missing features compensation. Its core novelty lies in a reformulated synergy between disentanglement learning and knowledge distillation, termed DLKD. Compensatory features are explicitly captured which, when fused with the features of the available modality, approximate the ideal fused representation of the full-modality case. To address the class-specific challenge, our Classwise Feature Learning Module (CFLM) adaptively learn discriminative evidence for each target depending on signal availability. Both DLKD and CFLM are supported by a hierarchical hybrid fusion (HF) structure using features across resolutions to strengthen prediction. Extensive experiments validate that our proposed approach significantly outperforms state-of-the-art methods across benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="168-Learning-Fine-Grained-Correspondence-with-Cross-Perspective-Perception-for-Open-Vocabulary-6D-Object-Pose-Estimation-cs-CV-cs-RO-eess-IVPDF"><a href="#168-Learning-Fine-Grained-Correspondence-with-Cross-Perspective-Perception-for-Open-Vocabulary-6D-Object-Pose-Estimation-cs-CV-cs-RO-eess-IVPDF" class="headerlink" title="[168] Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation cs.CV | cs.RO | eess.IVPDF"></a>[168] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13565">Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13565" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu Qin, Shimeng Fan, Fan Yang, Zixuan Xue, Zijie Mai</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为FiCoP的框架，用于解决开放词汇6D物体姿态估计中全局匹配策略带来的模糊性问题。该方法通过从全局匹配转向空间约束的块级对应，利用块间相关性矩阵作为结构先验来缩小匹配范围，从而有效过滤背景干扰，提升姿态估计的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有开放词汇6D姿态估计方法依赖于无约束的全局匹配策略，在开放世界场景中，目标特征容易与背景干扰物混淆，导致匹配模糊性高。本文旨在通过精细化的局部对应来解决这一问题。</p>
<p><strong>Result:</strong> 在REAL275和Toyota-Light数据集上的实验表明，FiCoP相比最先进的方法（SOTA）分别将平均召回率提高了8.0%和6.1%，证明了其在复杂开放世界环境中的鲁棒性和泛化能力。</p>
<p><strong>Insight:</strong> 核心创新在于利用块到块相关性矩阵作为结构先验来约束匹配范围，并设计了以物体为中心的分离预处理、跨视角全局感知模块和块相关性预测器，实现了从噪声敏感的全局匹配到精细、抗噪声的局部对应的转变，为机器人感知提供了新思路。</p>
<p><strong>Abstract:</strong> Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/zjjqinyu/FiCoP">https://github.com/zjjqinyu/FiCoP</a>.</p>
  </div>
</details>

<hr>
<h3 id="169-ChartVerse-Scaling-Chart-Reasoning-via-Reliable-Programmatic-Synthesis-from-Scratch-cs-CVPDF"><a href="#169-ChartVerse-Scaling-Chart-Reasoning-via-Reliable-Programmatic-Synthesis-from-Scratch-cs-CVPDF" class="headerlink" title="[169] ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch cs.CVPDF"></a>[169] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13606">ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13606" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao</span></p>
<p><strong>TL;DR:</strong> 本文提出了ChartVerse框架，旨在通过程序化合成方法，从零开始生成复杂图表和可靠的推理数据，以解决视觉语言模型在图表推理任务中高质量训练数据匮乏的问题。该框架通过引入量化图表复杂度的Rollout Posterior Entropy（RPE）指标和基于答案先行的逆向QA合成方法，生成了大规模、高质量的训练数据集ChartVerse-SFT-600K和ChartVerse-RL-40K，并训练出性能优异的ChartVerse-8B模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前开源视觉语言模型在图表推理能力上的发展受到高质量训练数据缺乏的严重制约，现有数据集存在合成图表过于简单重复、问答对易产生幻觉且推理深度不足的双重挑战。</p>
<p><strong>Result:</strong> 实验结果表明，基于ChartVerse框架训练的ChartVerse-8B模型在图表推理任务上取得了最先进的性能，显著超越了其教师模型Qwen3-VL-30B-A3B-Thinking，并与更强的Qwen3-VL-32B-Thinking模型性能相当。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于：1）提出了Rollout Posterior Entropy（RPE）这一量化图表复杂度的新指标，并以此指导复杂度感知的图表编码器自主合成多样化的高复杂度图表；2）提出了“答案先行”的逆向QA合成范式，直接从源代码提取确定性答案再生成问题，并执行严格的一致性验证，确保了推理的严谨性；3）通过模型失败率筛选和高质量思维链蒸馏，进一步提升了数据集的难度和推理深度。这些方法为解决数据合成中的复杂性和可靠性问题提供了系统性的技术路径。</p>
<p><strong>Abstract:</strong> Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.</p>
  </div>
</details>

<hr>
<h3 id="170-CARPE-Context-Aware-Image-Representation-Prioritization-via-Ensemble-for-Large-Vision-Language-Models-cs-CV-cs-AIPDF"><a href="#170-CARPE-Context-Aware-Image-Representation-Prioritization-via-Ensemble-for-Large-Vision-Language-Models-cs-CV-cs-AIPDF" class="headerlink" title="[170] CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models cs.CV | cs.AIPDF"></a>[170] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13622">CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13622" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Donghee Lee, Rui Cai, Zhe Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为CARPE的模型无关框架，旨在解决大型视觉语言模型在图像分类等视觉中心任务上表现不佳的问题。CARPE通过引入视觉集成层和上下文感知集成策略，使模型能够自适应地权衡视觉与文本模态，优先使用图像表示或依赖语言模型的推理能力，从而提升模型在分类和视觉语言基准上的泛化性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型视觉语言模型在视觉中心任务（如图像分类）上的表现仍落后于其基础视觉编码器（如CLIP），因此需要一种方法来增强模型对视觉信息的利用能力。</p>
<p><strong>Result:</strong> 大量实验表明，CARPE不仅提升了图像分类基准（如ImageNet）的性能，还在多种视觉语言基准（如VQA、GQA）上取得了改进，显示出其广泛的适用性和有效性。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种上下文感知的集成策略，使模型能动态调整视觉与文本的权重，从而更灵活地处理不同任务；从客观角度看，这种模型无关的设计可轻松集成到现有开源LVLM架构中，具有很好的可扩展性和实用性。</p>
<p><strong>Abstract:</strong> Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model’s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</p>
  </div>
</details>

<hr>
<h3 id="171-Scaling-Test-time-Inference-for-Visual-Grounding-cs-CVPDF"><a href="#171-Scaling-Test-time-Inference-for-Visual-Grounding-cs-CVPDF" class="headerlink" title="[171] Scaling Test-time Inference for Visual Grounding cs.CVPDF"></a>[171] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13633">Scaling Test-time Inference for Visual Grounding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13633" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guanqi Zhan, Changye Li, Zhijian Liu, Yao Lu, Yi Wu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为EGM的高效视觉定位方法，通过扩展小型视觉语言模型在测试时的计算量（即生成更多token），在保持部署友好性的同时，显著提升了视觉定位性能，使其能够达到甚至超越大型模型的精度，同时大幅降低推理延迟。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有最先进的视觉定位模型通常规模庞大，导致部署成本高、推理速度慢。作者观察到小型与大型视觉语言模型的主要差异在于语言模型部分，而非视觉编码器，因此希望通过增加小型模型在测试时的计算量来弥补其语言理解能力的不足。</p>
<p><strong>Result:</strong> 在RefCOCO基准测试中，EGM-Qwen3-VL-8B模型取得了91.4 IoU，平均延迟为737毫秒（比Qwen3-VL-235B快5.9倍），而后者需要4320毫秒达到90.5 IoU。在新建立的非模态视觉定位任务上，该方法也能持续显著提升小型模型的性能，达到或超越大型模型水平。</p>
<p><strong>Insight:</strong> 创新点在于提出通过扩展测试时计算（增加生成token数量）来提升小型视觉语言模型的视觉定位能力，这是一种部署友好的高效方法。其核心洞察是视觉定位性能差距主要源于语言理解能力，而非视觉处理，因此可以通过增加语言模型的推理计算来弥补模型规模的不足。</p>
<p><strong>Abstract:</strong> Visual grounding is an essential capability of Visual Language Models (VLMs) to understand the real physical world. Previous state-of-the-art grounding visual language models usually have large model sizes, making them heavy for deployment and slow for inference. However, we notice that the sizes of visual encoders are nearly the same for small and large VLMs and the major difference is the sizes of the language models. Small VLMs fall behind larger VLMs in grounding because of the difference in language understanding capability rather than visual information handling. To mitigate the gap, we introduce ‘Efficient visual Grounding language Models’ (EGM): a method to scale the test-time computation (#generated tokens). Scaling the test-time computation of a small model is deployment-friendly, and yields better end-to-end latency as the cost of each token is much cheaper compared to directly running a large model. On the RefCOCO benchmark, our EGM-Qwen3-VL-8B demonstrates 91.4 IoU with an average of 737ms (5.9x faster) latency while Qwen3-VL-235B demands 4,320ms to achieve 90.5 IoU. To validate our approach’s generality, we further set up a new amodal grounding setting that requires the model to predict both the visible and occluded parts of the objects. Experiments show our method can consistently and significantly improve the vanilla grounding and amodal grounding capabilities of small models to be on par with or outperform the larger models, thereby improving the efficiency for visual grounding.</p>
  </div>
</details>

<hr>
<h3 id="172-Face-Voice-Association-with-Inductive-Bias-for-Maximum-Class-Separation-cs-CVPDF"><a href="#172-Face-Voice-Association-with-Inductive-Bias-for-Maximum-Class-Separation-cs-CVPDF" class="headerlink" title="[172] Face-Voice Association with Inductive Bias for Maximum Class Separation cs.CVPDF"></a>[172] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13651">Face-Voice Association with Inductive Bias for Maximum Class Separation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13651" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marta Moscati, Oleksandr Kats, Mubashir Noman, Muhammad Zaigham Zaheer, Yufang Hou</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于人脸-语音关联的新方法，该方法通过引入最大类间分离作为归纳偏置来增强不同说话者的多模态表示之间的区分性。实验表明，该方法在两种人脸-语音关联任务上达到了最先进的性能，并通过消融研究验证了结合类间正交性损失的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的人脸-语音关联方法主要依赖损失函数来学习嵌入表示，但尚未利用最大类间分离这一在分类任务中已被证明能增强嵌入判别性的归纳偏置技术。本文旨在填补这一空白。</p>
<p><strong>Result:</strong> 定量实验表明，该方法在两种人脸-语音关联任务上实现了SOTA性能。消融研究进一步显示，当最大类间分离的归纳偏置与类间正交性损失结合时效果最佳。</p>
<p><strong>Insight:</strong> 创新点在于首次将最大类间分离作为归纳偏置引入多模态学习（特别是人脸-语音关联）领域，并证明了其有效性，这为建立新的多模态学习范式铺平了道路。</p>
<p><strong>Abstract:</strong> Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.</p>
  </div>
</details>

<hr>
<h3 id="173-VIAFormer-Voxel-Image-Alignment-Transformer-for-High-Fidelity-Voxel-Refinement-cs-CVPDF"><a href="#173-VIAFormer-Voxel-Image-Alignment-Transformer-for-High-Fidelity-Voxel-Refinement-cs-CVPDF" class="headerlink" title="[173] VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement cs.CVPDF"></a>[173] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13664">VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13664" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tiancheng Fang, Bowen Pan, Lingxi Chen, Jiangjing Lyu, Chengfei Lyu</span></p>
<p><strong>TL;DR:</strong> 本文提出了VIAFormer模型，用于多视图条件下的体素细化任务，即利用校准的多视图图像作为指导来修复不完整且有噪声的体素。其核心是一个协同设计，包括提供2D图像令牌显式3D空间定位的图像索引、学习直接体素细化轨迹的校正流目标，以及实现鲁棒跨模态融合的混合流Transformer。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决如何利用多视图图像作为引导，来修复从强大视觉基础模型获得的体素形状中存在的严重合成损坏和真实伪影的问题。</p>
<p><strong>Result:</strong> 实验表明，VIAFormer在纠正体素形状的严重合成损坏和真实伪影方面，建立了新的最先进水平（SOTA）。</p>
<p><strong>Insight:</strong> 创新点在于图像索引为2D特征提供3D空间基础、校正流目标直接学习细化轨迹，以及混合流Transformer的跨模态融合设计，为基于体素的方法在大模型大数据浪潮中提供了实用可靠的桥梁。</p>
<p><strong>Abstract:</strong> We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement–the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.</p>
  </div>
</details>

<hr>
<h3 id="174-Reasoning-or-Pattern-Matching-Probing-Large-Vision-Language-Models-with-Visual-Puzzles-cs-CVPDF"><a href="#174-Reasoning-or-Pattern-Matching-Probing-Large-Vision-Language-Models-with-Visual-Puzzles-cs-CVPDF" class="headerlink" title="[174] Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles cs.CVPDF"></a>[174] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13705">Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13705" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maria Lymperaiou, Vasileios Karampinis, Giorgos Filandrianos, Angelos Vlachos, Chrysoula Zerva</span></p>
<p><strong>TL;DR:</strong> 这篇论文系统综述了视觉谜题作为诊断工具，用于评估大型视觉语言模型（LVLMs）的推理能力，而非模式匹配。它通过统一的抽象框架对现有视觉谜题基准进行分类，并综合实证证据揭示了当前模型的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是利用视觉谜题作为紧凑且可控的探针，以最小化先验知识依赖的方式，隔离并评估LVLMs的抽象、规则发现和系统性推理能力，弥补开放式多模态基准的不足。</p>
<p><strong>Result:</strong> 综合实证证据表明，当前模型存在一致的局限性，包括脆弱的泛化能力、感知与推理的紧密纠缠，以及流畅解释与忠实执行之间的持续差距。</p>
<p><strong>Insight:</strong> 创新点在于将视觉谜题框架化为诊断工具而非任务格式，并通过按目标推理机制（归纳、类比、算法、演绎、几何&#x2F;空间）对基准进行分类，为未来基准和推理感知的多模态系统指明了方向。</p>
<p><strong>Abstract:</strong> Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric&#x2F;spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.</p>
  </div>
</details>

<hr>
<h3 id="175-Attention-space-Contrastive-Guidance-for-Efficient-Hallucination-Mitigation-in-LVLMs-cs-CV-cs-AI-cs-LGPDF"><a href="#175-Attention-space-Contrastive-Guidance-for-Efficient-Hallucination-Mitigation-in-LVLMs-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[175] Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs cs.CV | cs.AI | cs.LGPDF"></a>[175] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13707">Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13707" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujin Jo, Sangyoon Bae, Taesup Kim</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为注意力空间对比引导（ACG）的高效幻觉缓解方法，用于大型视觉语言模型（LVLMs）。该方法通过在自注意力层内构建视觉-语言和纯语言两种注意力路径，并进行正交化校正，以单次前向计算的方式减少模型对语言先验的过度依赖，从而生成更忠实于视觉内容的描述。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型视觉语言模型中的幻觉问题通常源于语言先验主导了视觉证据，导致物体误识别和视觉不一致的描述。本文旨在通过对比引导框架，将生成过程引导至视觉基础和语义忠实的文本，以解决此问题。</p>
<p><strong>Result:</strong> 在CHAIR和POPE基准测试上的实验表明，ACG在忠实度和描述质量方面达到了最先进水平（SOTA），同时显著降低了计算成本。与需要多次前向传播的先前对比解码方法相比，该方法将延迟降低了高达2倍。</p>
<p><strong>Insight:</strong> 创新点在于将幻觉缓解形式化为一种在注意力空间内操作的对比引导机制，通过单次前向计算同时构建两种注意力路径并引入正交化校正来选择性放大视觉贡献，实现了计算效率与性能的平衡。这为高效缓解幻觉提供了一种原则性方法。</p>
<p><strong>Abstract:</strong> Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model’s internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model’s representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.</p>
  </div>
</details>

<hr>
<h3 id="176-MVGD-Net-A-Novel-Motion-aware-Video-Glass-Surface-Detection-Network-cs-CVPDF"><a href="#176-MVGD-Net-A-Novel-Motion-aware-Video-Glass-Surface-Detection-Network-cs-CVPDF" class="headerlink" title="[176] MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network cs.CVPDF"></a>[176] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13715">MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13715" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiwei Lu, Hao Huang, Tao Yan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MVGD-Net的新型运动感知视频玻璃表面检测网络，通过利用玻璃表面反射或透射物体与非玻璃区域物体之间的运动不一致性线索来检测视频中的玻璃表面。该网络包含三个核心模块：用于融合空间特征和光流图的跨尺度多模态融合模块（CMFM）、历史引导注意力模块（HGAM）和时间交叉注意力模块（TCAM），以及一个用于生成玻璃区域掩码的时空解码器（TSD）。此外，作者还构建了一个包含312个场景、共19,268帧的大规模数据集用于训练和评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 玻璃表面在日常生活和专业环境中普遍存在，对机器人、无人机导航等基于视觉的系统构成潜在威胁。现有研究主要关注视频玻璃表面检测（VGSD），但缺乏对运动线索的有效利用。作者观察到，在视频运动场景中，玻璃表面上的反射或透射物体比同一空间平面内非玻璃区域的物体移动得更慢，这种运动不一致性可以有效揭示玻璃表面的存在。</p>
<p><strong>Result:</strong> 大量实验表明，MVGD-Net在作者构建的大规模数据集上超越了相关的state-of-the-art方法，实现了更优的性能。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于首次明确利用运动不一致性作为检测视频中玻璃表面的关键线索。网络架构的创新体现在三个专门设计的模块：CMFM用于跨模态（空间与运动）特征融合，HGAM和TCAM用于增强时序特征的建模能力。此外，构建的大规模、多样化数据集也为该领域的研究提供了有价值的基准。</p>
<p><strong>Abstract:</strong> Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="177-Hierarchical-Long-Video-Understanding-with-Audiovisual-Entity-Cohesion-and-Agentic-Search-cs-CV-cs-AI-cs-IRPDF"><a href="#177-Hierarchical-Long-Video-Understanding-with-Audiovisual-Entity-Cohesion-and-Agentic-Search-cs-CV-cs-AI-cs-IRPDF" class="headerlink" title="[177] Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search cs.CV | cs.AI | cs.IRPDF"></a>[177] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13719">Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13719" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu</span></p>
<p><strong>TL;DR:</strong> 论文提出了HAVEN框架，用于解决长视频理解中因上下文过长导致的信息碎片化和全局连贯性丢失问题。该框架通过整合视听实体凝聚和分层视频索引与智能搜索，实现了连贯且全面的推理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于简单分块策略和检索增强生成的方法在长视频理解中存在信息碎片化和全局连贯性丢失的问题，因此需要一种能够保持语义一致性和动态推理的新方法。</p>
<p><strong>Result:</strong> 在LVBench基准测试中，该方法达到了84.1%的整体准确率，在具有挑战性的推理类别中达到了80.1%，实现了新的SOTA水平，并展示了良好的时间连贯性、实体一致性和检索效率。</p>
<p><strong>Insight:</strong> 创新点包括整合视听实体级表示以保持语义一致性，构建从全局摘要到实体的结构化分层内容组织，以及采用智能搜索机制进行动态跨层检索和推理，从而提升长视频理解的连贯性和细粒度跟踪能力。</p>
<p><strong>Abstract:</strong> Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.</p>
  </div>
</details>

<hr>
<h3 id="178-PREGEN-Uncovering-Latent-Thoughts-in-Composed-Video-Retrieval-cs-CVPDF"><a href="#178-PREGEN-Uncovering-Latent-Thoughts-in-Composed-Video-Retrieval-cs-CVPDF" class="headerlink" title="[178] PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval cs.CVPDF"></a>[178] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13797">PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13797" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gabriele Serussi, David Vainshtein, Jonathan Kouchly, Dotan Di Castro, Chaim Baskin</span></p>
<p><strong>TL;DR:</strong> PREGEN是一个高效且强大的组合视频检索框架，它通过冻结预训练的视觉语言模型并提取其各层隐藏状态，结合轻量级编码器生成语义丰富的紧凑嵌入，无需微调VLM即可实现SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有组合视频检索方法未能充分利用现代视觉语言模型、需要昂贵微调和慢速字幕生成的问题。</p>
<p><strong>Result:</strong> 在标准CoVR基准测试中显著超越所有先前方法，Recall@1提升+27.23和+69.59，并在不同VLM骨干和复杂文本修改的零样本泛化中表现出鲁棒性。</p>
<p><strong>Insight:</strong> 创新性地从冻结VLM的各层提取隐藏状态作为语义表示，结合轻量编码器实现高效检索，避免了模型微调并提升了语义理解能力。</p>
<p><strong>Abstract:</strong> Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.</p>
  </div>
</details>

<hr>
<h3 id="179-Insight-Interpretable-Semantic-Hierarchies-in-Vision-Language-Encoders-cs-CV-cs-AI-cs-LGPDF"><a href="#179-Insight-Interpretable-Semantic-Hierarchies-in-Vision-Language-Encoders-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[179] Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders cs.CV | cs.AI | cs.LGPDF"></a>[179] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13798">Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13798" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele, Jonas Fischer</span></p>
<p><strong>TL;DR:</strong> 本文提出Insight，一种语言对齐的概念基础模型，旨在解决现有视觉-语言基础模型表征不透明、难以解释的问题。该模型通过分层稀疏自编码器和语义强大的基础模型自动提取多粒度、可解释且空间定位的概念，并利用概念间的共现依赖关系定义概念关系，从而改进概念命名和生成更丰富的解释。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有语言对齐的视觉基础模型在下游任务中表现强劲，但其学习到的表征不透明，决策过程难以解释。近期工作虽将表征分解为可解释概念，但空间定位能力差且仅限于图像分类任务。</p>
<p><strong>Result:</strong> 在基准数据上，Insight在分类和分割任务上达到了与不透明基础模型相当的性能，同时提供了细粒度、高质量的概念解释。</p>
<p><strong>Insight:</strong> 创新点在于结合分层稀疏自编码器和语义丰富的基础模型自动提取多粒度、可空间定位的概念，并通过分析概念间的局部共现依赖关系来定义概念关系和优化解释，实现了性能与可解释性的平衡。</p>
<p><strong>Abstract:</strong> Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at <a target="_blank" rel="noopener" href="https://github.com/kawi19/Insight">https://github.com/kawi19/Insight</a>.</p>
  </div>
</details>

<hr>
<h3 id="180-FastGHA-Generalized-Few-Shot-3D-Gaussian-Head-Avatars-with-Real-Time-Animation-cs-CVPDF"><a href="#180-FastGHA-Generalized-Few-Shot-3D-Gaussian-Head-Avatars-with-Real-Time-Animation-cs-CVPDF" class="headerlink" title="[180] FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation cs.CVPDF"></a>[180] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13837">FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13837" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xinya Ji, Sebastian Weiss, Manuel Kansy, Jacek Naruniec, Xun Cao</span></p>
<p><strong>TL;DR:</strong> 本文提出了FastGHA，一种前馈式方法，仅需少量输入图像即可生成高质量3D高斯人头化身，并支持实时动画。该方法通过基于Transformer的编码器融合DINOv3和Stable Diffusion VAE的图像特征，直接学习像素级高斯表示，并引入轻量级MLP动态网络预测3D高斯变形以实现动画。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于3D高斯的人头化身建模方法通常依赖大量多视图捕获设备或单目视频，并在推理时需要进行每身份优化，导致可扩展性和对未见主体的易用性受限。本文旨在克服这些效率缺陷。</p>
<p><strong>Result:</strong> 实验表明，该方法在渲染质量和推理效率上显著优于现有方法，同时支持实时动态化身动画。</p>
<p><strong>Insight:</strong> 创新点包括：使用基于Transformer的编码器融合DINOv3与Stable Diffusion VAE特征以聚合多视图信息；为显式高斯表示引入每高斯特征和轻量级MLP动态网络以实现实时动画；利用预训练大型重建模型的点云图作为几何监督以增强3D头部几何平滑性。</p>
<p><strong>Abstract:</strong> Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.</p>
  </div>
</details>

<hr>
<h3 id="181-DisasterVQA-A-Visual-Question-Answering-Benchmark-Dataset-for-Disaster-Scenes-cs-CVPDF"><a href="#181-DisasterVQA-A-Visual-Question-Answering-Benchmark-Dataset-for-Disaster-Scenes-cs-CVPDF" class="headerlink" title="[181] DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes cs.CVPDF"></a>[181] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13839">DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13839" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aisha Al-Mohannadi, Ayisha Firoz, Yin Yang, Muhammad Imran, Ferda Ofli</span></p>
<p><strong>TL;DR:</strong> 该论文提出了DisasterVQA，一个专门针对灾害场景的视觉问答基准数据集，包含1395张真实灾害图像和4405个专家标注的问答对，用于评估模型在灾害响应中的感知与推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于社交媒体图像是灾害期间获取态势信息的重要低延迟来源，但目前通用的视觉问答模型在应对灾害响应所需的复杂、安全关键型推理任务上的适用性尚不明确，因此需要专门的基准来评估和引导相关模型的发展。</p>
<p><strong>Result:</strong> 论文在DisasterVQA数据集上对七个最先进的视觉语言模型进行了基准测试，发现模型在二元问题上准确率高，但在细粒度定量推理、物体计数和上下文敏感解释方面表现不佳，尤其是在代表性不足的灾害场景中。</p>
<p><strong>Insight:</strong> 创新点在于创建了首个基于真实灾害图像、并根植于FEMA ESF和OCHA MIRA等人道主义框架的VQA基准数据集，为开发更鲁棒且具有实际操作意义的灾害响应视觉语言模型提供了具有挑战性的评估标准。</p>
<p><strong>Abstract:</strong> Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at <a target="_blank" rel="noopener" href="https://zenodo.org/records/18267770">https://zenodo.org/records/18267770</a>.</p>
  </div>
</details>

<hr>
<h3 id="182-Revisiting-Multi-Task-Visual-Representation-Learning-cs-CVPDF"><a href="#182-Revisiting-Multi-Task-Visual-Representation-Learning-cs-CVPDF" class="headerlink" title="[182] Revisiting Multi-Task Visual Representation Learning cs.CVPDF"></a>[182] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13886">Revisiting Multi-Task Visual Representation Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13886" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shangzhe Di, Zhonghua Zhai, Weidi Xie</span></p>
<p><strong>TL;DR:</strong> 本文提出了MTV，一个多任务视觉预训练框架，旨在整合视觉语言对比学习、自监督学习和密集空间监督的优势，以克服现有视觉表示学习方法在全局语义对齐和局部空间精度上的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视觉表示学习存在分裂：视觉语言模型（如CLIP）擅长全局语义对齐但缺乏空间精度，而自监督方法（如MAE、DINO）能捕捉局部结构却难以理解高级语义。论文动机是整合这些互补范式，通过多任务学习构建更全面的视觉编码器。</p>
<p><strong>Result:</strong> MTV框架在多个基准测试中实现了“两全其美”的性能，显著提升了细粒度空间推理能力，同时不损害全局语义理解，展示了多任务学习在视觉表示中的有效性。</p>
<p><strong>Insight:</strong> 创新点包括：1）提出一个统一的多任务框架，结合视觉语言对比、自监督和密集空间目标；2）利用大容量“专家”模型（如Depth Anything V2和OWLv2）生成大规模密集伪标签，减少人工标注需求；3）系统分析了多任务学习的机制，包括各目标的边际增益、任务协同与干扰，以及数据和模型规模的扩展行为。</p>
<p><strong>Abstract:</strong> Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity “expert” models – such as Depth Anything V2 and OWLv2 – to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves “best-of-both-worlds” performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</p>
  </div>
</details>

<hr>
<h3 id="183-On-the-Role-of-Rotation-Equivariance-in-Monocular-3D-Human-Pose-Estimation-cs-CVPDF"><a href="#183-On-the-Role-of-Rotation-Equivariance-in-Monocular-3D-Human-Pose-Estimation-cs-CVPDF" class="headerlink" title="[183] On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation cs.CVPDF"></a>[183] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13913">On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13913" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pavlo Melnyk, Cuong Le, Urs Waldmann, Per-Erik Forssén, Bastian Wandt</span></p>
<p><strong>TL;DR:</strong> 本文研究了旋转等变性在单目3D人体姿态估计中的作用，提出了一种通过数据增强学习旋转等变性的方法，而非显式设计等变网络。该方法在常见基准测试中超越了现有的显式等变设计方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有3D人体姿态估计方法在输入图像旋转时性能下降的问题，认为学习姿态及其平面内旋转比直接学习点对点映射更简单且几何基础更扎实。</p>
<p><strong>Result:</strong> 在常见3D人体姿态估计基准测试上，该方法通过数据增强学习旋转等变性，在图像平面内旋转的姿态上提升了模型性能，并超越了现有的显式等变设计方法（SOTA）。</p>
<p><strong>Insight:</strong> 创新点在于提出通过数据增强而非网络结构设计来隐式学习旋转等变性，这简化了学习过程并提升了泛化能力；客观分析认为，这种隐式学习策略可能更高效且易于实现，为几何感知任务提供了新思路。</p>
<p><strong>Abstract:</strong> Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.</p>
  </div>
</details>

<hr>
<h3 id="184-Glance-or-Gaze-Incentivizing-LMMs-to-Adaptively-Focus-Search-via-Reinforcement-Learning-cs-CV-cs-AIPDF"><a href="#184-Glance-or-Gaze-Incentivizing-LMMs-to-Adaptively-Focus-Search-via-Reinforcement-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[184] Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning cs.CV | cs.AIPDF"></a>[184] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13942">Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13942" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen</span></p>
<p><strong>TL;DR:</strong> 本文提出了Glance-or-Gaze (GoG)框架，旨在解决大型多模态模型在处理知识密集型视觉查询时的局限性。该框架通过强化学习激励模型自适应地选择是“瞥视”全局上下文还是“凝视”高价值区域，从而在检索前过滤冗余和噪声信息，实现主动的视觉规划。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于检索增强的大型多模态模型在处理涉及长尾实体或动态信息的复杂视觉查询时，存在因全图像检索引入大量视觉冗余和噪声，以及缺乏深度迭代反思的问题。</p>
<p><strong>Result:</strong> 在六个基准测试上取得了最先进的性能。消融研究证实了选择性凝视机制和复杂度自适应强化学习对于有效视觉搜索至关重要。</p>
<p><strong>Insight:</strong> 核心创新点在于从被动感知转向主动视觉规划的范式转变，提出了选择性凝视机制来动态决策视觉关注粒度，并设计了包含监督微调和复杂度自适应强化学习的双阶段训练策略，以增强模型对复杂查询的迭代推理能力。</p>
<p><strong>Abstract:</strong> Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model’s capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.</p>
  </div>
</details>

<hr>
<h3 id="185-STEC-A-Reference-Free-Spatio-Temporal-Entropy-Coverage-Metric-for-Evaluating-Sampled-Video-Frames-cs-CVPDF"><a href="#185-STEC-A-Reference-Free-Spatio-Temporal-Entropy-Coverage-Metric-for-Evaluating-Sampled-Video-Frames-cs-CVPDF" class="headerlink" title="[185] STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames cs.CVPDF"></a>[185] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13974">STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13974" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shih-Yao Lin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为STEC（时空熵覆盖）的无参考度量标准，用于评估视频帧采样的质量。该指标通过结合空间信息强度、时间分散性和非冗余性，为采样帧集是否充分捕捉视频内容提供了一种轻量级、原则性的评估方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频帧采样评估指标主要关注感知质量或重建保真度，无法有效评估采样帧集是否充分捕捉了信息丰富且具有代表性的视频内容，因此需要一种新的任务无关的评估工具。</p>
<p><strong>Result:</strong> 在MSR-VTT test-1k基准测试上的实验表明，STEC能够清晰区分随机、均匀和内容感知等常见采样策略，并揭示了平均性能无法捕捉的单个视频间的鲁棒性模式。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个联合建模空间信息（基于熵的结构复杂性）、时间覆盖和冗余性的无参考评估指标STEC，为在有限预算下分析帧采样行为提供了一个通用的诊断信号，而非直接预测下游任务精度。</p>
<p><strong>Abstract:</strong> Frame sampling is a fundamental component in video understanding and video–language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.   We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.   Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.   We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.</p>
  </div>
</details>

<hr>
<h3 id="186-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation-cs-CV-cs-ROPDF"><a href="#186-FantasyVLN-Unified-Multimodal-Chain-of-Thought-Reasoning-for-Vision-Language-Navigation-cs-CV-cs-ROPDF" class="headerlink" title="[186] FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation cs.CV | cs.ROPDF"></a>[186] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13976">FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13976" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu</span></p>
<p><strong>TL;DR:</strong> 本文提出了FantasyVLN，一个用于视觉语言导航的统一隐式推理框架，通过将想象的视觉标记编码到紧凑的潜在空间，避免了显式多模态思维链推理带来的令牌膨胀问题，实现了推理感知且实时的导航。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有视觉语言导航中思维链推理方法的不足：纯文本思维链缺乏空间基础且易过拟合稀疏标注的推理步骤，而多模态思维链因生成想象的视觉观察导致严重的令牌膨胀，使得实时导航不切实际。</p>
<p><strong>Result:</strong> 在LH-VLN基准上的大量实验表明，该方法实现了推理感知且实时的导航，提高了成功率和效率，与显式思维链方法相比，推理延迟降低了一个数量级。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个统一的隐式推理框架，通过预训练的视觉自回归器将想象的视觉标记编码到紧凑潜在空间，并在统一的多思维链策略下联合学习文本、视觉和多模态思维链模式，在推理时无需生成显式思维链即可享受推理感知的表征。</p>
<p><strong>Abstract:</strong> Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.</p>
  </div>
</details>

<hr>
<h3 id="187-Human-detectors-are-surprisingly-powerful-reward-models-cs-CVPDF"><a href="#187-Human-detectors-are-surprisingly-powerful-reward-models-cs-CVPDF" class="headerlink" title="[187] Human detectors are surprisingly powerful reward models cs.CVPDF"></a>[187] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14037">Human detectors are surprisingly powerful reward models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14037" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kumar Ashutosh, XuDong Wang, Xi Yin, Kristen Grauman, Adam Polyak</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为HuDA的简单奖励模型，用于量化和改善生成视频中的人类动作质量。HuDA结合了现成的人类检测器置信度（评估外观质量）和时序提示对齐分数（评估动作真实性），无需额外训练。通过将其用于视频模型的组奖励策略优化（GRPO）后训练，显著提升了复杂人类动作（如运动、舞蹈）的生成质量，在基准测试中优于Wan 2.1等最先进模型，胜率达73%。该方法还证明了对动物视频和人物-物体交互生成的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视频生成模型在视觉保真度和时序连贯性上已取得进展，但在合成复杂非刚性运动（如人类动态动作）时仍存在肢体缺失、姿态扭曲或物理上不合理等问题，需要一种方法来量化并改进生成视频中的人类动作质量。</p>
<p><strong>Result:</strong> HuDA奖励模型在无需额外训练的情况下，优于使用人工标注数据微调的专用模型。通过GRPO后训练，在生成复杂人类动作的视频上显著提升质量，优于Wan 2.1等SOTA模型，胜率达到73%。此外，在动物视频和人物-物体交互生成上也显示出改进效果。</p>
<p><strong>Insight:</strong> 创新点在于利用现成的、未经专门训练的人类检测器作为强大的奖励模型组件，结合外观和时序评估，以简单方法有效量化视频中人类动作的真实性。客观来看，这揭示了通用预训练模型在特定评估任务中的潜力，为视频生成的后训练优化提供了一种轻量且高效的解决方案。</p>
<p><strong>Abstract:</strong> Video generation models have recently achieved impressive visual fidelity and temporal coherence. Yet, they continue to struggle with complex, non-rigid motions, especially when synthesizing humans performing dynamic actions such as sports, dance, etc. Generated videos often exhibit missing or extra limbs, distorted poses, or physically implausible actions. In this work, we propose a remarkably simple reward model, HuDA, to quantify and improve the human motion in generated videos. HuDA integrates human detection confidence for appearance quality, and a temporal prompt alignment score to capture motion realism. We show this simple reward function that leverages off-the-shelf models without any additional training, outperforms specialized models finetuned with manually annotated data. Using HuDA for Group Reward Policy Optimization (GRPO) post-training of video models, we significantly enhance video generation, especially when generating complex human motions, outperforming state-of-the-art models like Wan 2.1, with win-rate of 73%. Finally, we demonstrate that HuDA improves generation quality beyond just humans, for instance, significantly improving generation of animal videos and human-object interactions.</p>
  </div>
</details>

<hr>
<h3 id="188-DermaBench-A-Clinician-Annotated-Benchmark-Dataset-for-Dermatology-Visual-Question-Answering-and-Reasoning-cs-CV-cs-AI-cs-CLPDF"><a href="#188-DermaBench-A-Clinician-Annotated-Benchmark-Dataset-for-Dermatology-Visual-Question-Answering-and-Reasoning-cs-CV-cs-AI-cs-CLPDF" class="headerlink" title="[188] DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning cs.CV | cs.AI | cs.CLPDF"></a>[188] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14084">DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14084" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas</span></p>
<p><strong>TL;DR:</strong> 本文介绍了DermaBench，一个由临床医生标注的皮肤病学视觉问答基准数据集，旨在评估视觉语言模型在皮肤病图像理解、细粒度形态推理和临床描述生成方面的能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有皮肤病数据集主要关注图像级分类任务（如病变识别），无法全面评估多模态模型的视觉理解、语言基础和临床推理能力，因此需要视觉问答基准来填补这一空白。</p>
<p><strong>Result:</strong> DermaBench基于DDI数据集构建，包含656张临床图像，覆盖Fitzpatrick皮肤类型I-VI，通过专家标注产生了约14,474个VQA风格注释，并作为元数据数据集公开发布。</p>
<p><strong>Insight:</strong> 创新点在于引入了分层标注模式，涵盖诊断、解剖部位、病变形态等多维度临床问题，为皮肤病学VQA提供了首个综合性基准，促进了模型在细粒度医学推理方面的评估。</p>
<p><strong>Abstract:</strong> Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.</p>
  </div>
</details>

<hr>
<h3 id="189-The-Side-Effects-of-Being-Smart-Safety-Risks-in-MLLMs’-Multi-Image-Reasoning-cs-CV-cs-CLPDF"><a href="#189-The-Side-Effects-of-Being-Smart-Safety-Risks-in-MLLMs’-Multi-Image-Reasoning-cs-CV-cs-CLPDF" class="headerlink" title="[189] The Side Effects of Being Smart: Safety Risks in MLLMs’ Multi-Image Reasoning cs.CV | cs.CLPDF"></a>[189] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14127">The Side Effects of Being Smart: Safety Risks in MLLMs’ Multi-Image Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14127" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Renmiao Chen, Yida Lu, Shiyao Cui, Xuan Ouyang, Victor Shea-Jay Huang</span></p>
<p><strong>TL;DR:</strong> 该论文研究了多模态大语言模型（MLLMs）在具备更强的多图像推理能力时可能引发的新的安全风险。作者构建了首个专注于多图像推理安全的基准测试MIR-SafetyBench，包含9类多图像关系的2676个实例，并对19个MLLMs进行了评估。研究发现，多图像推理能力更强的模型在安全基准上可能更脆弱，许多被标记为安全的回答是肤浅的、误解性的或回避性的。此外，不安全的生成内容平均表现出更低的注意力熵，表明模型可能过度专注于任务解决而忽视安全约束。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着多模态大语言模型（MLLMs）获得更强的处理复杂多图像指令的推理能力，这种进步可能带来新的安全风险，论文旨在研究这一问题。</p>
<p><strong>Result:</strong> 在构建的MIR-SafetyBench基准上对19个MLLMs的广泛评估显示，多图像推理能力更先进的模型在安全基准上可能更脆弱，揭示了模型安全性与推理能力之间的负相关趋势。</p>
<p><strong>Insight:</strong> 论文的创新点在于首次构建了专注于多图像推理安全的基准测试MIR-SafetyBench，并揭示了模型推理能力增强可能伴随安全脆弱性增加的矛盾现象，以及通过注意力熵分析发现不安全生成内容的内在特征，为模型安全评估提供了新的视角和工具。</p>
<p><strong>Abstract:</strong> As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at <a target="_blank" rel="noopener" href="https://github.com/thu-coai/MIR-SafetyBench">https://github.com/thu-coai/MIR-SafetyBench</a>.</p>
  </div>
</details>

<hr>
<h3 id="190-Weather-R1-Logically-Consistent-Reinforcement-Fine-Tuning-for-Multimodal-Reasoning-in-Meteorology-cs-CVPDF"><a href="#190-Weather-R1-Logically-Consistent-Reinforcement-Fine-Tuning-for-Multimodal-Reasoning-in-Meteorology-cs-CVPDF" class="headerlink" title="[190] Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology cs.CVPDF"></a>[190] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14044">Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14044" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaiyu Wu, Pucheng Han, Hualong Zhang, Naigeng Wu, Keze Wang</span></p>
<p><strong>TL;DR:</strong> 本文针对气象领域多模态推理任务，提出了WeatherQA基准和LoCo-RFT方法，以解决现有视觉语言模型存在的领域鸿沟和推理忠实性问题，并构建了首个具有逻辑忠实性的气象推理模型Weather-R1。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型在气象领域应用受限，主要存在领域鸿沟和推理忠实性鸿沟，特别是主流强化微调方法会导致模型推理过程与最终答案自相矛盾，这在气象等高风险领域是不可接受的。</p>
<p><strong>Result:</strong> 在提出的WeatherQA基准上，Weather-R1比基线模型性能提升了9.8个百分点，优于监督微调和传统强化微调，甚至超越了原始的Qwen2.5-VL-32B模型。</p>
<p><strong>Insight:</strong> 核心创新点在于提出了逻辑一致性奖励的强化微调方法，以解决自相矛盾推理问题，并构建了首个气象领域多模态推理基准和具有逻辑忠实性的专用模型。</p>
<p><strong>Abstract:</strong> While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model’s reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at <a target="_blank" rel="noopener" href="https://github.com/Marcowky/Weather-R1">https://github.com/Marcowky/Weather-R1</a>.</p>
  </div>
</details>

<hr>
<h3 id="191-Vision-Also-You-Need-Navigating-Out-of-Distribution-Detection-with-Multimodal-Large-Language-Model-cs-CVPDF"><a href="#191-Vision-Also-You-Need-Navigating-Out-of-Distribution-Detection-with-Multimodal-Large-Language-Model-cs-CVPDF" class="headerlink" title="[191] Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model cs.CVPDF"></a>[191] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14052">Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14052" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoran Xu, Yanlin Liu, Zizhao Tong, Jiaze Li, Kexue Fu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MM-OOD的新颖流程，利用多模态大语言模型（MLLMs）的多模态推理和多轮对话能力，以增强分布外（OOD）检测。该方法针对近OOD和远OOD任务分别设计了策略，在Food-101等数据集上取得了显著改进，并验证了在ImageNet-1K上的可扩展性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于CLIP的零样本OOD检测方法过度依赖文本空间的大语言模型知识，忽视了图像空间检测分布外样本的固有挑战，本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，该方法在广泛使用的多模态数据集（如Food-101）上取得了显著改进，并在ImageNet-1K上验证了其可扩展性。</p>
<p><strong>Insight:</strong> 创新点在于利用MLLMs的多模态能力，并针对近&#x2F;远OOD任务分别设计策略（如为远OOD任务引入“草图-生成-阐述”框架），将视觉信息更系统地整合到OOD检测流程中，而非仅依赖文本知识。</p>
<p><strong>Abstract:</strong> Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.</p>
  </div>
</details>

<hr>
<h3 id="192-Decoder-Free-Supervoxel-GNN-for-Accurate-Brain-Tumor-Localization-in-Multi-Modal-MRI-cs-CV-cs-AIPDF"><a href="#192-Decoder-Free-Supervoxel-GNN-for-Accurate-Brain-Tumor-Localization-in-Multi-Modal-MRI-cs-CV-cs-AIPDF" class="headerlink" title="[192] Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI cs.CV | cs.AIPDF"></a>[192] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14055">Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14055" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa Barbosa Da Silva, Paolo Cacace</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为SVGFormer的无解码器超体素图神经网络，用于多模态MRI中的脑肿瘤精确定位。该方法通过内容感知分组将3D医学图像分割成语义图，并采用分层编码器结合补丁级Transformer和超体素级图注意力网络来学习特征，实现了从局部到区域的双尺度可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有3D医学视觉主干网络通常采用参数密集的编码器-解码器结构，将大量参数用于空间重建而非特征学习的问题，旨在设计一种更专注于特征编码且具有内在可解释性的替代方案。</p>
<p><strong>Result:</strong> 在BraTS数据集上训练的两个专用模型（节点级分类和肿瘤比例回归）均表现出色：分类模型的F1分数达到0.875，回归模型的MAE为0.028，验证了编码器学习判别性和局部化特征的能力。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种基于图的无解码器范式，通过内容感知分组构建语义图，并联合使用Transformer和GAT进行分层特征学习，实现了参数高效、特征专注且具有双尺度可解释性的3D医学图像表示方法。</p>
<p><strong>Abstract:</strong> Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework’s flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder’s ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.</p>
  </div>
</details>

<hr>
<h3 id="193-Fine-Grained-Zero-Shot-Composed-Image-Retrieval-with-Complementary-Visual-Semantic-Integration-cs-CVPDF"><a href="#193-Fine-Grained-Zero-Shot-Composed-Image-Retrieval-with-Complementary-Visual-Semantic-Integration-cs-CVPDF" class="headerlink" title="[193] Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration cs.CVPDF"></a>[193] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14060">Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14060" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongcong Ye, Kai Zhang, Yanghai Zhang, Enhong Chen, Longfei Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的细粒度零样本组合图像检索方法CVSI，通过互补的视觉-语义集成来解决现有方法在捕捉细粒度变化和有效整合多模态信息方面的不足。该方法包含视觉信息提取、语义信息提取和互补信息检索三个核心组件，在多个公开数据集上实现了显著的性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有零样本组合图像检索方法难以有效捕捉细粒度修改以及未能充分整合互补的视觉和语义信息的问题，这些方法通常过度依赖图像到文本转换或大语言模型生成描述，导致信息丢失或不完整。</p>
<p><strong>Result:</strong> 在CIRR、CIRCO和FashionIQ三个公开数据集上的大量实验表明，CVSI方法显著优于现有的最先进方法。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个互补的视觉-语义集成框架，不仅提取全局图像特征，还通过伪令牌和对象预测来增强视觉表示，同时利用预训练描述模型和LLM生成多描述和修改对象，最后通过集成查询和数据库图像信息进行检索，实现了更鲁棒和精细的检索能力。</p>
<p><strong>Abstract:</strong> Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yyc6631/CVSI">https://github.com/yyc6631/CVSI</a>.</p>
  </div>
</details>

<hr>
<h3 id="194-Unsupervised-Video-Class-Incremental-Learning-via-Deep-Embedded-Clustering-Management-cs-CV-cs-AI-cs-LGPDF"><a href="#194-Unsupervised-Video-Class-Incremental-Learning-via-Deep-Embedded-Clustering-Management-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[194] Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management cs.CV | cs.AI | cs.LGPDF"></a>[194] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14069">Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14069" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nattapong Kurpukdee, Adrian G. Bors</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种无监督视频类增量学习（uVCIL）方法，通过深度嵌入聚类管理来学习视频信息而不遗忘，且无需任何数据标签。该方法首先使用深度特征提取网络获取每个任务的代表性视频特征，然后逐步构建一系列深度聚类，并在连续任务学习中通过模型初始化实现知识迁移。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决无监督视频类增量学习问题，避免依赖标签和任务边界，减少人工标注成本，实现更现实的学习场景。</p>
<p><strong>Result:</strong> 在UCF101、HMDB51和Something-to-Something V2三个标准视频动作识别数据集上进行了评估，忽略监督设置中的标签，该方法在所有数据集上显著优于其他基线方法。</p>
<p><strong>Insight:</strong> 创新点在于结合深度特征提取和渐进式聚类管理，实现无监督下的知识迁移和类增量学习，为视频分析提供了标签无关的持续学习框架。</p>
<p><strong>Abstract:</strong> Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.</p>
  </div>
</details>

<hr>
<h3 id="195-VENI-Variational-Encoder-for-Natural-Illumination-cs-CVPDF"><a href="#195-VENI-Variational-Encoder-for-Natural-Illumination-cs-CVPDF" class="headerlink" title="[195] VENI: Variational Encoder for Natural Illumination cs.CVPDF"></a>[195] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14079">VENI: Variational Encoder for Natural Illumination</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14079" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Paul Walker, James A. D. Gardner, Andreea Ardelean, William A. P. Smith, Bernhard Egger</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为VENI的旋转等变变分自编码器，用于在球面上建模自然光照，避免了依赖2D投影，并利用新颖的向量神经元视觉变换器（VN-ViT）作为编码器和旋转等变条件神经场作为解码器，以保持环境贴图的SO(2)等变性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决逆渲染这一不适定问题，现有方法要么忽略了光照环境的球面和旋转等变特性，要么未能提供良好行为的潜在空间，因此需要一种能直接建模球面自然光照并保持等变性的方法。</p>
<p><strong>Result:</strong> 论文提出的SO(2)-等变全连接层在SO(2)-等变模型中优于标准向量神经元，且变分自编码器在潜在空间中实现了更平滑的插值，提供了更良好行为的潜在空间。</p>
<p><strong>Insight:</strong> 创新点包括：使用VN-ViT作为编码器和旋转等变条件神经场作为解码器，以保持SO(2)-等变性；提出了一种新颖的SO(2)-等变全连接层作为向量神经元的扩展，提升了模型性能；直接建模球面光照，避免了2D投影的局限性。</p>
<p><strong>Abstract:</strong> Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.</p>
  </div>
</details>

<hr>
<h3 id="196-Two-Stream-temporal-transformer-for-video-action-classification-cs-CV-cs-AI-cs-LGPDF"><a href="#196-Two-Stream-temporal-transformer-for-video-action-classification-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[196] Two-Stream temporal transformer for video action classification cs.CV | cs.AI | cs.LGPDF"></a>[196] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14086">Two-Stream temporal transformer for video action classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14086" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nattapong Kurpukdee, Adrian G. Bors</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新的双流Transformer视频分类器，通过结合内容流和光流来提取时空信息，利用自注意力机制在联合光流和时间帧域中识别特征，并在Transformer编码器中表示它们的关系。该方法在三个人类活动视频数据集上取得了优异的分类结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 运动表示在视频理解中至关重要，但现有方法可能未充分利用时空信息。本文旨在通过结合内容流和光流，利用Transformer的自注意力机制来改进视频动作分类，以更有效地捕捉运动特征。</p>
<p><strong>Result:</strong> 实验结果显示，该方法在三个人类活动视频数据集上提供了优秀的分类结果，具体数据集未在摘要中明确提及，但暗示达到了较高水平，可能接近或达到SOTA。</p>
<p><strong>Insight:</strong> 创新点在于将双流架构与Transformer结合，通过自注意力机制在联合光流和时间帧域中提取特征，这增强了模型对时空信息的捕捉能力，为视频动作分类提供了新的思路。</p>
<p><strong>Abstract:</strong> Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.</p>
  </div>
</details>

<hr>
<h3 id="197-LLM-Augmented-Intervenable-Multimodal-Adaptor-for-Post-operative-Complication-Prediction-in-Lung-Cancer-Surgery-cs-CV-cs-AIPDF"><a href="#197-LLM-Augmented-Intervenable-Multimodal-Adaptor-for-Post-operative-Complication-Prediction-in-Lung-Cancer-Surgery-cs-CV-cs-AIPDF" class="headerlink" title="[197] LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery cs.CV | cs.AIPDF"></a>[197] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14154">LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14154" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju, Kenneth Seastedt</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MIRACLE的深度学习架构，用于预测肺癌手术后的并发症风险。该模型通过整合术前临床数据和放射学数据，在超球面嵌入空间中融合异构输入，并引入可干预模块以提高预测的透明度和临床实用性。在包含3094名患者的真实世界数据集POC-L上验证，MIRACLE在个性化和可解释的术后风险管理方面优于传统机器学习模型和现有大型语言模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 术后并发症严重影响患者预后并增加医疗成本，现有方法在整合多模态数据、提供可解释且可操作的预测方面存在不足，需要一种更有效且透明的预测工具。</p>
<p><strong>Result:</strong> 在真实世界数据集POC-L（来自Roswell Park综合癌症中心的3094名患者）上，MIRACLE在预测术后并发症风险方面，性能优于各种传统机器学习模型和当代大型语言模型变体。</p>
<p><strong>Insight:</strong> 主要创新点包括：1）采用超球面嵌入空间进行异构多模态数据（临床记录与放射图像）的融合；2）引入了可干预的深度学习模块，使模型不仅能优化预测，还能提供可解释的、可操作的见解，允许领域专家基于临床知识交互式调整建议，增强了模型的透明度和临床实用性。</p>
<p><strong>Abstract:</strong> Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.</p>
  </div>
</details>

<hr>
<h3 id="198-One-Shot-Refiner-Boosting-Feed-forward-Novel-View-Synthesis-via-One-Step-Diffusion-cs-CVPDF"><a href="#198-One-Shot-Refiner-Boosting-Feed-forward-Novel-View-Synthesis-via-One-Step-Diffusion-cs-CVPDF" class="headerlink" title="[198] One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion cs.CVPDF"></a>[198] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14161">One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14161" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yitong Dong, Qi Zhang, Minchao Jiang, Zhiqiang Wu, Qingnan Fan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为One-Shot Refiner的新框架，用于从稀疏图像进行高保真新视角合成。该框架通过一个双域细节感知模块处理高分辨率输入，并利用特征引导的扩散网络进行细节恢复，同时采用统一的训练策略联合优化基于ViT的几何主干和基于扩散的细化模块，以解决现有前馈式3D高斯溅射方法在处理高分辨率输入和跨视角结构一致性方面的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有基于ViT主干的前馈式3D高斯溅射方法因计算成本而受限于低分辨率输入，以及现有生成式增强方法缺乏3D感知导致跨视角（尤其是未见区域）结构不一致的问题。</p>
<p><strong>Result:</strong> 实验表明，该方法在多个数据集上均能保持卓越的生成质量。</p>
<p><strong>Insight:</strong> 主要创新点包括：1. 双域细节感知模块，使模型能处理高分辨率图像并赋予高斯表示存储高频细节的能力；2. 特征引导的扩散网络，能在恢复过程中保留高频细节；3. 统一的训练策略，实现了基于ViT的几何主干与基于扩散的细化模块的联合优化。从客观角度看，该工作将扩散模型的生成能力与3DGS的显式表示相结合，为提升稀疏视图新视角合成的保真度和一致性提供了一种有效途径。</p>
<p><strong>Abstract:</strong> We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.</p>
  </div>
</details>

<hr>
<h3 id="199-IIR-VLM-In-Context-Instance-level-Recognition-for-Large-Vision-Language-Models-cs-CVPDF"><a href="#199-IIR-VLM-In-Context-Instance-level-Recognition-for-Large-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[199] IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models cs.CVPDF"></a>[199] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14188">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14188" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Liang Shi, Wei Li, Kevin M Beussman, Lin Chen, Yun Fu</span></p>
<p><strong>TL;DR:</strong> 本文提出IIR-VLM，一种增强型视觉语言模型，用于上下文实例级识别。该方法通过集成预训练的ILR专家模型作为辅助视觉编码器，提供专门特征，使VLM能够以单样本方式在上下文中学习新实例，并利用该知识进行实例感知的视觉理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代VLM在实例级识别任务上表现不佳，远逊于领域专用模型，限制了其在需要识别熟悉人物或物体等实际应用中的有效性。现有方法通常需要针对每个实例收集数据并单独训练，成本高且难以进行细粒度区分。</p>
<p><strong>Result:</strong> 在现有的实例个性化基准上验证了IIR-VLM的有效性，并在一个包含人物、人脸、宠物和通用物体等不同难度和类别的新基准上展示了其优越的ILR性能。</p>
<p><strong>Insight:</strong> 创新点在于将预训练的ILR专家模型作为辅助编码器集成到VLM中，实现了在上下文中以单样本方式学习新实例的能力，从而提升了VLM的实例级识别和细粒度区分性能。</p>
<p><strong>Abstract:</strong> Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM’s efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.</p>
  </div>
</details>

<hr>
<h3 id="200-Rig-Aware-3D-Reconstruction-of-Vehicle-Undercarriages-using-Gaussian-Splatting-cs-CV-cs-GR-cs-LGPDF"><a href="#200-Rig-Aware-3D-Reconstruction-of-Vehicle-Undercarriages-using-Gaussian-Splatting-cs-CV-cs-GR-cs-LGPDF" class="headerlink" title="[200] Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting cs.CV | cs.GR | cs.LGPDF"></a>[200] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14208">Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.GR | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14208" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nitin Kulkarni, Akhil Devarashetti, Charlie Cluss, Livio Forte, Dan Buckmaster</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于车辆底盘三维重建的端到端流水线，利用三摄像头装置采集车辆驶过时的视频，并生成可交互的3D模型。该模型允许检查员和客户旋转、缩放和切片查看底盘，从而快速检测锈蚀、泄漏或撞击损伤。核心贡献是一个专门设计的、考虑装置几何约束的运动恢复结构（SfM）流水线，以克服广角镜头畸变和低视差场景的挑战，并最终通过高斯溅射生成逼真的实时渲染模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决二手车底盘检查劳动强度大、在线买家难以查看底盘照片的问题，旨在通过自动化3D重建提升检查效率和买家信心。</p>
<p><strong>Result:</strong> 实验和消融研究表明，该方法的设计选择对于实现最先进的质量至关重要，其生成的稀疏点云质量通常超过标准SfM流水线，并最终能生成逼真的实时渲染模型。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个“装置感知”的SfM流水线，通过整合精确标定、同步视频流和装置提供的强几何先验，结合使用DISK特征提取器和基于注意力的LightGlue匹配器的约束匹配策略，有效解决了广角畸变和低视差难题，为类似受限或结构化场景的3D重建提供了新思路。</p>
<p><strong>Abstract:</strong> Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.</p>
  </div>
</details>

<hr>
<h3 id="201-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer-cs-CVPDF"><a href="#201-OmniTransfer-All-in-one-Framework-for-Spatio-temporal-Video-Transfer-cs-CVPDF" class="headerlink" title="[201] OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer cs.CVPDF"></a>[201] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14250">OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14250" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pengze Zhang, Yanze Wu, Mengtian Li, Xu Bai, Songtao Zhao</span></p>
<p><strong>TL;DR:</strong> OmniTransfer是一个统一的时空视频迁移框架，通过多帧多视图信息增强外观一致性，并利用时序线索实现细粒度时序控制，以解决现有视频定制方法依赖参考图像或特定任务时序先验、未能充分利用视频时空信息的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频定制方法通常依赖参考图像或特定任务的时序先验，无法充分利用视频固有的丰富时空信息，导致视频生成的灵活性和泛化性受限。</p>
<p><strong>Result:</strong> 在ID、风格等外观迁移以及摄像机运动、视频特效等时序迁移任务上，OmniTransfer优于现有方法；在运动迁移任务上，无需使用姿态信息即可与姿态引导方法性能相当，为灵活、高保真视频生成建立了新范式。</p>
<p><strong>Insight:</strong> 创新点包括：任务感知的位置偏置自适应利用参考视频信息以改善时序对齐或外观一致性；参考解耦的因果学习分离参考和目标分支以实现精确参考迁移并提升效率；任务自适应的多模态对齐使用多模态语义引导动态区分和处理不同任务。</p>
<p><strong>Abstract:</strong> Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.</p>
  </div>
</details>

<hr>
<h3 id="202-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR-cs-CVPDF"><a href="#202-LightOnOCR-A-1B-End-to-End-Multilingual-Vision-Language-Model-for-State-of-the-Art-OCR-cs-CVPDF" class="headerlink" title="[202] LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR cs.CVPDF"></a>[202] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14251">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14251" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin</span></p>
<p><strong>TL;DR:</strong> 本文提出了LightOnOCR-2-1B，一个拥有10亿参数、端到端的多语言视觉-语言模型，可直接将文档图像（如PDF）转换为干净、自然顺序的文本，无需传统的脆弱OCR流程。该模型在包含大量扫描件、法语文档和科学PDF的高质量蒸馏数据集上训练，在OlmOCR-Bench上取得了最先进的结果，同时比之前性能最佳的模型小9倍且速度显著更快。此外，模型扩展了输出格式以预测嵌入图像的归一化边界框，通过一种恢复策略在预训练中引入定位能力，并使用基于IoU奖励的RLVR进行微调。最后，通过检查点平均和任务算术合并提高了模型的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决传统OCR流程脆弱、复杂且难以处理多语言、多格式文档（尤其是扫描件和科学PDF）的问题，旨在构建一个更高效、更强大的端到端文档理解模型。</p>
<p><strong>Result:</strong> 在OlmOCR-Bench基准测试上达到了最先进（SOTA）水平，模型规模比之前最佳模型小9倍，且推理速度显著更快。</p>
<p><strong>Insight:</strong> 创新点包括：1) 采用大规模、高质量的多语言蒸馏数据进行端到端训练，避免了传统OCR流水线；2) 扩展模型输出以支持图像边界框预测，通过恢复策略和基于IoU的强化学习（RLVR）进行定位微调；3) 使用检查点平均和任务算术合并提升模型鲁棒性。从客观角度看，其将视觉文档理解、文本识别和图像定位统一到一个轻量级模型中，并在效率和性能上取得平衡，是值得借鉴的方向。</p>
<p><strong>Abstract:</strong> We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision–language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.</p>
  </div>
</details>

<hr>
<h3 id="203-Motion-3-to-4-3D-Motion-Reconstruction-for-4D-Synthesis-cs-CVPDF"><a href="#203-Motion-3-to-4-3D-Motion-Reconstruction-for-4D-Synthesis-cs-CVPDF" class="headerlink" title="[203] Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis cs.CVPDF"></a>[203] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14253">Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14253" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongyuan Chen, Xingyu Chen, Youjia Zhang, Zexiang Xu, Anpei Chen</span></p>
<p><strong>TL;DR:</strong> Motion 3-to-4是一个前馈框架，用于从单目视频和可选的3D参考网格合成高质量的4D动态物体。它将4D合成分解为静态3D形状生成和运动重建两个步骤，通过一个规范参考网格学习紧凑的运动潜在表示，并预测逐帧顶点轨迹来恢复完整、时间一致的几何形状。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决从单目视频进行4D合成（动态3D物体生成）的困难，这些困难源于训练数据有限以及从单视角恢复几何和运动固有的模糊性。</p>
<p><strong>Result:</strong> 在标准基准测试和一个具有精确真实几何的新数据集上的评估表明，Motion 3-to-4在保真度和空间一致性方面优于先前的工作，达到了SOTA水平。</p>
<p><strong>Insight:</strong> 主要创新点在于将4D合成任务解耦为静态形状和动态运动，并引入基于规范网格的紧凑运动潜在表示和可扩展的逐帧Transformer来处理不同长度的序列，这提高了对单目视频输入的鲁棒性和输出质量。</p>
<p><strong>Abstract:</strong> We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at <a target="_blank" rel="noopener" href="https://motion3-to-4.github.io/">https://motion3-to-4.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="204-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior-cs-CV-cs-AIPDF"><a href="#204-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior-cs-CV-cs-AIPDF" class="headerlink" title="[204] VideoMaMa: Mask-Guided Video Matting via Generative Prior cs.CV | cs.AIPDF"></a>[204] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14255">VideoMaMa: Mask-Guided Video Matting via Generative Prior</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14255" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim</span></p>
<p><strong>TL;DR:</strong> 本文提出VideoMaMa模型，利用预训练视频扩散模型将粗糙分割掩码转换为精确的alpha遮罩，实现了对真实世界视频的零样本泛化。基于此，构建了大规模伪标注视频抠图数据集MA-V，并微调SAM2得到SAM2-Matte模型，在真实视频上展现出更强的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决视频抠图模型因标注数据稀缺而难以泛化到真实世界视频的问题。</p>
<p><strong>Result:</strong> VideoMaMa在合成数据训练下对真实视频表现出零样本泛化能力；SAM2-Matte在MA-V数据集上微调后，在真实世界视频上的鲁棒性优于基于现有抠图数据集训练的同类模型。</p>
<p><strong>Insight:</strong> 利用生成先验（预训练视频扩散模型）和易得的分割线索（粗糙掩码）实现可扩展的视频抠图；通过大规模伪标注构建高质量数据集推动领域进展。</p>
<p><strong>Abstract:</strong> Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.</p>
  </div>
</details>

<hr>
<h3 id="205-Implicit-Neural-Representation-Facilitates-Unified-Universal-Vision-Encoding-cs-CVPDF"><a href="#205-Implicit-Neural-Representation-Facilitates-Unified-Universal-Vision-Encoding-cs-CVPDF" class="headerlink" title="[205] Implicit Neural Representation Facilitates Unified Universal Vision Encoding cs.CVPDF"></a>[205] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14256">Implicit Neural Representation Facilitates Unified Universal Vision Encoding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14256" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matthew Gwilliam, Xiao Wang, Xuefeng Hu, Zhenheng Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种统一视觉编码模型，通过隐式神经表示（INR）超网络架构，同时实现图像识别与生成能力，并利用知识蒸馏提升泛化性能，在多种视觉任务上达到SOTA水平。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有图像表示学习模型通常分别针对识别或生成任务设计，本文旨在统一这两个方向，学习同时适用于识别和生成的表示。</p>
<p><strong>Result:</strong> 模型在图像表示学习任务中与SOTA结果竞争，其压缩嵌入空间在多种视觉任务上表现优异，并支持高质量生成。</p>
<p><strong>Insight:</strong> 创新点包括首次将INR超网络用于统一视觉编码，结合知识蒸馏提升性能，并学习到前所未有的压缩嵌入空间，为多任务视觉模型设计提供了新思路。</p>
<p><strong>Abstract:</strong> Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at <a target="_blank" rel="noopener" href="https://github.com/tiktok/huvr">https://github.com/tiktok/huvr</a>.</p>
  </div>
</details>

<hr>
<div id='q-bio.GN'></div>

<h1 id="q-bio-GN-Back"><a href="#q-bio-GN-Back" class="headerlink" title="q-bio.GN [Back]"></a>q-bio.GN <a href="#toc">[Back]</a></h1><h3 id="206-SciHorizon-GENE-Benchmarking-LLM-for-Life-Sciences-Inference-from-Gene-Knowledge-to-Functional-Understanding-q-bio-GN-cs-AI-cs-CLPDF"><a href="#206-SciHorizon-GENE-Benchmarking-LLM-for-Life-Sciences-Inference-from-Gene-Knowledge-to-Functional-Understanding-q-bio-GN-cs-AI-cs-CLPDF" class="headerlink" title="[206] SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding q-bio.GN | cs.AI | cs.CLPDF"></a>[206] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12805">SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">q-bio.GN | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12805" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen</span></p>
<p><strong>TL;DR:</strong> 该论文介绍了SciHorizon-GENE，一个针对生命科学领域构建的大规模、以基因为中心的基准测试，旨在评估大型语言模型从基因知识到功能理解的推理能力。基准包含超过190K个人类基因的精选知识和540K多个问题，覆盖细胞类型注释、功能解释和机制导向分析等推理场景。论文系统评估了多种通用和生物医学LLM，揭示了它们在基因层面推理能力的显著异质性以及在生成忠实、完整、基于文献的功能解释方面存在的持续挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决LLM在生物医学研究中从基因级知识到功能理解的可信推理能力尚未被充分探索的问题，特别是针对知识增强的细胞图谱解释这一核心需求，构建一个系统性的评估基准。</p>
<p><strong>Result:</strong> 在SciHorizon-GENE基准上系统评估了多种最先进的通用和生物医学LLM，结果显示模型在基因级推理能力上存在显著异质性，且在生成忠实、完整、基于文献的功能解释方面面临持续挑战。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个大规模、以基因为中心、整合权威生物数据库知识的基准，并从研究关注敏感性、幻觉倾向、答案完整性和文献影响四个关键生物学视角评估LLM，明确针对限制LLM在生物解释流程中安全应用的失败模式，为基因尺度的LLM行为分析和模型选择与开发提供了系统性基础。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="207-CSyMR-Benchmarking-Compositional-Symbolic-Muisc-Reasoning-With-MIR-Tool-Integration-cs-LG-cs-AI-cs-CL-cs-SD-eess-ASPDF"><a href="#207-CSyMR-Benchmarking-Compositional-Symbolic-Muisc-Reasoning-With-MIR-Tool-Integration-cs-LG-cs-AI-cs-CL-cs-SD-eess-ASPDF" class="headerlink" title="[207] CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration cs.LG | cs.AI | cs.CL | cs.SD | eess.ASPDF"></a>[207] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11556">CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11556" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Boyang Wang, Yash Vishe, Xin Xu, Zachary Novack, Julian McAuley</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为CSyMR-Bench的基准测试，用于评估大语言模型在组合式符号音乐推理方面的能力。该基准包含126个来自专家论坛和专业考试的多选题，要求结合多个原子分析得出最终答案。同时，论文还引入了一个工具增强的智能体框架，利用music21库中的符号音乐分析工具来解决CSyMR-Bench提出的挑战。实验表明，该基准对现有模型构成了显著挑战，而所提出的工具增强智能体在所有基线模型上均表现更优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大语言模型在符号音乐推理方面的基准测试主要关注孤立的知识或原子分析，缺乏对连接音乐结构所需的综合性组合推理能力的评估。</p>
<p><strong>Result:</strong> 实验验证了CSyMR-Bench在社区来源和考试风格问题上都构成了非平凡的挑战。所提出的工具增强智能体在所有基线模型上表现一致更优，实现了5-7%的绝对准确率提升。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了一个专注于组合式推理的音乐基准测试，并提出了一个集成领域专用工具（music21）的智能体框架来应对这一复杂任务，这为评估和增强AI在结构化、多步骤推理领域的性能提供了新思路。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.</p>
  </div>
</details>

<hr>
<h3 id="208-AGGC-Adaptive-Group-Gradient-Clipping-for-Stabilizing-Large-Language-Model-Training-cs-LG-cs-CLPDF"><a href="#208-AGGC-Adaptive-Group-Gradient-Clipping-for-Stabilizing-Large-Language-Model-Training-cs-LG-cs-CLPDF" class="headerlink" title="[208] AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training cs.LG | cs.CLPDF"></a>[208] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11864">AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11864" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiyuan Li, Yuan Wu, Yi Chang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为自适应分组梯度裁剪（AGGC）的新方法，旨在解决大语言模型训练中的梯度不稳定问题。该方法根据参数的功能类型进行分组，并利用指数移动平均为每组参数自适应地调整裁剪阈值，以缓解梯度爆炸和消失，同时平衡探索与收敛。实验表明，AGGC在多个7B模型上优于LoRA和全参数微调，并在GSM8K基准测试和RLVR任务中提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统全局范数梯度裁剪假设所有参数梯度同质，但实际上不同功能模块的梯度行为差异很大，导致对稳定参数的不必要缩放（“溢出”效应）。AGGC旨在克服这种梯度异质性，实现更稳定的训练。</p>
<p><strong>Result:</strong> 在LLaMA 2-7B、Mistral-7B和Gemma-7B上的实验表明，AGGC性能持续优于LoRA，并经常超越全参数微调。具体在GSM8K基准上，使用AGGC微调的Mistral-7B达到72.93%的准确率，超过LoRA的69.5%。AGGC还能有效稳定RLVR训练，提升了Qwen 2.5和Llama 3.2模型的逻辑推理能力。</p>
<p><strong>Insight:</strong> 核心创新点在于模块化的自适应分组裁剪策略：1) 按功能类型对参数分组，承认并处理梯度异质性；2) 利用EMA历史行为为每组构建自适应裁剪区间，同时应对梯度爆炸和消失；3) 引入时间相关的调度机制平衡探索与收敛。其轻量级设计便于集成到现有训练流程中。</p>
<p><strong>Abstract:</strong> To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse “spill-over” effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA’s 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.</p>
  </div>
</details>

<hr>
<h3 id="209-R-2-PO-Decoupling-Training-Trajectories-from-Inference-Responses-for-LLM-Reasoning-cs-LG-cs-AI-cs-CLPDF"><a href="#209-R-2-PO-Decoupling-Training-Trajectories-from-Inference-Responses-for-LLM-Reasoning-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[209] R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning cs.LG | cs.AI | cs.CLPDF"></a>[209] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11960">R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11960" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun</span></p>
<p><strong>TL;DR:</strong> 本文提出了R²PO（Residual Rollout Policy Optimization）方法，旨在解决大型语言模型推理任务中强化学习训练的一个核心矛盾：即使用单一策略同时生成稳定的推理响应和多样化的训练轨迹会导致探索不足。该方法通过在策略之上引入一个轻量级的“残差展开头”，将训练轨迹的生成与推理响应的生成解耦，从而在训练时实现可控的轨迹多样化，同时保持推理生成的稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于强化学习改进LLM推理的方法使用单一策略来同时产生推理响应和训练优化轨迹，这导致了生成稳定推理响应与探索多样化训练轨迹之间的目标冲突，从而损害了模型的推理能力。</p>
<p><strong>Result:</strong> 在多个基准测试上的实验表明，该方法一致优于基线模型，在MATH-500和APPS数据集上分别实现了平均3.1%和2.4%的准确率提升，同时减少了格式错误并缓解了长度偏差，实现了稳定的优化。</p>
<p><strong>Insight:</strong> 核心创新点在于通过引入一个额外的、轻量级的“残差展开头”来解耦训练和推理阶段的目标，这是一种新颖的架构设计。从客观角度看，这种解耦思想为解决强化学习中探索与利用的平衡问题提供了一个具体且有效的技术路径，其轻量级设计也保证了方法的实用性。</p>
<p><strong>Abstract:</strong> Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/RRPO-ARR/Code">https://github.com/RRPO-ARR/Code</a>.</p>
  </div>
</details>

<hr>
<h3 id="210-CooperBench-Why-Coding-Agents-Cannot-be-Your-Teammates-Yet-cs-LG-cs-AI-cs-CL-cs-MA-cs-SIPDF"><a href="#210-CooperBench-Why-Coding-Agents-Cannot-be-Your-Teammates-Yet-cs-LG-cs-AI-cs-CL-cs-MA-cs-SIPDF" class="headerlink" title="[210] CooperBench: Why Coding Agents Cannot be Your Teammates Yet cs.LG | cs.AI | cs.CL | cs.MA | cs.SIPDF"></a>[210] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13295">CooperBench: Why Coding Agents Cannot be Your Teammates Yet</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | cs.MA | cs.SI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13295" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic Sadrieh</span></p>
<p><strong>TL;DR:</strong> 本文介绍了CooperBench基准测试，包含超过600个跨4种编程语言和12个库的协作编码任务，用于评估AI代理在团队协作中的协调能力。研究发现当前最先进的编码代理在协作时成功率平均降低30%，揭示了协调诅咒现象，并识别出通信堵塞、承诺偏离和错误预期三个关键问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着AI代理在复杂工作中日益协作，需要发展协调能力以成为有效队友，但假设当前代理缺乏这些能力，因此创建CooperBench来测试其协作表现。</p>
<p><strong>Result:</strong> 在CooperBench基准上评估最先进的编码代理，发现协作时成功率比单独执行任务平均低30%，与人类团队通常提高生产力的趋势形成鲜明对比。</p>
<p><strong>Insight:</strong> 论文创新点在于提出了首个专注于协作编码的基准测试，揭示了AI代理在社交智能方面的不足，并呼吁从追求个体能力转向发展社交智能；客观分析认为其通过大规模模拟发现了罕见的涌现协调行为（如角色分工和谈判），为未来研究提供了新方向。</p>
<p><strong>Abstract:</strong> Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others’ plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.</p>
  </div>
</details>

<hr>
<h3 id="211-Self-Improvement-as-Coherence-Optimization-A-Theoretical-Account-cs-LG-cs-AI-cs-CLPDF"><a href="#211-Self-Improvement-as-Coherence-Optimization-A-Theoretical-Account-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[211] Self-Improvement as Coherence Optimization: A Theoretical Account cs.LG | cs.AI | cs.CLPDF"></a>[211] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13566">Self-Improvement as Coherence Optimization: A Theoretical Account</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13566" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianyi Qiu, Ahmed Hani Ismail, Zhonghao He, Shi Feng</span></p>
<p><strong>TL;DR:</strong> 这篇论文从理论角度解释了语言模型无需外部监督即可自我提升的现象，揭示了辩论、自举和内部一致性最大化等方法本质上是‘一致性优化’的特例，即寻找最可压缩和联合可预测的上下文到行为的映射。论文证明了该优化等价于描述长度正则化，并推导出其在半监督学习中的最优性条件。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在从理论上解释语言模型为何能通过无监督方法（如辩论、自举）实现自我提升，并匹配有监督微调的性能，其核心动机是阐明这些方法背后统一的工作原理和理论依据。</p>
<p><strong>Result:</strong> 初步实验支持了理论分析，表明一致性优化能有效解释无反馈自我提升的成功机制，并理论预测了其成功与失败的条件。</p>
<p><strong>Insight:</strong> 创新点在于提出了‘一致性优化’这一统一理论框架，将多种无监督自我提升方法归纳为寻求可压缩和可预测的映射，并建立了其与描述长度正则化的等价性及在半监督学习中的最优性理论，为理解模型自我改进提供了新的理论视角。</p>
<p><strong>Abstract:</strong> Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that’s most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.</p>
  </div>
</details>

<hr>
<h3 id="212-A-model-of-errors-in-transformers-cs-LG-cs-AI-cs-CL-hep-thPDF"><a href="#212-A-model-of-errors-in-transformers-cs-LG-cs-AI-cs-CL-hep-thPDF" class="headerlink" title="[212] A model of errors in transformers cs.LG | cs.AI | cs.CL | hep-thPDF"></a>[212] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14175">A model of errors in transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | hep-th</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14175" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Suvrat Raju, Praneeth Netrapalli</span></p>
<p><strong>TL;DR:</strong> 本文研究了大型语言模型（LLM）在需要确定性输出的任务（如算术）上的错误率。作者认为，错误源于注意力机制中的微小误差累积超过阈值，并据此推导出一个描述任务准确率与复杂性之间关系的双参数定量模型。该模型将LLM的众多参数简化为两个关键参数，并通过在Gemini和DeepSeek等模型上的广泛实验验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决LLM在需要确定性输出和重复处理小集合标记的任务（如算术）中产生错误的原因，并挑战关于这些错误源于“推理崩溃”或无法表达“组合”功能的现有观点。</p>
<p><strong>Result:</strong> 在Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1模型上进行了广泛的实证测试，发现预测准确率与观测准确率在多种任务上高度吻合，尽管在某些情况下也存在偏差。</p>
<p><strong>Insight:</strong> 创新点在于提出一个基于注意力机制误差累积的定量双参数模型来解释LLM错误率，该模型受“有效场论”启发，将复杂参数简化为两个可解释参数（基本噪声率和可能错误标记数），并展示了如何通过提示工程来降低错误率。</p>
<p><strong>Abstract:</strong> We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an <code>effective field theory&#39;&#39; perspective: the LLM&#39;s many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the </code>collapse of reasoning’’, or an inability to express &#96;&#96;compositional’’ functions. Finally, we show how to construct prompts to reduce the error rate.</p>
  </div>
</details>

<hr>
<h3 id="213-InT-Self-Proposed-Interventions-Enable-Credit-Assignment-in-LLM-Reasoning-cs-LG-cs-AI-cs-CLPDF"><a href="#213-InT-Self-Proposed-Interventions-Enable-Credit-Assignment-in-LLM-Reasoning-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[213] InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning cs.LG | cs.AI | cs.CLPDF"></a>[213] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14209">InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14209" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为干预训练（InT）的新训练范式，旨在解决大语言模型推理中的信用分配问题。该方法通过模型自主识别推理轨迹中的首个错误并生成单步干预，将轨迹导向正确解，随后结合监督微调进行训练，以提升模型作为强化学习初始化的质量。实验表明，该方法在IMO-AnswerBench基准上显著提升了4B参数基础模型的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准结果奖励强化学习在信用分配上存在局限，仅基于最终答案奖惩整个推理轨迹，导致正确中间步骤在失败轨迹中被惩罚，而错误步骤在成功轨迹中被强化。本文旨在通过模型自主干预，实现更细粒度的信用分配，以优化推理过程。</p>
<p><strong>Result:</strong> 在IMO-AnswerBench基准上，经过InT和后续强化学习微调后，模型准确率比4B参数基础模型提升了近14%，超越了如gpt-oss-20b等更大的开源模型。</p>
<p><strong>Insight:</strong> 创新点在于利用数学推理数据集中常见的参考解，结合“验证生成解比从头生成正确解更容易”的特性，让模型自主提出单步干预来纠正推理错误，从而实现局部化错误定位和更有效的信用分配，为强化学习训练提供了更好的初始化模型。</p>
<p><strong>Abstract:</strong> Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</p>
  </div>
</details>

<hr>
<h3 id="214-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow-cs-LG-cs-CLPDF"><a href="#214-Jet-RL-Enabling-On-Policy-FP8-Reinforcement-Learning-with-Unified-Training-and-Rollout-Precision-Flow-cs-LG-cs-CLPDF" class="headerlink" title="[214] Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow cs.LG | cs.CLPDF"></a>[214] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14243">Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai</span></p>
<p><strong>TL;DR:</strong> 本文提出了Jet-RL，一个用于强化学习（RL）训练的FP8精度框架，旨在解决现有RL训练流程中因训练与推理阶段精度不匹配（如BF16训练+FP8推理）导致的训练不稳定和精度崩溃问题。Jet-RL通过在训练和推理阶段统一使用FP8精度流，显著减少了数值差异，无需低效的步间校准，从而实现了稳定的优化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有RL训练流程计算效率低、资源密集，其中推理阶段占训练总时间的70%以上。量化RL训练（如使用FP8精度）是缓解此瓶颈的有前景的方法，但广泛采用的BF16训练+FP8推理策略在长序列推理和挑战性任务下存在严重的训练不稳定和灾难性精度崩溃问题，其根源在于该策略的离策略性质引入了训练与推理间的显著数值不匹配。</p>
<p><strong>Result:</strong> 大量实验验证了Jet-RL的有效性：与BF16训练相比，该方法在推理阶段实现了高达33%的加速，在训练阶段实现了高达41%的加速，端到端加速达16%，同时在所有设置下保持稳定收敛，且精度下降可忽略不计。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于提出了一个统一的FP8精度流，用于RL的训练和推理阶段，从而最小化数值差异并消除低效的校准需求。从客观角度看，这种统一的精度设计不仅解决了现有混合精度策略的稳定性问题，还通过减少计算开销实现了显著的端到端加速，为高效、稳定的量化RL训练提供了新思路。</p>
<p><strong>Abstract:</strong> Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.</p>
  </div>
</details>

<hr>
<h3 id="215-Constraint-Aware-Neurosymbolic-Uncertainty-Quantification-with-Bayesian-Deep-Learning-for-Scientific-Discovery-cs-LG-cs-AI-cs-CVPDF"><a href="#215-Constraint-Aware-Neurosymbolic-Uncertainty-Quantification-with-Bayesian-Deep-Learning-for-Scientific-Discovery-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[215] Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery cs.LG | cs.AI | cs.CVPDF"></a>[215] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12442">Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12442" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha</span></p>
<p><strong>TL;DR:</strong> 本文提出了约束感知神经符号不确定性框架（CANUF），将贝叶斯深度学习与可微符号推理相结合，为科学AI应用提供可信的不确定性估计，同时确保满足领域约束。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有不确定性量化方法缺乏融入符号科学知识的机制，而神经符号方法又缺乏原则性的不确定性建模，因此需要一种能同时处理不确定性估计和领域约束的框架。</p>
<p><strong>Result:</strong> 在Materials Project、QM9分子性质和气候基准测试上的实验表明，CANUF相较于贝叶斯神经网络将预期校准误差降低了34.7%，同时保持了99.2%的约束满足率；消融实验显示约束引导的重新校准贡献了18.3%的性能增益。</p>
<p><strong>Insight:</strong> 主要创新点在于构建了首个端到端可微管道，统一了贝叶斯不确定性量化与可微符号约束满足，并通过从科学文献中自动提取约束，实现了物理一致性保障与可解释性。</p>
<p><strong>Abstract:</strong> Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.</p>
  </div>
</details>

<hr>
<h3 id="216-LLMOrbit-A-Circular-Taxonomy-of-Large-Language-Models-From-Scaling-Walls-to-Agentic-AI-Systems-cs-LG-cs-AI-cs-CV-cs-MA-eess-IVPDF"><a href="#216-LLMOrbit-A-Circular-Taxonomy-of-Large-Language-Models-From-Scaling-Walls-to-Agentic-AI-Systems-cs-LG-cs-AI-cs-CV-cs-MA-eess-IVPDF" class="headerlink" title="[216] LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems cs.LG | cs.AI | cs.CV | cs.MA | eess.IVPDF"></a>[216] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14053">LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | cs.MA | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14053" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Badri N. Patro, Vijay S. Agneeswaran</span></p>
<p><strong>TL;DR:</strong> 本文提出了LLMOrbit，一个涵盖2019至2025年大语言模型发展的综合性环形分类法。该综述通过八个相互关联的维度，分析了超过50个模型，记录了定义现代LLM、生成式AI和智能体系统的架构创新、训练方法和效率模式。论文识别了数据稀缺、成本指数增长和能耗不可持续三大危机构成的‘扩展墙’，并揭示了突破此墙的六大范式（如测试时计算、量化、分布式边缘计算等）和三大范式转变（如训练后增益、效率革命、民主化）。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是系统地梳理和分类大语言模型（LLM）的快速发展历程，从基础Transformer架构到接近人类水平的推理系统，并识别当前领域面临的重大挑战（如数据、成本、能耗危机构成的‘扩展墙’），从而为未来研究提供导航和见解。</p>
<p><strong>Result:</strong> 论文是综述性质，未提出具体新模型，因此没有在特定基准上的定量结果。但其分析指出，通过所总结的范式（如测试时计算、效率技术），模型能在保持性能（如达到GPT-4水平）的同时大幅降低成本（如&lt;0.30美元&#x2F;百万token）或提升效率（如10倍推理计算达到同等性能）。同时，开源模型（如Llama 3）在MMLU基准上超越了GPT-4。</p>
<p><strong>Insight:</strong> 宣称的创新点在于提出了一个新颖的‘环形分类法’（LLMOrbit）来结构化地分析LLM生态。客观的创新之处在于系统性地识别了限制领域发展的‘扩展墙’及其三大具体危机，并归纳了突破此墙的六大技术范式和三大宏观范式转变，为理解LLM从被动生成到智能体系统的演进、以及未来的效率与民主化趋势提供了清晰的框架和分析视角。</p>
<p><strong>Abstract:</strong> The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &lt;$0.30&#x2F;M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.</p>
  </div>
</details>

<hr>
<h3 id="217-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning-cs-LG-cs-AI-cs-CVPDF"><a href="#217-KAGE-Bench-Fast-Known-Axis-Visual-Generalization-Evaluation-for-Reinforcement-Learning-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[217] KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning cs.LG | cs.AI | cs.CVPDF"></a>[217] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14232">KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14232" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</span></p>
<p><strong>TL;DR:</strong> 本文提出了KAGE-Env环境与KAGE-Bench基准，用于系统性评估强化学习智能体在纯视觉分布偏移下的泛化能力。该工作将观测过程分解为独立可控的视觉轴，并构建了包含34个配置对的基准，以隔离不同视觉偏移的影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准测试往往将多种偏移源混杂在一起，阻碍了对像素强化学习智能体在视觉分布偏移下失败原因的系统性分析。本文旨在提供一个干净、可控的抽象环境来专门研究视觉泛化问题。</p>
<p><strong>Result:</strong> 使用标准PPO-CNN基线在KAGE-Bench上测试，观察到强烈的轴依赖性失败：背景和光度偏移常导致任务完全失败，而智能体外观偏移影响相对较小。某些偏移能保持前进运动但破坏任务完成，表明仅看回报会掩盖泛化失败。</p>
<p><strong>Insight:</strong> 核心创新在于构建了可分解视觉轴的JAX原生2D平台环境，实现了对单一视觉偏移的隔离评估。其实验设计揭示了不同视觉因素对泛化性能的差异化影响，且其完全向量化实现（单GPU可达3300万步&#x2F;秒）为快速、可重复的视觉因素扫描提供了可能。</p>
<p><strong>Abstract:</strong> Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: <a target="_blank" rel="noopener" href="https://avanturist322.github.io/KAGEBench/">https://avanturist322.github.io/KAGEBench/</a>.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="218-Mobile-friendly-Image-de-noising-Hardware-Conscious-Optimization-for-Edge-Application-eess-IV-cs-AI-cs-CVPDF"><a href="#218-Mobile-friendly-Image-de-noising-Hardware-Conscious-Optimization-for-Edge-Application-eess-IV-cs-AI-cs-CVPDF" class="headerlink" title="[218] Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application eess.IV | cs.AI | cs.CVPDF"></a>[218] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11684">Mobile-friendly Image de-noising: Hardware Conscious Optimization for Edge Application</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11684" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Srinivas Miriyala, Sowmya Vajrala, Hitesh Kumar, Sravanth Kodavanti, Vikram Rajendiran</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种面向移动设备的图像去噪网络，通过熵正则化的可微分神经架构搜索（NAS）在硬件感知的搜索空间中优化U-Net架构，实现了参数减少12%、设备延迟提升约2倍、内存占用减少1.5倍，同时PSNR仅下降0.7%。该网络在多个基准测试中表现出色，并在三星Galaxy S24 Ultra上验证了其高效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统图像信号处理（ISP）在去噪任务上效果有限，而深度学习方法的成功依赖于其在智能手机等边缘设备上的易部署性。本文旨在设计一种硬件友好的轻量级去噪网络，以平衡性能与效率。</p>
<p><strong>Result:</strong> 在三星Galaxy S24 Ultra上部署测试，PSNR仅下降0.7%，但参数减少12%、延迟提升约2倍、内存占用减少1.5倍；与SOTA的Swin-Transformer相比，在竞争性精度下GMACs减少约18倍。在4个基准测试上验证了高斯去噪（3种强度）和1个真实世界去噪基准，展示了泛化能力。</p>
<p><strong>Insight:</strong> 创新点包括首次将熵正则化可微分NAS应用于硬件感知的U-Net搜索空间，实现了移动端去噪网络的轻量化设计；客观来看，该方法通过NAS自动优化架构，在保持精度的同时显著提升了边缘设备的计算效率，为移动视觉应用提供了可借鉴的硬件-算法协同优化思路。</p>
<p><strong>Abstract:</strong> Image enhancement is a critical task in computer vision and photography that is often entangled with noise. This renders the traditional Image Signal Processing (ISP) ineffective compared to the advances in deep learning. However, the success of such methods is increasingly associated with the ease of their deployment on edge devices, such as smartphones. This work presents a novel mobile-friendly network for image de-noising obtained with Entropy-Regularized differentiable Neural Architecture Search (NAS) on a hardware-aware search space for a U-Net architecture, which is first-of-its-kind. The designed model has 12% less parameters, with ~2-fold improvement in ondevice latency and 1.5-fold improvement in the memory footprint for a 0.7% drop in PSNR, when deployed and profiled on Samsung Galaxy S24 Ultra. Compared to the SOTA Swin-Transformer for Image Restoration, the proposed network had competitive accuracy with ~18-fold reduction in GMACs. Further, the network was tested successfully for Gaussian de-noising with 3 intensities on 4 benchmarks and real-world de-noising on 1 benchmark demonstrating its generalization ability.</p>
  </div>
</details>

<hr>
<h3 id="219-Bridging-Modalities-Joint-Synthesis-and-Registration-Framework-for-Aligning-Diffusion-MRI-with-T1-Weighted-Images-eess-IV-cs-CVPDF"><a href="#219-Bridging-Modalities-Joint-Synthesis-and-Registration-Framework-for-Aligning-Diffusion-MRI-with-T1-Weighted-Images-eess-IV-cs-CVPDF" class="headerlink" title="[219] Bridging Modalities: Joint Synthesis and Registration Framework for Aligning Diffusion MRI with T1-Weighted Images eess.IV | cs.CVPDF"></a>[219] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11689">Bridging Modalities: Joint Synthesis and Registration Framework for Aligning Diffusion MRI with T1-Weighted Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11689" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaofan Wang, Junyi Wang, Yuqian Chen, Lauren J. O’ Donnell, Fan Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于生成式配准网络的无监督多模态图像配准框架，用于对齐扩散MRI（dMRI）与T1加权（T1w）MRI图像。该方法通过图像合成模型生成具有T1w对比度的图像，将b0与T1w图像间的多模态配准问题转化为生成图像与真实T1w图像间的单模态配准任务，从而降低跨模态配准的复杂性。配准网络联合优化局部结构相似性和跨模态统计依赖性以提高变形场估计精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决扩散MRI与高分辨率T1w MRI图像之间由于强度差异大而导致传统配准方法精度不足的问题。</p>
<p><strong>Result:</strong> 在两个独立数据集上的实验表明，该方法在多模态配准任务中优于多种最先进（SOTA）方法。</p>
<p><strong>Insight:</strong> 创新点在于通过生成式方法将跨模态配准转化为单模态问题，并联合优化结构相似性与统计依赖性以提升配准精度；可借鉴的思路是利用图像合成来简化复杂的多模态对齐任务。</p>
<p><strong>Abstract:</strong> Multimodal image registration between diffusion MRI (dMRI) and T1-weighted (T1w) MRI images is a critical step for aligning diffusion-weighted imaging (DWI) data with structural anatomical space. Traditional registration methods often struggle to ensure accuracy due to the large intensity differences between diffusion data and high-resolution anatomical structures. This paper proposes an unsupervised registration framework based on a generative registration network, which transforms the original multimodal registration problem between b0 and T1w images into a unimodal registration task between a generated image and the real T1w image. This effectively reduces the complexity of cross-modal registration. The framework first employs an image synthesis model to generate images with T1w-like contrast, and then learns a deformation field from the generated image to the fixed T1w image. The registration network jointly optimizes local structural similarity and cross-modal statistical dependency to improve deformation estimation accuracy. Experiments conducted on two independent datasets demonstrate that the proposed method outperforms several state-of-the-art approaches in multimodal registration tasks.</p>
  </div>
</details>

<hr>
<h3 id="220-SHARE-A-Fully-Unsupervised-Framework-for-Single-Hyperspectral-Image-Restoration-eess-IV-cs-CVPDF"><a href="#220-SHARE-A-Fully-Unsupervised-Framework-for-Single-Hyperspectral-Image-Restoration-eess-IV-cs-CVPDF" class="headerlink" title="[220] SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration eess.IV | cs.CVPDF"></a>[220] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13987">SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13987" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiangwei Xie, Zhang Wen, Mike Davies, Dongdong Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了SHARE，一种完全无监督的高光谱图像（HSI）修复框架，通过结合几何等变性原理和低秩光谱建模，无需地面真值数据即可处理修复和超分辨率等病态逆问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有深度学习方法依赖精心策划的地面真值数据集，这在真实场景中往往不可得，限制了其适用性。本文旨在开发一种无需地面真值的完全无监督HSI修复方法。</p>
<p><strong>Result:</strong> 在HSI修复和超分辨率任务上的大量实验表明，SHARE优于许多最先进的无监督方法，并达到了与有监督方法相当的性能水平。</p>
<p><strong>Insight:</strong> 核心创新点在于利用高光谱结构在可微几何变换下的内在不变性，通过等变性一致性约束实现自监督，并引入了动态自适应光谱注意力（DASA）模块来显式编码HSI的全局低秩特性并自适应优化局部光谱-空间相关性。</p>
<p><strong>Abstract:</strong> Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE’s core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/xuwayyy/SHARE">https://github.com/xuwayyy/SHARE</a>.</p>
  </div>
</details>

<hr>
<div id='cs.MA'></div>

<h1 id="cs-MA-Back"><a href="#cs-MA-Back" class="headerlink" title="cs.MA [Back]"></a>cs.MA <a href="#toc">[Back]</a></h1><h3 id="221-Rethinking-the-Value-of-Multi-Agent-Workflow-A-Strong-Single-Agent-Baseline-cs-MA-cs-CL-cs-LGPDF"><a href="#221-Rethinking-the-Value-of-Multi-Agent-Workflow-A-Strong-Single-Agent-Baseline-cs-MA-cs-CL-cs-LGPDF" class="headerlink" title="[221] Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline cs.MA | cs.CL | cs.LGPDF"></a>[221] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12307">Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent Baseline</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MA | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12307" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiawei Xu, Arief Koesdwiady, Sisong Bei, Yan Han, Baixiang Huang</span></p>
<p><strong>TL;DR:</strong> 这篇论文重新评估了多智能体工作流（MAS）的价值，通过实验发现，在多个基准测试中，一个单一的LLM智能体通过多轮对话可以模拟同构多智能体工作流的性能，甚至匹配自动优化的异构工作流。基于此，作者提出了OneFlow算法，旨在为单智能体执行自动定制工作流，以降低推理成本而不牺牲准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于LLM的多智能体系统（MAS）通常采用同构设计（所有智能体共享相同的基础LLM），这引发了一个问题：这种工作流是否可以通过单智能体的多轮对话来模拟？论文旨在探究单智能体能否达到或超越多智能体工作流的性能，从而重新思考MAS的价值。</p>
<p><strong>Result:</strong> 在涵盖编码、数学、通用问答、领域特定推理以及现实世界规划和工具使用的七个基准测试中，单智能体能够达到同构工作流的性能，并受益于KV缓存重用的效率优势；它甚至能匹配自动优化的异构工作流的性能。OneFlow算法在保持准确性的同时，降低了推理成本。</p>
<p><strong>Insight:</strong> 论文的创新点在于挑战了多智能体工作流的必要性，提出单智能体可以作为MAS研究的强基线，并引入了OneFlow算法来自动优化单智能体工作流。从客观角度看，这揭示了当前同构MAS可能存在的冗余，并强调了未来开发真正异构多智能体系统的机会，因为单LLM方法无法跨不同LLM共享KV缓存。</p>
<p><strong>Abstract:</strong> Recent advances in LLM-based multi-agent systems (MAS) show that workflows composed of multiple LLM agents with distinct roles, tools, and communication patterns can outperform single-LLM baselines on complex tasks. However, most frameworks are homogeneous, where all agents share the same base LLM and differ only in prompts, tools, and positions in the workflow. This raises the question of whether such workflows can be simulated by a single agent through multi-turn conversations. We investigate this across seven benchmarks spanning coding, mathematics, general question answering, domain-specific reasoning, and real-world planning and tool use. Our results show that a single agent can reach the performance of homogeneous workflows with an efficiency advantage from KV cache reuse, and can even match the performance of an automatically optimized heterogeneous workflow. Building on this finding, we propose \textbf{OneFlow}, an algorithm that automatically tailors workflows for single-agent execution, reducing inference costs compared to existing automatic multi-agent design frameworks without trading off accuracy. These results position the single-LLM implementation of multi-agent workflows as a strong baseline for MAS research. We also note that single-LLM methods cannot capture heterogeneous workflows due to the lack of KV cache sharing across different LLMs, highlighting future opportunities in developing \textit{truly} heterogeneous multi-agent systems.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="222-PAIR-SAFE-A-Paired-Agent-Approach-for-Runtime-Auditing-and-Refining-AI-Mediated-Mental-Health-Support-cs-HC-cs-AI-cs-CL-cs-CYPDF"><a href="#222-PAIR-SAFE-A-Paired-Agent-Approach-for-Runtime-Auditing-and-Refining-AI-Mediated-Mental-Health-Support-cs-HC-cs-AI-cs-CL-cs-CYPDF" class="headerlink" title="[222] PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support cs.HC | cs.AI | cs.CL | cs.CYPDF"></a>[222] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12754">PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.AI | cs.CL | cs.CY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12754" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan, Koustuv Saha</span></p>
<p><strong>TL;DR:</strong> PAIR-SAFE是一个用于审计和优化AI心理健康支持响应的双智能体框架，它通过一个基于临床验证的MITI-4框架的监督者智能体（Judge）来实时审核和指导响应者智能体（Responder）的回复，从而提升对话的临床一致性和关系质量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前用于心理健康支持的LLM可能产生过于指令化、不一致或临床不匹配的回应，尤其在敏感或高风险情境下，而现有的缓解方法（如训练或提示）缺乏透明度和运行时问责。</p>
<p><strong>Result:</strong> 在使用基于人类标注数据构建的求助者模拟器进行的咨询交互模拟中，经Judge监督的交互在MITI-4关键维度（如伙伴关系、寻求协作和整体关系质量）上显示出显著改善，定量结果得到了定性专家评估的支持。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个运行时、基于临床框架（MITI-4）的配对智能体审计与精炼机制，将监督与生成解耦，增强了AI心理健康支持的透明度、问责制和临床对齐性。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly used for mental health support, yet they can produce responses that are overly directive, inconsistent, or clinically misaligned, particularly in sensitive or high-risk contexts. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judgeaudits each response and provides structuredALLOW or REVISE decisions that guide runtime response refinement. We simulate counseling interactions using a support-seeker simulator derived from human-annotated motivational interviewing data. We find that Judge-supervised interactions show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality. Our quantitative findings are supported by qualitative expert evaluation, which further highlights the nuances of runtime supervision. Together, our results reveal that such pairedagent approach can provide clinically grounded auditing and refinement for AI-assisted conversational mental health support.</p>
  </div>
</details>

<hr>
<h3 id="223-“Jutters”-cs-HC-cs-AI-cs-CVPDF"><a href="#223-“Jutters”-cs-HC-cs-AI-cs-CVPDF" class="headerlink" title="[223] “Jutters” cs.HC | cs.AI | cs.CVPDF"></a>[223] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11532">“Jutters”</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11532" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Meike Driessen, Selina Khan, Gonçalo Marcelino</span></p>
<p><strong>TL;DR:</strong> 该项目通过荷兰海岸拾荒者（jutter）的隐喻，探讨人类与AI生成内容的互动方式，构建了一个融合真实海岸废弃物与AI转换图像&#x2F;视频的沉浸式装置，邀请参观者以当代拾荒者身份筛选内容，将AI图像重构为反思媒介。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在回应AI生成媒体日益影响人类生活的现状，通过艺术装置形式激发人们对AI内容的批判性思考与选择性参与。</p>
<p><strong>Result:</strong> 项目未提及定量实验，但通过视频预览（<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=L6319Ii7MT8%EF%BC%89%E5%B1%95%E7%A4%BA%E4%BA%86%E6%B2%89%E6%B5%B8%E5%BC%8F%E8%A3%85%E7%BD%AE%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%95%88%E6%9E%9C%EF%BC%8C%E5%BC%BA%E8%B0%83%E5%8F%82%E4%B8%8E%E5%BC%8F%E4%BD%93%E9%AA%8C%E8%80%8C%E9%9D%9E%E6%8A%80%E6%9C%AF%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0%E3%80%82">https://www.youtube.com/watch?v=L6319Ii7MT8）展示了沉浸式装置的实现效果，强调参与式体验而非技术性能评估。</a></p>
<p><strong>Insight:</strong> 创新点在于将AI生成内容类比为海岸漂流物，通过物理装置与隐喻结合，推动从被动消费到主动反思的交互范式转变；其跨学科方法为AI伦理与公众参与提供了新颖的艺术实践视角。</p>
<p><strong>Abstract:</strong> This project explores how we engage with AI-generated content through the lens of the jutter: Dutch coastal foragers who comb the shoreline after storms, gathering and repurposing what the sea leaves behind. Reflecting how our lives are increasingly shaped by AI-generated media, we create a beach-like installation that blends real shoreline debris with AI-transformed images and videos. Visitors are invited to explore this space as contemporary jutters, deciding what to keep and what to discard. In doing so, the project reimagines AI-imagery as material for reflection, encouraging a more discerning engagement with the content that drifts through our feeds. A video preview of the installation can be found at <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=L6319Ii7MT8">https://www.youtube.com/watch?v=L6319Ii7MT8</a>.</p>
  </div>
</details>

<hr>
<h3 id="224-Multimodal-Feedback-for-Handheld-Tool-Guidance-Combining-Wrist-Based-Haptics-with-Augmented-Reality-cs-HC-cs-CV-eess-SYPDF"><a href="#224-Multimodal-Feedback-for-Handheld-Tool-Guidance-Combining-Wrist-Based-Haptics-with-Augmented-Reality-cs-HC-cs-CV-eess-SYPDF" class="headerlink" title="[224] Multimodal Feedback for Handheld Tool Guidance: Combining Wrist-Based Haptics with Augmented Reality cs.HC | cs.CV | eess.SYPDF"></a>[224] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12037">Multimodal Feedback for Handheld Tool Guidance: Combining Wrist-Based Haptics with Augmented Reality</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CV | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12037" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Yang, Christoph Leuze, Brian Hargreaves, Bruce Daniel, Fred M Baik</span></p>
<p><strong>TL;DR:</strong> 本文研究如何将振动触觉腕部反馈与光学透视增强现实（AR）结合，以提升手持工具空间引导的精度和用户体验。通过设计多模态系统，结合AR视觉与定制腕戴触觉设备提供方向和状态提示，实验表明AR与触觉结合能显著提高空间精度和可用性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决AR视觉引导在手术等任务中因视觉遮挡、光照条件和界面模糊导致的精度和信心不足问题。</p>
<p><strong>Result:</strong> 在引导任务中，结合AR与触觉的参与者空间精度显著更高（5.8毫米），可用性评分更高（SUS&#x3D;88.1），尽管任务时间略有增加；在提示识别实验中，参与者能准确识别五种振动模式。</p>
<p><strong>Insight:</strong> 创新点在于将腕部触觉反馈集成到AR系统中，提供多模态确认以减少认知负荷，适用于高精度视觉复杂任务如手术引导；设计启示包括结合方向与状态提示以增强用户信心和效率。</p>
<p><strong>Abstract:</strong> We investigate how vibrotactile wrist feedback can enhance spatial guidance for handheld tool movement in optical see-through augmented reality (AR). While AR overlays are widely used to support surgical tasks, visual occlusion, lighting conditions, and interface ambiguity can compromise precision and confidence. To address these challenges, we designed a multimodal system combining AR visuals with a custom wrist-worn haptic device delivering directional and state-based cues. A formative study with experienced surgeons and residents identified key tool maneuvers and preferences for reference mappings, guiding our cue design. In a cue identification experiment (N&#x3D;21), participants accurately recognized five vibration patterns under visual load, with higher recognition for full-actuator states than spatial direction cues. In a guidance task (N&#x3D;27), participants using both AR and haptics achieved significantly higher spatial precision (5.8 mm) and usability (SUS &#x3D; 88.1) than those using either modality alone, despite having modest increases in task time. Participants reported that haptic cues provided reassuring confirmation and reduced cognitive effort during alignment. Our results highlight the promise of integrating wrist-based haptics into AR systems for high-precision, visually complex tasks such as surgical guidance. We discuss design implications for multimodal interfaces supporting confident, efficient tool manipulation.</p>
  </div>
</details>

<hr>
<h3 id="225-Breaking-Coordinate-Overfitting-Geometry-Aware-WiFi-Sensing-for-Cross-Layout-3D-Pose-Estimation-cs-HC-cs-CVPDF"><a href="#225-Breaking-Coordinate-Overfitting-Geometry-Aware-WiFi-Sensing-for-Cross-Layout-3D-Pose-Estimation-cs-HC-cs-CVPDF" class="headerlink" title="[225] Breaking Coordinate Overfitting: Geometry-Aware WiFi Sensing for Cross-Layout 3D Pose Estimation cs.HC | cs.CVPDF"></a>[225] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12252">Breaking Coordinate Overfitting: Geometry-Aware WiFi Sensing for Cross-Layout 3D Pose Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12252" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Songming Jia, Yan Lu, Bin Liu, Xiang Zhang, Peng Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了PerceptAlign，一个几何感知的WiFi传感框架，用于解决基于WiFi的3D人体姿态估计中的坐标过拟合问题。该框架通过一个轻量级的坐标统一过程，将WiFi和视觉测量对齐到一个共享的3D空间，并显式地将校准后的收发器位置编码为条件变量，使模型能够解耦人体运动和部署布局，从而实现跨布局的鲁棒姿态估计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于WiFi的3D姿态估计方法依赖于视觉3D姿态作为监督，并直接将信道状态信息（CSI）回归到基于摄像头的坐标系，这导致了坐标过拟合问题：模型记住了特定部署的WiFi收发器布局，而非仅学习与活动相关的表示，从而导致严重的泛化失败。</p>
<p><strong>Result:</strong> 实验表明，在构建的迄今为止最大的跨域3D WiFi姿态估计数据集（包含21个受试者、5个场景、18个动作和7种设备布局）上，PerceptAlign与最先进的基线相比，将域内误差降低了12.3%，并将跨域误差降低了60%以上。</p>
<p><strong>Insight:</strong> 论文的核心创新在于提出了几何条件学习框架，通过显式地将设备几何布局作为条件变量融入模型，强制网络解耦人体运动和部署布局，首次实现了布局不变的WiFi姿态估计。这为可扩展和实用的WiFi感知提供了一条可行路径。</p>
<p><strong>Abstract:</strong> WiFi-based 3D human pose estimation offers a low-cost and privacy-preserving alternative to vision-based systems for smart interaction. However, existing approaches rely on visual 3D poses as supervision and directly regress CSI to a camera-based coordinate system. We find that this practice leads to coordinate overfitting: models memorize deployment-specific WiFi transceiver layouts rather than only learning activity-relevant representations, resulting in severe generalization failures. To address this challenge, we present PerceptAlign, the first geometry-conditioned framework for WiFi-based cross-layout pose estimation. PerceptAlign introduces a lightweight coordinate unification procedure that aligns WiFi and vision measurements in a shared 3D space using only two checkerboards and a few photos. Within this unified space, it encodes calibrated transceiver positions into high-dimensional embeddings and fuses them with CSI features, making the model explicitly aware of device geometry as a conditional variable. This design forces the network to disentangle human motion from deployment layouts, enabling robust and, for the first time, layout-invariant WiFi pose estimation. To support systematic evaluation, we construct the largest cross-domain 3D WiFi pose estimation dataset to date, comprising 21 subjects, 5 scenes, 18 actions, and 7 device layouts. Experiments show that PerceptAlign reduces in-domain error by 12.3% and cross-domain error by more than 60% compared to state-of-the-art baselines. These results establish geometry-conditioned learning as a viable path toward scalable and practical WiFi sensing.</p>
  </div>
</details>

<hr>
<div id='cs.CY'></div>

<h1 id="cs-CY-Back"><a href="#cs-CY-Back" class="headerlink" title="cs.CY [Back]"></a>cs.CY <a href="#toc">[Back]</a></h1><h3 id="226-AI-generated-data-contamination-erodes-pathological-variability-and-diagnostic-reliability-cs-CY-cs-AI-cs-CL-cs-CV-cs-LGPDF"><a href="#226-AI-generated-data-contamination-erodes-pathological-variability-and-diagnostic-reliability-cs-CY-cs-AI-cs-CL-cs-CV-cs-LGPDF" class="headerlink" title="[226] AI-generated data contamination erodes pathological variability and diagnostic reliability cs.CY | cs.AI | cs.CL | cs.CV | cs.LGPDF"></a>[226] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12946">AI-generated data contamination erodes pathological variability and diagnostic reliability</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CL | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12946" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang</span></p>
<p><strong>TL;DR:</strong> 这篇论文研究了生成式AI在医疗记录中生成合成数据所导致的数据污染问题，发现这种自我参照循环会迅速侵蚀病理变异性和诊断可靠性，尤其是在缺乏人工验证的情况下。通过分析超过80万个合成数据点，研究发现AI模型生成的合成内容会逐渐收敛到通用表型，导致罕见但关键的病理发现消失，并产生虚假的诊断信心。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 论文的动机是探索生成式AI在医疗领域广泛使用后，其生成的合成数据可能污染未来模型训练数据，从而对临床诊断可靠性产生的未知后果，特别是缺乏人工监督时的风险。</p>
<p><strong>Result:</strong> 在临床文本生成、视觉语言报告和医学图像合成等任务中，模型经过两代后诊断可靠性急剧下降，虚假安慰率增加至40%，医生盲评确认其临床无用；缓解策略中，合成数据量扩展无效，而混合真实数据与质量感知过滤能有效保持多样性。</p>
<p><strong>Insight:</strong> 论文的创新点在于首次系统揭示了生成式AI数据污染对医疗诊断的具体危害机制，如病理变异性的侵蚀和虚假信心的产生，并客观分析了不同缓解策略的效果，强调了政策强制人工监督的必要性。</p>
<p><strong>Abstract:</strong> Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="227-Thinking-Traps-in-Long-Chain-of-Thought-A-Measurable-Study-and-Trap-Aware-Adaptive-Restart-cs-AI-cs-CLPDF"><a href="#227-Thinking-Traps-in-Long-Chain-of-Thought-A-Measurable-Study-and-Trap-Aware-Adaptive-Restart-cs-AI-cs-CLPDF" class="headerlink" title="[227] Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart cs.AI | cs.CLPDF"></a>[227] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11940">Thinking Traps in Long Chain-of-Thought: A Measurable Study and Trap-Aware Adaptive Restart</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11940" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng</span></p>
<p><strong>TL;DR:</strong> 本文研究了长思维链推理中存在的’思维陷阱’现象，即模型在早期做出错误承诺后，后续的反思、尝试或验证都无法修正根本错误，导致生成自洽但错误的答案。作者提出了一个名为TAAR的测试时控制框架，通过训练诊断策略来预测陷阱位置和逃脱概率，并在推理时自适应地截断轨迹并重启解码，以提升推理性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于，尽管通过延长思维链可以提升模型的推理能力，但更长的生成过程并不保证正确性。模型一旦在早期陷入错误路径，后续的自我修正机制可能失效，形成难以摆脱的’思维陷阱’。本文旨在量化分析这一现象并设计解决方案。</p>
<p><strong>Result:</strong> 在DAPO-MATH的精选子集上，89%的失败案例表现出思维陷阱。在AIME24、AIME25、GPQA-Diamond、HMMT25、BRUMO25等具有挑战性的数学和科学推理基准测试中，TAAR方法在不微调基础模型参数的情况下，显著提升了推理性能。</p>
<p><strong>Insight:</strong> 创新点在于：1) 对长思维链推理失败模式进行了细粒度的轨迹分析，并定义了’思维陷阱’这一可测量的概念；2) 提出了TAAR框架，通过预测陷阱位置和逃脱概率，在测试时自适应地干预解码过程，包括截断、重启以及引入更强扰动（如高温重采样和结构化重启后缀）。该方法为提升大模型推理可靠性提供了一种无需修改模型参数的轻量级干预策略。</p>
<p><strong>Abstract:</strong> Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly enhances reasoning capabilities, yet extended generation does not guarantee correctness: after an early wrong commitment, models may keep elaborating a self-consistent but incorrect prefix. Through fine-grained trajectory analysis, we identify Thinking Traps, prefix-dominant deadlocks where later reflection, alternative attempts, or verification fails to revise the root error. On a curated subset of DAPO-MATH, 89% of failures exhibit such traps. To solve this problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control framework that trains a diagnostic policy to predict two signals from partial trajectories: a trap index for where to truncate and an escape probability for whether and how strongly to intervene. At inference time, TAAR truncates the trajectory before the predicted trap segment and adaptively restarts decoding; for severely trapped cases, it applies stronger perturbations, including higher-temperature resampling and an optional structured reboot suffix. Experiments on challenging mathematical and scientific reasoning benchmarks (AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves reasoning performance without fine-tuning base model parameters.</p>
  </div>
</details>

<hr>
<h3 id="228-A-Multi-Agent-System-for-Generating-Actionable-Business-Advice-cs-AI-cs-CLPDF"><a href="#228-A-Multi-Agent-System-for-Generating-Actionable-Business-Advice-cs-AI-cs-CLPDF" class="headerlink" title="[228] A Multi-Agent System for Generating Actionable Business Advice cs.AI | cs.CLPDF"></a>[228] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12024">A Multi-Agent System for Generating Actionable Business Advice</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12024" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Dhruv Kumar, Praveen Kumar</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于大语言模型的多智能体框架，用于将大规模用户评论转化为可执行的商业建议。该框架通过聚类选择代表性评论、生成建议、迭代评估和可行性排序四个组件，结合语料库提炼与反馈驱动的建议优化，生成具体、可操作且实用的输出。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有分析方法（如情感分析或方面提取）多停留在描述性任务，而大语言模型生成的建议往往缺乏准确性和深度推理，因此需要一种能够从用户评论中提取可执行商业建议的框架。</p>
<p><strong>Result:</strong> 在三个服务领域和多个模型系列上的实验表明，该框架在可操作性、具体性和非冗余性方面持续优于单模型基线，且中等规模模型的性能接近大型模型框架。</p>
<p><strong>Insight:</strong> 创新点在于将多智能体系统与LLM结合，通过聚类、生成、评估和排序的集成流程，实现从评论到可执行建议的端到端转化，并利用反馈循环优化建议质量，使中等模型也能达到接近大型模型的性能。</p>
<p><strong>Abstract:</strong> Customer reviews contain rich signals about product weaknesses and unmet user needs, yet existing analytic methods rarely move beyond descriptive tasks such as sentiment analysis or aspect extraction. While large language models (LLMs) can generate free-form suggestions, their outputs often lack accuracy and depth of reasoning. In this paper, we present a multi-agent, LLM-based framework for prescriptive decision support, which transforms large scale review corpora into actionable business advice. The framework integrates four components: clustering to select representative reviews, generation of advices, iterative evaluation, and feasibility based ranking. This design couples corpus distillation with feedback driven advice refinement to produce outputs that are specific, actionable, and practical. Experiments across three service domains and multiple model families show that our framework consistently outperform single model baselines on actionability, specificity, and non-redundancy, with medium sized models approaching the performance of large model frameworks.</p>
  </div>
</details>

<hr>
<h3 id="229-Agentic-Reasoning-for-Large-Language-Models-cs-AI-cs-CLPDF"><a href="#229-Agentic-Reasoning-for-Large-Language-Models-cs-AI-cs-CLPDF" class="headerlink" title="[229] Agentic Reasoning for Large Language Models cs.AI | cs.CLPDF"></a>[229] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12538">Agentic Reasoning for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12538" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang</span></p>
<p><strong>TL;DR:</strong> 这篇综述论文系统性地探讨了大型语言模型（LLMs）的智能体推理范式，将其重新定义为能够通过持续交互进行规划、行动和学习的自主智能体。论文从三个互补维度组织智能体推理：基础智能体推理、自我进化智能体推理和集体多智能体推理，并区分了上下文推理与训练后推理。综述涵盖了从科学到数学等多个现实应用和基准测试，旨在构建一个连接思维与行动的统一路线图。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管大型语言模型在封闭世界环境中展现出强大的推理能力，但在开放、动态环境中表现不佳。论文旨在通过引入智能体推理范式，解决LLMs在开放性和动态环境中的适应与交互问题，推动其从被动响应向主动规划与学习的转变。</p>
<p><strong>Result:</strong> 论文综述了多个现实应用领域（如科学、机器人、医疗、自主研究和数学）的代表性智能体推理框架和基准测试，展示了该范式在不同场景下的应用潜力和现有进展，但未提供具体的定量比较或SOTA结果。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个系统化的智能体推理三维分类框架（基础、自我进化、集体），并明确区分了上下文推理与训练后推理两种扩展交互的方式。从客观角度看，该框架为理解和设计LLM驱动的自主智能体提供了清晰的结构化路线图，强调了从单智能体能力到多智能体协作的演进路径，以及持续学习与适应的重要性。</p>
<p><strong>Abstract:</strong> Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.</p>
  </div>
</details>

<hr>
<h3 id="230-MemeLens-Multilingual-Multitask-VLMs-for-Memes-cs-AI-cs-CLPDF"><a href="#230-MemeLens-Multilingual-Multitask-VLMs-for-Memes-cs-AI-cs-CLPDF" class="headerlink" title="[230] MemeLens: Multilingual Multitask VLMs for Memes cs.AI | cs.CLPDF"></a>[230] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12539">MemeLens: Multilingual Multitask VLMs for Memes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12539" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ali Ezzat Shahroor, Mohamed Bayan Kmainasi, Abul Hasnat, Dimitar Dimitrov, Giovanni Da San Martino</span></p>
<p><strong>TL;DR:</strong> 本文提出MemeLens，一个统一的多语言多任务解释增强视觉语言模型，用于理解网络迷因。作者整合了38个公开迷因数据集，将数据集特定标签映射到涵盖危害、目标、比喻&#x2F;语用意图和情感等20个任务的共享分类法中，并进行了全面的实证分析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有迷因研究分散在不同任务（如仇恨、厌女、宣传、情感、幽默）和语言中，限制了跨领域泛化能力，因此需要构建一个统一的多语言多任务模型来弥合这一差距。</p>
<p><strong>Result:</strong> 实证分析表明，稳健的迷因理解需要多模态训练，在不同语义类别间表现出显著差异，并且在单个数据集上微调而非统一训练时，模型容易对过度专业化敏感。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个统一的多语言多任务解释增强VLM框架，并整合了大量数据集以创建共享任务分类法，为迷因理解提供了系统化的基准和分析视角。</p>
<p><strong>Abstract:</strong> Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative&#x2F;pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.</p>
  </div>
</details>

<hr>
<h3 id="231-CURE-Med-Curriculum-Informed-Reinforcement-Learning-for-Multilingual-Medical-Reasoning-cs-AI-cs-CLPDF"><a href="#231-CURE-Med-Curriculum-Informed-Reinforcement-Learning-for-Multilingual-Medical-Reasoning-cs-AI-cs-CLPDF" class="headerlink" title="[231] CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning cs.AI | cs.CLPDF"></a>[231] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13262">CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13262" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出了CURE-MED，一个基于课程学习的强化学习框架，旨在提升大型语言模型在多语言医疗推理任务上的性能。为此，作者首先构建了一个高质量的多语言医疗推理数据集CUREMED-BENCH，涵盖13种语言。提出的框架结合了代码切换感知的监督微调和组相对策略优化，以联合提升逻辑正确性和语言稳定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型在单语言数学和常识推理上表现良好，但在多语言医疗推理应用中仍不可靠，这阻碍了其在多语言医疗环境中的部署。本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 在13种语言上，该方法持续优于强基线模型，并展现出良好的扩展性。在7B参数规模下，实现了85.21%的语言一致性和54.35%的逻辑正确性；在32B参数规模下，实现了94.96%的语言一致性和70.04%的逻辑正确性。</p>
<p><strong>Insight:</strong> 创新点包括：1) 构建了一个高质量、覆盖多种代表性不足语言的多语言开放式医疗推理基准数据集CUREMED-BENCH；2) 提出了一个课程式强化学习框架CURE-MED，通过整合代码切换感知的监督微调和组相对策略优化，同时优化逻辑推理和语言一致性，这为解决多语言领域特定推理任务提供了新的技术路径。</p>
<p><strong>Abstract:</strong> While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at <a target="_blank" rel="noopener" href="https://cure-med.github.io/">https://cure-med.github.io/</a></p>
  </div>
</details>

<hr>
<h3 id="232-DSAEval-Evaluating-Data-Science-Agents-on-a-Wide-Range-of-Real-World-Data-Science-Problems-cs-AI-cs-CLPDF"><a href="#232-DSAEval-Evaluating-Data-Science-Agents-on-a-Wide-Range-of-Real-World-Data-Science-Problems-cs-AI-cs-CLPDF" class="headerlink" title="[232] DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems cs.AI | cs.CLPDF"></a>[232] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13591">DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World Data Science Problems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13591" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maojun Sun, Yifei Xie, Yue Wu, Ruijian Han, Binyan Jiang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一个名为DSAEval的基准测试，用于评估基于LLM的数据科学代理在真实世界数据科学问题上的能力。该基准包含641个基于285个多样化数据集的问题，涵盖结构化和非结构化数据。研究还系统评估了11个先进的代理模型，并分析了其性能、效率和成本效益。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于LLM的数据科学代理旨在自动化从数据分析到深度学习的任务，但真实世界数据科学问题的开放性、跨分类和缺乏标准答案的特性给评估带来了重大挑战。</p>
<p><strong>Result:</strong> 在DSAEval基准上的评估结果显示，Claude-Sonnet-4.5整体性能最强，GPT-5.2效率最高，MiMo-V2-Flash最具成本效益。多模态感知在视觉相关任务上能带来2.04%到11.30%的性能提升。当前代理在结构化数据和常规分析流程上表现良好，但在非结构化领域仍面临重大挑战。</p>
<p><strong>Insight:</strong> 论文的创新点在于构建了一个具有多模态环境感知、多轮查询交互和多维度评估（推理、代码、结果）三大特征的综合性评估基准。这为数据科学代理的迭代开发和能力评估提供了一个更贴近真实场景的框架。</p>
<p><strong>Abstract:</strong> Recent LLM-based data agents aim to automate data science tasks ranging from data analysis to deep learning. However, the open-ended nature of real-world data science problems, which often span multiple taxonomies and lack standard answers, poses a significant challenge for evaluation. To address this, we introduce DSAEval, a benchmark comprising 641 real-world data science problems grounded in 285 diverse datasets, covering both structured and unstructured data (e.g., vision and text). DSAEval incorporates three distinctive features: (1) Multimodal Environment Perception, which enables agents to interpret observations from multiple modalities including text and vision; (2) Multi-Query Interactions, which mirror the iterative and cumulative nature of real-world data science projects; and (3) Multi-Dimensional Evaluation, which provides a holistic assessment across reasoning, code, and results. We systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2 is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further demonstrate that multimodal perception consistently improves performance on vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while current data science agents perform well on structured data and routine data anlysis workflows, substantial challenges remain in unstructured domains. Finally, we offer critical insights and outline future research directions to advance the development of data science agents.</p>
  </div>
</details>

<hr>
<h3 id="233-Finding-RELIEF-Shaping-Reasoning-Behavior-without-Reasoning-Supervision-via-Belief-Engineering-cs-AI-cs-CLPDF"><a href="#233-Finding-RELIEF-Shaping-Reasoning-Behavior-without-Reasoning-Supervision-via-Belief-Engineering-cs-AI-cs-CLPDF" class="headerlink" title="[233] Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering cs.AI | cs.CLPDF"></a>[233] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13752">Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13752" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee</span></p>
<p><strong>TL;DR:</strong> 本文提出RELIEF框架，通过信念工程塑造大型推理模型的行为，无需依赖推理轨迹监督。该方法利用模型内部的潜在推理信念，通过简单的logit探测捕获，并通过微调自反性问答对来内化目标信念，从而提升模型的效率和忠实性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型推理模型在复杂问题解决中表现出色，但常存在计算冗余或推理不忠实的问题。现有方法通常依赖强化学习或使用黄金标准推理轨迹进行微调，计算成本高且难以扩展。本文旨在探索一种无需推理监督即可塑造模型行为的方法。</p>
<p><strong>Result:</strong> 在效率和忠实性任务上的大量实验表明，RELIEF在匹配或超越基于行为监督和偏好基线方法的同时，训练成本更低。进一步分析验证了改变模型的推理信念能有效塑造其实际行为。</p>
<p><strong>Insight:</strong> 创新点在于揭示了大型推理模型内部存在可探测的潜在推理信念，并提出了通过信念工程直接对齐模型自我概念与目标信念蓝图来塑造行为的方法，完全绕过了对推理轨迹监督的需求，降低了训练成本。</p>
<p><strong>Abstract:</strong> Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model’s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model’s reasoning belief effectively shapes its actual behavior.</p>
  </div>
</details>

<hr>
<h3 id="234-DARC-Decoupled-Asymmetric-Reasoning-Curriculum-for-LLM-Evolution-cs-AI-cs-CLPDF"><a href="#234-DARC-Decoupled-Asymmetric-Reasoning-Curriculum-for-LLM-Evolution-cs-AI-cs-CLPDF" class="headerlink" title="[234] DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution cs.AI | cs.CLPDF"></a>[234] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13761">DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13761" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shengda Fan, Xuyan Ye, Yankai Lin</span></p>
<p><strong>TL;DR:</strong> DARC是一种两阶段自进化框架，旨在解决大型语言模型自对弈中的优化不稳定问题。它通过解耦问题生成器（Questioner）和解答器（Solver）的训练，首先训练问题生成器合成难度可控的问题，然后通过非对称自蒸馏机制训练解答器，利用文档增强的教师模型生成高质量伪标签来监督学生解答器。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有自对弈框架存在优化不稳定问题，原因包括：（i）问题生成器的奖励反馈依赖于解答器，导致目标非平稳；（ii）解答器使用自生成的伪标签进行监督，存在自举误差。DARC旨在通过解耦和课程学习机制来稳定自进化过程。</p>
<p><strong>Result:</strong> DARC在九个推理基准测试和三个骨干模型上平均提升了10.9分，一致优于所有基线方法，并在不依赖人工标注的情况下接近全监督模型的性能。</p>
<p><strong>Insight:</strong> 创新点包括：将自对弈解耦为两个独立阶段，引入显式难度级别和外部语料库来校准问题生成，以及采用非对称自蒸馏（文档增强教师监督无文档访问的学生）来减少伪标签噪声。这提供了一种更稳定的模型自进化路径，且与模型无关。</p>
<p><strong>Abstract:</strong> Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at <a target="_blank" rel="noopener" href="https://github.com/RUCBM/DARC">https://github.com/RUCBM/DARC</a>.</p>
  </div>
</details>

<hr>
<h3 id="235-Look-Ahead-Bench-a-Standardized-Benchmark-of-Look-ahead-Bias-in-Point-in-Time-LLMs-for-Finance-cs-AI-cs-CL-cs-LG-q-fin-CP-q-fin-GNPDF"><a href="#235-Look-Ahead-Bench-a-Standardized-Benchmark-of-Look-ahead-Bias-in-Point-in-Time-LLMs-for-Finance-cs-AI-cs-CL-cs-LG-q-fin-CP-q-fin-GNPDF" class="headerlink" title="[235] Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance cs.AI | cs.CL | cs.LG | q-fin.CP | q-fin.GNPDF"></a>[235] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13770">Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG | q-fin.CP | q-fin.GN</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13770" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mostapha Benhenda</span></p>
<p><strong>TL;DR:</strong> 本文介绍了Look-Ahead-Bench，一个用于评估金融领域点对点大语言模型中前瞻性偏差的标准化基准。该基准通过模拟真实金融工作流程，分析模型在不同市场周期下的性能衰减，以区分模型的真实预测能力与基于记忆的表现。研究评估了多个开源LLM和PiT-Inference系列模型，发现标准LLM存在显著的前瞻性偏差，而PiT模型随着规模扩大展现出更好的泛化和推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法主要通过问答测试LLM的内部前瞻知识，缺乏对实际应用场景中模型行为的评估。本文旨在解决这一问题，为金融LLM的时间偏差提供标准化评估基础，并识别适合实际部署的模型。</p>
<p><strong>Result:</strong> 在Look-Ahead-Bench上评估了Llama 3.1（8B和70B）、DeepSeek 3.2以及PiT-Inference的Pitinf-Small、Pitinf-Medium和Pitinf-Large模型。结果显示，标准LLM表现出显著的前瞻性偏差（通过alpha衰减衡量），而PiT模型随着规模扩大，泛化和推理能力得到改善，达到了前沿水平。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个专注于实际金融工作流程的标准化基准，通过分析跨时间市场周期的性能衰减来区分预测能力与记忆效应，为评估金融LLM的时间偏差提供了实用框架。客观来看，该方法强调了在真实场景中测试模型的重要性，有助于推动更可靠的金融AI模型开发。</p>
<p><strong>Abstract:</strong> We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\&amp;A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs – Llama 3.1 (8B and 70B) and DeepSeek 3.2 – against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: <a target="_blank" rel="noopener" href="https://github.com/benstaf/lookaheadbench">https://github.com/benstaf/lookaheadbench</a></p>
  </div>
</details>

<hr>
<h3 id="236-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning-cs-AI-cs-CLPDF"><a href="#236-Toward-Efficient-Agents-Memory-Tool-learning-and-Planning-cs-AI-cs-CLPDF" class="headerlink" title="[236] Toward Efficient Agents: Memory, Tool learning, and Planning cs.AI | cs.CLPDF"></a>[236] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14192">Toward Efficient Agents: Memory, Tool learning, and Planning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14192" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu</span></p>
<p><strong>TL;DR:</strong> 本文系统性地探讨了基于大语言模型的智能体系统的效率问题，聚焦于内存、工具学习和规划三个核心组件，并考虑了延迟、令牌消耗、步骤数等成本。文章综述了多种提升效率的方法，如通过压缩和管理来限制上下文、设计强化学习奖励以最小化工具调用、采用受控搜索机制等，并提出了从固定成本预算下的效能和同等效能下的成本两个互补角度来评估效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于大语言模型的智能体系统研究主要关注效能的提升，而忽略了效率这一对实际部署至关重要的因素，因此本文旨在全面研究智能体系统本身的效率问题。</p>
<p><strong>Result:</strong> 本文是一篇综述性论文，未提出具体模型或进行定量实验，但总结了现有方法中提升效率的共享高级原则，并整理了针对内存、工具学习和规划组件的评估协议与常用效率指标。</p>
<p><strong>Insight:</strong> 创新点在于首次系统性地从效率角度审视智能体系统的核心组件，提出了基于帕累托前沿的效能-成本权衡分析框架，并为未来效率导向的基准测试和研究方向提供了见解。</p>
<p><strong>Abstract:</strong> Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.</p>
  </div>
</details>

<hr>
<h3 id="237-VIRO-Robust-and-Efficient-Neuro-Symbolic-Reasoning-with-Verification-for-Referring-Expression-Comprehension-cs-AI-cs-CVPDF"><a href="#237-VIRO-Robust-and-Efficient-Neuro-Symbolic-Reasoning-with-Verification-for-Referring-Expression-Comprehension-cs-AI-cs-CVPDF" class="headerlink" title="[237] VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension cs.AI | cs.CVPDF"></a>[237] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12781">VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12781" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyejin Park, Junhyuk Kwon, Suha Kwak, Jungseul Ok</span></p>
<p><strong>TL;DR:</strong> 本文提出VIRO（Verification-Integrated Reasoning Operators）框架，一种用于指代表达理解（REC）的神经符号推理方法。该方法通过在每个推理步骤中嵌入轻量级验证器来检测和纠正错误，从而解决现有方法中因中间步骤错误导致的级联误差问题，特别是在目标不存在的情况下。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有神经符号REC方法依赖大型语言模型（LLM）和视觉语言模型（VLM）进行组合推理，但假设中间推理步骤准确，这会导致级联错误（如虚假检测和无效关系传播），甚至在图像中无目标时产生高置信度误报。</p>
<p><strong>Result:</strong> VIRO在目标存在和无目标设置下达到61.1%的平衡准确率，实现SOTA性能；在真实世界第一人称数据上展示泛化能力；具有高吞吐量计算效率，程序失败率低于0.3%，并通过解耦程序生成与执行实现可扩展性。</p>
<p><strong>Insight:</strong> 创新点在于将验证机制集成到神经符号推理操作符中，通过逐步骤验证（如对象存在性、空间关系）增强鲁棒性，有效处理无目标情况；框架设计兼顾了可解释性、效率与可靠性，为组合视觉推理提供了新思路。</p>
<p><strong>Abstract:</strong> Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.</p>
  </div>
</details>

<hr>
<h3 id="238-Reasoning-is-a-Modality-cs-AI-cs-CV-cs-LGPDF"><a href="#238-Reasoning-is-a-Modality-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[238] Reasoning is a Modality cs.AI | cs.CV | cs.LGPDF"></a>[238] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13562">Reasoning is a Modality</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13562" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiguang Liu, Yi Shang</span></p>
<p><strong>TL;DR:</strong> 本文提出将推理视为一种独立模态，并设计了一种角色分离的Transformer块，将全局控制器令牌与网格工作空间令牌分离，以迭代执行规则。该方法在视觉为中心的VARC协议下训练和评估，在ARC-1任务上达到62.6%的准确率，超越了人类平均表现（60.2%）和先前方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代AI系统（如LLMs和ViTs）主要作为行为序列预测机器运行，缺乏可读的持久心理状态，与人类可解释内部状态的行为存在差距。作者假设推理应作为一个独立于低层工作空间的通道存在，以弥合这一差距。</p>
<p><strong>Result:</strong> 在ARC-1任务上，该方法准确率达到62.6%，超过人类平均表现（60.2%），并显著优于先前方法，在视觉为中心的VARC协议下实现了SOTA水平。</p>
<p><strong>Insight:</strong> 创新点在于将推理建模为独立模态，并通过角色分离的Transformer块实现控制器与工作空间的解耦，使模型从概率驱动转向控制器驱动的推理，展现出更连贯的规则应用结构。</p>
<p><strong>Abstract:</strong> The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for studying abstract reasoning, an ability central to human intelligence. Modern AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior prediction machines: they match observable behaviors by modeling token statistics without a persistent, readable mental state. This creates a gap with human-like behavior: humans can explain an action by decoding internal state, while AI systems can produce fluent post-hoc rationalizations that are not grounded in such a state. We hypothesize that reasoning is a modality: reasoning should exist as a distinct channel separate from the low-level workspace on which rules are applied. To test this hypothesis, on solving ARC tasks as a visual reasoning problem, we designed a novel role-separated transformer block that splits global controller tokens from grid workspace tokens, enabling iterative rule execution. Trained and evaluated within the VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1, surpassing average human performance (60.2%) and outperforming prior methods significantly. Qualitatively, our models exhibit more coherent rule-application structure than the dense ViT baseline, consistent with a shift away from plausible probability blobs toward controller-driven reasoning.</p>
  </div>
</details>

<hr>
<div id='cs.MM'></div>

<h1 id="cs-MM-Back"><a href="#cs-MM-Back" class="headerlink" title="cs.MM [Back]"></a>cs.MM <a href="#toc">[Back]</a></h1><h3 id="239-Chain-of-Thought-Compression-Should-Not-Be-Blind-V-Skip-for-Efficient-Multimodal-Reasoning-via-Dual-Path-Anchoring-cs-MM-cs-CL-cs-CVPDF"><a href="#239-Chain-of-Thought-Compression-Should-Not-Be-Blind-V-Skip-for-Efficient-Multimodal-Reasoning-via-Dual-Path-Anchoring-cs-MM-cs-CL-cs-CVPDF" class="headerlink" title="[239] Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring cs.MM | cs.CL | cs.CVPDF"></a>[239] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13879">Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.MM | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13879" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongxu Zhang, Yiding Sun, Cheng Tan, Wenbiao Yan, Ning Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为V-Skip的视觉锚定信息瓶颈优化方法，旨在解决多模态大语言模型中思维链推理因自回归特性导致的高延迟问题。该方法通过双路径门控机制，结合语言惊奇度和跨模态注意力流来评估令牌重要性，有效避免因盲目压缩而导致的视觉遗忘现象，从而在显著提升推理速度的同时保持模型精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前多模态大语言模型中的思维链推理虽然能提升性能，但其自回归特性带来了难以承受的延迟；现有的令牌压缩方法往往盲目地将以文本为中心的度量应用于多模态场景，导致视觉遗忘（即错误剪枝语言上冗余但视觉上重要的令牌，引发幻觉），因此需要一种能有效保留视觉关键信息的压缩方法。</p>
<p><strong>Result:</strong> 在Qwen2-VL和Llama-3.2系列模型上的大量实验表明，V-Skip实现了2.9倍的加速，且精度损失可忽略；特别是在DocVQA基准测试上，其性能优于其他基线方法超过30%，有效保留了细粒度视觉细节。</p>
<p><strong>Insight:</strong> 论文的创新点在于将令牌剪枝重新定义为视觉锚定信息瓶颈优化问题，并设计了结合语言惊奇度和跨模态注意力流的双路径门控机制来评估令牌重要性，从而在压缩过程中保护视觉上关键的锚点信息；从客观角度看，该方法强调了多模态场景下压缩策略需考虑跨模态依赖，而非单纯依赖文本度量，为高效多模态推理提供了新思路。</p>
<p><strong>Abstract:</strong> While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30% on the DocVQA.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="240-FRoM-W1-Towards-General-Humanoid-Whole-Body-Control-with-Language-Instructions-cs-RO-cs-CL-cs-CVPDF"><a href="#240-FRoM-W1-Towards-General-Humanoid-Whole-Body-Control-with-Language-Instructions-cs-RO-cs-CL-cs-CVPDF" class="headerlink" title="[240] FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions cs.RO | cs.CL | cs.CVPDF"></a>[240] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12799">FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12799" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peng Li, Zihan Zhuang, Yangfan Gao, Yi Dong, Sixian Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了FRoM-W1，一个开源的、基于自然语言指令实现通用人形机器人全身运动控制的框架。该框架分为两个阶段：H-GPT利用大规模人类数据训练语言驱动的全身运动生成模型，并采用思维链技术提升指令理解泛化能力；H-ACT通过强化学习在物理仿真中预训练并微调运动控制器，将生成的人体动作重定向到机器人上，实现稳定、精确的执行，并通过仿真到现实的模块部署到真实机器人。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前人形机器人的动作通常是硬编码或专门训练的，限制了其通用性和灵活性。本文旨在通过自然语言指令实现通用的人形机器人全身运动控制，以提升机器人的多功能性和适应性。</p>
<p><strong>Result:</strong> 在HumanML3D-X基准测试中，FRoM-W1在人体全身运动生成方面表现出优越性能；在Unitree H1和G1机器人上的评估表明，所引入的强化学习微调持续提高了运动跟踪精度和任务成功率。</p>
<p><strong>Insight:</strong> 创新点包括：1) 将大规模语言驱动的人体运动生成与机器人特定控制解耦的两阶段框架；2) 在运动生成模型中应用思维链技术以增强对自然语言指令的理解和泛化；3) 通过仿真预训练和强化学习微调的结合，以及模块化的仿真到现实部署，实现稳定、准确的物理执行。</p>
<p><strong>Abstract:</strong> Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model’s generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.</p>
  </div>
</details>

<hr>
<h3 id="241-AI-for-Green-Spaces-Leveraging-Autonomous-Navigation-and-Computer-Vision-for-Park-Litter-Removal-cs-RO-cs-AI-cs-CV-eess-SYPDF"><a href="#241-AI-for-Green-Spaces-Leveraging-Autonomous-Navigation-and-Computer-Vision-for-Park-Litter-Removal-cs-RO-cs-AI-cs-CV-eess-SYPDF" class="headerlink" title="[241] AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal cs.RO | cs.AI | cs.CV | eess.SYPDF"></a>[241] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11876">AI for Green Spaces: Leveraging Autonomous Navigation and Computer Vision for Park Litter Removal</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11876" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Christopher Kao, Akhil Pathapati, James Davis</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种用于公园垃圾清理的自主机器人系统，该系统结合了自主导航和计算机视觉技术。通过使用生成树覆盖算法进行路径规划，实时动态GPS实现厘米级定位，以及基于ResNet50的卷积神经网络进行垃圾检测，机器人能够自动识别并拾取草地上的垃圾。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决公园草地上因野餐者遗留大量垃圾（仅美国就有500亿件）而造成的环境问题，旨在开发一个能够自主导航、识别并拾取垃圾的机器人。</p>
<p><strong>Result:</strong> 在垃圾检测方面，ResNet50 CNN模型达到了94.52%的准确率；整个系统的总体成功率达到了80%，证明了该方案在草地上的可行性。</p>
<p><strong>Insight:</strong> 创新点在于将成熟的STC覆盖路径规划算法、高精度RTK GPS定位与ResNet50 CNN目标检测模型集成到一个针对公园草地垃圾清理的专用机器人系统中，并通过设计针对性的拾取机构来优化整体性能，为特定场景的自动化环境清理提供了可行的技术方案。</p>
<p><strong>Abstract:</strong> There are 50 billion pieces of litter in the U.S. alone. Grass fields contribute to this problem because picnickers tend to leave trash on the field. We propose building a robot that can autonomously navigate, identify, and pick up trash in parks. To autonomously navigate the park, we used a Spanning Tree Coverage (STC) algorithm to generate a coverage path the robot could follow. To navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which provides a centimeter-level reading every second. For computer vision, we utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We select a new pickup mechanism that specifically targets the trash we encounter on the field. Our solution achieved an overall success rate of 80%, demonstrating that autonomous trash pickup robots on grass fields are a viable solution.</p>
  </div>
</details>

<hr>
<h3 id="242-ReWorld-Multi-Dimensional-Reward-Modeling-for-Embodied-World-Models-cs-RO-cs-CVPDF"><a href="#242-ReWorld-Multi-Dimensional-Reward-Modeling-for-Embodied-World-Models-cs-RO-cs-CVPDF" class="headerlink" title="[242] ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models cs.RO | cs.CVPDF"></a>[242] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12428">ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12428" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Baorui Peng, Wenyao Zhang, Liang Xu, Zekun Qi, Jiazhao Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了ReWorld框架，旨在利用强化学习来对齐基于视频的具身世界模型，使其在物理真实性、任务完成能力、具身合理性和视觉质量等多个维度上符合人类偏好。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于视频的世界模型主要关注视觉生成质量，而忽略了物理保真度、动态一致性和任务逻辑，尤其是在接触丰富的操作任务中，这限制了其在下游任务中的应用。</p>
<p><strong>Result:</strong> 综合实验和理论分析表明，ReWorld显著提高了生成轨迹的物理保真度、逻辑连贯性、具身性和视觉质量，超越了先前的方法。</p>
<p><strong>Insight:</strong> 核心创新在于构建了一个大规模视频偏好数据集，并训练了一个分层奖励模型来捕捉与人类偏好一致的多维度奖励，进而通过一种计算高效的PPO风格算法对基于流的世界模型进行后训练对齐。</p>
<p><strong>Abstract:</strong> Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.</p>
  </div>
</details>

<hr>
<h3 id="243-Sparse-ActionGen-Accelerating-Diffusion-Policy-with-Real-time-Pruning-cs-RO-cs-CVPDF"><a href="#243-Sparse-ActionGen-Accelerating-Diffusion-Policy-with-Real-time-Pruning-cs-RO-cs-CVPDF" class="headerlink" title="[243] Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning cs.RO | cs.CVPDF"></a>[243] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12894">Sparse ActionGen: Accelerating Diffusion Policy with Real-time Pruning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12894" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kangye Ji, Yuan Meng, Zhou Jianbo, Ye Li, Hanyun Cui</span></p>
<p><strong>TL;DR:</strong> 本文提出Sparse ActionGen（SAG）方法，通过实时剪枝加速扩散策略（Diffusion Policy）的动作生成，以解决其多步去噪过程在实时视觉运动控制中不实用的问题。SAG采用基于rollout动态的自适应剪枝-重用机制，全局识别可剪枝计算并用缓存激活值替代，同时引入参数高效、推理高效的扩散剪枝器进行环境感知适配，以及跨时间步和块的zig-zag重用策略最小化冗余。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散策略虽能建模多模态动作分布，但其多步去噪过程导致实时控制困难；现有基于缓存的加速方法依赖静态调度，无法适应机器人-环境交互的动态变化，导致性能次优。</p>
<p><strong>Result:</strong> 在多个机器人基准测试上的广泛实验表明，SAG在不牺牲性能的情况下实现了高达4倍的生成加速。</p>
<p><strong>Insight:</strong> 创新点包括：rollout自适应的剪枝-重用机制、观察条件化的扩散剪枝器参数化设计以实现环境感知适配、以及跨时间步和块的zig-zag重用策略以最小化全局冗余；从客观角度看，该方法将动态适应机制引入扩散策略加速，提升了实时控制场景下的实用性和效率。</p>
<p><strong>Abstract:</strong> Diffusion Policy has dominated action generation due to its strong capabilities for modeling multi-modal action distributions, but its multi-step denoising processes make it impractical for real-time visuomotor control. Existing caching-based acceleration methods typically rely on $\textit{static}$ schedules that fail to adapt to the $\textit{dynamics}$ of robot-environment interactions, thereby leading to suboptimal performance. In this paper, we propose $\underline{\textbf{S}}$parse $\underline{\textbf{A}}$ction$\underline{\textbf{G}}$en ($\textbf{SAG}$) for extremely sparse action generation. To accommodate the iterative interactions, SAG customizes a rollout-adaptive prune-then-reuse mechanism that first identifies prunable computations globally and then reuses cached activations to substitute them during action diffusion. To capture the rollout dynamics, SAG parameterizes an observation-conditioned diffusion pruner for environment-aware adaptation and instantiates it with a highly parameter- and inference-efficient design for real-time prediction. Furthermore, SAG introduces a one-for-all reusing strategy that reuses activations across both timesteps and blocks in a zig-zag manner, minimizing the global redundancy. Extensive experiments on multiple robotic benchmarks demonstrate that SAG achieves up to 4$\times$ generation speedup without sacrificing performance. Project Page: <a target="_blank" rel="noopener" href="https://sparse-actiongen.github.io/">https://sparse-actiongen.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="244-LLM-VLM-Fusion-Framework-for-Autonomous-Maritime-Port-Inspection-using-a-Heterogeneous-UAV-USV-System-cs-RO-cs-CVPDF"><a href="#244-LLM-VLM-Fusion-Framework-for-Autonomous-Maritime-Port-Inspection-using-a-Heterogeneous-UAV-USV-System-cs-RO-cs-CVPDF" class="headerlink" title="[244] LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System cs.RO | cs.CVPDF"></a>[244] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13096">LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13096" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Muhayy Ud Din, Waseem Akram, Ahsan B. Bakht, Irfan Hussain</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的LLM-VLM融合框架，用于异构无人机-无人水面艇系统的自主海事港口巡检。该框架利用大语言模型进行符号化任务规划，并结合视觉语言模型进行语义化感知与合规性评估，以替代传统状态机规划器和常规计算机视觉方法，实现了上下文感知和自适应的监控。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有海事港口巡检方法主要依赖人工操作和缺乏可扩展性及上下文理解能力的传统计算机视觉技术，难以满足复杂海事环境下的安全、合规与高效运营需求。</p>
<p><strong>Result:</strong> 框架在扩展的MBZIRC海事模拟器（包含真实港口基础设施）中进行了验证，并进一步通过真实机器人巡检试验进行了评估。其轻量化的机载设计确保了其在资源受限的海事平台上的适用性。</p>
<p><strong>Insight:</strong> 核心创新点在于将LLM与VLM协同用于异构机器人系统的自主巡检：LLM负责将自然语言指令转化为包含依赖图和操作约束的可执行符号计划，确保无人机与无人艇的安全协调；VLM则负责实时语义巡检与合规评估，并生成带有上下文推理的结构化报告。这为智能自主巡检系统的发展提供了新思路。</p>
<p><strong>Abstract:</strong> Maritime port inspection plays a critical role in ensuring safety, regulatory compliance, and operational efficiency in complex maritime environments. However, existing inspection methods often rely on manual operations and conventional computer vision techniques that lack scalability and contextual understanding. This study introduces a novel integrated engineering framework that utilizes the synergy between Large Language Models (LLMs) and Vision Language Models (VLMs) to enable autonomous maritime port inspection using cooperative aerial and surface robotic platforms. The proposed framework replaces traditional state-machine mission planners with LLM-driven symbolic planning and improved perception pipelines through VLM-based semantic inspection, enabling context-aware and adaptive monitoring. The LLM module translates natural language mission instructions into executable symbolic plans with dependency graphs that encode operational constraints and ensure safe UAV-USV coordination. Meanwhile, the VLM module performs real-time semantic inspection and compliance assessment, generating structured reports with contextual reasoning. The framework was validated using the extended MBZIRC Maritime Simulator with realistic port infrastructure and further assessed through real-world robotic inspection trials. The lightweight on-board design ensures suitability for resource-constrained maritime platforms, advancing the development of intelligent, autonomous inspection systems. Project resources (code and videos) can be found here: <a target="_blank" rel="noopener" href="https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection">https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection</a></p>
  </div>
</details>

<hr>
<h3 id="245-Event-based-Heterogeneous-Information-Processing-for-Online-Vision-based-Obstacle-Detection-and-Localization-cs-RO-cs-CVPDF"><a href="#245-Event-based-Heterogeneous-Information-Processing-for-Online-Vision-based-Obstacle-Detection-and-Localization-cs-RO-cs-CVPDF" class="headerlink" title="[245] Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization cs.RO | cs.CVPDF"></a>[245] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.13451">Event-based Heterogeneous Information Processing for Online Vision-based Obstacle Detection and Localization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.13451" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Reza Ahmadvand, Sarah Safura Sharif, Yaser Mike Banad</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的机器人视觉导航框架，该框架将混合神经网络（HNNs）与基于脉冲神经网络（SNNs）的滤波相结合，以增强对未建模障碍物的检测与定位的情境感知能力。系统利用人工神经网络（ANNs）和SNNs的互补优势，实现了准确的环境理解和快速、节能的处理。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决机器人在不可预测和动态环境中导航时，对未建模障碍物进行快速、准确检测与定位的问题，旨在提升情境感知和导航策略的预见性。</p>
<p><strong>Result:</strong> 仿真结果表明，所提方法在保持接近纯SNN实现的计算效率（资源成本极低）的同时，提供了可接受的检测精度。</p>
<p><strong>Insight:</strong> 主要创新点在于采用了一种双通路架构：ANN通路低频处理静态空间特征，而SNN通路实时处理基于事件的动态传感器数据，并直接利用脉冲编码输入进行定位和状态估计，无需传统的域转换机制。这为神经形态导航系统在动态环境中的应用提供了新思路。</p>
<p><strong>Abstract:</strong> This paper introduces a novel framework for robotic vision-based navigation that integrates Hybrid Neural Networks (HNNs) with Spiking Neural Network (SNN)-based filtering to enhance situational awareness for unmodeled obstacle detection and localization. By leveraging the complementary strengths of Artificial Neural Networks (ANNs) and SNNs, the system achieves both accurate environmental understanding and fast, energy-efficient processing. The proposed architecture employs a dual-pathway approach: an ANN component processes static spatial features at low frequency, while an SNN component handles dynamic, event-based sensor data in real time. Unlike conventional hybrid architectures that rely on domain conversion mechanisms, our system incorporates a pre-developed SNN-based filter that directly utilizes spike-encoded inputs for localization and state estimation. Detected anomalies are validated using contextual information from the ANN pathway and continuously tracked to support anticipatory navigation strategies. Simulation results demonstrate that the proposed method offers acceptable detection accuracy while maintaining computational efficiency close to SNN-only implementations, which operate at a fraction of the resource cost. This framework represents a significant advancement in neuromorphic navigation systems for robots operating in unpredictable and dynamic environments.</p>
  </div>
</details>

<hr>
<h3 id="246-Diffusion-Guided-Backdoor-Attacks-in-Real-World-Reinforcement-Learning-cs-RO-cs-CVPDF"><a href="#246-Diffusion-Guided-Backdoor-Attacks-in-Real-World-Reinforcement-Learning-cs-RO-cs-CVPDF" class="headerlink" title="[246] Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning cs.RO | cs.CVPDF"></a>[246] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14104">Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14104" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tairan Huang, Qingqing Ye, Yulin Jin, Jiawei Lian, Yi Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对现实世界强化学习（RL）系统的扩散引导后门攻击框架（DGBA），通过使用条件扩散模型生成多样化的视觉补丁触发器，并采用基于优势的投毒策略，在保持正常任务性能的同时，在物理机器人（TurtleBot3）上实现了可靠的目标攻击激活。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有后门攻击主要在仿真环境中验证，其在现实世界机器人系统中因安全约束控制管道（如速度限制、动作平滑、碰撞避免）的抑制而效果不佳，本文旨在解决这一被忽视的问题。</p>
<p><strong>Result:</strong> 在TurtleBot3移动机器人上的评估表明，该方法能够可靠激活目标攻击，同时保持正常任务性能。</p>
<p><strong>Insight:</strong> 创新点包括：利用条件扩散模型生成适应现实世界视觉变化的多样化触发器；将机器人控制栈视为黑盒系统；引入基于优势的投毒策略，仅在决策关键训练状态注入触发器，以提高攻击的隐蔽性和有效性。</p>
<p><strong>Abstract:</strong> Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.</p>
  </div>
</details>

<hr>
<h3 id="247-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers-cs-RO-cs-CVPDF"><a href="#247-TwinBrainVLA-Unleashing-the-Potential-of-Generalist-VLMs-for-Embodied-Tasks-via-Asymmetric-Mixture-of-Transformers-cs-RO-cs-CVPDF" class="headerlink" title="[247] TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers cs.RO | cs.CVPDF"></a>[247] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14133">TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14133" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen</span></p>
<p><strong>TL;DR:</strong> 本文提出TwinBrainVLA模型，通过非对称混合Transformer机制协调通用视觉语言模型和专用具身感知模型，以解决标准VLA模型在微调时存在的通用语义理解与精细运动技能学习之间的冲突，在保持预训练模型开放世界能力的同时提升机器人操控性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决标准视觉-语言-动作模型在微调过程中因专注于低层次传感器运动技能学习而导致的对高层次通用语义理解的灾难性遗忘问题。</p>
<p><strong>Result:</strong> 在SimplerEnv和RoboCasa基准测试中，TwinBrainVLA相比现有最先进基线取得了更优的操作性能，同时明确保留了预训练VLM的全面视觉理解能力。</p>
<p><strong>Insight:</strong> 创新性地采用非对称混合Transformer架构，将冻结的通用VLM作为‘左脑’与可训练的专用具身感知VLM作为‘右脑’协同工作，使专用模型能动态查询通用语义知识并与本体感知状态融合，为生成精确连续控制提供丰富条件。</p>
<p><strong>Abstract:</strong> Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to “catastrophic forgetting” of the model’s open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen “Left Brain”, which retains robust general visual reasoning, with a trainable “Right Brain”, specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.</p>
  </div>
</details>

<hr>
<div id='cs.GR'></div>

<h1 id="cs-GR-Back"><a href="#cs-GR-Back" class="headerlink" title="cs.GR [Back]"></a>cs.GR <a href="#toc">[Back]</a></h1><h3 id="248-Copy-Trasform-Paste-Zero-Shot-Object-Object-Alignment-Guided-by-Vision-Language-and-Geometric-Constraints-cs-GR-cs-CVPDF"><a href="#248-Copy-Trasform-Paste-Zero-Shot-Object-Object-Alignment-Guided-by-Vision-Language-and-Geometric-Constraints-cs-GR-cs-CVPDF" class="headerlink" title="[248] Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints cs.GR | cs.CVPDF"></a>[248] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14207">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.GR | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14207" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rotem Gatenyo, Ohad Fried</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种零样本3D网格对齐方法，通过文本提示描述物体间的空间关系，利用CLIP驱动的梯度和可微分渲染器直接优化相对位姿（平移、旋转和各向同性缩放），并结合几何感知目标（如软ICP项和穿透损失）来增强语言监督，从而在无需训练新模型的情况下实现语义准确且物理合理的对齐。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决零样本3D物体对齐问题，即根据文本描述的空间关系对齐两个给定网格，这是内容创建和场景组装中的关键能力；现有方法主要依赖几何对齐或预训练的2D扩散模型，而本文旨在通过直接优化和结合语言与几何约束来改进对齐效果。</p>
<p><strong>Result:</strong> 在包含多样类别和关系的基准测试中，该方法优于所有基线，实现了语义忠实且物理合理的对齐效果，达到了当前最佳水平（SOTA）。</p>
<p><strong>Insight:</strong> 创新点包括：直接优化相对位姿的零样本框架，无需训练新模型；结合CLIP驱动的语言监督与几何感知目标（软ICP和穿透损失）；采用分阶段调度和相机控制来强化接触约束和聚焦交互区域。从客观角度看，该方法有效融合了视觉语言模型与几何约束，提升了3D对齐的语义和物理合理性。</p>
<p><strong>Abstract:</strong> We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation – an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</p>
  </div>
</details>

<hr>
<div id='q-bio.QM'></div>

<h1 id="q-bio-QM-Back"><a href="#q-bio-QM-Back" class="headerlink" title="q-bio.QM [Back]"></a>q-bio.QM <a href="#toc">[Back]</a></h1><h3 id="249-Karhunen-Loeve-Expansion-Based-Residual-Anomaly-Map-for-Resource-Efficient-Glioma-MRI-Segmentation-q-bio-QM-cs-CV-cs-LG-eess-IVPDF"><a href="#249-Karhunen-Loeve-Expansion-Based-Residual-Anomaly-Map-for-Resource-Efficient-Glioma-MRI-Segmentation-q-bio-QM-cs-CV-cs-LG-eess-IVPDF" class="headerlink" title="[249] Karhunen-Loève Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation q-bio.QM | cs.CV | cs.LG | eess.IVPDF"></a>[249] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11833">Karhunen-Loève Expansion-Based Residual Anomaly Map for Resource-Efficient Glioma MRI Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">q-bio.QM | cs.CV | cs.LG | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11833" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anthony Hur</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于Karhunen-Loève展开（KLE）的残差异常图方法，用于资源高效的脑胶质瘤MRI分割。该方法通过KLE对下采样、归一化的多模态MRI扫描进行特征提取和压缩，生成残差异常图作为额外通道输入到一个紧凑的3D U-Net中，从而在显著降低计算资源和数据需求的同时，保持了与最先进方法相当的分割性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决当前最先进的脑肿瘤分割深度学习模型对大规模数据集和巨大计算资源（如超级计算机）的严重依赖问题，使其在资源受限（如消费级工作站）和数据有限的环境中也能实现高性能分割。</p>
<p><strong>Result:</strong> 在BraTS GLI 2023数据集上，该方法在消费级工作站上训练，取得了后处理的Dice分数：全肿瘤（WT）0.929，肿瘤核心（TC）0.856，增强肿瘤（ET）0.821；HD95距离分别为2.93、6.78和10.35体素。在HD95距离和WT Dice分数上显著优于BraTS 2023的获胜方法。</p>
<p><strong>Insight:</strong> 主要创新点在于将KLE作为一种高效的特征提取和压缩工具，用于生成残差异常图，该图能有效捕捉重建误差信息，作为额外的输入通道来增强紧凑模型的分割能力。这为在资源受限条件下实现高性能医学图像分割提供了一种新颖的、数据&#x2F;计算高效的预处理和特征增强范式。</p>
<p><strong>Abstract:</strong> Accurate segmentation of brain tumors is essential for clinical diagnosis and treatment planning. Deep learning is currently the state-of-the-art for brain tumor segmentation, yet it requires either large datasets or extensive computational resources that are inaccessible in most areas. This makes the problem increasingly difficult: state-of-the-art models use thousands of training cases and vast computational power, where performance drops sharply when either is limited. The top performer in the Brats GLI 2023 competition relied on supercomputers trained on over 92,000 augmented MRI scans using an AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM over multiple weeks. To address this, the Karhunen–Loève Expansion (KLE) was implemented as a feature extraction step on downsampled, z-score normalized MRI volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four $48^3$ channels and compressed into 32 KL coefficients. The resulting approximate reconstruction enables a residual-based anomaly map, which is upsampled and added as a fifth channel to a compact 3D U-Net. All experiments were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB VRAM), and 64GB RAM while using far fewer training cases. This model achieves post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95 distances of 2.93, 6.78, and 10.35 voxels. These results are significantly better than the winning BraTS 2023 methodology for HD95 distances and WT dice scores. This demonstrates that a KLE-based residual anomaly map can dramatically reduce computational cost and data requirements while retaining state-of-the-art performance.</p>
  </div>
</details>

<hr>
<div id='q-bio.NC'></div>

<h1 id="q-bio-NC-Back"><a href="#q-bio-NC-Back" class="headerlink" title="q-bio.NC [Back]"></a>q-bio.NC <a href="#toc">[Back]</a></h1><h3 id="250-MooneyMaker-A-Python-package-to-create-ambiguous-two-tone-images-q-bio-NC-cs-CVPDF"><a href="#250-MooneyMaker-A-Python-package-to-create-ambiguous-two-tone-images-q-bio-NC-cs-CVPDF" class="headerlink" title="[250] MooneyMaker: A Python package to create ambiguous two-tone images q-bio.NC | cs.CVPDF"></a>[250] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14077">MooneyMaker: A Python package to create ambiguous two-tone images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">q-bio.NC | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.14077" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lars C. Reining, Thabo Matthies, Luisa Haussner, Rabea Turon, Thomas S. A. Wallis</span></p>
<p><strong>TL;DR:</strong> MooneyMaker是一个开源的Python包，用于自动化生成模糊的双色调Mooney图像，这些图像通过阈值化处理照片获得，用于视觉感知研究。该工具提供多种生成方法，包括基于图像统计和深度学习的技术，旨在平衡图像的初始不可识别性和在呈现原始模板后的可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统上，研究人员手动创建Mooney图像，这既耗时又可能导致研究间的不一致；MooneyMaker旨在通过自动化生成过程，提供标准化、可复现的视觉刺激创建方案，以支持更一致的视觉感知研究。</p>
<p><strong>Result:</strong> 实验验证了MooneyMaker生成图像的有效性：使用不同技术生成的Mooney图像，初始识别率越低，在呈现模板后的识别率越高（即去模糊效应越大），这为技术选择提供了实用指南。</p>
<p><strong>Insight:</strong> 创新点在于整合了多种互补的自动化生成方法（从图像统计到深度学习），通过策略性改变边缘信息来增加初始模糊性，并允许用户直接比较不同方法的结果，从而标准化了Mooney图像的创建过程，提升了研究的可重复性。</p>
<p><strong>Abstract:</strong> Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.</p>
  </div>
</details>

<hr>
<div id='cs.SE'></div>

<h1 id="cs-SE-Back"><a href="#cs-SE-Back" class="headerlink" title="cs.SE [Back]"></a>cs.SE <a href="#toc">[Back]</a></h1><h3 id="251-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey-cs-SE-cs-CLPDF"><a href="#251-Advances-and-Frontiers-of-LLM-based-Issue-Resolution-in-Software-Engineering-A-Comprehensive-Survey-cs-SE-cs-CLPDF" class="headerlink" title="[251] Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey cs.SE | cs.CLPDF"></a>[251] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11655">Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SE | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11655" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Caihua Li, Lianghong Guo, Yanlin Wang, Daya Guo, Wei Tao</span></p>
<p><strong>TL;DR:</strong> 本文对基于大语言模型（LLM）的软件工程问题解决领域进行了系统性综述，涵盖数据构建、方法论（包括免训练框架与基于训练的技术）、关键分析、实际应用，并指出了未来研究挑战与方向。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决软件工程中的问题（Issue Resolution）是现实开发中的复杂任务，已成为人工智能面临的重要挑战；SWE-bench等基准的建立表明LLM在此任务上存在显著困难，这推动了自主编码代理的发展，本文旨在系统梳理这一新兴领域。</p>
<p><strong>Result:</strong> 本文为综述性论文，未报告具体定量结果，但全面分析了该领域的方法论、数据质量与代理行为，并总结了现有技术进展。</p>
<p><strong>Insight:</strong> 创新点在于对LLM在软件工程问题解决领域的系统化梳理，包括数据构建、方法论分类（训练免费与训练基）以及关键挑战的识别；客观来看，该综述为研究人员提供了结构化知识框架与开源资源，有助于推动该领域发展。</p>
<p><strong>Abstract:</strong> Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at <a target="_blank" rel="noopener" href="https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution">https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution</a> to serve as a dynamic resource in this field.</p>
  </div>
</details>

<hr>
<h3 id="252-The-Stability-Trap-Evaluating-the-Reliability-of-LLM-Based-Instruction-Adherence-Auditing-cs-SE-cs-CLPDF"><a href="#252-The-Stability-Trap-Evaluating-the-Reliability-of-LLM-Based-Instruction-Adherence-Auditing-cs-SE-cs-CLPDF" class="headerlink" title="[252] The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing cs.SE | cs.CLPDF"></a>[252] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11783">The Stability Trap: Evaluating the Reliability of LLM-Based Instruction Adherence Auditing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SE | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.11783" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Murtuza N. Shergadwala</span></p>
<p><strong>TL;DR:</strong> 本研究探讨了基于大语言模型的指令遵循审计的可靠性问题，通过提出的范围化指令分解框架，揭示了在评估不同类型指令时存在的’稳定性陷阱’，即判决稳定性与推理稳定性之间的脱节现象。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在受监管领域（如人力资源）中，企业需要对生成式AI进行可扩展且可复现的审计，而基于LLM的评估方法虽然提供了可扩展性，但其在评估不同类型系统指令遵循性方面的可靠性尚未得到验证。</p>
<p><strong>Result:</strong> 研究在两个代表性的人力资源生成式AI应用上评估了四种法官架构的稳定性，结果显示，对于客观和主观评估，判决一致性均接近完美（&gt;99%），但推理稳定性差异巨大：客观定量分析指令（如字数统计）的推理稳定性低至约19%，而专注于离散实体提取的客观指令则实现了高推理稳定性（&gt;90%）；主观指令的推理稳定性则在35%到83%之间波动。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了范围化指令分解框架来分类和隔离导致评估不稳定的因素，并揭示了高判决稳定性可能掩盖脆弱的推理过程这一关键洞察；从客观角度看，其核心贡献是指出了自动化评估协议应严格限定范围：将所有可确定性验证的逻辑委托给代码执行，而将LLM法官仅用于复杂的语义评估。</p>
<p><strong>Abstract:</strong> The enterprise governance of Generative AI (GenAI) in regulated sectors, such as Human Resources (HR), demands scalable yet reproducible auditing mechanisms. While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their reliability in evaluating adherence of different types of system instructions remains unverified. This study asks: To what extent does the instruction type of an Application Under Test (AUT) influence the stability of judge evaluations? To address this, we introduce the Scoped Instruction Decomposition Framework to classify AUT instructions into Objective and Subjective types, isolating the factors that drive judge instability. We applied this framework to two representative HR GenAI applications, evaluating the stability of four judge architectures over variable runs. Our results reveal a &#96;&#96;Stability Trap’’ characterized by a divergence between Verdict Stability and Reasoning Stability. While judges achieved near-perfect verdict agreement ($&gt;99%$) for both objective and subjective evaluations, their accompanying justification traces diverged significantly. Objective instructions requiring quantitative analysis, such as word counting, exhibited reasoning stability as low as $\approx19%$, driven by variances in numeric justifications. Similarly, reasoning stability for subjective instructions varied widely ($35%$–$83%$) based on evidence granularity, with feature-specific checks failing to reproduce consistent rationale. Conversely, objective instructions focusing on discrete entity extraction achieved high reasoning stability ($&gt;90%$). These findings demonstrate that high verdict stability can mask fragile reasoning. Thus, we suggest that auditors scope automated evaluation protocols strictly: delegate all deterministically verifiable logic to code, while reserving LLM judges for complex semantic evaluation.</p>
  </div>
</details>

<hr>
<div id='cs.SD'></div>

<h1 id="cs-SD-Back"><a href="#cs-SD-Back" class="headerlink" title="cs.SD [Back]"></a>cs.SD <a href="#toc">[Back]</a></h1><h3 id="253-Harmonizing-the-Arabic-Audio-Space-with-Data-Scheduling-cs-SD-cs-AI-cs-CL-eess-ASPDF"><a href="#253-Harmonizing-the-Arabic-Audio-Space-with-Data-Scheduling-cs-SD-cs-AI-cs-CL-eess-ASPDF" class="headerlink" title="[253] Harmonizing the Arabic Audio Space with Data Scheduling cs.SD | cs.AI | cs.CL | eess.ASPDF"></a>[253] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.12494">Harmonizing the Arabic Audio Space with Data Scheduling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SD | cs.AI | cs.CL | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2601.12494" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury</span></p>
<p><strong>TL;DR:</strong> 本文首次系统研究了面向阿拉伯语的多任务指令微调音频大语言模型，涵盖生成任务（如语音识别、语音摘要）和判别任务（如方言和情感识别）。研究引入了阿拉伯语语音摘要数据集AraMega-SSum，并基于Qwen2.5-Omni (7B)模型，提出了任务渐进课程学习（TPC）和对齐器引导的多样化采样（ADS）策略，以构建信息密集的批次。研究发现ADS能加速初始收敛并提升副语言F1分数，但长期训练可能导致生成解码不稳定；TPC能稳定核心声学映射，但可能引发下游任务的负迁移。最终，混合TPC+ADS策略被证明为最优训练方案，先在复杂低资源多模态环境中建立稳健基础，再通过多样性感知优化捕捉细粒度差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决音频大语言模型在语言复杂、方言丰富的阿拉伯语环境中适应不足的问题，探索多任务指令微调的有效方法。</p>
<p><strong>Result:</strong> 在阿拉伯语语音摘要数据集AraMega-SSum上实验，ADS策略提升了副语言任务（如方言和情感识别）的F1分数，但生成任务（如ASR）在长期训练中可能不稳定；TPC稳定了核心声学映射，但可能对下游任务产生负迁移；混合TPC+ADS策略在效率和鲁棒性上达到最优平衡，为低资源多模态环境提供了实用训练指导。</p>
<p><strong>Insight:</strong> 创新点包括引入阿拉伯语语音摘要数据集AraMega-SSum，以及提出任务渐进课程学习（TPC）和对齐器引导的多样化采样（ADS）策略。从客观角度看，论文揭示了多任务音频LLM训练中效率与鲁棒性的权衡，并通过混合策略优化了这一过程，为复杂低资源语言的模型适配提供了可借鉴的方法论。</p>
<p><strong>Abstract:</strong> Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training &#96;&#96;recipe’’, first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2026-01-23/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2026-01-20/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
