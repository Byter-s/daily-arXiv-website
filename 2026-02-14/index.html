<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2026-02-14 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2026-02-14
            
            
        </div>
        <span class="post-date">
            Feb 14, 2026
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 33]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 43]</li>
<li><a href="#eess.SP">eess.SP</a> [Total: 1]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 1]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 3]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 1]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 1]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 10]</li>
<li><a href="#cs.NE">cs.NE</a> [Total: 1]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 15]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-Response-Based-Knowledge-Distillation-for-Multilingual-Jailbreak-Prevention-Unwittingly-Compromises-Safety-cs-CLPDF"><a href="#1-Response-Based-Knowledge-Distillation-for-Multilingual-Jailbreak-Prevention-Unwittingly-Compromises-Safety-cs-CLPDF" class="headerlink" title="[1] Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety cs.CLPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11157">Response-Based Knowledge Distillation for Multilingual Jailbreak Prevention Unwittingly Compromises Safety</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11157" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Max Zhang, Derek Liu, Kai Zhang, Joshua Franco, Haihao Liu</span></p>
<p><strong>TL;DR:</strong> 本文探讨了在大型语言模型（LLMs）多语言安全对齐中应用知识蒸馏（KD）的挑战。研究通过基于响应的参数高效微调（PEFT），将专有教师模型（OpenAI o1-mini）的拒绝行为蒸馏到三个开源学生模型（Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, Qwen3-8B）中，以应对多语言越狱攻击。然而，评估发现，对教师模型“安全”拒绝数据的标准微调反而意外地提高了所有学生模型的越狱成功率（JSR），最高达16.6个百分点。通过移除导致安全性能下降的主要来源——微妙的“边界”拒绝，可以缓解甚至逆转安全性能下降，但推理性能（GSM8K）的降低仍然存在。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLMs）的部署日益全球化，但其安全对齐主要集中于英语，导致在非英语（尤其是低资源语言）环境中存在漏洞。本研究旨在探索知识蒸馏（KD）作为一种技术，在多语言越狱预防中的有效性和潜在挑战。</p>
<p><strong>Result:</strong> 在MultiJail基准测试上的评估显示，标准微调反而提高了所有学生模型的越狱成功率（JSR），最高达16.6个百分点。通过移除“边界”拒绝数据，可以缓解安全性能下降，但推理性能（在GSM8K基准上）持续降低。</p>
<p><strong>Insight:</strong> 论文的创新点在于首次在多语言安全对齐的背景下系统性地应用基于响应的知识蒸馏，并揭示了其反直觉的副作用（即安全性能下降）。客观分析认为，其核心洞察在于：1） 在多语言安全蒸馏中，对教师模型“安全”响应的简单模仿可能导致学生模型泛化到未见语言时出现安全漏洞；2） 微妙的“边界”拒绝是导致安全性能下降的关键因素，移除它们可以改善安全性能，但会牺牲推理能力，这揭示了安全与能力之间的权衡。这为未来多语言安全对齐研究提供了重要的警示和基础。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly deployed worldwide, yet their safety alignment remains predominantly English-centric. This allows for vulnerabilities in non-English contexts, especially with low-resource languages. We introduce a novel application of knowledge distillation (KD) in the context of multilingual jailbreak prevention, examining its efficacy. We distill the refusal behaviors of a proprietary teacher model (OpenAI o1-mini) with Low-Rank Adaptation (LoRA) into three open-source student models: Meta-Llama-3-8B-Instruct, Gemma-2-2B-IT, and Qwen3-8B, using ~28,000 multilingual jailbreak prompts from XSafety via black-box response-based, parameter-efficient fine-tuning (PEFT). Evaluation on the MultiJail benchmark reveals a counterintuitive behavior: standard fine-tuning on the teacher’s &#96;&#96;safe’’ refusal data inadvertently increases Jailbreak Success Rate (JSR) for all student models, up to 16.6 percentage points. Our experiments reveal a divergent generalization to unseen languages during distillation, with varying outcomes depending on the base model. By removing a primary source of safety degradation, nuanced &#96;boundary’ refusals, we mitigate or even reverse safety declines in student models, although reductions in reasoning performance (GSM8K) persist. Overall, our exploratory study highlights the challenges and potential of KD as a technique for multilingual safety alignment, offering a foundation for future research in this direction.</p>
  </div>
</details>

<hr>
<h3 id="2-PRIME-Policy-Reinforced-Iterative-Multi-agent-Execution-for-Algorithmic-Reasoning-in-Large-Language-Models-cs-CLPDF"><a href="#2-PRIME-Policy-Reinforced-Iterative-Multi-agent-Execution-for-Algorithmic-Reasoning-in-Large-Language-Models-cs-CLPDF" class="headerlink" title="[2] PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models cs.CLPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11170">PRIME: Policy-Reinforced Iterative Multi-agent Execution for Algorithmic Reasoning in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11170" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiawei Xu, Zhenyu Yu, Ziqian Bi, Minh Duc Pham, Xiaoyi Qu</span></p>
<p><strong>TL;DR:</strong> 本文提出PRIME框架，通过三个专门代理（执行器、验证器、协调器）和群体相对策略优化，提升大语言模型在算法推理任务上的性能，并在新构建的大规模基准PRIME-Bench上实现了从26.8%到93.8%的准确率提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大语言模型在算法推理任务上表现有限的问题，特别是处理需要持续状态跟踪的复杂算法时错误传播导致的失败。</p>
<p><strong>Result:</strong> 在PRIME-Bench（包含86个任务、51,600个实例）上，PRIME将平均准确率从基线的26.8%提升至93.8%（相对提升250%），在图灵机模拟（9%→92%）和长除法（16%→94%）等任务上提升显著；消融研究表明迭代验证是关键贡献者，且小模型（8B参数）能达到与大8倍模型相当的准确率。</p>
<p><strong>Insight:</strong> 创新点在于采用策略强化的多代理迭代执行框架，通过分工协作（执行、验证、回溯控制）和群体相对策略优化来防止错误传播；客观来看，其构建的大规模、多样化的算法推理基准和验证机制的设计对提升模型鲁棒性具有借鉴意义。</p>
<p><strong>Abstract:</strong> Large language models have demonstrated remarkable capabilities across diverse reasoning tasks, yet their performance on algorithmic reasoning remains limited. To handle this limitation, we propose PRIME (Policy-Reinforced Iterative Multi-agent Execution), a framework comprising three specialized agents, an executor for step-by-step reasoning, a verifier for constraint checking, and a coordinator for backtracking control, optimized through group relative policy optimization. For comprehensive evaluation, we introduce PRIME-Bench, the largest algorithmic reasoning benchmark to date, comprising 86 tasks across 12 categories with 51,600 instances. Tasks span sorting algorithms, graph and tree structures, automata and state machines, symbolic reasoning, and constraint-based puzzles, with execution traces reaching over one million steps. Compared to baseline approach, PRIME improves average accuracy from 26.8% to 93.8%, a 250% relative gain. The largest improvements occur on tasks requiring sustained state tracking, with Turing machine simulation improving from 9% to 92% and long division from 16% to 94%. Ablation studies identify iterative verification as the primary contributor, preventing the error propagation that causes baseline approaches to fail catastrophically. Analysis across model scales (8B-120B parameters) reveals that smaller models benefit disproportionately, achieving accuracy comparable to models 8x larger.</p>
  </div>
</details>

<hr>
<h3 id="3-Author-in-the-Loop-Response-Generation-and-Evaluation-Integrating-Author-Expertise-and-Intent-in-Responses-to-Peer-Review-cs-CLPDF"><a href="#3-Author-in-the-Loop-Response-Generation-and-Evaluation-Integrating-Author-Expertise-and-Intent-in-Responses-to-Peer-Review-cs-CLPDF" class="headerlink" title="[3] Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review cs.CLPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11173">Author-in-the-Loop Response Generation and Evaluation: Integrating Author Expertise and Intent in Responses to Peer Review</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11173" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qian Ruan, Iryna Gurevych</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种作者参与循环的同行评审回复生成框架REspGen和评估套件REspEval，通过整合作者的专业知识和意图（如领域知识、私有信息、修订策略）来辅助生成高质量的审稿回复，并构建了首个大规模对齐的评审-回复-修订三元组数据集Re$^3$Align以支持该任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有自动生成同行评审回复的方法未能充分利用作者的专业知识和意图，而实践中作者需要整合这些信号来有效回应审稿意见，因此需要设计能够融合作者输入的自然语言处理辅助工具。</p>
<p><strong>Result:</strong> 实验使用最先进的LLMs进行，结果表明作者输入和评估引导的优化能提升回复质量，输入设计对质量有影响，并在可控性和质量之间存在权衡。</p>
<p><strong>Insight:</strong> 创新点在于将作者回复生成重新定义为作者参与循环的任务，提出了整合显式作者输入、多属性控制和评估引导优化的生成框架，以及覆盖输入利用、可控性、回复质量和话语结构的全面评估指标，并构建了首个对齐的三元组数据集以捕捉作者专业知识和意图信号。</p>
<p><strong>Abstract:</strong> Author response (rebuttal) writing is a critical stage of scientific peer review that demands substantial author effort. Recent work frames this task as automatic text generation, underusing author expertise and intent. In practice, authors possess domain expertise, author-only information, revision and response strategies–concrete forms of author expertise and intent–to address reviewer concerns, and seek NLP assistance that integrates these signals to support effective response writing in peer review. We reformulate author response generation as an author-in-the-loop task and introduce REspGen, a generation framework that integrates explicit author input, multi-attribute control, and evaluation-guided refinement, together with REspEval, a comprehensive evaluation suite with 20+ metrics covering input utilization, controllability, response quality, and discourse. To support this formulation, we construct Re$^3$Align, the first large-scale dataset of aligned review–response–revision triplets, where revisions provide signals of author expertise and intent. Experiments with state-of-the-art LLMs show the benefits of author input and evaluation-guided refinement, the impact of input design on response quality, and trade-offs between controllability and quality. We make our dataset, generation and evaluation tools publicly available.</p>
  </div>
</details>

<hr>
<h3 id="4-Barriers-to-Discrete-Reasoning-with-Transformers-A-Survey-Across-Depth-Exactness-and-Bandwidth-cs-CLPDF"><a href="#4-Barriers-to-Discrete-Reasoning-with-Transformers-A-Survey-Across-Depth-Exactness-and-Bandwidth-cs-CLPDF" class="headerlink" title="[4] Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth cs.CLPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11175">Barriers to Discrete Reasoning with Transformers: A Survey Across Depth, Exactness, and Bandwidth</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11175" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Michelle Yuan, Weiyi Sun, Amir H. Rezaeian, Jyotika Singh, Sandip Ghoshal</span></p>
<p><strong>TL;DR:</strong> 本文是一篇综述性论文，系统性地探讨了Transformer架构在执行离散推理任务（如算术、逻辑推理和算法组合）时面临的理论局限性。论文从电路复杂性、近似理论和通信复杂性三个理论视角，综合分析了Transformer在深度约束、难以精确逼近不连续性以及token间通信瓶颈等方面的结构性障碍。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管Transformer已成为序列建模的基础架构，并在自然语言处理和视觉等领域取得了最先进的性能，但其在离散推理任务中的理论局限性仍是一个关键且未解决的问题。本文旨在通过整合多个理论框架，阐明Transformer在执行符号计算时遇到的根本性障碍。</p>
<p><strong>Result:</strong> 本文是一篇理论综述，未提出新模型或进行具体实验，因此未报告定量结果或基准测试。它综合了现有研究，从理论角度分析了Transformer的局限性。</p>
<p><strong>Insight:</strong> 论文的创新之处在于将电路复杂性、近似理论和通信复杂性这三个不同的理论框架统一起来，为理解Transformer在离散推理中的失败提供了一个系统且易于理解的理论解释。这为未来模型设计（如克服深度限制或通信瓶颈）指明了有前景的研究方向。</p>
<p><strong>Abstract:</strong> Transformers have become the foundational architecture for a broad spectrum of sequence modeling applications, underpinning state-of-the-art systems in natural language processing, vision, and beyond. However, their theoretical limitations in discrete reasoning tasks, such as arithmetic, logical inference, and algorithmic composition, remain a critical open problem. In this survey, we synthesize recent studies from three theoretical perspectives: circuit complexity, approximation theory, and communication complexity, to clarify the structural and computational barriers that transformers face when performing symbolic computations. By connecting these established theoretical frameworks, we provide an accessible and unified account of why current transformer architectures struggle to implement exact discrete algorithms, even as they excel at pattern matching and interpolation. We review key definitions, seminal results, and illustrative examples, highlighting challenges such as depth constraints, difficulty approximating discontinuities, and bottlenecks in inter-token communication. Finally, we discuss implications for model design and suggest promising directions for overcoming these foundational limitations.</p>
  </div>
</details>

<hr>
<h3 id="5-Evaluating-Few-Shot-Temporal-Reasoning-of-LLMs-for-Human-Activity-Prediction-in-Smart-Environments-cs-CL-cs-AI-cs-CE-cs-LGPDF"><a href="#5-Evaluating-Few-Shot-Temporal-Reasoning-of-LLMs-for-Human-Activity-Prediction-in-Smart-Environments-cs-CL-cs-AI-cs-CE-cs-LGPDF" class="headerlink" title="[5] Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments cs.CL | cs.AI | cs.CE | cs.LGPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11176">Evaluating Few-Shot Temporal Reasoning of LLMs for Human Activity Prediction in Smart Environments</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CE | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11176" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maral Doctorarastoo, Katherine A. Flanigan, Mario Bergés, Christopher McComb</span></p>
<p><strong>TL;DR:</strong> 本文研究了大型语言模型在低数据环境下对人类活动进行时序推理的能力，应用于智能环境中的活动预测。通过采用检索增强提示策略整合时间、空间、行为历史和人物角色四种上下文信息，并在CASAS Aruba智能家居数据集上评估了模型在零样本和少样本设置下的表现。结果表明，LLMs展现出强大的内在时序理解能力，能够生成连贯的日常活动预测，少量示例即可显著提升持续时间校准和分类准确性，但性能在超过几个示例后趋于饱和。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有数据驱动的基于代理的模型（从基于规则到深度学习）在低数据环境中表现不佳的问题，探索利用预训练于广泛人类知识的大型语言模型，通过紧凑的上下文线索对日常活动进行推理，以增强智能环境应用中的适应性系统。</p>
<p><strong>Result:</strong> 在CASAS Aruba智能家居数据集上评估了两个互补任务：带持续时间估计的下一个活动预测和多步日常序列生成。结果显示，即使在零样本设置下，LLMs也能产生连贯的预测；添加一两个少样本示例可进一步优化持续时间校准和分类准确性，但超过几个示例后性能饱和，表明收益递减。序列级评估证实了跨少样本条件的一致时序对齐。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种检索增强提示策略，整合多维上下文进行时序推理，并系统评估了少样本效应以平衡数据效率和预测准确性。客观分析表明，预训练语言模型可作为有前景的时序推理器，捕捉重复性常规和上下文依赖的行为变化，从而强化基于代理模型的行为模块，尤其在低数据环境中具有实用潜力。</p>
<p><strong>Abstract:</strong> Anticipating human activities and their durations is essential in applications such as smart-home automation, simulation-based architectural and urban design, activity-based transportation system simulation, and human-robot collaboration, where adaptive systems must respond to human activities. Existing data-driven agent-based models–from rule-based to deep learning–struggle in low-data environments, limiting their practicality. This paper investigates whether large language models, pre-trained on broad human knowledge, can fill this gap by reasoning about everyday activities from compact contextual cues. We adopt a retrieval-augmented prompting strategy that integrates four sources of context–temporal, spatial, behavioral history, and persona–and evaluate it on the CASAS Aruba smart-home dataset. The evaluation spans two complementary tasks: next-activity prediction with duration estimation, and multi-step daily sequence generation, each tested with various numbers of few-shot examples provided in the prompt. Analyzing few-shot effects reveals how much contextual supervision is sufficient to balance data efficiency and predictive accuracy, particularly in low-data environments. Results show that large language models exhibit strong inherent temporal understanding of human behavior: even in zero-shot settings, they produce coherent daily activity predictions, while adding one or two demonstrations further refines duration calibration and categorical accuracy. Beyond a few examples, performance saturates, indicating diminishing returns. Sequence-level evaluation confirms consistent temporal alignment across few-shot conditions. These findings suggest that pre-trained language models can serve as promising temporal reasoners, capturing both recurring routines and context-dependent behavioral variations, thereby strengthening the behavioral modules of agent-based models.</p>
  </div>
</details>

<hr>
<h3 id="6-Mechanistic-Interpretability-for-Large-Language-Model-Alignment-Progress-Challenges-and-Future-Directions-cs-CLPDF"><a href="#6-Mechanistic-Interpretability-for-Large-Language-Model-Alignment-Progress-Challenges-and-Future-Directions-cs-CLPDF" class="headerlink" title="[6] Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions cs.CLPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11180">Mechanistic Interpretability for Large Language Model Alignment: Progress, Challenges, and Future Directions</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11180" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Usman Naseem</span></p>
<p><strong>TL;DR:</strong> 本文综述了大型语言模型（LLM）对齐领域中机制可解释性（Mechanistic Interpretability）的研究进展、挑战与未来方向。它探讨了从电路发现到特征可视化、激活引导和因果干预等一系列技术，分析了这些可解释性见解如何为RLHF、宪法AI和可扩展监督等对齐策略提供信息，并指出了叠加假设、神经元多义性等关键挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLM在各种任务上表现出色，但其内部决策过程仍不透明，机制可解释性研究对于理解和对齐这些模型至关重要。</p>
<p><strong>Result:</strong> 本文是一篇综述性论文，未报告具体的定量实验结果，但系统梳理了该领域的研究进展并分析了现有方法。</p>
<p><strong>Insight:</strong> 论文强调将机制可解释性见解直接应用于对齐策略（如RLHF）的重要性，并提出了自动化可解释性、跨模型电路泛化以及可扩展至前沿模型的可解释性驱动对齐技术等未来方向。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have achieved remarkable capabilities across diverse tasks, yet their internal decision-making processes remain largely opaque. Mechanistic interpretability (i.e., the systematic study of how neural networks implement algorithms through their learned representations and computational structures) has emerged as a critical research direction for understanding and aligning these models. This paper surveys recent progress in mechanistic interpretability techniques applied to LLM alignment, examining methods ranging from circuit discovery to feature visualization, activation steering, and causal intervention. We analyze how interpretability insights have informed alignment strategies including reinforcement learning from human feedback (RLHF), constitutional AI, and scalable oversight. Key challenges are identified, including the superposition hypothesis, polysemanticity of neurons, and the difficulty of interpreting emergent behaviors in large-scale models. We propose future research directions focusing on automated interpretability, cross-model generalization of circuits, and the development of interpretability-driven alignment techniques that can scale to frontier models.</p>
  </div>
</details>

<hr>
<h3 id="7-MetaMem-Evolving-Meta-Memory-for-Knowledge-Utilization-through-Self-Reflective-Symbolic-Optimization-cs-CLPDF"><a href="#7-MetaMem-Evolving-Meta-Memory-for-Knowledge-Utilization-through-Self-Reflective-Symbolic-Optimization-cs-CLPDF" class="headerlink" title="[7] MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization cs.CLPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11182">MetaMem: Evolving Meta-Memory for Knowledge Utilization through Self-Reflective Symbolic Optimization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11182" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haidong Xin, Xinze Li, Zhenghao Liu, Yukun Yan, Shuo Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了MetaMem框架，通过自演化的元记忆来增强大语言模型（LLM）的记忆系统，旨在教会LLM如何有效利用已记忆的知识。该框架通过自我反思推理过程并执行操作来更新元记忆状态，从而迭代地提炼跨任务的可迁移知识利用经验，以指导LLM从分散的记忆片段中系统性地识别和整合关键证据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有记忆系统虽然能支持LLM进行长程人机交互，但往往会破坏交互会话中固有的逻辑和时间关系，导致记忆单元碎片化和推理性能下降。本文旨在解决如何让LLM更有效地利用记忆知识的问题。</p>
<p><strong>Result:</strong> 大量实验表明，MetaMem显著优于强基线模型，性能提升超过3.6%。</p>
<p><strong>Insight:</strong> 创新点在于引入了自演化的元记忆，通过自我反思的符号优化来积累知识利用经验，从而显式地指导LLM整合碎片化记忆，提升推理能力。这为记忆系统的设计提供了新的视角，即不仅存储信息，还学习如何利用信息。</p>
<p><strong>Abstract:</strong> Existing memory systems enable Large Language Models (LLMs) to support long-horizon human-LLM interactions by persisting historical interactions beyond limited context windows. However, while recent approaches have succeeded in constructing effective memories, they often disrupt the inherent logical and temporal relationships within interaction sessions, resulting in fragmented memory units and degraded reasoning performance. In this paper, we propose MetaMem, a novel framework that augments memory systems with a self-evolving meta-memory, aiming to teach LLMs how to effectively utilize memorized knowledge. During meta-memory optimization, MetaMem iteratively distills transferable knowledge utilization experiences across different tasks by self-reflecting on reasoning processes and performing actions to update the current meta-memory state. The accumulated meta-memory units serve as explicit knowledge utilization experiences, guiding the LLM to systematically identify and integrate critical evidence from scattered memory fragments. Extensive experiments demonstrate the effectiveness of MetaMem, which significantly outperforms strong baselines by over 3.6%. All codes and datasets are available at <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/MetaMem">https://github.com/OpenBMB/MetaMem</a>.</p>
  </div>
</details>

<hr>
<h3 id="8-DDL2PropBank-Agent-Benchmarking-Multi-Agent-Frameworks’-Developer-Experience-Through-a-Novel-Relational-Schema-Mapping-Task-cs-CL-cs-AIPDF"><a href="#8-DDL2PropBank-Agent-Benchmarking-Multi-Agent-Frameworks’-Developer-Experience-Through-a-Novel-Relational-Schema-Mapping-Task-cs-CL-cs-AIPDF" class="headerlink" title="[8] DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks’ Developer Experience Through a Novel Relational Schema Mapping Task cs.CL | cs.AIPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11198">DDL2PropBank Agent: Benchmarking Multi-Agent Frameworks’ Developer Experience Through a Novel Relational Schema Mapping Task</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11198" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shafiuddin Rehan Ahmed, Wei Wei</span></p>
<p><strong>TL;DR:</strong> 本文提出了DDL2PropBank基准任务，用于评估多智能体框架的开发者体验。该任务要求将关系数据库模式映射到PropBank角色集，涉及候选框架的自主检索以及对表名、列和关系的细粒度语言推理。作者在10个框架上实现了相同的智能体逻辑，并从代码复杂性和AI可辅助性两个维度进行评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前缺乏在受控环境中评估多智能体框架开发者体验的系统方法，因此需要一种新的基准任务来量化比较不同框架的易用性和开发效率。</p>
<p><strong>Result:</strong> 评估结果显示，Pydantic AI和Agno的实现开销最低。在AI可辅助性方面，对于具有单一规范模式的框架，结构对齐分数能可靠地代理运行时成功率，但对多模式框架会高估正确性。Agno综合表现最佳，具有最低的复杂度和最高的结构对齐分数，其pass@1达到83%。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个新颖的关系模式映射基准任务（DDL2PropBank），并采用“智能体即工具”模式在多个框架上实现统一逻辑进行横向评估。客观来看，该方法为量化多智能体框架的开发者体验提供了可复现的评估体系，特别是代码复杂性和AI可辅助性这两个维度的度量具有借鉴意义。</p>
<p><strong>Abstract:</strong> Multi-agent frameworks promise to simplify LLM-driven software development, yet there is no principled way to evaluate their developer experience in a controlled setting. We introduce DDL2PropBank, a novel benchmark task that maps relational database schemas to PropBank rolesets, requiring autonomous retrieval of candidate frames and fine-grained linguistic reasoning over table names, columns, and relations. Using the Agent-as-a-Tool pattern, we implement identical agent logic across 10 frameworks and evaluate along two dimensions: (i) code complexity via static analysis, and (ii) AI-assistability – the extent to which LLMs can autonomously generate correct, framework-specific code. Our results reveal a threefold complexity spectrum, with Pydantic AI and Agno requiring the least implementation overhead. For AI-assistability, structural alignment scores reliably proxy runtime success for frameworks with single canonical patterns, but overestimate correctness for multi-pattern frameworks. Agno emerges as the strongest overall performer, combining lowest complexity with highest structural alignment and 83% pass@1.</p>
  </div>
</details>

<hr>
<h3 id="9-When-and-What-to-Ask-AskBench-and-Rubric-Guided-RLVR-for-LLM-Clarification-cs-CL-cs-LGPDF"><a href="#9-When-and-What-to-Ask-AskBench-and-Rubric-Guided-RLVR-for-LLM-Clarification-cs-CL-cs-LGPDF" class="headerlink" title="[9] When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification cs.CL | cs.LGPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11199">When and What to Ask: AskBench and Rubric-Guided RLVR for LLM Clarification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11199" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiale Zhao, Ke Fang, Lu Cheng</span></p>
<p><strong>TL;DR:</strong> 该论文提出了AskBench基准测试和RLVR方法，用于评估和改进大型语言模型在需要澄清时决定何时提问以及提问什么内容的能力，而不牺牲任务性能。AskBench将标准问答对转换为包含明确检查点的多轮交互，涵盖意图缺失查询和包含错误前提查询两种场景。RLVR方法通过结构化评分标准和基于验证器的奖励机制，引导模型进行有针对性的澄清。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决大型语言模型在提示信息缺失关键细节或包含误导信息时仍会直接回应，导致产生幻觉或强化错误观念的问题，旨在提升模型在需要时主动寻求澄清的能力。</p>
<p><strong>Result:</strong> 实验表明，所提方法在准确性、评分标准遵循度和交互效率方面均取得了一致性提升，并且在未见过的领域也表现出很强的泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于构建了交互式基准AskBench来系统评估澄清能力，并提出了结合结构化评分标准和验证器奖励的强化学习框架RLVR，以引导模型进行更精准和高效的澄清提问。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) often respond even when prompts omit critical details or include misleading information, leading to hallucinations or reinforced misconceptions. We study how to evaluate and improve LLMs’ ability to decide when and what to ask for clarification without sacrificing task performance. We introduce AskBench, an interactive benchmark that converts standard QA pairs into multi-turn interactions with explicit checkpoints. A unified judge loop evaluates final answers and simulates user responses as needed. AskBench covers two settings: AskMind, with intent-deficient queries requiring clarification, and AskOverconfidence, with queries containing false premises that must be identified and corrected. We further propose rubric-guided reinforcement learning with verifier-based rewards (RLVR), which uses structured rubrics to encourage targeted clarification. Experiments show consistent improvements in accuracy, rubric adherence, and interaction efficiency, with strong generalization to unseen domains.</p>
  </div>
</details>

<hr>
<h3 id="10-Mechanistic-Evidence-for-Faithfulness-Decay-in-Chain-of-Thought-Reasoning-cs-CLPDF"><a href="#10-Mechanistic-Evidence-for-Faithfulness-Decay-in-Chain-of-Thought-Reasoning-cs-CLPDF" class="headerlink" title="[10] Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning cs.CLPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11201">Mechanistic Evidence for Faithfulness Decay in Chain-of-Thought Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11201" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Donald Ye, Max Loffgren, Om Kotadia, Linus Wong</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为归一化对数差异衰减（NLDD）的指标，用于评估思维链（CoT）解释中各个推理步骤对模型决策过程的忠实度。通过在句法、逻辑和算术任务上测试三个模型系列，研究发现存在一个一致的推理视界（k*），大约在链长的70-85%处，超过此点的推理标记对最终答案几乎没有或产生负面影响。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在解决思维链解释是否真实反映语言模型解决复杂问题的内部推理过程，还是仅仅是事后合理化的问题。</p>
<p><strong>Result:</strong> 在句法、逻辑和算术任务上测试了三个模型系列，发现存在一个一致的推理视界（k*），超过此点的推理步骤对最终答案影响甚微或为负。同时发现模型可能编码了正确的内部表示却完全无法完成任务。</p>
<p><strong>Insight:</strong> 创新点在于提出了NLDD这一标准化指标，能够严格进行跨模型比较，以衡量思维链何时真正重要。客观分析表明，该研究揭示了仅凭准确性无法判断模型是否真正通过其思维链进行推理，为评估模型推理的忠实性提供了新方法。</p>
<p><strong>Abstract:</strong> Chain-of-Thought (CoT) explanations are widely used to interpret how language models solve complex problems, yet it remains unclear whether these step-by-step explanations reflect how the model actually reaches its answer, or merely post-hoc justifications. We propose Normalized Logit Difference Decay (NLDD), a metric that measures whether individual reasoning steps are faithful to the model’s decision-making process. Our approach corrupts individual reasoning steps from the explanation and measures how much the model’s confidence in its answer drops, to determine if a step is truly important. By standardizing these measurements, NLDD enables rigorous cross-model comparison across different architectures. Testing three model families across syntactic, logical, and arithmetic tasks, we discover a consistent Reasoning Horizon (k*) at 70–85% of chain length, beyond which reasoning tokens have little or negative effect on the final answer. We also find that models can encode correct internal representations while completely failing the task. These results show that accuracy alone does not reveal whether a model actually reasons through its chain. NLDD offers a way to measure when CoT matters.</p>
  </div>
</details>

<hr>
<h3 id="11-Finding-the-Cracks-Improving-LLMs-Reasoning-with-Paraphrastic-Probing-and-Consistency-Verification-cs-CL-cs-AI-cs-LGPDF"><a href="#11-Finding-the-Cracks-Improving-LLMs-Reasoning-with-Paraphrastic-Probing-and-Consistency-Verification-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[11] Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification cs.CL | cs.AI | cs.LGPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11361">Finding the Cracks: Improving LLMs Reasoning with Paraphrastic Probing and Consistency Verification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11361" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weili Shi, Dongliang Guo, Lehan Yang, Tianlong Wang, Hanzhang Yuan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PPCV（Paraphrastic Probing and Consistency Verification）的两阶段框架，旨在通过识别和替换推理过程中的关键令牌（critical tokens）来提升大型语言模型在复杂任务中的推理能力。该方法首先通过对比原始问题与转述问题在初始推理路径上的令牌预测差异来定位关键令牌，然后替换这些令牌并生成新的并行推理路径，最终通过验证这些路径输出的一致性来确定答案。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型在复杂推理任务中，常因幻觉和中间步骤错误累积导致性能下降。现有研究指出替换推理过程中的关键令牌可以优化推理轨迹，但如何可靠地识别和利用这些关键令牌仍是一个挑战。</p>
<p><strong>Result:</strong> 在多个主流大型语言模型和基准测试上的广泛实验表明，PPCV框架相比基线方法显著提升了模型的推理性能。</p>
<p><strong>Insight:</strong> 创新点在于将转述探测与一致性验证相结合，系统性地识别并替换影响推理路径的关键令牌，从而减少错误传播。该方法提供了一种无需修改模型内部参数即可增强推理可靠性的通用框架。</p>
<p><strong>Abstract:</strong> Large language models have demonstrated impressive performance across a variety of reasoning tasks. However, their problem-solving ability often declines on more complex tasks due to hallucinations and the accumulation of errors within these intermediate steps. Recent work has introduced the notion of critical tokens–tokens in the reasoning process that exert significant influence on subsequent steps. Prior studies suggest that replacing critical tokens can refine reasoning trajectories. Nonetheless, reliably identifying and exploiting critical tokens remains challenging. To address this, we propose the Paraphrastic Probing and Consistency Verification~(PPCV) framework. PPCV operates in two stages. In the first stage, we roll out an initial reasoning path from the original question and then concatenate paraphrased versions of the question with this reasoning path. And we identify critical tokens based on mismatches between the predicted top-1 token and the expected token in the reasoning path. A criterion is employed to confirm the final critical token. In the second stage, we substitute critical tokens with candidate alternatives and roll out new reasoning paths for both the original and paraphrased questions. The final answer is determined by checking the consistency of outputs across these parallel reasoning processes. We evaluate PPCV on mainstream LLMs across multiple benchmarks. Extensive experiments demonstrate PPCV substantially enhances the reasoning performance of LLMs compared to baselines.</p>
  </div>
</details>

<hr>
<h3 id="12-Gradients-Must-Earn-Their-Influence-Unifying-SFT-with-Generalized-Entropic-Objectives-cs-CL-cs-AI-cs-LGPDF"><a href="#12-Gradients-Must-Earn-Their-Influence-Unifying-SFT-with-Generalized-Entropic-Objectives-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[12] Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives cs.CL | cs.AI | cs.LGPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11424">Gradients Must Earn Their Influence: Unifying SFT with Generalized Entropic Objectives</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11424" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zecheng Wang, Deyuan Liu, Chunshan Li, Yupeng Zhang, Zhengyun Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为动态熵微调（DEFT）的新方法，用于解决监督微调（SFT）中标准负对数似然（NLL）目标函数因均匀令牌级加权而导致的塑性-稳定性困境。该方法通过广义变形对数族统一了令牌级SFT目标，利用Cayley变换将模型的不确定性映射到连续的聚焦轨迹上，并使用Rényi-2熵作为模型预测状态的代理来动态调节信任门控，从而在探索与利用之间实现更好的平衡。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准SFT使用的NLL损失函数采用均匀令牌级加权，这导致两个失败模式：一是过度强调低概率目标会放大噪声监督的梯度并破坏鲁棒先验；二是当模型已很自信时，均匀加权提供的锐化作用很弱。现有方法未能解决由此产生的塑性-稳定性困境，常常在抑制有害信号的同时也抑制了必要的学习信号。</p>
<p><strong>Result:</strong> 广泛的实验和分析表明，DEFT在探索与利用之间实现了更好的平衡，从而提升了整体性能。</p>
<p><strong>Insight:</strong> 论文的创新点在于将令牌级SFT目标统一在广义变形对数族框架内，揭示了通用的‘门控×误差’梯度结构，并利用Cayley变换和Rényi-2熵动态调节信任门控，这是一种无参数的、能自适应模型预测状态的目标函数设计方法。</p>
<p><strong>Abstract:</strong> Standard negative log-likelihood (NLL) for Supervised Fine-Tuning (SFT) applies uniform token-level weighting. This rigidity creates a two-fold failure mode: (i) overemphasizing low-probability targets can amplify gradients on noisy supervision and disrupt robust priors, and (ii) uniform weighting provides weak sharpening when the model is already confident. Existing methods fail to resolve the resulting plasticity–stability dilemma, often suppressing necessary learning signals alongside harmful ones. To address this issue, we unify token-level SFT objectives within a generalized deformed-log family and expose a universal gate $\times$ error gradient structure, where the gate controls how much the model trusts its current prediction. By employing the Cayley transform, we map the model’s continuously evolving uncertainty onto a continuous focus trajectory, which enables seamless interpolation between scenarios involving uncertain novel concepts and those involving well-established knowledge. We then introduce Dynamic Entropy Fine-Tuning (DEFT), a parameter-free objective that modulates the trust gate using distribution concentration (Rényi-2 entropy) as a practical proxy for the model’s predictive state. Extensive experiments and analyses demonstrate that DEFT achieves a better balance between exploration and exploitation, leading to improved overall performance.</p>
  </div>
</details>

<hr>
<h3 id="13-LoopFormer-Elastic-Depth-Looped-Transformers-for-Latent-Reasoning-via-Shortcut-Modulation-cs-CLPDF"><a href="#13-LoopFormer-Elastic-Depth-Looped-Transformers-for-Latent-Reasoning-via-Shortcut-Modulation-cs-CLPDF" class="headerlink" title="[13] LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation cs.CLPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11451">LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11451" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ahmadreza Jeddi, Marco Ciccone, Babak Taati</span></p>
<p><strong>TL;DR:</strong> LoopFormer是一种弹性深度的循环Transformer模型，通过捷径调制实现潜在推理。该模型在可变长度轨迹上进行训练，能够根据计算预算灵活调整计算深度，并在语言建模和推理任务中表现出稳健性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有循环Transformer模型在训练和推理中固定循环迭代次数的问题，探索模型能否在可变计算预算下灵活适应计算深度，以实现可控和预算感知的大型语言模型。</p>
<p><strong>Result:</strong> 在语言建模和推理基准测试中，即使面临激进的计算约束，LoopFormer也表现出稳健性能，并能随着额外预算的增加而优雅扩展。</p>
<p><strong>Insight:</strong> 核心创新是捷径一致性训练方案，通过对齐不同长度的轨迹，确保短循环产生信息丰富的表示，而长循环继续优化它们，使表示在不同长度轨迹上一致演化，避免漂移或停滞。这展示了循环Transformer天生适合自适应语言建模，为可控和预算感知的LLM开辟了路径。</p>
<p><strong>Abstract:</strong> Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive language modeling, opening a path toward controllable and budget-aware large language models.</p>
  </div>
</details>

<hr>
<h3 id="14-ADRD-Bench-A-Preliminary-LLM-Benchmark-for-Alzheimer’s-Disease-and-Related-Dementias-cs-CLPDF"><a href="#14-ADRD-Bench-A-Preliminary-LLM-Benchmark-for-Alzheimer’s-Disease-and-Related-Dementias-cs-CLPDF" class="headerlink" title="[14] ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer’s Disease and Related Dementias cs.CLPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11460">ADRD-Bench: A Preliminary LLM Benchmark for Alzheimer’s Disease and Related Dementias</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11460" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guangxin Zhao, Jiahao Zheng, Malaz Boustani, Jarek Nabrzyski, Meng Jiang</span></p>
<p><strong>TL;DR:</strong> 本文介绍了ADRD-Bench，这是首个针对阿尔茨海默病及相关痴呆症（ADRD）的LLM基准测试数据集，包含ADRD统一问答和ADRD照护问答两个部分，用于评估LLM在ADRD领域的临床知识和实际照护能力。作者评估了33个先进LLM，发现顶级模型准确率虽高，但推理质量和稳定性存在不一致性，凸显了基于日常照护数据进行领域特定改进的必要性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有评估基准对阿尔茨海默病及相关痴呆症（ADRD）的覆盖不足，缺乏实际照护背景，因此需要构建一个专门的基准来严格评估LLM在该领域的应用潜力。</p>
<p><strong>Result:</strong> 在提出的ADRD-Bench上评估了33个SOTA LLM：开源通用模型准确率0.63-0.93（均值0.78），开源医疗模型0.48-0.93（均值0.82），闭源通用模型0.83-0.91（均值0.89）。顶级模型准确率超过0.9，但案例研究显示其推理质量和稳定性不一致。</p>
<p><strong>Insight:</strong> 创新点在于构建了首个ADRD专用基准，特别引入了源自真实、循证脑健康管理项目（ABC）的照护问答，弥补了现有基准缺乏实际照护背景的不足。客观来看，该工作强调了在医疗等专业领域，评估LLM不仅需要通用知识，还需结合具体、真实的日常应用场景数据，以揭示模型在复杂推理和稳定性方面的深层缺陷，推动领域特定改进。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have shown great potential for healthcare applications. However, existing evaluation benchmarks provide minimal coverage of Alzheimer’s Disease and Related Dementias (ADRD). To address this gap, we introduce ADRD-Bench, the first ADRD-specific benchmark dataset designed for rigorous evaluation of LLMs. ADRD-Bench has two components: 1) ADRD Unified QA, a synthesis of 1,352 questions consolidated from seven established medical benchmarks, providing a unified assessment of clinical knowledge; and 2) ADRD Caregiving QA, a novel set of 149 questions derived from the Aging Brain Care (ABC) program, a widely used, evidence-based brain health management program. Guided by a program with national expertise in comprehensive ADRD care, this new set was designed to mitigate the lack of practical caregiving context in existing benchmarks. We evaluated 33 state-of-the-art LLMs on the proposed ADRD-Bench. Results showed that the accuracy of open-weight general models ranged from 0.63 to 0.93 (mean: 0.78; std: 0.09). The accuracy of open-weight medical models ranged from 0.48 to 0.93 (mean: 0.82; std: 0.13). The accuracy of closed-source general models ranged from 0.83 to 0.91 (mean: 0.89; std: 0.03). While top-tier models achieved high accuracies (&gt;0.9), case studies revealed that inconsistent reasoning quality and stability limit their reliability, highlighting a critical need for domain-specific improvement to enhance LLMs’ knowledge and reasoning grounded in daily caregiving data. The entire dataset is available at <a target="_blank" rel="noopener" href="https://github.com/IIRL-ND/ADRD-Bench">https://github.com/IIRL-ND/ADRD-Bench</a>.</p>
  </div>
</details>

<hr>
<h3 id="15-When-Audio-LLMs-Don’t-Listen-A-Cross-Linguistic-Study-of-Modality-Arbitration-cs-CL-cs-SD-eess-ASPDF"><a href="#15-When-Audio-LLMs-Don’t-Listen-A-Cross-Linguistic-Study-of-Modality-Arbitration-cs-CL-cs-SD-eess-ASPDF" class="headerlink" title="[15] When Audio-LLMs Don’t Listen: A Cross-Linguistic Study of Modality Arbitration cs.CL | cs.SD | eess.ASPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11488">When Audio-LLMs Don’t Listen: A Cross-Linguistic Study of Modality Arbitration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11488" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jayadev Billa</span></p>
<p><strong>TL;DR:</strong> 这篇论文研究了音频-语言模型在音频与文本信息冲突时的模态仲裁行为，发现模型即使在明确指令下也更倾向于遵循文本而非音频，且这种文本主导现象在跨语言和跨模型中普遍存在。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是探究音频-语言模型在处理音频与文本冲突信息时的仲裁机制，揭示模型在模态选择上的不对称性及其潜在原因。</p>
<p><strong>Result:</strong> 在ALME基准测试（包含8种语言的57,602个音频-文本冲突刺激）中，Gemini 2.0 Flash在音频-文本冲突下的文本主导率为16.6%，远高于文本-文本冲突的1.6%；实验覆盖四种SOTA音频-语言模型，均显示一致趋势。</p>
<p><strong>Insight:</strong> 创新点在于提出模态仲裁可及性框架，指出文本主导源于模型对文本表征的推理更易访问，而非音频信息不足；通过微调实验定位文本主导源于语言模型推理层，而非音频编码器。</p>
<p><strong>Abstract:</strong> When audio and text conflict, speech-enabled language models follow the text 10 times more often than when arbitrating between two text sources, even when explicitly instructed to trust the audio. Using ALME, a benchmark of 57,602 controlled audio-text conflict stimuli across 8 languages, we find that Gemini 2.0 Flash exhibits 16.6% text dominance under audio-text conflict versus 1.6% under text-text conflict with identical reliability cues. This gap is not explained by audio quality: audio-only accuracy (97.2%) exceeds cascade accuracy (93.9%), indicating audio embeddings preserve more information than text transcripts. We propose that text dominance reflects an asymmetry not in information content but in arbitration accessibility: how easily the model can reason over competing representations.   This framework explains otherwise puzzling findings. Forcing transcription before answering increases text dominance (19% to 33%), sacrificing audio’s information advantage without improving accessibility. Framing text as &#96;&#96;deliberately corrupted’’ reduces text dominance by 80%. A fine-tuning ablation provides interventional evidence: training only the audio projection layer increases text dominance (+26.5%), while LoRA on the language model halves it ($-$23.9%), localizing text dominance to the LLM’s reasoning rather than the audio encoder. Experiments across four state-of-the-art audio-LLMs and 8 languages show consistent trends with substantial cross-linguistic and cross-model variation, establishing modality arbitration as a distinct reliability dimension not captured by standard speech benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="16-Multimodal-Fact-Level-Attribution-for-Verifiable-Reasoning-cs-CL-cs-AI-cs-CVPDF"><a href="#16-Multimodal-Fact-Level-Attribution-for-Verifiable-Reasoning-cs-CL-cs-AI-cs-CVPDF" class="headerlink" title="[16] Multimodal Fact-Level Attribution for Verifiable Reasoning cs.CL | cs.AI | cs.CVPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11509">Multimodal Fact-Level Attribution for Verifiable Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11509" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">David Wan, Han Wang, Ziyang Wang, Elias Stengel-Eskin, Hyunji Lee</span></p>
<p><strong>TL;DR:</strong> 本文提出了MuRGAt基准，用于评估多模态大语言模型在复杂推理任务中的事实级归因能力，要求模型在生成答案时提供明确的推理过程和精确的引用（包括模态和时间段），并开发了与人工评估高度相关的自动评估框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准和评估方法主要关注简化的、基于观察的场景或有限模态，无法评估复杂多模态推理中的归因能力，因此需要新的基准来验证模型在需要超越直接观察的推理任务中的可靠性。</p>
<p><strong>Result:</strong> 在MuRGAt基准上，即使强大的多模态大语言模型也经常产生幻觉引用，且增加推理深度或强制结构化归因往往会降低准确性，揭示了内部推理与可验证归因之间的显著差距。</p>
<p><strong>Insight:</strong> 创新点在于引入了多模态事实级归因基准，强调精确引用（模态和时间段）的重要性，并开发了可靠的自动评估框架；客观分析认为，该研究突出了多模态推理中归因与准确性之间的权衡问题，为提升模型可靠性提供了关键方向。</p>
<p><strong>Abstract:</strong> Multimodal large language models (MLLMs) are increasingly used for real-world tasks involving multi-step reasoning and long-form generation, where reliability requires grounding model outputs in heterogeneous input sources and verifying individual factual claims. However, existing multimodal grounding benchmarks and evaluation methods focus on simplified, observation-based scenarios or limited modalities and fail to assess attribution in complex multimodal reasoning. We introduce MuRGAt (Multimodal Reasoning with Grounded Attribution), a benchmark for evaluating fact-level multimodal attribution in settings that require reasoning beyond direct observation. Given inputs spanning video, audio, and other modalities, MuRGAt requires models to generate answers with explicit reasoning and precise citations, where each citation specifies both modality and temporal segments. To enable reliable assessment, we introduce an automatic evaluation framework that strongly correlates with human judgments. Benchmarking with human and automated scores reveals that even strong MLLMs frequently hallucinate citations despite correct reasoning. Moreover, we observe a key trade-off: increasing reasoning depth or enforcing structured grounding often degrades accuracy, highlighting a significant gap between internal reasoning and verifiable attribution.</p>
  </div>
</details>

<hr>
<h3 id="17-SIGHT-Reinforcement-Learning-with-Self-Evidence-and-Information-Gain-Diverse-Branching-for-Search-Agent-cs-CLPDF"><a href="#17-SIGHT-Reinforcement-Learning-with-Self-Evidence-and-Information-Gain-Diverse-Branching-for-Search-Agent-cs-CLPDF" class="headerlink" title="[17] SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent cs.CLPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11551">SIGHT: Reinforcement Learning with Self-Evidence and Information-Gain Diverse Branching for Search Agent</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11551" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenlin Zhong, Jinluan Yang, Yiquan Wu, Yi Liu, Jianhang Yao</span></p>
<p><strong>TL;DR:</strong> SIGHT是一个强化学习框架，旨在解决多轮搜索场景中LLM智能体面临的搜索冗余和信噪比低的问题，通过自我证据支持和信息增益驱动的多样化分支来增强搜索推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多轮搜索中因早期噪声检索导致的’隧道视觉’问题，即错误累积不可逆，以及搜索结果冗余和信噪比低的挑战。</p>
<p><strong>Result:</strong> 在单跳和多跳问答基准测试中，SIGHT显著优于现有方法，特别是在复杂推理场景下，且使用更少的搜索步骤。</p>
<p><strong>Insight:</strong> 创新点包括自我证据支持（SES）提炼高保真证据、信息增益评分识别关键状态以指导动态提示干预（如去重、反思或自适应分支），以及通过组相对策略优化整合SES和正确性奖励来内化稳健探索策略。</p>
<p><strong>Abstract:</strong> Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to master autonomous search for complex question answering. However, particularly within multi-turn search scenarios, this interaction introduces a critical challenge: search results often suffer from high redundancy and low signal-to-noise ratios. Consequently, agents easily fall into “Tunnel Vision,” where the forced interpretation of early noisy retrievals leads to irreversible error accumulation. To address these challenges, we propose SIGHT, a framework that enhances search-based reasoning through Self-Evidence Support (SES) and Information-Gain Driven Diverse Branching. SIGHT distills search results into high-fidelity evidence via SES and calculates an Information Gain score to pinpoint pivotal states where observations maximally reduce uncertainty. This score guides Dynamic Prompting Interventions - including de-duplication, reflection, or adaptive branching - to spawn new branches with SES. Finally, by integrating SES and correctness rewards via Group Relative Policy Optimization, SIGHT internalizes robust exploration strategies without external verifiers. Experiments on single-hop and multi-hop QA benchmarks demonstrate that SIGHT significantly outperforms existing approaches, particularly in complex reasoning scenarios, using fewer search steps.</p>
  </div>
</details>

<hr>
<h3 id="18-PRIME-A-Process-Outcome-Alignment-Benchmark-for-Verifiable-Reasoning-in-Mathematics-and-Engineering-cs-CLPDF"><a href="#18-PRIME-A-Process-Outcome-Alignment-Benchmark-for-Verifiable-Reasoning-in-Mathematics-and-Engineering-cs-CLPDF" class="headerlink" title="[18] PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering cs.CLPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11570">PRIME: A Process-Outcome Alignment Benchmark for Verifiable Reasoning in Mathematics and Engineering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11570" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangfeng Wang, Hangyu Guo, Yanlin Lai, Mitt Huang, Liang Zhao</span></p>
<p><strong>TL;DR:</strong> 本文介绍了PRIME基准，用于评估数学和工程领域中推理验证器的过程-结果对齐能力。该基准包含2530个高难度STEM问题样本，旨在解决现有结果中心验证范式忽视推导过程错误的问题。实验表明，当前验证器常无法检测推导缺陷，而利用PRIME筛选的验证器进行过程感知的RLVR训练能显著提升模型性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于结果的验证范式主要关注最终结果与真实答案的一致性，常忽略推导过程中的潜在错误，导致对错误推导产生的正确答案给予正向奖励。本文旨在填补这一空白，关注过程与结果的对齐验证。</p>
<p><strong>Result:</strong> 在AIME24、AIME25和Beyond-AIME基准上，使用PRIME筛选验证器的过程感知RLVR训练方法相比仅基于结果的基线，为Qwen3-14B-Base模型分别带来8.29%、9.12%和7.31%的绝对性能提升。验证器在PRIME上的准确性与RLVR训练效果呈现强线性相关（R² &gt; 0.92）。</p>
<p><strong>Insight:</strong> 创新点在于提出了首个专注于过程-结果对齐验证的基准PRIME，并引入了过程感知的RLVR训练范式。客观来看，其通过一致性过滤流程构建高质量数据集的方法，以及将验证器性能与下游训练效果强关联的发现，对构建可靠的AI推理系统具有重要借鉴意义。</p>
<p><strong>Abstract:</strong> While model-based verifiers are essential for scaling Reinforcement Learning with Verifiable Rewards (RLVR), current outcome-centric verification paradigms primarily focus on the consistency between the final result and the ground truth, often neglecting potential errors in the derivation process. This leads to assigning positive rewards to correct answers produced from incorrect derivations. To bridge this gap, we introduce PRIME, a benchmark for evaluating verifiers on Process-Outcome Alignment verification in Mathematics and Engineering. Curated from a comprehensive collection of college-level STEM problems, PRIME comprises 2,530 high-difficulty samples through a consistency-based filtering pipeline. Through extensive evaluation, we find that current verifiers frequently fail to detect derivation flaws. Furthermore, we propose a process-aware RLVR training paradigm utilizing verifiers selected via PRIME. This approach substantially outperforms the outcome-only verification baseline, achieving absolute performance gains of 8.29%, 9.12%, and 7.31% on AIME24, AIME25, and Beyond-AIME, respectively, for the Qwen3-14B-Base model. Finally, we demonstrate a strong linear correlation ($R^2 &gt; 0.92$) between verifier accuracy on PRIME and RLVR training effectiveness, validating PRIME as a reliable predictor for verifier selection.</p>
  </div>
</details>

<hr>
<h3 id="19-PACE-Prefix-Protected-and-Difficulty-Aware-Compression-for-Efficient-Reasoning-cs-CLPDF"><a href="#19-PACE-Prefix-Protected-and-Difficulty-Aware-Compression-for-Efficient-Reasoning-cs-CLPDF" class="headerlink" title="[19] PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning cs.CLPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11639">PACE: Prefix-Protected and Difficulty-Aware Compression for Efficient Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11639" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ruixiang Feng, Yuntao Wen, Silin Zhou, Ke Shi, Yifan Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为PACE的双层压缩框架，旨在解决语言推理模型（LRMs）因“过度思考”而产生过长推理链、导致延迟和内存使用增加的问题。该方法在序列级别通过前缀保护优化来保持有效推理路径，在组级别通过难度感知惩罚来动态调整长度约束。实验表明，PACE能显著减少token使用并提升数学基准上的准确率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有LRMs通常采用统一的长度惩罚来强制简洁性，但这在序列级别过度压缩了关键的早期推理步骤，在组级别则不加区分地惩罚所有查询，导致效率低下。</p>
<p><strong>Result:</strong> 在DeepSeek-R1-Distill-Qwen（1.5B&#x2F;7B）模型上的大量实验表明，PACE在数学基准上实现了token使用量大幅减少（高达55.7%），同时准确率提升（高达4.1%），并能泛化到代码、科学和通用领域。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个分层监督的双层压缩框架：序列级别的前缀保护优化通过衰减混合rollouts平衡推理有效性与简洁性；组级别的难度感知惩罚根据查询复杂度动态调整长度约束，实现对不同难度问题的自适应压缩。这为高效推理提供了一种结构化的压缩方法。</p>
<p><strong>Abstract:</strong> Language Reasoning Models (LRMs) achieve strong performance by scaling test-time computation but often suffer from &#96;&#96;overthinking’’, producing excessively long reasoning traces that increase latency and memory usage. Existing LRMs typically enforce conciseness with uniform length penalties, which over-compress crucial early deduction steps at the sequence level and indiscriminately penalize all queries at the group level. To solve these limitations, we propose \textbf{\model}, a dual-level framework for prefix-protected and difficulty-aware compression under hierarchical supervision. At the sequence level, prefix-protected optimization employs decaying mixed rollouts to maintain valid reasoning paths while promoting conciseness. At the group level, difficulty-aware penalty dynamically scales length constraints based on query complexity, maintaining exploration for harder questions while curbing redundancy on easier ones. Extensive experiments on DeepSeek-R1-Distill-Qwen (1.5B&#x2F;7B) demonstrate that \model achieves a substantial reduction in token usage (up to \textbf{55.7%}) while simultaneously improving accuracy (up to \textbf{4.1%}) on math benchmarks, with generalization ability to code, science, and general domains.</p>
  </div>
</details>

<hr>
<h3 id="20-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction-cs-CLPDF"><a href="#20-Thinking-with-Drafting-Optical-Decompression-via-Logical-Reconstruction-cs-CLPDF" class="headerlink" title="[20] Thinking with Drafting: Optical Decompression via Logical Reconstruction cs.CLPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11731">Thinking with Drafting: Optical Decompression via Logical Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11731" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jingxuan Wei, Honghao He, Caijun Jia, Siyuan Li, Zheng Sun</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为’Thinking with Drafting’的新方法，将视觉推理重新定义为’光学解压缩’过程，即从压缩的视觉标记中重建潜在的逻辑结构。该方法利用一个极简的领域特定语言作为中间表示，强制模型将其心智模型草拟为可执行代码，以生成确定性的视觉证明进行自我验证。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有多模态大语言模型在复杂推理任务中存在精度悖论：光学感知系统转录符号时无法捕获逻辑拓扑，而基于像素的生成模型会产生缺乏数学精确性的视觉伪影。本文旨在弥合这一差距。</p>
<p><strong>Result:</strong> 为验证方法，作者提出了VisAlg视觉代数基准。实验表明，TwD作为一种优越的认知支架，在视觉推理任务中表现出色，建立了一个视觉生成作为逻辑验证器而非创造性输出的闭环系统。</p>
<p><strong>Insight:</strong> 核心创新在于将’解析即推理’作为公理，通过DSL中间表示强制模型进行可执行的逻辑草拟和视觉证明自验证，将视觉生成的角色从创造性输出转变为逻辑验证器，为视觉推理提供了一条可泛化的路径。</p>
<p><strong>Abstract:</strong> Existing multimodal large language models have achieved high-fidelity visual perception and exploratory visual generation. However, a precision paradox persists in complex reasoning tasks: optical perception systems transcribe symbols without capturing logical topology, while pixel-based generative models produce visual artifacts lacking mathematical exactness. To bridge this gap, we propose that reasoning over visual inputs be reconceptualized as optical decompression-the process of reconstructing latent logical structures from compressed visual tokens. Guided by the axiom that Parsing is Reasoning, we introduce Thinking with Drafting (TwD), which utilizes a minimalist Domain-Specific Language (DSL) as a grounding intermediate representation. Unlike standard approaches that hallucinate answers directly, TwD forces the model to draft its mental model into executable code, rendering deterministic visual proofs for self-verification. To validate this, we present VisAlg, a visual algebra benchmark. Experiments demonstrate that TwD serve as a superior cognitive scaffold. Our work establishes a closed-loop system where visual generation acts not as a creative output but as a logical verifier, offering a generalizable path for visual reasoning.</p>
  </div>
</details>

<hr>
<h3 id="21-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning-cs-CLPDF"><a href="#21-Think-Longer-to-Explore-Deeper-Learn-to-Explore-In-Context-via-Length-Incentivized-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[21] Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning cs.CLPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11748">Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11748" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Futing Wang, Jianhao Yan, Yun Luo, Ganqu Cui, Zhi Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为长度激励探索（Length-Incentivized Exploration）的方法，旨在解决大语言模型在测试时扩展中面临的“浅层探索陷阱”问题。该方法通过基于长度的奖励和冗余惩罚，激励模型在单个连续上下文中生成、验证和优化更长的推理轨迹，从而增强其上下文内探索能力。实验表明，该方法能有效提升模型在领域内和领域外任务上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是解决大语言模型在测试时扩展中的一个关键瓶颈：为了实现有效的上下文内探索（即生成、验证和优化多个推理假设），模型需要生成更长的推理序列以获得更广泛的状态覆盖，但自回归生成过程中采样长序列的概率呈指数衰减，这被称为“浅层探索陷阱”。</p>
<p><strong>Result:</strong> 在Qwen3和Llama等不同模型上的综合实验表明，该方法有效激励了上下文内探索。具体结果是在领域内任务上平均提升了4.4%，在领域外基准测试上平均提升了2.7%。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于提出了一个简单而有效的长度激励探索方法，通过结合长度奖励和冗余惩罚，以两步法最大化状态覆盖。从客观角度看，其核心创新在于将强化学习中的奖励机制（基于长度）与探索理论（状态覆盖）相结合，为缓解大语言模型在复杂推理任务中的探索不足问题提供了一个新颖的解决方案。</p>
<p><strong>Abstract:</strong> Achieving effective test-time scaling requires models to engage in In-Context Exploration – the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context.   Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the &#96;&#96;Shallow Exploration Trap’’.   To bridge this gap, we propose Length-Incentivized Exploration(\method).   This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner.   Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \method effectively incentivize in-context exploration.   As a result, our method achieves an average improvement of 4.4% on in-domain tasks and a 2.7% gain on out-of-domain benchmarks.</p>
  </div>
</details>

<hr>
<h3 id="22-Benchmark-Illusion-Disagreement-among-LLMs-and-Its-Scientific-Consequences-cs-CLPDF"><a href="#22-Benchmark-Illusion-Disagreement-among-LLMs-and-Its-Scientific-Consequences-cs-CLPDF" class="headerlink" title="[22] Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences cs.CLPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11898">Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11898" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Eddie Yang, Dashun Wang</span></p>
<p><strong>TL;DR:</strong> 本文揭示了大型语言模型（LLM）在基准测试中存在的‘基准幻觉’现象：尽管不同模型在MMLU-Pro和GPQA等推理基准上取得了相近的准确率，但它们对16%-66%的题目判断存在分歧，顶级前沿模型之间也有16%-38%的分歧。这种隐藏的分歧会导致模型在用于科学数据标注和推理时，成为影响研究结果可重复性的关键变量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基准测试是衡量和信任LLM进展的基础，但作者发现，基准准确率的表面趋同可能掩盖了深层的认知差异，这可能会对依赖LLM进行科学研究的可重复性产生严重后果。</p>
<p><strong>Result:</strong> 在MMLU-Pro和GPQA基准上，不同LLM间存在显著判断分歧；在教育和政治学的已发表研究重新分析中，更换标注模型可使估计的处理效应变化超过80%，甚至改变其符号。</p>
<p><strong>Insight:</strong> 论文的核心创新在于提出了‘基准幻觉’概念，并实证揭示了基准测试总分趋同下模型间存在的系统性认知差异及其对科学研究的潜在影响，强调了在科学研究中考虑模型选择作为关键变量的必要性。</p>
<p><strong>Abstract:</strong> Benchmarks underpin how progress in large language models (LLMs) is measured and trusted. Yet our analyses reveal that apparent convergence in benchmark accuracy can conceal deep epistemic divergence. Using two major reasoning benchmarks - MMLU-Pro and GPQA - we show that LLMs achieving comparable accuracy still disagree on 16-66% of items, and 16-38% among top-performing frontier models. These discrepancies suggest distinct error profiles for different LLMs. When such models are used for scientific data annotation and inference, their hidden disagreements propagate into research results: in re-analyses of published studies in education and political science, switching the annotation model can change estimated treatment effects by more than 80%, and in some cases reverses their sign. Together, these findings illustrate a benchmark illusion, where equal accuracy may conceal disagreement, with model choice becoming a hidden yet consequential variable for scientific reproducibility.</p>
  </div>
</details>

<hr>
<h3 id="23-AdaptEvolve-Improving-Efficiency-of-Evolutionary-AI-Agents-through-Adaptive-Model-Selection-cs-CL-cs-AIPDF"><a href="#23-AdaptEvolve-Improving-Efficiency-of-Evolutionary-AI-Agents-through-Adaptive-Model-Selection-cs-CL-cs-AIPDF" class="headerlink" title="[23] AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection cs.CL | cs.AIPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11931">AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11931" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pretam Ray, Pratik Prabhanjan Brahma, Zicheng Liu, Emad Barsoum</span></p>
<p><strong>TL;DR:</strong> 本文提出AdaptEvolve方法，用于在进化式AI智能体框架中，通过自适应选择不同规模的大语言模型来平衡计算效率与推理能力。该方法利用模型生成的内在置信度来实时估计任务可解性，从而动态选择模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决进化智能体在推理过程中反复调用大语言模型所导致的计算效率与推理能力之间的权衡问题，现有级联模型路由策略依赖静态启发式方法或外部控制器，且未明确考虑模型不确定性。</p>
<p><strong>Result:</strong> 在多个基准测试上，置信度驱动的选择方法形成了有利的帕累托前沿，平均减少37.9%的总推理成本，同时保持了静态大模型基线97.5%的上界准确率。</p>
<p><strong>Insight:</strong> 创新点在于将模型内在生成置信度作为实时路由决策依据，实现自适应模型选择，而非依赖静态规则，这为多模型协同推理系统提供了一种轻量且有效的动态调度机制。</p>
<p><strong>Abstract:</strong> Evolutionary agentic systems intensify the trade-off between computational efficiency and reasoning capability by repeatedly invoking large language models (LLMs) during inference. This setting raises a central question: how can an agent dynamically select an LLM that is sufficiently capable for the current generation step while remaining computationally efficient? While model cascades offer a practical mechanism for balancing this trade-off, existing routing strategies typically rely on static heuristics or external controllers and do not explicitly account for model uncertainty. We introduce AdaptEvolve: Adaptive LLM Selection for Multi-LLM Evolutionary Refinement within an evolutionary sequential refinement framework that leverages intrinsic generation confidence to estimate real-time solvability. Empirical results show that confidence-driven selection yields a favourable Pareto frontier, reducing total inference cost by an average of 37.9% across benchmarks while retaining 97.5% of the upper-bound accuracy of static large-model baselines. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/raypretam/adaptive_llm_selection">https://github.com/raypretam/adaptive_llm_selection</a>.</p>
  </div>
</details>

<hr>
<h3 id="24-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models-cs-CLPDF"><a href="#24-Composition-RL-Compose-Your-Verifiable-Prompts-for-Reinforcement-Learning-of-Large-Language-Models-cs-CLPDF" class="headerlink" title="[24] Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models cs.CLPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12036">Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12036" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Xu, Clive Bai, Kai Yang, Tianhao Chen, Yangkun Chen</span></p>
<p><strong>TL;DR:</strong> 本文提出Composition-RL方法，通过自动组合多个可验证问题生成新的复合提示，用于强化学习训练，以更有效地利用有限的可验证提示数据，特别是针对通过率为1的简单提示，从而提升大语言模型的推理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于可验证奖励的强化学习依赖大规模可验证提示，但数据包含大量无信息样本且扩展成本高；同时，训练中简单提示（通过率1）占比增加导致有效数据规模减小，需要更高效利用有限提示数据的方法。</p>
<p><strong>Result:</strong> 在4B到30B不同规模模型上的广泛实验表明，Composition-RL相比在原始数据集上进行RL训练，能持续提升推理能力；采用逐步增加组合深度的课程学习变体可进一步提升性能；该方法还能通过组合不同领域提示实现更有效的跨领域RL。</p>
<p><strong>Insight:</strong> 核心创新在于提出自动组合多个可验证问题以创建新训练样本的机制，这为数据高效利用提供了新思路；其课程学习变体展示了渐进式难度调整的有效性；组合跨领域提示的能力也增强了方法的泛化性和实用性。</p>
<p><strong>Abstract:</strong> Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at <a target="_blank" rel="noopener" href="https://github.com/XinXU-USTC/Composition-RL">https://github.com/XinXU-USTC/Composition-RL</a>.</p>
  </div>
</details>

<hr>
<h3 id="25-DeepSight-An-All-in-One-LM-Safety-Toolkit-cs-CL-cs-AI-cs-CR-cs-CVPDF"><a href="#25-DeepSight-An-All-in-One-LM-Safety-Toolkit-cs-CL-cs-AI-cs-CR-cs-CVPDF" class="headerlink" title="[25] DeepSight: An All-in-One LM Safety Toolkit cs.CL | cs.AI | cs.CR | cs.CVPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12092">DeepSight: An All-in-One LM Safety Toolkit</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CR | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12092" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bo Zhang, Jiaxuan Guo, Lijun Li, Dongrui Liu, Sujin Chen</span></p>
<p><strong>TL;DR:</strong> DeepSight是一个开源的大型模型安全工具包，旨在通过整合评估与诊断阶段，解决现有安全流程中工具分离、评估黑盒化、诊断脱离具体风险场景等问题。它包含DeepSafe评估工具和DeepScan诊断工具，实现了从行为风险定位到内部根因分析的白盒化安全洞察。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大语言模型和多模态大语言模型的安全工作流中，评估、诊断和对齐通常由独立工具处理，导致安全评估只能定位外部行为风险而无法分析内部根因，安全诊断则往往脱离具体风险场景且停留在可解释性层面，使得安全对齐缺乏对内部机制变化的专门解释，可能损害模型的通用能力。</p>
<p><strong>Result:</strong> 论文宣称DeepSight是首个支持前沿AI风险评估以及联合安全评估与诊断的开源工具包，具有低成本、可复现、高效和高可扩展性。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了一个评估-诊断一体化的新范式，通过统一任务和数据协议，在评估和诊断阶段之间建立连接，将安全评估从黑盒视角转变为白盒洞察，从而系统性地提升大模型安全工作的深度和效率。</p>
<p><strong>Abstract:</strong> As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis.</p>
  </div>
</details>

<hr>
<h3 id="26-P-GenRM-Personalized-Generative-Reward-Model-with-Test-time-User-based-Scaling-cs-CLPDF"><a href="#26-P-GenRM-Personalized-Generative-Reward-Model-with-Test-time-User-based-Scaling-cs-CLPDF" class="headerlink" title="[26] P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling cs.CLPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12116">P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12116" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pinyi Zhang, Ting-En Lin, Yuchuan Wu, Jingyang Chen, Zongqi Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了P-GenRM，一种个性化的生成式奖励模型，旨在解决大语言模型个性化对齐中用户偏好信号获取和泛化到新用户的挑战。其核心创新在于将偏好信号转化为结构化的评估链，并引入基于用户原型的双粒度缩放机制，在测试时根据用户进行自适应调整。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有个性化奖励模型存在两大局限：一是将多样化的场景特定偏好过度简化为少量固定评估原则；二是难以泛化到反馈有限的新用户。论文旨在解决开放场景下获取准确、用户特定奖励信号的挑战。</p>
<p><strong>Result:</strong> 在广泛使用的个性化奖励模型基准测试中，P-GenRM取得了SOTA结果，平均提升2.31%，并在分布外数据集上表现出强大的泛化能力。测试时基于用户的缩放机制额外带来了3%的性能提升。</p>
<p><strong>Insight:</strong> 主要创新点包括：1) 将偏好信号转化为动态生成的自适应角色和评分标准的结构化评估链；2) 通过用户原型聚类和双粒度（个体和原型层面）缩放机制，有效缓解偏好推断噪声并增强对未见用户的泛化能力，实现了测试时可扩展的个性化对齐。</p>
<p><strong>Abstract:</strong> Personalized alignment of large language models seeks to adapt responses to individual user preferences, typically via reinforcement learning. A key challenge is obtaining accurate, user-specific reward signals in open-ended scenarios. Existing personalized reward models face two persistent limitations: (1) oversimplifying diverse, scenario-specific preferences into a small, fixed set of evaluation principles, and (2) struggling with generalization to new users with limited feedback. To this end, we propose P-GenRM, the first Personalized Generative Reward Model with test-time user-based scaling. P-GenRM transforms preference signals into structured evaluation chains that derive adaptive personas and scoring rubrics across various scenarios. It further clusters users into User Prototypes and introduces a dual-granularity scaling mechanism: at the individual level, it adaptively scales and aggregates each user’s scoring scheme; at the prototype level, it incorporates preferences from similar users. This design mitigates noise in inferred preferences and enhances generalization to unseen users through prototype-based transfer. Empirical results show that P-GenRM achieves state-of-the-art results on widely-used personalized reward model benchmarks, with an average improvement of 2.31%, and demonstrates strong generalization on an out-of-distribution dataset. Notably, Test-time User-based scaling provides an additional 3% boost, demonstrating stronger personalized alignment with test-time scalability.</p>
  </div>
</details>

<hr>
<h3 id="27-WavBench-Benchmarking-Reasoning-Colloquialism-and-Paralinguistics-for-End-to-End-Spoken-Dialogue-Models-cs-CLPDF"><a href="#27-WavBench-Benchmarking-Reasoning-Colloquialism-and-Paralinguistics-for-End-to-End-Spoken-Dialogue-Models-cs-CLPDF" class="headerlink" title="[27] WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models cs.CLPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12135">WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12135" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yangzhuo Li, Shengpeng Ji, Yifu Chen, Tianle Liang, Haorong Ying</span></p>
<p><strong>TL;DR:</strong> 该论文提出了WavBench，一个用于评估端到端口语对话模型在推理、口语化和副语言能力方面的综合性基准测试。它包含三个子集：Pro子集用于挑战推理增强模型，Basic子集定义口语化新标准，Acoustic子集评估副语言能力。通过评估五个SOTA模型，该基准为鲁棒口语对话模型的发展提供了关键见解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前口语对话模型的评估主要遵循文本生成标准，忽视了副语言和口语化等音频中心特性以及现代智能体所需的认知深度，因此需要能应对现实世界复杂性的新基准。</p>
<p><strong>Result:</strong> 论文通过评估五个最先进的模型，为复杂问题解决、口语化表达和副语言保真度的交叉领域提供了关键见解，但摘要中未提及具体的定量结果或与SOTA的对比数据。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个独特的三元评估框架，将推理难度、口语化‘可听性’（而非书面准确性）和全面的副语言能力（包括显式理解、生成和隐式对话）整合到一个统一的基准中，以更真实地评估对话模型。</p>
<p><strong>Abstract:</strong> With the rapid integration of advanced reasoning capabilities into spoken dialogue models, the field urgently demands benchmarks that transcend simple interactions to address real-world complexity. However, current evaluations predominantly adhere to text-generation standards, overlooking the unique audio-centric characteristics of paralinguistics and colloquialisms, alongside the cognitive depth required by modern agents. To bridge this gap, we introduce WavBench, a comprehensive benchmark designed to evaluate realistic conversational abilities where prior works fall short. Uniquely, WavBench establishes a tripartite framework: 1) Pro subset, designed to rigorously challenge reasoning-enhanced models with significantly increased difficulty; 2) Basic subset, defining a novel standard for spoken colloquialism that prioritizes “listenability” through natural vocabulary, linguistic fluency, and interactive rapport, rather than rigid written accuracy; and 3) Acoustic subset, covering explicit understanding, generation, and implicit dialogue to rigorously evaluate comprehensive paralinguistic capabilities within authentic real-world scenarios. Through evaluating five state-of-the-art models, WavBench offers critical insights into the intersection of complex problem-solving, colloquial delivery, and paralinguistic fidelity, guiding the evolution of robust spoken dialogue models. The benchmark dataset and evaluation toolkit are available at <a target="_blank" rel="noopener" href="https://naruto-2024.github.io/wavbench.github.io/">https://naruto-2024.github.io/wavbench.github.io/</a>.</p>
  </div>
</details>

<hr>
<h3 id="28-CitiLink-Minutes-A-Multilayer-Annotated-Dataset-of-Municipal-Meeting-Minutes-cs-CLPDF"><a href="#28-CitiLink-Minutes-A-Multilayer-Annotated-Dataset-of-Municipal-Meeting-Minutes-cs-CLPDF" class="headerlink" title="[28] CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes cs.CLPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12137">CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12137" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ricardo Campos, Ana Filipa Pacheco, Ana Luísa Fernandes, Inês Cantante, Rute Rebouças</span></p>
<p><strong>TL;DR:</strong> 本文介绍了CitiLink-Minutes数据集，这是一个包含120份欧洲葡萄牙语市政会议记录的多层标注数据集，旨在填补市政会议记录在信息检索和自然语言处理领域缺乏标注数据的空白。数据集提供了元数据、讨论主题和投票结果三个维度的标注，并附带了基线实验结果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 市政会议记录作为地方治理的重要文件，在信息检索和自然语言处理领域缺乏标注数据集，限制了相关计算模型的发展。本文旨在通过构建一个多层标注数据集来解决这一问题。</p>
<p><strong>Result:</strong> 数据集包含超过100万个词元，提供了超过38,000个个体标注，并在元数据提取、主题分类和投票标签任务上提供了基线结果，展示了其在自然语言处理和下游任务中的潜力。</p>
<p><strong>Insight:</strong> 创新点在于首次为市政会议记录构建了多层标注数据集，提供了结构化的官方书面记录链接，并遵循FAIR原则发布，促进了市政决策的透明访问和计算模型的发展。</p>
<p><strong>Abstract:</strong> City councils play a crucial role in local governance, directly influencing citizens’ daily lives through decisions made during municipal meetings. These deliberations are formally documented in meeting minutes, which serve as official records of discussions, decisions, and voting outcomes. Despite their importance, municipal meeting records have received little attention in Information Retrieval (IR) and Natural Language Processing (NLP), largely due to the lack of annotated datasets, which ultimately limit the development of computational models. To address this gap, we introduce CitiLink-Minutes, a multilayer dataset of 120 European Portuguese municipal meeting minutes from six municipalities. Unlike prior annotated datasets of parliamentary or video records, CitiLink-Minutes provides multilayer annotations and structured linkage of official written minutes. The dataset contains over one million tokens, with all personal identifiers de-identified. Each minute was manually annotated by two trained annotators and curated by an experienced linguist across three complementary dimensions: (1) metadata, (2) subjects of discussion, and (3) voting outcomes, totaling over 38,000 individual annotations. Released under FAIR principles and accompanied by baseline results on metadata extraction, topic classification, and vote labeling, CitiLink-Minutes demonstrates its potential for downstream NLP and IR tasks, while promoting transparent access to municipal decisions.</p>
  </div>
</details>

<hr>
<h3 id="29-dVoting-Fast-Voting-for-dLLMs-cs-CL-cs-AIPDF"><a href="#29-dVoting-Fast-Voting-for-dLLMs-cs-CL-cs-AIPDF" class="headerlink" title="[29] dVoting: Fast Voting for dLLMs cs.CL | cs.AIPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12153">dVoting: Fast Voting for dLLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12153" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sicheng Feng, Zigeng Chen, Xinyin Ma, Gongfan Fang, Xinchao Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了dVoting，一种用于扩散大语言模型（dLLMs）的快速投票技术，旨在无需训练即可提升模型的推理能力。该方法利用dLLMs可并行生成任意位置token的特性，通过采样、一致性分析识别不确定token、投票再生以及迭代优化直至收敛的过程，以可接受的计算开销显著提升模型在多个基准测试上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散大语言模型（dLLMs）作为一种超越自回归建模的新范式，具有并行生成任意位置token的潜力，但现有方法在推理时存在效率瓶颈。dVoting的动机在于观察到同一提示下多个样本的token预测大多一致，而性能差异主要由少数不确定token决定，因此提出通过投票机制迭代优化这些不确定token，以提升推理能力。</p>
<p><strong>Result:</strong> 在多个基准测试上，dVoting一致提升了性能：在GSM8K上获得6.22%-7.66%的增益，在MATH500上为4.40%-7.20%，在ARC-C上为3.16%-14.84%，在MMLU上为4.83%-5.74%。这些结果表明dVoting能有效增强dLLMs的推理能力，且计算开销可接受。</p>
<p><strong>Insight:</strong> 论文的创新点在于利用dLLMs的任意位置生成能力，设计了一种基于一致性分析和投票的迭代优化方法，无需训练即可提升模型性能。从客观角度看，该方法将不确定token的识别与并行再生相结合，为扩散模型的测试时缩放提供了高效解决方案，可借鉴于其他需要提升推理稳定性的生成模型。</p>
<p><strong>Abstract:</strong> Diffusion Large Language Models (dLLMs) represent a new paradigm beyond autoregressive modeling, offering competitive performance while naturally enabling a flexible decoding process. Specifically, dLLMs can generate tokens at arbitrary positions in parallel, endowing them with significant potential for parallel test-time scaling, which was previously constrained by severe inefficiency in autoregressive modeling. In this work, we introduce dVoting, a fast voting technique that boosts reasoning capability without training, with only an acceptable extra computational overhead. dVoting is motivated by the observation that, across multiple samples for the same prompt, token predictions remain largely consistent, whereas performance is determined by a small subset of tokens exhibiting cross-sample variability. Leveraging the arbitrary-position generation capability of dLLMs, dVoting performs iterative refinement by sampling, identifying uncertain tokens via consistency analysis, regenerating them through voting, and repeating this process until convergence. Extensive evaluations demonstrate that dVoting consistently improves performance across various benchmarks. It achieves gains of 6.22%-7.66% on GSM8K, 4.40%-7.20% on MATH500, 3.16%-14.84% on ARC-C, and 4.83%-5.74% on MMLU. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/fscdc/dVoting">https://github.com/fscdc/dVoting</a></p>
  </div>
</details>

<hr>
<h3 id="30-Query-focused-and-Memory-aware-Reranker-for-Long-Context-Processing-cs-CLPDF"><a href="#30-Query-focused-and-Memory-aware-Reranker-for-Long-Context-Processing-cs-CLPDF" class="headerlink" title="[30] Query-focused and Memory-aware Reranker for Long Context Processing cs.CLPDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12192">Query-focused and Memory-aware Reranker for Long Context Processing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12192" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuqing Li, Jiangnan Li, Mo Yu, Guoxuan Ding, Zheng Lin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于注意力分数的轻量级重排序框架，通过训练模型利用选定注意力头的分数来估计段落-查询相关性，提供了一种利用候选列表整体信息的列表式解决方案，并在多个领域和基准测试中实现了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有大型语言模型中的检索头分析表明，需要一种能够利用候选列表整体信息、无需Likert-scale监督且能产生连续相关性分数的重排序方法，以提升长上下文处理中的检索效果。</p>
<p><strong>Result:</strong> 在包括Wikipedia和长叙事数据集在内的多个领域上，该方法超越了现有的最先进的点式和列表式重排序器，并在评估对话理解和记忆使用能力的LoCoMo基准上建立了新的最先进水平。</p>
<p><strong>Insight:</strong> 创新点在于利用注意力分数进行轻量级列表式重排序，无需Likert-scale监督即可训练，并支持通过增强候选段落上下文信息或使用中间层注意力头来灵活扩展，在保持高效的同时提升准确性。</p>
<p><strong>Abstract:</strong> Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.</p>
  </div>
</details>

<hr>
<h3 id="31-Visual-Reasoning-Benchmark-Evaluating-Multimodal-LLMs-on-Classroom-Authentic-Visual-Problems-from-Primary-Education-cs-CL-cs-AIPDF"><a href="#31-Visual-Reasoning-Benchmark-Evaluating-Multimodal-LLMs-on-Classroom-Authentic-Visual-Problems-from-Primary-Education-cs-CL-cs-AIPDF" class="headerlink" title="[31] Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education cs.CL | cs.AIPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12196">Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12196" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohamed Huti, Alasdair Mackintosh, Amy Waldock, Dominic Andrews, Maxime Lelièvre</span></p>
<p><strong>TL;DR:</strong> 本文介绍了视觉推理基准（VRB），这是一个用于评估多模态大语言模型（MLLMs）解决小学课堂真实视觉问题能力的新数据集。该基准包含701道来自赞比亚和印度小学考试的题目，涵盖类比推理、模式完成和空间匹配等任务。研究发现，模型在静态技能（如计数和缩放）上表现较好，但在动态操作（如折叠、反射和旋转）上存在明显的“空间天花板”。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> AI模型在文本推理上已达到SOTA，但其在空间和关系结构上的推理能力仍是关键瓶颈，尤其是在依赖视觉的早期数学教育中。本文旨在通过构建一个基于真实课堂视觉问题的基准，来评估MLLMs的实际应用能力。</p>
<p><strong>Result:</strong> 在VRB基准上的评估揭示了模型能力的“锯齿状前沿”：在静态技能（如计数）上表现较好，但在动态空间操作（如折叠、反射、旋转）上存在显著弱点，表明其尚未达到可靠课堂应用的水平。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个基于真实、未经编辑的小学考试视觉问题的基准，强调测试模型在真实教育场景中的能力。客观分析认为，该工作揭示了当前MLLMs在动态空间推理上的具体短板，为教育AI工具的功能边界评估提供了重要基准。</p>
<p><strong>Abstract:</strong> AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck – particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a <code>jagged frontier&#39;&#39; of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct </code>spatial ceiling’’ when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms.</p>
  </div>
</details>

<hr>
<h3 id="32-ExStrucTiny-A-Benchmark-for-Schema-Variable-Structured-Information-Extraction-from-Document-Images-cs-CLPDF"><a href="#32-ExStrucTiny-A-Benchmark-for-Schema-Variable-Structured-Information-Extraction-from-Document-Images-cs-CLPDF" class="headerlink" title="[32] ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images cs.CLPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12203">ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12203" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mathieu Sibue, Andres Muñoz Garza, Samuel Mensah, Pranav Shetty, Zhiqiang Ma</span></p>
<p><strong>TL;DR:</strong> 该论文提出了ExStrucTiny，一个用于文档图像结构化信息提取的新基准数据集，旨在评估通用视觉语言模型在多样化文档类型和灵活模式下的细粒度结构化提取能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文档理解基准在实体本体、查询复杂性和文档类型多样性方面存在局限，无法充分评估模型在灵活模式下的结构化信息提取能力。</p>
<p><strong>Result:</strong> 论文在ExStrucTiny基准上分析了开放和封闭的视觉语言模型，揭示了模式适应、查询欠规范和答案定位等挑战，但未提及具体的定量SOTA结果。</p>
<p><strong>Insight:</strong> 创新点在于构建了一个统一关键实体提取、关系提取和视觉问答任务的新基准，并通过结合人工和合成验证样本的新颖流程创建了更丰富的数据集，为提升文档结构化信息提取的通用模型提供了基础。</p>
<p><strong>Abstract:</strong> Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.</p>
  </div>
</details>

<hr>
<h3 id="33-On-Policy-Context-Distillation-for-Language-Models-cs-CLPDF"><a href="#33-On-Policy-Context-Distillation-for-Language-Models-cs-CLPDF" class="headerlink" title="[33] On-Policy Context Distillation for Language Models cs.CLPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12275">On-Policy Context Distillation for Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12275" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei</span></p>
<p><strong>TL;DR:</strong> 本文提出了On-Policy Context Distillation (OPCD)框架，将策略蒸馏与上下文蒸馏相结合，通过让学生在自身生成的轨迹上进行训练，并最小化与上下文条件教师模型的反向KL散度，来使语言模型将上下文知识内化到参数中。该方法在经验知识蒸馏和系统提示蒸馏两个应用上表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决如何让语言模型更有效地将上下文中的知识（如历史解决方案或优化提示）内化到其自身参数中的问题，以提升任务性能并保持分布外泛化能力。</p>
<p><strong>Result:</strong> 在数学推理、文本游戏和特定领域任务上，OPCD持续优于基线方法，获得了更高的任务准确率，并更好地保持了分布外能力。同时，它支持有效的跨尺寸蒸馏，使更小的学生模型能从更大的教师模型中内化经验知识。</p>
<p><strong>Insight:</strong> 创新点在于将策略蒸馏的在线学习思想引入上下文蒸馏，通过让学生模型在自身生成的数据上进行训练并与上下文条件教师对齐，实现了更稳定、更有效的知识内化。这为模型压缩和从交互经验中学习提供了新思路。</p>
<p><strong>Abstract:</strong> Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.</p>
  </div>
</details>

<hr>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="34-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning-cs-CV-cs-CL-cs-ROPDF"><a href="#34-ABot-M0-VLA-Foundation-Model-for-Robotic-Manipulation-with-Action-Manifold-Learning-cs-CV-cs-CL-cs-ROPDF" class="headerlink" title="[34] ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning cs.CV | cs.CL | cs.ROPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11236">ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11236" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yandan Yang, Shuang Zeng, Tong Lin, Xinyuan Chang, Dekang Qi</span></p>
<p><strong>TL;DR:</strong> 本文提出了ABot-M0框架，这是一个面向机器人操作的视觉-语言-动作（VLA）基础模型。它通过构建统一的数据集UniACT-dataset，并引入动作流形学习（AML）假设，将动作预测建模为向低维可行流形的投影，从而提升了策略的效率和稳定性。该框架还采用双流感知机制，结合VLM语义与几何先验，增强了对3D空间的理解。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决构建通用具身智能体（’one-brain, many-forms’）的挑战，这些挑战包括数据碎片化、表示不一致以及训练目标不统一。</p>
<p><strong>Result:</strong> 实验表明，框架的各个组件独立运行且具有叠加效益。基于六个公共数据集构建了包含超过600万条轨迹、9500小时数据的大规模数据集UniACT-dataset，并通过统一预训练提升了跨平台和跨任务的知识迁移与泛化能力。</p>
<p><strong>Insight:</strong> 主要创新点包括：1）提出了动作流形假设，将动作学习从去噪转变为向低维可行流形的投影，使用DiT主干直接预测干净连续的动作序列，提高了解码速度和策略稳定性；2）设计了系统化的数据治理流程，构建了大规模、标准化的统一数据集；3）采用了模块化的双流感知机制，在不修改主干网络的情况下，结合VLM语义与可插拔3D模块的几何先验，增强了3D空间理解能力。</p>
<p><strong>Abstract:</strong> Building general-purpose embodied agents across diverse hardware remains a central challenge in robotics, often framed as the ‘’one-brain, many-forms’’ paradigm. Progress is hindered by fragmented data, inconsistent representations, and misaligned training objectives. We present ABot-M0, a framework that builds a systematic data curation pipeline while jointly optimizing model architecture and training strategies, enabling end-to-end transformation of heterogeneous raw data into unified, efficient representations. From six public datasets, we clean, standardize, and balance samples to construct UniACT-dataset, a large-scale dataset with over 6 million trajectories and 9,500 hours of data, covering diverse robot morphologies and task scenarios. Unified pre-training improves knowledge transfer and generalization across platforms and tasks, supporting general-purpose embodied intelligence. To improve action prediction efficiency and stability, we propose the Action Manifold Hypothesis: effective robot actions lie not in the full high-dimensional space but on a low-dimensional, smooth manifold governed by physical laws and task constraints. Based on this, we introduce Action Manifold Learning (AML), which uses a DiT backbone to predict clean, continuous action sequences directly. This shifts learning from denoising to projection onto feasible manifolds, improving decoding speed and policy stability. ABot-M0 supports modular perception via a dual-stream mechanism that integrates VLM semantics with geometric priors and multi-view inputs from plug-and-play 3D modules such as VGGT and Qwen-Image-Edit, enhancing spatial understanding without modifying the backbone and mitigating standard VLM limitations in 3D reasoning. Experiments show components operate independently with additive benefits. We will release all code and pipelines for reproducibility and future research.</p>
  </div>
</details>

<hr>
<h3 id="35-Active-Zero-Self-Evolving-Vision-Language-Models-through-Active-Environment-Exploration-cs-CV-cs-LGPDF"><a href="#35-Active-Zero-Self-Evolving-Vision-Language-Models-through-Active-Environment-Exploration-cs-CV-cs-LGPDF" class="headerlink" title="[35] Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration cs.CV | cs.LGPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11241">Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11241" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jinghan He, Junfeng Fang, Feng Xiong, Zijun Yao, Fei Shen</span></p>
<p><strong>TL;DR:</strong> 本文提出Active-Zero框架，通过主动探索视觉环境而非被动交互静态图像，使视觉语言模型能够自主进化。该框架包含三个协同进化的智能体：根据模型能力边界从开放世界检索图像的Searcher、合成校准推理任务的Questioner以及通过准确性奖励优化的Solver，形成一个自我搭建课程学习的闭环系统。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言模型的自对弈方法依赖于与静态图像集的被动交互，导致对初始数据集的强依赖和学习效率低下。模型无法主动寻找与其进化能力相匹配的视觉数据，浪费计算资源在过于简单或困难的样本上。</p>
<p><strong>Result:</strong> 在Qwen2.5-VL-7B-Instruct模型上，于12个基准测试中，Active-Zero在推理任务上达到53.97的平均准确率（提升5.7%），在通用理解任务上达到59.77（提升3.9%），持续超越现有自对弈基线方法。</p>
<p><strong>Insight:</strong> 创新点在于将自对弈范式从被动交互转变为主动环境探索，通过协同进化智能体实现自我搭建的自动课程学习。客观来看，其核心是构建了一个动态、自适应的数据检索与任务生成闭环，使模型能自主引导其学习轨迹，这为可扩展的自进化视觉语言系统提供了关键思路。</p>
<p><strong>Abstract:</strong> Self-play has enabled large language models to autonomously improve through self-generated challenges. However, existing self-play methods for vision-language models rely on passive interaction with static image collections, resulting in strong dependence on initial datasets and inefficient learning. Without the ability to actively seek visual data tailored to their evolving capabilities, agents waste computational effort on samples that are either trivial or beyond their current skill level. To address these limitations, we propose Active-Zero, a framework that shifts from passive interaction to active exploration of visual environments. Active-Zero employs three co-evolving agents: a Searcher that retrieves images from open-world repositories based on the model’s capability frontier, a Questioner that synthesizes calibrated reasoning tasks, and a Solver refined through accuracy rewards. This closed loop enables self-scaffolding auto-curricula where the model autonomously constructs its learning trajectory. On Qwen2.5-VL-7B-Instruct across 12 benchmarks, Active-Zero achieves 53.97 average accuracy on reasoning tasks (5.7% improvement) and 59.77 on general understanding (3.9% improvement), consistently outperforming existing self-play baselines. These results highlight active exploration as a key ingredient for scalable and adaptive self-evolving vision-language systems.</p>
  </div>
</details>

<hr>
<h3 id="36-ReTracing-An-Archaeological-Approach-Through-Body-Machine-and-Generative-Systems-cs-CVPDF"><a href="#36-ReTracing-An-Archaeological-Approach-Through-Body-Machine-and-Generative-Systems-cs-CVPDF" class="headerlink" title="[36] ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems cs.CVPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11242">ReTracing: An Archaeological Approach Through Body, Machine, and Generative Systems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11242" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yitong Wang, Yue Yao</span></p>
<p><strong>TL;DR:</strong> ReTracing是一个多智能体具身表演艺术项目，采用考古学方法研究人工智能如何塑造、约束并生成身体动作。项目从科幻小说中提取描述人机交互的句子，利用大语言模型生成’做什么’和’不做什么’的配对提示，再通过扩散式文本到视频模型将这些提示转化为人类表演者的编舞指导和四足机器人的运动指令。双方在镜面地板上执行动作，通过多摄像头运动捕捉重建为3D点云和运动轨迹，形成动作痕迹的数字档案。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在通过具身表演揭示生成式系统如何通过编排的动作编码社会文化偏见，并探讨在能够移动、思考和留下痕迹的AI时代，’为人’意味着什么这一时代关键问题。</p>
<p><strong>Result:</strong> 论文未提及具体定量结果或基准测试，但构建了一个包含人类与机器人动作痕迹的数字档案，作为分析生成系统偏见的媒介。</p>
<p><strong>Insight:</strong> 创新点在于将考古学方法论与生成式AI（LLM和扩散模型）结合，通过多智能体（人类、机器人）具身交互和3D运动重建，创造性地将文本指令转化为跨媒介（文本-视频-动作-点云）的表演档案，为研究AI系统中的偏见提供了新颖的表演艺术与计算分析融合的框架。</p>
<p><strong>Abstract:</strong> We present ReTracing, a multi-agent embodied performance art that adopts an archaeological approach to examine how artificial intelligence shapes, constrains, and produces bodily movement. Drawing from science-fiction novels, the project extracts sentences that describe human-machine interaction. We use large language models (LLMs) to generate paired prompts “what to do” and “what not to do” for each excerpt. A diffusion-based text-to-video model transforms these prompts into choreographic guides for a human performer and motor commands for a quadruped robot. Both agents enact the actions on a mirrored floor, captured by multi-camera motion tracking and reconstructed into 3D point clouds and motion trails, forming a digital archive of motion traces. Through this process, ReTracing serves as a novel approach to reveal how generative systems encode socio-cultural biases through choreographed movements. Through an immersive interplay of AI, human, and robot, ReTracing confronts a critical question of our time: What does it mean to be human among AIs that also move, think, and leave traces behind?</p>
  </div>
</details>

<hr>
<h3 id="37-Stress-Tests-REVEAL-Fragile-Temporal-and-Visual-Grounding-in-Video-Language-Models-cs-CVPDF"><a href="#37-Stress-Tests-REVEAL-Fragile-Temporal-and-Visual-Grounding-in-Video-Language-Models-cs-CVPDF" class="headerlink" title="[37] Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models cs.CVPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11244">Stress Tests REVEAL Fragile Temporal and Visual Grounding in Video-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11244" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sethuraman T, Savya Khosla, Aditi Tiwari, Vidya Ganesh, Rakshana Jayaprakash</span></p>
<p><strong>TL;DR:</strong> 本文通过REVEAL基准测试揭示了当前视频-语言模型在视频内容理解、时序序列和运动感知方面的脆弱性，发现这些模型存在时序期望偏差、依赖纯语言捷径、视频附和、相机运动敏感性和时空遮挡鲁棒性差等问题，而人类却能轻松应对这些任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探究视频-语言模型是否能稳健地处理视频内容、时序和运动，并揭示其潜在弱点。</p>
<p><strong>Result:</strong> 在REVEAL基准的五个压力测试中，主流开源和闭源VidLMs表现不佳，例如将反向场景误判为正向、忽略视频内容回答问题、同意错误主张、难以处理基本相机运动以及在简单时空遮挡下无法聚合时序信息，而人类表现轻松超越模型。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个可自动生成诊断示例的基准测试框架，系统性评估VidLM的脆弱性；客观分析表明，该研究强调了当前模型在时空基础任务上的严重不足，为未来改进提供了具体方向。</p>
<p><strong>Abstract:</strong> This work investigates a fundamental question: Do Video-Language Models (VidLMs) robustly account for video content, temporal sequence, and motion? Our investigation shows that, surprisingly, they often do not. We introduce REVEAL{}, a diagnostic benchmark that probes fundamental weaknesses of contemporary VidLMs through five controlled stress tests; assessing temporal expectation bias, reliance on language-only shortcuts, video sycophancy, camera motion sensitivity, and robustness to spatiotemporal occlusion. We test leading open- and closed-source VidLMs and find that these models confidently describe reversed scenes as forward, answer questions while neglecting video content, agree with false claims, struggle with basic camera motion, and fail to aggregate temporal information amidst simple spatiotemporal masking. Humans, on the other hand, succeed at these tasks with ease. Alongside our benchmark, we provide a data pipeline that automatically generates diagnostic examples for our stress tests, enabling broader and more scalable evaluation. We will release our benchmark and code to support future research.</p>
  </div>
</details>

<hr>
<h3 id="38-MDE-VIO-Enhancing-Visual-Inertial-Odometry-Using-Learned-Depth-Priors-cs-CVPDF"><a href="#38-MDE-VIO-Enhancing-Visual-Inertial-Odometry-Using-Learned-Depth-Priors-cs-CVPDF" class="headerlink" title="[38] MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors cs.CVPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11323">MDE-VIO: Enhancing Visual-Inertial Odometry Using Learned Depth Priors</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11323" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arda Alniak, Sinan Kalkan, Mustafa Mert Ankarali, Afsar Saranli, Abdullah Aydin Alatan</span></p>
<p><strong>TL;DR:</strong> 本文提出MDE-VIO，一种将学习到的深度先验直接集成到VINS-Mono优化后端的方法，以增强单目视觉惯性里程计（VIO）在低纹理环境下的性能。该方法通过实施仿射不变深度一致性和成对序数约束，并基于方差门控过滤不稳定伪影，在严格遵循边缘设备计算限制的同时，鲁棒地恢复度量尺度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统单目VIO系统在低纹理环境中表现不佳，因为稀疏的视觉特征不足以进行精确的位姿估计。虽然基于ViT的复杂基础模型能提供密集且几何一致的深度，但其计算需求通常使其无法实时部署在边缘设备上。本文旨在弥合这一差距。</p>
<p><strong>Result:</strong> 在TartanGround和M3ED数据集上的大量实验表明，该方法在具有挑战性的场景中防止了发散，并带来了显著的精度提升，将绝对轨迹误差（ATE）降低了高达28.3%。</p>
<p><strong>Insight:</strong> 主要创新点在于将学习到的深度先验直接集成到VIO优化后端，并提出了仿射不变深度一致性约束、成对序数约束以及基于方差的门控机制来过滤不稳定深度估计，从而在保证边缘设备实时性的前提下，有效利用密集深度信息提升VIO的鲁棒性和精度。</p>
<p><strong>Abstract:</strong> Traditional monocular Visual-Inertial Odometry (VIO) systems struggle in low-texture environments where sparse visual features are insufficient for accurate pose estimation. To address this, dense Monocular Depth Estimation (MDE) has been widely explored as a complementary information source. While recent Vision Transformer (ViT) based complex foundational models offer dense, geometrically consistent depth, their computational demands typically preclude them from real-time edge deployment. Our work bridges this gap by integrating learned depth priors directly into the VINS-Mono optimization backend. We propose a novel framework that enforces affine-invariant depth consistency and pairwise ordinal constraints, explicitly filtering unstable artifacts via variance-based gating. This approach strictly adheres to the computational limits of edge devices while robustly recovering metric scale. Extensive experiments on the TartanGround and M3ED datasets demonstrate that our method prevents divergence in challenging scenarios and delivers significant accuracy gains, reducing Absolute Trajectory Error (ATE) by up to 28.3%. Code will be made available.</p>
  </div>
</details>

<hr>
<h3 id="39-Exploring-Real-Time-Super-Resolution-Benchmarking-and-Fine-Tuning-for-Streaming-Content-cs-CVPDF"><a href="#39-Exploring-Real-Time-Super-Resolution-Benchmarking-and-Fine-Tuning-for-Streaming-Content-cs-CVPDF" class="headerlink" title="[39] Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content cs.CVPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11339">Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11339" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Evgeney Bogatyrev, Khaled Abud, Ivan Molodetskikh, Nikita Alutis, Dmitry Vatolin</span></p>
<p><strong>TL;DR:</strong> 本文针对实时超分辨率在流媒体视频应用中的挑战，提出了一个从YouTube采集的流媒体专用数据集StreamSR，并基于此对11种SOTA模型进行了基准测试。同时，作者提出了一种名为EfRLFN的高效实时模型，该模型集成了高效通道注意力机制和双曲正切激活函数，并通过复合损失函数优化训练。实验表明，在该数据集上微调其他模型也能带来显著的性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有实时超分辨率方法在处理压缩视频流时面临独特挑战，且常用数据集无法准确反映流媒体特性，导致现有基准测试与实际应用脱节。</p>
<p><strong>Result:</strong> 在提出的StreamSR数据集上对11种SOTA模型进行了基准测试。提出的EfRLFN模型在视觉质量和运行效率上均有提升。此外，在其他模型上使用该数据集进行微调，也能在多个标准基准测试上带来显著的、泛化性好的性能增益。</p>
<p><strong>Insight:</strong> 创新点包括：1）构建了首个专注于流媒体场景的、具有代表性的实时超分辨率数据集StreamSR，填补了领域空白；2）提出EfRLFN模型，将高效通道注意力与双曲正切激活函数结合用于实时超分辨率，并设计了复合损失函数以改善训练收敛，这是一种新颖的架构设计选择；3）证明了在领域特定数据集上进行微调的有效性，为提升模型在实际应用中的性能提供了实用路径。</p>
<p><strong>Abstract:</strong> Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a comprehensive dataset - StreamSR - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.   Furthermore, we propose EfRLFN, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.   Finally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at <a target="_blank" rel="noopener" href="https://github.com/EvgeneyBogatyrev/EfRLFN">https://github.com/EvgeneyBogatyrev/EfRLFN</a>.</p>
  </div>
</details>

<hr>
<h3 id="40-Ctrl-Shift-High-Quality-Geometry-Aware-Object-Manipulation-in-Visual-Generation-cs-CVPDF"><a href="#40-Ctrl-Shift-High-Quality-Geometry-Aware-Object-Manipulation-in-Visual-Generation-cs-CVPDF" class="headerlink" title="[40] Ctrl&amp;Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation cs.CVPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11440">Ctrl&amp;Shift: High-Quality Geometry-Aware Object Manipulation in Visual Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11440" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Penghui Ruan, Bojia Zi, Xianbiao Qi, Youze Huang, Rong Xiao</span></p>
<p><strong>TL;DR:</strong> 本文提出了Ctrl&amp;Shift，一个端到端的扩散模型框架，用于实现无需显式3D表示的、几何一致的对象级图像&#x2F;视频编辑。其核心是将操作分解为对象移除和相机姿态控制下的参考引导修复两个阶段，并通过统一的多任务、多阶段训练策略分离背景、对象身份和姿态信号。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有对象级编辑方法难以同时实现背景保持、视点变换下的几何一致性以及用户可控变换这三个核心目标的问题。基于几何的方法控制精确但泛化差，而基于扩散的方法泛化好但缺乏细粒度几何控制。</p>
<p><strong>Result:</strong> 广泛的实验表明，Ctrl&amp;Shift在保真度、视点一致性和可控性方面取得了最先进（SOTA）的结果。</p>
<p><strong>Insight:</strong> 主要创新点在于将编辑任务分解为移除与姿态引导修复的两阶段扩散过程，并设计了多任务训练策略来解耦控制信号。此外，构建了一个可扩展的真实世界数据集生成流程，以提升泛化能力。这是首个在不依赖任何显式3D建模的情况下，统一了细粒度几何控制和真实世界泛化能力的对象操作框架。</p>
<p><strong>Abstract:</strong> Object-level manipulation, relocating or reorienting objects in images or videos while preserving scene realism, is central to film post-production, AR, and creative editing. Yet existing methods struggle to jointly achieve three core goals: background preservation, geometric consistency under viewpoint shifts, and user-controllable transformations. Geometry-based approaches offer precise control but require explicit 3D reconstruction and generalize poorly; diffusion-based methods generalize better but lack fine-grained geometric control. We present Ctrl&amp;Shift, an end-to-end diffusion framework to achieve geometry-consistent object manipulation without explicit 3D representations. Our key insight is to decompose manipulation into two stages, object removal and reference-guided inpainting under explicit camera pose control, and encode both within a unified diffusion process. To enable precise, disentangled control, we design a multi-task, multi-stage training strategy that separates background, identity, and pose signals across tasks. To improve generalization, we introduce a scalable real-world dataset construction pipeline that generates paired image and video samples with estimated relative camera poses. Extensive experiments demonstrate that Ctrl&amp;Shift achieves state-of-the-art results in fidelity, viewpoint consistency, and controllability. To our knowledge, this is the first framework to unify fine-grained geometric control and real-world generalization for object manipulation, without relying on any explicit 3D modeling.</p>
  </div>
</details>

<hr>
<h3 id="41-A-Dual-Branch-Framework-for-Semantic-Change-Detection-with-Boundary-and-Temporal-Awareness-cs-CVPDF"><a href="#41-A-Dual-Branch-Framework-for-Semantic-Change-Detection-with-Boundary-and-Temporal-Awareness-cs-CVPDF" class="headerlink" title="[41] A Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness cs.CVPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11466">A Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11466" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yun-Cheng Li, Sen Lei, Heng-Chao Li, Ke Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为DBTANet的双分支框架，用于语义变化检测，该框架结合了边界和时间感知。通过使用双分支孪生编码器，其中冻结的SAM分支捕获全局语义上下文和边界先验，ResNet34分支提供局部空间细节，并设计了双向时间感知模块（BTAM）来聚合多尺度特征和对称捕获时间依赖，以及高斯平滑投影模块（GSPM）来细化浅层SAM特征以增强边界信息。在两个公共基准测试上的实验表明，DBTANet有效整合了全局语义、局部细节、时间推理和边界感知，实现了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有语义变化检测方法中边界模糊和时间建模不足的问题，以提高分割精度。</p>
<p><strong>Result:</strong> 在两个公共基准测试上进行了广泛实验，DBTANet实现了最先进的性能（SOTA）。</p>
<p><strong>Insight:</strong> 创新点包括双分支编码器结合SAM和ResNet34的互补特征表示、双向时间感知模块（BTAM）对称捕获时间依赖，以及高斯平滑投影模块（GSPM）增强边界信息；从客观角度看，该方法有效整合了全局与局部特征、时间推理和边界感知，为语义变化检测提供了新思路。</p>
<p><strong>Abstract:</strong> Semantic Change Detection (SCD) aims to detect and categorize land-cover changes from bi-temporal remote sensing images. Existing methods often suffer from blurred boundaries and inadequate temporal modeling, limiting segmentation accuracy. To address these issues, we propose a Dual-Branch Framework for Semantic Change Detection with Boundary and Temporal Awareness, termed DBTANet. Specifically, we utilize a dual-branch Siamese encoder where a frozen SAM branch captures global semantic context and boundary priors, while a ResNet34 branch provides local spatial details, ensuring complementary feature representations. On this basis, we design a Bidirectional Temporal Awareness Module (BTAM) to aggregate multi-scale features and capture temporal dependencies in a symmetric manner. Furthermore, a Gaussian-smoothed Projection Module (GSPM) refines shallow SAM features, suppressing noise while enhancing edge information for boundary-aware constraints. Extensive experiments on two public benchmarks demonstrate that DBTANet effectively integrates global semantics, local details, temporal reasoning, and boundary awareness, achieving state-of-the-art performance.</p>
  </div>
</details>

<hr>
<h3 id="42-Arbitrary-Ratio-Feature-Compression-via-Next-Token-Prediction-cs-CVPDF"><a href="#42-Arbitrary-Ratio-Feature-Compression-via-Next-Token-Prediction-cs-CVPDF" class="headerlink" title="[42] Arbitrary Ratio Feature Compression via Next Token Prediction cs.CVPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11494">Arbitrary Ratio Feature Compression via Next Token Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11494" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yufan Liu, Daoyuan Ren, Zhipeng Zhang, Wenyang Luo, Bing Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖且灵活的任意比率特征压缩（ARFC）框架，通过单个模型支持任意压缩比，无需为不同比率训练专用模型。其核心是自回归的任意比率压缩器（ARC），通过下一令牌预测进行压缩，推理时通过调整生成令牌数量控制压缩比。为提高压缩特征质量，引入了混合解决方案（MoS）模块和实体关系图约束（ERGC）。在跨模态检索、图像分类和图像检索任务上的实验表明，该方法在各种压缩比下均优于现有方法，有时甚至超过原始未压缩特征的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有特征压缩方法通常依赖专用模型实现特定压缩比，缺乏灵活性且泛化能力有限，适应新压缩比需要重新训练。本文旨在解决这一限制，提出一个支持任意压缩比的通用框架。</p>
<p><strong>Result:</strong> 在多个数据集（如跨模态检索、图像分类、图像检索任务）上的广泛实验表明，该方法在各种压缩比下始终优于现有方法，在某些情况下甚至超越了原始未压缩特征的性能，验证了其有效性和通用性。</p>
<p><strong>Insight:</strong> 创新点包括：1) 通过自回归下一令牌预测实现任意比率压缩，使压缩比在推理时可灵活调整；2) 引入MoS模块利用多个压缩结果减少不确定性；3) 集成ERGC约束以保持压缩过程中的语义和结构关系。从客观角度看，将序列生成思想应用于特征压缩是一个有前景的方向，提高了模型的实用性和效率。</p>
<p><strong>Abstract:</strong> Feature compression is increasingly important for improving the efficiency of downstream tasks, especially in applications involving large-scale or multi-modal data. While existing methods typically rely on dedicated models for achieving specific compression ratios, they are often limited in flexibility and generalization. In particular, retraining is necessary when adapting to a new compression ratio. To address this limitation, we propose a novel and flexible Arbitrary Ratio Feature Compression (ARFC) framework, which supports any compression ratio with a single model, eliminating the need for multiple specialized models. At its core, the Arbitrary Ratio Compressor (ARC) is an auto-regressive model that performs compression via next-token prediction. This allows the compression ratio to be controlled at inference simply by adjusting the number of generated tokens. To enhance the quality of the compressed features, two key modules are introduced. The Mixture of Solutions (MoS) module refines the compressed tokens by utilizing multiple compression results (solutions), reducing uncertainty and improving robustness. The Entity Relation Graph Constraint (ERGC) is integrated into the training process to preserve semantic and structural relationships during compression. Extensive experiments on cross-modal retrieval, image classification, and image retrieval tasks across multiple datasets demonstrate that our method consistently outperforms existing approaches at various compression ratios. Notably, in some cases, it even surpasses the performance of the original, uncompressed features. These results validate the effectiveness and versatility of ARFC for practical, resource-constrained scenarios.</p>
  </div>
</details>

<hr>
<h3 id="43-What-if-Agents-Could-Imagine-Reinforcing-Open-Vocabulary-HOI-Comprehension-through-Generation-cs-CVPDF"><a href="#43-What-if-Agents-Could-Imagine-Reinforcing-Open-Vocabulary-HOI-Comprehension-through-Generation-cs-CVPDF" class="headerlink" title="[43] What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation cs.CVPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11499">What if Agents Could Imagine? Reinforcing Open-Vocabulary HOI Comprehension through Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11499" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhenlong Yuan, Xiangyan Qu, Jing Tang, Rui Chen, Lei Sun</span></p>
<p><strong>TL;DR:</strong> 本文提出了ImagineAgent框架，通过结合认知推理与生成式想象来增强开放词汇人-物交互理解，利用认知图建模实体与动作关系，并动态调用检索增强、图像裁剪和扩散模型等工具来获取领域知识和视觉证据，以解决多模态大语言模型在OV-HOI任务中的幻觉和遮挡模糊问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态大语言模型在开放词汇人-物交互理解中存在的跨模态幻觉和遮挡引起的模糊性问题。</p>
<p><strong>Result:</strong> 在SWIG-HOI和HICO-DET数据集上实现了SOTA性能，且仅需约20%的训练数据，验证了方法的鲁棒性和效率。</p>
<p><strong>Insight:</strong> 创新点在于构建认知图来显式建模实体与动作的合理关系，并通过动态工具调用实现跨模态对齐；可借鉴之处包括结合生成式想象增强视觉理解，以及使用复合奖励平衡预测准确性和工具效率。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models have shown promising capabilities in bridging visual and textual reasoning, yet their reasoning capabilities in Open-Vocabulary Human-Object Interaction (OV-HOI) are limited by cross-modal hallucinations and occlusion-induced ambiguity. To address this, we propose \textbf{ImagineAgent}, an agentic framework that harmonizes cognitive reasoning with generative imagination for robust visual understanding. Specifically, our method innovatively constructs cognitive maps that explicitly model plausible relationships between detected entities and candidate actions. Subsequently, it dynamically invokes tools including retrieval augmentation, image cropping, and diffusion models to gather domain-specific knowledge and enriched visual evidence, thereby achieving cross-modal alignment in ambiguous scenarios. Moreover, we propose a composite reward that balances prediction accuracy and tool efficiency. Evaluations on SWIG-HOI and HICO-DET datasets demonstrate our SOTA performance, requiring approximately 20% of training data compared to existing methods, validating our robustness and efficiency.</p>
  </div>
</details>

<hr>
<h3 id="44-Supervise-assisted-Multi-modality-Fusion-Diffusion-Model-for-PET-Restoration-cs-CVPDF"><a href="#44-Supervise-assisted-Multi-modality-Fusion-Diffusion-Model-for-PET-Restoration-cs-CVPDF" class="headerlink" title="[44] Supervise-assisted Multi-modality Fusion Diffusion Model for PET Restoration cs.CVPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11545">Supervise-assisted Multi-modality Fusion Diffusion Model for PET Restoration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11545" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yingkai Zhang, Shuang Chen, Ye Tian, Yunyi Gao, Jianyong Jiang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种监督辅助的多模态融合扩散模型（MFdiff），用于从低剂量PET（LPET）和磁共振（MR）图像中恢复高质量的标准剂量PET（SPET）图像。该方法通过多模态特征融合模块学习优化的融合特征，并以此作为扩散模型的额外条件进行迭代生成。同时，采用两阶段监督辅助学习策略，结合模拟分布内数据的通用先验和针对体内分布外数据的特定先验，以解决多模态融合中的结构纹理不一致和分布外数据不匹配问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 降低PET扫描的放射性示踪剂剂量或扫描时间会损害图像质量，而利用具有清晰解剖信息的MR图像从LPET恢复SPET是一种有前景的方法，但面临多模态融合的结构纹理不一致以及分布外数据不匹配的挑战。</p>
<p><strong>Result:</strong> 实验表明，所提出的MFdiff方法能够从多模态输入中有效恢复高质量的SPET图像，在定性和定量评估上均优于现有最先进方法。</p>
<p><strong>Insight:</strong> 创新点包括设计多模态特征融合模块以避免引入无关细节，将融合特征作为扩散模型的条件进行迭代生成，以及采用两阶段监督辅助学习策略以结合通用和特定先验知识来处理分布外数据，从而提升恢复图像的质量和鲁棒性。</p>
<p><strong>Abstract:</strong> Positron emission tomography (PET) offers powerful functional imaging but involves radiation exposure. Efforts to reduce this exposure by lowering the radiotracer dose or scan time can degrade image quality. While using magnetic resonance (MR) images with clearer anatomical information to restore standard-dose PET (SPET) from low-dose PET (LPET) is a promising approach, it faces challenges with the inconsistencies in the structure and texture of multi-modality fusion, as well as the mismatch in out-of-distribution (OOD) data. In this paper, we propose a supervise-assisted multi-modality fusion diffusion model (MFdiff) for addressing these challenges for high-quality PET restoration. Firstly, to fully utilize auxiliary MR images without introducing extraneous details in the restored image, a multi-modality feature fusion module is designed to learn an optimized fusion feature. Secondly, using the fusion feature as an additional condition, high-quality SPET images are iteratively generated based on the diffusion model. Furthermore, we introduce a two-stage supervise-assisted learning strategy that harnesses both generalized priors from simulated in-distribution datasets and specific priors tailored to in-vivo OOD data. Experiments demonstrate that the proposed MFdiff effectively restores high-quality SPET images from multi-modality inputs and outperforms state-of-the-art methods both qualitatively and quantitatively.</p>
  </div>
</details>

<hr>
<h3 id="45-LUVE-Latent-Cascaded-Ultra-High-Resolution-Video-Generation-with-Dual-Frequency-Experts-cs-CVPDF"><a href="#45-LUVE-Latent-Cascaded-Ultra-High-Resolution-Video-Generation-with-Dual-Frequency-Experts-cs-CVPDF" class="headerlink" title="[45] LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts cs.CVPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11564">LUVE : Latent-Cascaded Ultra-High-Resolution Video Generation with Dual Frequency Experts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11564" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chen Zhao, Jiawei Chen, Hongyu Li, Zhuoliang Kang, Shilin Lu</span></p>
<p><strong>TL;DR:</strong> 本文提出LUVE，一种用于超高分辨率视频生成的潜在空间级联框架。它采用三阶段架构：低分辨率运动生成、潜在空间视频上采样以及集成低频与高频专家的高分辨率内容细化，旨在解决运动建模、语义规划和细节合成的复合挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视频扩散模型在视觉质量上虽有进步，但超高分辨率视频生成仍面临运动建模、语义规划和细节合成等多重困难，需要一种高效且高质量的方法。</p>
<p><strong>Result:</strong> 大量实验表明，LUVE在超高分辨率视频生成中实现了卓越的逼真度和内容保真度，消融研究进一步验证了各组件有效性。</p>
<p><strong>Insight:</strong> 创新点包括潜在空间级联设计以减少计算开销，以及双频专家（低频与高频）的集成以共同增强语义连贯性和细粒度细节生成，为高效UHR视频生成提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in video diffusion models have significantly improved visual quality, yet ultra-high-resolution (UHR) video generation remains a formidable challenge due to the compounded difficulties of motion modeling, semantic planning, and detail synthesis. To address these limitations, we propose \textbf{LUVE}, a \textbf{L}atent-cascaded \textbf{U}HR \textbf{V}ideo generation framework built upon dual frequency \textbf{E}xperts. LUVE employs a three-stage architecture comprising low-resolution motion generation for motion-consistent latent synthesis, video latent upsampling that performs resolution upsampling directly in the latent space to mitigate memory and computational overhead, and high-resolution content refinement that integrates low-frequency and high-frequency experts to jointly enhance semantic coherence and fine-grained detail generation. Extensive experiments demonstrate that our LUVE achieves superior photorealism and content fidelity in UHR video generation, and comprehensive ablation studies further validate the effectiveness of each component. The project is available at \href{<a target="_blank" rel="noopener" href="https://unicornanrocinu.github.io/LUVE_web/%7D%7Bhttps://github.io/LUVE/%7D">https://unicornanrocinu.github.io/LUVE_web/}{https://github.io/LUVE/}</a>.</p>
  </div>
</details>

<hr>
<h3 id="46-Move-What-Matters-Parameter-Efficient-Domain-Adaptation-via-Optimal-Transport-Flow-for-Collaborative-Perception-cs-CVPDF"><a href="#46-Move-What-Matters-Parameter-Efficient-Domain-Adaptation-via-Optimal-Transport-Flow-for-Collaborative-Perception-cs-CVPDF" class="headerlink" title="[46] Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11565">Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11565" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zesheng Jia, Jin Wang, Siao Liu, Lingzhi Li, Ziyao Huang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为FlowAdapt的参数高效领域自适应框架，用于解决V2X协同感知中多智能体系统跨域部署的挑战。该框架基于最优传输理论，通过Wasserstein贪婪采样策略过滤冗余样本，并设计渐进知识传输模块来缓解深层语义退化，仅使用1%的可训练参数即可实现SOTA性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决多智能体协同感知中直接应用参数高效微调（PEFT）时出现的性能下降和训练不稳定问题，具体归因于异构传感器流中的帧间冗余和PEFT适应下深层表示的细粒度语义侵蚀。</p>
<p><strong>Result:</strong> 在三个基准测试上的广泛实验表明，FlowAdapt仅使用1%的可训练参数就实现了最先进的性能，有效弥合了领域差距，并具有优异的样本效率和泛化能力。</p>
<p><strong>Insight:</strong> 创新点包括基于最优传输理论最小化跨数据分布和网络层次的信息传输成本，引入Wasserstein贪婪采样策略选择性过滤冗余样本，以及设计渐进知识传输模块通过可学习路径将压缩的早期表示注入后期阶段以减轻语义退化。</p>
<p><strong>Abstract:</strong> Fast domain adaptation remains a fundamental challenge for deploying multi-agent systems across diverse environments in Vehicle-to-Everything (V2X) collaborative perception. Despite the success of Parameter-Efficient Fine-Tuning (PEFT) in natural language processing and conventional vision tasks, directly applying PEFT to multi-agent settings leads to significant performance degradation and training instability. In this work, we conduct a detailed analysis and identify two key factors: (i) inter-frame redundancy in heterogeneous sensory streams, and (ii) erosion of fine-grained semantics in deep-layer representations under PEFT adaptation. To address these issues, we propose FlowAdapt, a parameter-efficient framework grounded in optimal transport theory, which minimizes information transport costs across both data distributions and network hierarchies. Specifically, we introduce a Wasserstein Greedy Sampling strategy to selectively filter redundant samples via a bounded covering radius. Furthermore, Progressive Knowledge Transfer module is designed to progressively inject compressed early-stage representations into later stages through learnable pathways, alleviating semantic degradation in late-stage adaptation. Extensive experiments on three benchmarks demonstrate that FlowAdapt achieves state-of-the-art performance with only 1% of trainable parameters, effectively bridging domain gaps with superior sample efficiency and generalization.</p>
  </div>
</details>

<hr>
<h3 id="47-A-Large-Language-Model-for-Disaster-Structural-Reconnaissance-Summarization-cs-CVPDF"><a href="#47-A-Large-Language-Model-for-Disaster-Structural-Reconnaissance-Summarization-cs-CVPDF" class="headerlink" title="[47] A Large Language Model for Disaster Structural Reconnaissance Summarization cs.CVPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11588">A Large Language Model for Disaster Structural Reconnaissance Summarization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11588" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuqing Gao, Guanren Zhou, Khalid M. Mosalam</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于大语言模型（LLM）的灾害结构侦察总结框架（LLM-DRS），用于改进基于视觉的结构健康监测（SHM）。该框架通过标准化的侦察计划收集图像数据和元数据，利用深度卷积神经网络提取关键属性，并借助精心设计的提示词驱动LLM生成针对单个结构或受灾区域的总结报告。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的基于AI视觉的SHM方法通常只输出离散的损伤类别标签和区域坐标，需要工程师进一步重组和分析以进行评估和决策，过程繁琐。LLM的兴起为自动化、智能化的灾害侦察报告生成提供了新思路。</p>
<p><strong>Result:</strong> 研究结果表明，将LLM集成到基于视觉的SHM中，特别是在快速灾后侦察方面，展现出通过有效侦察提高建筑环境韧性的巨大潜力。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个将标准化侦察流程、深度视觉特征提取与大语言模型文本生成能力相结合的端到端框架（LLM-DRS），实现了从原始视觉数据到结构化、可读性强的总结报告的自动化生成，提升了灾后评估的效率和智能化水平。</p>
<p><strong>Abstract:</strong> Artificial Intelligence (AI)-aided vision-based Structural Health Monitoring (SHM) has emerged as an effective approach for monitoring and assessing structural condition by analyzing image and video data. By integrating Computer Vision (CV) and Deep Learning (DL), vision-based SHM can automatically identify and localize visual patterns associated with structural damage. However, previous works typically generate only discrete outputs, such as damage class labels and damage region coordinates, requiring engineers to further reorganize and analyze these results for evaluation and decision-making. In late 2022, Large Language Models (LLMs) became popular across multiple fields, providing new insights into AI-aided vision-based SHM. In this study, a novel LLM-based Disaster Reconnaissance Summarization (LLM-DRS) framework is proposed. It introduces a standard reconnaissance plan in which the collection of vision data and corresponding metadata follows a well-designed on-site investigation process. Text-based metadata and image-based vision data are then processed and integrated into a unified format, where well-trained Deep Convolutional Neural Networks extract key attributes, including damage state, material type, and damage level. Finally, all data are fed into an LLM with carefully designed prompts, enabling the LLM-DRS to generate summary reports for individual structures or affected regions based on aggregated attributes and metadata. Results show that integrating LLMs into vision-based SHM, particularly for rapid post-disaster reconnaissance, demonstrates promising potential for improving resilience of the built environment through effective reconnaissance.</p>
  </div>
</details>

<hr>
<h3 id="48-PLESS-Pseudo-Label-Enhancement-with-Spreading-Scribbles-for-Weakly-Supervised-Segmentation-cs-CV-cs-LGPDF"><a href="#48-PLESS-Pseudo-Label-Enhancement-with-Spreading-Scribbles-for-Weakly-Supervised-Segmentation-cs-CV-cs-LGPDF" class="headerlink" title="[48] PLESS: Pseudo-Label Enhancement with Spreading Scribbles for Weakly Supervised Segmentation cs.CV | cs.LGPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11628">PLESS: Pseudo-Label Enhancement with Spreading Scribbles for Weakly Supervised Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11628" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yeva Gabrielyan, Varduhi Yeghiazaryan, Irina Voiculescu</span></p>
<p><strong>TL;DR:</strong> PLESS是一种用于弱监督分割的伪标签增强策略，通过构建图像的分层空间一致性区域，在语义一致区域内传播涂鸦标注信息以提升伪标签的可靠性和空间一致性。该方法模型无关，可轻松集成到现有伪标签方法中，在心脏MRI数据集上验证了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决基于涂鸦标注的弱监督分割中，由于稀疏标注导致的噪声和不完整监督问题，特别是现有伪标签方法中伪标签质量限制性能的瓶颈。</p>
<p><strong>Result:</strong> 在两个公开心脏MRI数据集（ACDC和MSCMRseg）上，对四种涂鸦监督算法进行实验，均取得分割精度的一致提升。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种通用的、基于分层空间一致性区域构建的伪标签增强框架，通过区域内的信息传播来提升伪标签质量；其模型无关性使其易于集成到现有方法中，具有较好的通用性和实用性。</p>
<p><strong>Abstract:</strong> Weakly supervised learning with scribble annotations uses sparse user-drawn strokes to indicate segmentation labels on a small subset of pixels. This annotation reduces the cost of dense pixel-wise labeling, but suffers inherently from noisy and incomplete supervision. Recent scribble-based approaches in medical image segmentation address this limitation using pseudo-label-based training; however, the quality of the pseudo-labels remains a key performance limit. We propose PLESS, a generic pseudo-label enhancement strategy which improves reliability and spatial consistency. It builds on a hierarchical partitioning of the image into a hierarchy of spatially coherent regions. PLESS propagates scribble information to refine pseudo-labels within semantically coherent regions. The framework is model-agnostic and easily integrates into existing pseudo-label methods. Experiments on two public cardiac MRI datasets (ACDC and MSCMRseg) across four scribble-supervised algorithms show consistent improvements in segmentation accuracy. Code will be made available on GitHub upon acceptance.</p>
  </div>
</details>

<hr>
<h3 id="49-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning-cs-CV-cs-AIPDF"><a href="#49-ScalSelect-Scalable-Training-Free-Multimodal-Data-Selection-for-Efficient-Visual-Instruction-Tuning-cs-CV-cs-AIPDF" class="headerlink" title="[49] ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning cs.CV | cs.AIPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11636">ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11636" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Changti Wu, Jiahuai Mao, Yuzhuo Miao, Shijie Lian, Bin Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出ScalSelect，一种可扩展的无训练多模态数据选择方法，用于高效视觉指令调优。该方法通过提取目标视觉语言模型中指令令牌最关注的视觉特征构建样本表示，并识别最能近似完整数据集表示主导子空间的样本，实现线性时间复杂度的数据选择。实验表明，仅使用16%的数据即可达到全数据集训练97.5%以上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模视觉指令调优（VIT）计算成本高且数据冗余，现有数据选择方法需昂贵训练或梯度计算，无训练方法依赖代理模型或数据集、指令无关表示及二次复杂度成对相似性，限制了可扩展性和表示保真度。</p>
<p><strong>Result:</strong> 在多个视觉语言模型、数据集和选择预算上的实验表明，ScalSelect仅使用16%数据即可达到全数据集训练97.5%以上的性能，在某些设置下甚至超越全数据训练。</p>
<p><strong>Insight:</strong> 创新点包括：无训练、线性时间复杂度、无需外部模型或辅助数据集；通过指令相关视觉特征构建样本表示，并基于主导子空间近似进行可扩展重要性评分，避免了成对比较。</p>
<p><strong>Abstract:</strong> Large-scale Visual Instruction Tuning (VIT) has become a key paradigm for advancing the performance of vision-language models (VLMs) across various multimodal tasks. However, training on the large-scale datasets is computationally expensive and inefficient due to redundancy in the data, which motivates the need for multimodal data selection to improve training efficiency. Existing data selection methods for VIT either require costly training or gradient computation. Training-free alternatives often depend on proxy models or datasets, instruction-agnostic representations, and pairwise similarity with quadratic complexity, limiting scalability and representation fidelity. In this work, we propose ScalSelect, a scalable training-free multimodal data selection method with linear-time complexity with respect to the number of samples, eliminating the need for external models or auxiliary datasets. ScalSelect first constructs sample representations by extracting visual features most attended by instruction tokens in the target VLM, capturing instruction-relevant information. It then identifies samples whose representations best approximate the dominant subspace of the full dataset representations, enabling scalable importance scoring without pairwise comparisons. Extensive experiments across multiple VLMs, datasets, and selection budgets demonstrate that ScalSelect achieves over 97.5% of the performance of training on the full dataset using only 16% of the data, and even outperforms full-data training in some settings. The code is available at \href{<a target="_blank" rel="noopener" href="https://github.com/ChangtiWu/ScalSelect%7D%7BScalSelect%7D">https://github.com/ChangtiWu/ScalSelect}{ScalSelect}</a>.</p>
  </div>
</details>

<hr>
<h3 id="50-GR-Diffusion-3D-Gaussian-Representation-Meets-Diffusion-in-Whole-Body-PET-Reconstruction-cs-CVPDF"><a href="#50-GR-Diffusion-3D-Gaussian-Representation-Meets-Diffusion-in-Whole-Body-PET-Reconstruction-cs-CVPDF" class="headerlink" title="[50] GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction cs.CVPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11653">GR-Diffusion: 3D Gaussian Representation Meets Diffusion in Whole-Body PET Reconstruction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11653" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mengxiao Geng, Zijie Chen, Ran Hong, Bingxuan Li, Qiegen Liu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为GR-Diffusion的新型框架，用于三维低剂量全身PET图像重建。该方法将三维离散高斯表示（GR）的几何先验与扩散模型的生成能力相结合，通过GR从投影数据生成参考图像，并在扩散过程中利用该参考进行分层指导，以提升重建图像的质量和细节。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决PET重建中因稀疏采样和逆问题不适定性导致的噪声放大、结构模糊和细节丢失等挑战，旨在提升低剂量全身PET图像的重建质量。</p>
<p><strong>Result:</strong> 在UDPET和Clinical数据集上，针对不同剂量水平的实验结果表明，GR-Diffusion在提升三维全身PET图像质量和保留生理细节方面优于现有最先进方法。</p>
<p><strong>Insight:</strong> 创新点在于将三维离散高斯表示与扩散模型协同集成，利用GR生成具有物理基础和结构明确性的参考图像，并设计分层指导机制（细粒度与粗粒度）来引导扩散过程，从而整合几何先验并恢复亚体素信息。从客观角度看，该方法为医学图像重建提供了一种结合显式几何表示与生成模型的新范式。</p>
<p><strong>Abstract:</strong> Positron emission tomography (PET) reconstruction is a critical challenge in molecular imaging, often hampered by noise amplification, structural blurring, and detail loss due to sparse sampling and the ill-posed nature of inverse problems. The three-dimensional discrete Gaussian representation (GR), which efficiently encodes 3D scenes using parameterized discrete Gaussian distributions, has shown promise in computer vision. In this work, we pro-pose a novel GR-Diffusion framework that synergistically integrates the geometric priors of GR with the generative power of diffusion models for 3D low-dose whole-body PET reconstruction. GR-Diffusion employs GR to generate a reference 3D PET image from projection data, establishing a physically grounded and structurally explicit benchmark that overcomes the low-pass limitations of conventional point-based or voxel-based methods. This reference image serves as a dual guide during the diffusion process, ensuring both global consistency and local accuracy. Specifically, we employ a hierarchical guidance mechanism based on the GR reference. Fine-grained guidance leverages differences to refine local details, while coarse-grained guidance uses multi-scale difference maps to correct deviations. This strategy allows the diffusion model to sequentially integrate the strong geometric prior from GR and recover sub-voxel information. Experimental results on the UDPET and Clinical datasets with varying dose levels show that GR-Diffusion outperforms state-of-the-art methods in enhancing 3D whole-body PET image quality and preserving physiological details.</p>
  </div>
</details>

<hr>
<h3 id="51-SToRM-Supervised-Token-Reduction-for-Multi-modal-LLMs-toward-efficient-end-to-end-autonomous-driving-cs-CV-cs-AI-cs-ROPDF"><a href="#51-SToRM-Supervised-Token-Reduction-for-Multi-modal-LLMs-toward-efficient-end-to-end-autonomous-driving-cs-CV-cs-AI-cs-ROPDF" class="headerlink" title="[51] SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving cs.CV | cs.AI | cs.ROPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11656">SToRM: Supervised Token Reduction for Multi-modal LLMs toward efficient end-to-end autonomous driving</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11656" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Seo Hyun Kim, Jin Bok Park, Do Yeon Koo, Ho Gun Park, Il Yong Chun</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为SToRM的监督令牌缩减框架，旨在为多模态大语言模型驱动的端到端自动驾驶系统实现高效计算。该方法通过轻量级重要性预测器、监督训练策略和锚点-上下文合并模块，在显著减少视觉令牌数量的同时，保持了与使用全部令牌相当的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在端到端自动驾驶系统中，使用多模态大语言模型处理传感器数据和自然语言指令虽能提升交互与性能，但因其依赖大量视觉令牌和LLM计算，导致计算资源需求过高，难以在车载设备上部署。现有令牌缩减方法常导致端任务性能下降。</p>
<p><strong>Result:</strong> 在LangAuto基准测试上，SToRM在相同的缩减令牌预算下，超越了最先进的端到端驾驶MLLMs，在保持全令牌性能的同时，将计算成本降低了高达30倍。</p>
<p><strong>Insight:</strong> 创新点在于首次为多模态LLM提出了监督令牌缩减框架，其核心是结合短期滑动窗口的轻量级重要性预测、利用全令牌LLM路径生成伪监督信号的训练方法，以及通过锚点-上下文分区与合并来最小化信息损失的冗余消除机制。这为资源受限场景下的高效多模态理解提供了可借鉴的架构设计思路。</p>
<p><strong>Abstract:</strong> In autonomous driving, end-to-end (E2E) driving systems that predict control commands directly from sensor data have achieved significant advancements. For safe driving in unexpected scenarios, these systems may additionally rely on human interventions such as natural language instructions. Using a multi-modal large language model (MLLM) facilitates human-vehicle interaction and can improve performance in such scenarios. However, this approach requires substantial computational resources due to its reliance on an LLM and numerous visual tokens from sensor inputs, which are limited in autonomous vehicles. Many MLLM studies have explored reducing visual tokens, but often suffer end-task performance degradation compared to using all tokens.   To enable efficient E2E driving while maintaining performance comparable to using all tokens, this paper proposes the first Supervised Token Reduction framework for multi-modal LLMs (SToRM). The proposed framework consists of three key elements. First, a lightweight importance predictor with short-term sliding windows estimates token importance scores. Second, a supervised training approach uses an auxiliary path to obtain pseudo-supervision signals from an all-token LLM pass. Third, an anchor-context merging module partitions tokens into anchors and context tokens, and merges context tokens into relevant anchors to reduce redundancy while minimizing information loss. Experiments on the LangAuto benchmark show that SToRM outperforms state-of-the-art E2E driving MLLMs under the same reduced-token budget, maintaining all-token performance while reducing computational cost by up to 30x.</p>
  </div>
</details>

<hr>
<h3 id="52-EmoSpace-Fine-Grained-Emotion-Prototype-Learning-for-Immersive-Affective-Content-Generation-cs-CVPDF"><a href="#52-EmoSpace-Fine-Grained-Emotion-Prototype-Learning-for-Immersive-Affective-Content-Generation-cs-CVPDF" class="headerlink" title="[52] EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation cs.CVPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11658">EmoSpace: Fine-Grained Emotion Prototype Learning for Immersive Affective Content Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11658" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bingyuan Wang, Xingbei Chen, Zongyang Qiu, Linping Yuan, Zeyu Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出EmoSpace框架，通过视觉-语言对齐学习动态可解释的情感原型，实现细粒度情感控制的沉浸式内容生成，支持情感图像外绘、风格化生成和VR全景生成等应用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有生成方法难以捕捉细微情感语义和实现细粒度控制，无法满足沉浸式体验需求，因此需要开发能够学习动态情感原型并支持精细情感调控的生成框架。</p>
<p><strong>Result:</strong> 实验表明EmoSpace在定性和定量评估上均优于现有方法，并通过用户研究比较了VR与桌面环境下情感感知的差异。</p>
<p><strong>Insight:</strong> 创新点包括分层情感表示与可学习原型、多原型引导与时间混合的控制生成流程，以及无需显式情感标签的细粒度情感控制方法，为治疗、教育等领域提供技术支持。</p>
<p><strong>Abstract:</strong> Emotion is important for creating compelling virtual reality (VR) content. Although some generative methods have been applied to lower the barrier to creating emotionally rich content, they fail to capture the nuanced emotional semantics and the fine-grained control essential for immersive experiences. To address these limitations, we introduce EmoSpace, a novel framework for emotion-aware content generation that learns dynamic, interpretable emotion prototypes through vision-language alignment. We employ a hierarchical emotion representation with rich learnable prototypes that evolve during training, enabling fine-grained emotional control without requiring explicit emotion labels. We develop a controllable generation pipeline featuring multi-prototype guidance, temporal blending, and attention reweighting that supports diverse applications, including emotional image outpainting, stylized generation, and emotional panorama generation for VR environments. Our experiments demonstrate the superior performance of EmoSpace over existing methods in both qualitative and quantitative evaluations. Additionally, we present a comprehensive user study investigating how VR environments affect emotional perception compared to desktop settings. Our work facilitates immersive visual content generation with fine-grained emotion control and supports applications like therapy, education, storytelling, artistic creation, and cultural preservation. Code and models will be made publicly available.</p>
  </div>
</details>

<hr>
<h3 id="53-Egocentric-Gaze-Estimation-via-Neck-Mounted-Camera-cs-CVPDF"><a href="#53-Egocentric-Gaze-Estimation-via-Neck-Mounted-Camera-cs-CVPDF" class="headerlink" title="[53] Egocentric Gaze Estimation via Neck-Mounted Camera cs.CVPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11669">Egocentric Gaze Estimation via Neck-Mounted Camera</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11669" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoyu Huang, Yoichi Sato</span></p>
<p><strong>TL;DR:</strong> 本文提出了颈戴式视角的视线估计新任务，通过颈戴摄像头估计用户视线方向。为填补该领域空白，作者收集了首个包含8名参与者日常活动约4小时视频的数据集，并评估了基于Transformer的GLC模型，同时提出了视线越界分类辅助任务和多视角协同学习方法。实验表明视线越界分类能提升性能，但协同学习方法未带来增益。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有第一人称视线估计研究主要集中于头戴式摄像头，而其他视角（如颈戴式）尚未充分探索，本文旨在填补这一空白并探索颈戴摄像头在视线估计任务中的可行性。</p>
<p><strong>Result:</strong> 在自建数据集上实验表明，引入视线越界分类辅助任务相比标准微调提升了性能，但提出的多视角协同学习方法未取得明显增益。</p>
<p><strong>Insight:</strong> 创新点包括提出颈戴式视线估计新任务、构建首个对应数据集，以及设计视线越界分类辅助任务；从客观角度看，将视线估计视角从头部扩展到颈部为可穿戴计算提供了新思路，而视线越界分类作为辅助任务的设计思路具有借鉴意义。</p>
<p><strong>Abstract:</strong> This paper introduces neck-mounted view gaze estimation, a new task that estimates user gaze from the neck-mounted camera perspective. Prior work on egocentric gaze estimation, which predicts device wearer’s gaze location within the camera’s field of view, mainly focuses on head-mounted cameras while alternative viewpoints remain underexplored. To bridge this gap, we collect the first dataset for this task, consisting of approximately 4 hours of video collected from 8 participants during everyday activities. We evaluate a transformer-based gaze estimation model, GLC, on the new dataset and propose two extensions: an auxiliary gaze out-of-bound classification task and a multi-view co-learning approach that jointly trains head-view and neck-view models using a geometry-aware auxiliary loss. Experimental results show that incorporating gaze out-of-bound classification improves performance over standard fine-tuning, while the co-learning approach does not yield gains. We further analyze these results and discuss implications for neck-mounted gaze estimation.</p>
  </div>
</details>

<hr>
<h3 id="54-U-Net-with-Hadamard-Transform-and-DCT-Latent-Spaces-for-Next-day-Wildfire-Spread-Prediction-cs-CVPDF"><a href="#54-U-Net-with-Hadamard-Transform-and-DCT-Latent-Spaces-for-Next-day-Wildfire-Spread-Prediction-cs-CVPDF" class="headerlink" title="[54] U-Net with Hadamard Transform and DCT Latent Spaces for Next-day Wildfire Spread Prediction cs.CVPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11672">U-Net with Hadamard Transform and DCT Latent Spaces for Next-day Wildfire Spread Prediction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11672" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yingyi Luo, Shuaiang Rong, Adam Watts, Ahmet Enis Cetin</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种轻量级且计算高效的次日野火蔓延预测工具TD-FusionUNet，该模型利用多模态卫星数据作为输入，通过引入可训练的哈达玛变换和离散余弦变换层在正交化潜在空间中捕获关键频率成分，并结合随机边缘裁剪和高斯混合模型等预处理技术增强泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 旨在开发一个适用于资源受限环境的实时野火预测工具，解决现有模型参数量大、计算成本高的问题，并提升对稀疏预燃掩码的表征能力。</p>
<p><strong>Result:</strong> 在Google Research 2023年发布的Next-Day Wildfire Spread数据集和WildfireSpreadTS数据集上评估，TD-FusionUNet以37万参数取得了0.591的F1分数，优于使用ResNet18编码器的UNet基线，且在参数量大幅减少的同时保持了准确性。</p>
<p><strong>Insight:</strong> 创新点在于将可训练的二维正交变换（哈达玛变换和DCT）集成到U-Net架构中，以在潜在空间融合频率信息；同时，针对稀疏数据设计的预处理方法（如随机边缘裁剪和高斯混合模型）有效提升了模型泛化性能，实现了轻量化与精度的平衡。</p>
<p><strong>Abstract:</strong> We developed a lightweight and computationally efficient tool for next-day wildfire spread prediction using multimodal satellite data as input. The deep learning model, which we call Transform Domain Fusion UNet (TD-FusionUNet), incorporates trainable Hadamard Transform and Discrete Cosine Transform layers that apply two-dimensional transforms, enabling the network to capture essential “frequency” components in orthogonalized latent spaces. Additionally, we introduce custom preprocessing techniques, including random margin cropping and a Gaussian mixture model, to enrich the representation of the sparse pre-fire masks and enhance the model’s generalization capability. The TD-FusionUNet is evaluated on two datasets which are the Next-Day Wildfire Spread dataset released by Google Research in 2023, and WildfireSpreadTS dataset. Our proposed TD-FusionUNet achieves an F1 score of 0.591 with 370k parameters, outperforming the UNet baseline using ResNet18 as the encoder reported in the WildfireSpreadTS dataset while using substantially fewer parameters. These results show that the proposed latent space fusion model balances accuracy and efficiency under a lightweight setting, making it suitable for real time wildfire prediction applications in resource limited environments.</p>
  </div>
</details>

<hr>
<h3 id="55-LLM-Driven-3D-Scene-Generation-of-Agricultural-Simulation-Environments-cs-CV-cs-AI-cs-ROPDF"><a href="#55-LLM-Driven-3D-Scene-Generation-of-Agricultural-Simulation-Environments-cs-CV-cs-AI-cs-ROPDF" class="headerlink" title="[55] LLM-Driven 3D Scene Generation of Agricultural Simulation Environments cs.CV | cs.AI | cs.ROPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11706">LLM-Driven 3D Scene Generation of Agricultural Simulation Environments</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11706" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arafa Yoncalik, Wouter Jansen, Nico Huebel, Mohammad Hasan Rahmani, Jan Steckel</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种由大型语言模型驱动的模块化多LLM流水线，用于从自然语言提示生成农业合成仿真3D场景。该系统集成了3D资产检索、领域知识注入和针对Unreal渲染引擎的代码生成，并通过结合少样本提示、检索增强生成、微调和验证等LLM优化技术来提高准确性和可扩展性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有基于LLM的3D场景生成方法缺乏领域特定推理、验证机制和模块化设计的问题，导致控制力减弱和可扩展性差，特别是在农业仿真环境生成领域。</p>
<p><strong>Result:</strong> 系统通过结构化提示和语义准确性指标进行评估。用户研究表明，生成的场景在真实感和熟悉度上可与真实世界图像媲美；专家对比显示，相比手动场景设计，该方法能显著节省时间。结果证实了多LLM流水线在自动化领域特定3D场景生成方面的有效性，并提高了可靠性和精确性。</p>
<p><strong>Insight:</strong> 创新点在于采用模块化多LLM架构，结合了领域知识注入和混合LLM优化策略（如RAG、微调），实现了结构化数据处理、中间验证和灵活扩展，从而提升了生成场景的领域准确性和系统可扩展性。从客观角度看，将LLM驱动的代码生成与专业渲染引擎API深度集成，为特定领域仿真环境的高效、可控生成提供了可借鉴的范式。</p>
<p><strong>Abstract:</strong> Procedural generation techniques in 3D rendering engines have revolutionized the creation of complex environments, reducing reliance on manual design. Recent approaches using Large Language Models (LLMs) for 3D scene generation show promise but often lack domain-specific reasoning, verification mechanisms, and modular design. These limitations lead to reduced control and poor scalability. This paper investigates the use of LLMs to generate agricultural synthetic simulation environments from natural language prompts, specifically to address the limitations of lacking domain-specific reasoning, verification mechanisms, and modular design. A modular multi-LLM pipeline was developed, integrating 3D asset retrieval, domain knowledge injection, and code generation for the Unreal rendering engine using its API. This results in a 3D environment with realistic planting layouts and environmental context, all based on the input prompt and the domain knowledge. To enhance accuracy and scalability, the system employs a hybrid strategy combining LLM optimization techniques such as few-shot prompting, Retrieval-Augmented Generation (RAG), finetuning, and validation. Unlike monolithic models, the modular architecture enables structured data handling, intermediate verification, and flexible expansion. The system was evaluated using structured prompts and semantic accuracy metrics. A user study assessed realism and familiarity against real-world images, while an expert comparison demonstrated significant time savings over manual scene design. The results confirm the effectiveness of multi-LLM pipelines in automating domain-specific 3D scene generation with improved reliability and precision. Future work will explore expanding the asset hierarchy, incorporating real-time generation, and adapting the pipeline to other simulation domains beyond agriculture.</p>
  </div>
</details>

<hr>
<h3 id="56-STVG-R1-Incentivizing-Instance-Level-Reasoning-and-Grounding-in-Videos-via-Reinforcement-Learning-cs-CVPDF"><a href="#56-STVG-R1-Incentivizing-Instance-Level-Reasoning-and-Grounding-in-Videos-via-Reinforcement-Learning-cs-CVPDF" class="headerlink" title="[56] STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning cs.CVPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11730">STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11730" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaowen Zhang, Zhi Gao, Licheng Jiao, Lingling Li, Qing Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种新颖的视觉提示范式STVG-R1，用于解决时空视频定位任务中文本描述与视觉坐标错位导致的幻觉问题。该方法将逐帧坐标预测重新定义为紧凑的实例级识别问题，为每个对象分配唯一且时间一致的ID作为视觉提示，并首次引入强化学习框架来联合优化时间准确性、空间一致性和结构格式正则化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决视觉语言模型在密集预测任务（如时空视频定位）中，文本描述与视觉坐标错位导致的严重幻觉问题，同时避免现有方法引入额外可训练模块带来的高标注成本和计算开销。</p>
<p><strong>Result:</strong> 在六个基准测试上进行了广泛实验，在HCSTVG-v2基准上，STVG-R1的m_IoU显著超越基线模型Qwen2.5-VL-7B达20.9%，达到了新的SOTA水平。同时，在零样本泛化到多目标指代视频目标分割任务上，在MeViS基准上取得了SOTA的47.3% J&amp;F分数。</p>
<p><strong>Insight:</strong> 核心创新在于将坐标对齐的难题转化为实例级识别问题，通过引入时间一致的对象ID作为视觉提示，为VLM提供显式且可解释的输入。同时，首次将强化学习框架应用于STVG任务，通过任务驱动的奖励机制联合优化多个关键指标，实现了性能的显著提升和强大的零样本泛化能力。</p>
<p><strong>Abstract:</strong> In vision-language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial-temporal video grounding (STVG). Prior approaches typically focus on enhancing visual-textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation tasks, achieving a SOTA 47.3% J&amp;F on MeViS.</p>
  </div>
</details>

<hr>
<h3 id="57-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale-cs-CV-cs-AIPDF"><a href="#57-Adapting-Vision-Language-Models-for-E-commerce-Understanding-at-Scale-cs-CV-cs-AIPDF" class="headerlink" title="[57] Adapting Vision-Language Models for E-commerce Understanding at Scale cs.CV | cs.AIPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11733">Adapting Vision-Language Models for E-commerce Understanding at Scale</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11733" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Matteo Nulli, Vladimir Orshulevich, Tala Bazazo, Christian Herold, Michael Kozielski</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种针对电子商务场景的视觉语言模型（VLM）适配方法，通过大规模实验研究，证明了有针对性的适配能在保持模型通用多模态能力的同时，显著提升其在电商产品理解任务上的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决通用视觉语言模型在适应电商数据（如属性中心、多图像、噪声多）时缺乏明确策略，且可能牺牲通用性能的问题。</p>
<p><strong>Result:</strong> 通过提出的适配方法，在涵盖深度产品理解、严格指令遵循和动态属性提取的综合性评估套件上，取得了性能的显著提升。</p>
<p><strong>Insight:</strong> 创新点在于提出了针对电商领域的VLM适配策略，并构建了一个全面的评估套件，为领域特定适配提供了系统性的方法论和评估基准。</p>
<p><strong>Abstract:</strong> E-commerce product understanding demands by nature, strong multimodal comprehension from text, images, and structured attributes. General-purpose Vision-Language Models (VLMs) enable generalizable multimodal latent modelling, yet there is no documented, well-known strategy for adapting them to the attribute-centric, multi-image, and noisy nature of e-commerce data, without sacrificing general performance. In this work, we show through a large-scale experimental study, how targeted adaptation of general VLMs can substantially improve e-commerce performance while preserving broad multimodal capabilities. Furthermore, we propose a novel extensive evaluation suite covering deep product understanding, strict instruction following, and dynamic attribute extraction.</p>
  </div>
</details>

<hr>
<h3 id="58-Mask-What-Matters-Mitigating-Object-Hallucinations-in-Multimodal-Large-Language-Models-with-Object-Aligned-Visual-Contrastive-Decoding-cs-CV-cs-CLPDF"><a href="#58-Mask-What-Matters-Mitigating-Object-Hallucinations-in-Multimodal-Large-Language-Models-with-Object-Aligned-Visual-Contrastive-Decoding-cs-CV-cs-CLPDF" class="headerlink" title="[58] Mask What Matters: Mitigating Object Hallucinations in Multimodal Large Language Models with Object-Aligned Visual Contrastive Decoding cs.CV | cs.CLPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11737">Mask What Matters: Mitigating Object Hallucinations in Multimodal Large Language Models with Object-Aligned Visual Contrastive Decoding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11737" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Boqi Chen, Xudong Liu, Jianing Qiu</span></p>
<p><strong>TL;DR:</strong> 本文针对多模态大语言模型中的物体幻觉问题，提出了一种基于物体对齐视觉对比解码的方法。通过利用自监督视觉Transformer中的物体中心注意力机制，移除最显著的视觉证据来构建辅助视图，从而抑制无视觉支持的文本生成，并在两个主流物体幻觉基准上验证了方法的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态大语言模型中普遍存在的物体幻觉问题，即模型生成与输入图像内容不符的物体描述。</p>
<p><strong>Result:</strong> 在两个流行的物体幻觉基准（如POPE和CHAIR）上，该方法在两种不同的MLLM模型上均取得了稳定的性能提升，有效减少了幻觉现象。</p>
<p><strong>Insight:</strong> 创新点在于利用自监督视觉Transformer的物体中心注意力机制构建物体对齐的辅助视图，增强视觉对比解码的信号；该方法具有提示无关、模型无关、计算开销低（仅需一次可缓存的单次前向传播）的优点，可无缝集成到现有VCD流程中。</p>
<p><strong>Abstract:</strong> We study object hallucination in Multimodal Large Language Models (MLLMs) and improve visual contrastive decoding (VCD) by constructing an object-aligned auxiliary view. We leverage object-centric attention in self-supervised Vision Transformers. In particular, we remove the most salient visual evidence to construct an auxiliary view that disrupts unsupported tokens and produces a stronger contrast signal. Our method is prompt-agnostic, model-agnostic, and can be seamlessly plugged into the existing VCD pipeline with little computation overhead, i.e., a single cacheable forward pass. Empirically, our method demonstrates consistent gains on two popular object hallucination benchmarks across two MLLMs.</p>
  </div>
</details>

<hr>
<h3 id="59-Adaptive-Debiasing-Tsallis-Entropy-for-Test-Time-Adaptation-cs-CVPDF"><a href="#59-Adaptive-Debiasing-Tsallis-Entropy-for-Test-Time-Adaptation-cs-CVPDF" class="headerlink" title="[59] Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation cs.CVPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11743">Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11743" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangyu Wu, Dongming Jiang, Feng Yu, Yueying Tian, Jiaqi Tang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种自适应去偏Tsallis熵（ADTE）方法，用于测试时适应（TTA）中的视觉语言模型（如CLIP）。该方法通过引入一个类别特定的非广延参数q^l，来替代传统使用的香农熵（SE），以更准确地衡量预测不确定性并缓解模型因预训练数据不平衡而产生的固有偏差。ADTE能够自适应地选择高置信度视图，并结合标签调整策略增强适应效果，无需针对特定分布进行超参数调优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 主流TTA方法依赖香农熵来衡量预测不确定性，但CLIP等模型在高度不平衡的网络爬取数据上预训练后存在固有偏差，导致香农熵产生有偏的不确定性估计。本文旨在解决这一问题。</p>
<p><strong>Result:</strong> 实验结果表明，ADTE在ImageNet及其五个变体上超越了现有最先进方法，并在10个跨域基准测试中取得了最高的平均性能，且不受模型架构或文本提示词的影响。</p>
<p><strong>Insight:</strong> 创新点在于发现并论证了Tsallis熵（TE）作为香农熵的广义形式，通过引入非广延参数q，天然适合描述有偏分布，且SE的性能是TE的下界。进一步将TE推广为自适应的ADTE，通过基于连续流入测试实例估计的标签偏差来定制类别特定参数q^l，实现了无需分布特定超参数调优的自适应去偏和性能提升。ADTE和TE均可作为SE在TTA中的直接、高级替代方案。</p>
<p><strong>Abstract:</strong> Mainstream Test-Time Adaptation (TTA) methods for adapting vision-language models, e.g., CLIP, typically rely on Shannon Entropy (SE) at test time to measure prediction uncertainty and inconsistency. However, since CLIP has a built-in bias from pretraining on highly imbalanced web-crawled data, SE inevitably results in producing biased estimates of uncertainty entropy. To address this issue, we notably find and demonstrate that Tsallis Entropy (TE), a generalized form of SE, is naturally suited for characterizing biased distributions by introducing a non-extensive parameter q, with the performance of SE serving as a lower bound for TE. Building upon this, we generalize TE into Adaptive Debiasing Tsallis Entropy (ADTE) for TTA, customizing a class-specific parameter q^l derived by normalizing the estimated label bias from continuously incoming test instances, for each category. This adaptive approach allows ADTE to accurately select high-confidence views and seamlessly integrate with a label adjustment strategy to enhance adaptation, without introducing distribution-specific hyperparameter tuning. Besides, our investigation reveals that both TE and ADTE can serve as direct, advanced alternatives to SE in TTA, without any other modifications. Experimental results show that ADTE outperforms state-of-the-art methods on ImageNet and its five variants, and achieves the highest average performance on 10 cross-domain benchmarks, regardless of the model architecture or text prompts used. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Jinx630/ADTE">https://github.com/Jinx630/ADTE</a>.</p>
  </div>
</details>

<hr>
<h3 id="60-Code2Worlds-Empowering-Coding-LLMs-for-4D-World-Generation-cs-CVPDF"><a href="#60-Code2Worlds-Empowering-Coding-LLMs-for-4D-World-Generation-cs-CVPDF" class="headerlink" title="[60] Code2Worlds: Empowering Coding LLMs for 4D World Generation cs.CVPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11757">Code2Worlds: Empowering Coding LLMs for 4D World Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11757" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi Zhang, Yunshuang Wang, Zeyu Zhang, Hao Tang</span></p>
<p><strong>TL;DR:</strong> Code2Worlds是一个将4D世界生成任务（3D空间+时间动态）构建为语言到模拟代码生成问题的框架。它通过双流架构解耦对象生成与环境编排，并引入物理感知的闭环机制（包含后处理代理和视觉语言模型-运动评判器）来迭代优化模拟代码，从而生成具有动态保真度的物理世界。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于编码大语言模型的方法主要关注静态3D场景生成，而扩展到包含时间动态的4D世界生成面临两大挑战：多尺度上下文纠缠（难以平衡局部对象结构与全局环境布局）以及语义-物理执行鸿沟（开环代码生成导致缺乏动态保真度的物理幻觉）。</p>
<p><strong>Result:</strong> 在Code4D基准测试上，Code2Worlds优于基线方法，取得了41%的SGS增益和49%的Richness提升，并且能够生成先前静态方法所不具备的物理感知动态。</p>
<p><strong>Insight:</strong> 主要创新点在于：1）将4D生成任务形式化为语言到模拟代码的生成问题；2）提出双流架构解耦对象生成与环境编排；3）建立物理感知的闭环机制，通过后处理代理和VLM-Motion Critic进行自我反思和迭代优化，以弥合语义与物理执行之间的鸿沟，确保动态保真度。</p>
<p><strong>Abstract:</strong> Achieving spatial intelligence requires moving beyond visual plausibility to build world simulators grounded in physical laws. While coding LLMs have advanced static 3D scene generation, extending this paradigm to 4D dynamics remains a critical frontier. This task presents two fundamental challenges: multi-scale context entanglement, where monolithic generation fails to balance local object structures with global environmental layouts; and a semantic-physical execution gap, where open-loop code generation leads to physical hallucinations lacking dynamic fidelity. We introduce Code2Worlds, a framework that formulates 4D generation as language-to-simulation code generation. First, we propose a dual-stream architecture that disentangles retrieval-augmented object generation from hierarchical environmental orchestration. Second, to ensure dynamic fidelity, we establish a physics-aware closed-loop mechanism in which a PostProcess Agent scripts dynamics, coupled with a VLM-Motion Critic that performs self-reflection to iteratively refine simulation code. Evaluations on the Code4D benchmark show Code2Worlds outperforms baselines with a 41% SGS gain and 49% higher Richness, while uniquely generating physics-aware dynamics absent in prior static methods. Code: <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/Code2Worlds">https://github.com/AIGeeksGroup/Code2Worlds</a>. Website: <a target="_blank" rel="noopener" href="https://aigeeksgroup.github.io/Code2Worlds">https://aigeeksgroup.github.io/Code2Worlds</a>.</p>
  </div>
</details>

<hr>
<h3 id="61-Light4D-Training-Free-Extreme-Viewpoint-4D-Video-Relighting-cs-CVPDF"><a href="#61-Light4D-Training-Free-Extreme-Viewpoint-4D-Video-Relighting-cs-CVPDF" class="headerlink" title="[61] Light4D: Training-Free Extreme Viewpoint 4D Video Relighting cs.CVPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11769">Light4D: Training-Free Extreme Viewpoint 4D Video Relighting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11769" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhenghuang Wu, Kang Chen, Zeyu Zhang, Hao Tang</span></p>
<p><strong>TL;DR:</strong> 本文提出Light4D，一种无需训练的新框架，用于在目标光照下合成一致的4D视频，即使面对极端视角变化。该方法通过解耦流引导和时间一致注意力等技术，解决了4D重光照中数据稀缺和时序一致性的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决基于扩散模型的图像和视频重光照方法在扩展到4D（动态3D场景）时，因缺乏配对训练数据和难以在极端视角下保持时间一致性而面临的挑战。</p>
<p><strong>Result:</strong> 大量实验表明，该方法在时间一致性和光照保真度上取得了有竞争力的性能，能够鲁棒地处理-90度到90度的相机旋转。</p>
<p><strong>Insight:</strong> 主要创新点包括：1）解耦流引导，一种时间感知策略，可在潜在空间有效注入光照控制同时保持几何完整性；2）在IC-Light架构内开发的时间一致注意力，并结合确定性正则化以消除外观闪烁。这些技术为无需训练、保持4D视频时序一致性的重光照提供了新思路。</p>
<p><strong>Abstract:</strong> Recent advances in diffusion-based generative models have established a new paradigm for image and video relighting. However, extending these capabilities to 4D relighting remains challenging, due primarily to the scarcity of paired 4D relighting training data and the difficulty of maintaining temporal consistency across extreme viewpoints. In this work, we propose Light4D, a novel training-free framework designed to synthesize consistent 4D videos under target illumination, even under extreme viewpoint changes. First, we introduce Disentangled Flow Guidance, a time-aware strategy that effectively injects lighting control into the latent space while preserving geometric integrity. Second, to reinforce temporal consistency, we develop Temporal Consistent Attention within the IC-Light architecture and further incorporate deterministic regularization to eliminate appearance flickering. Extensive experiments demonstrate that our method achieves competitive performance in temporal consistency and lighting fidelity, robustly handling camera rotations from -90 to 90. Code: <a target="_blank" rel="noopener" href="https://github.com/AIGeeksGroup/Light4D">https://github.com/AIGeeksGroup/Light4D</a>. Website: <a target="_blank" rel="noopener" href="https://aigeeksgroup.github.io/Light4D">https://aigeeksgroup.github.io/Light4D</a>.</p>
  </div>
</details>

<hr>
<h3 id="62-How-to-Sample-High-Quality-3D-Fractals-for-Action-Recognition-Pre-Training-cs-CV-cs-LGPDF"><a href="#62-How-to-Sample-High-Quality-3D-Fractals-for-Action-Recognition-Pre-Training-cs-CV-cs-LGPDF" class="headerlink" title="[62] How to Sample High Quality 3D Fractals for Action Recognition Pre-Training? cs.CV | cs.LGPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11810">How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11810" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Marko Putak, Thomas B. Moeslund, Joakim Bruslund Haurum</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Targeted Smart Filtering的新方法，用于高效生成高质量的3D分形数据，以预训练动作识别模型。该方法通过改进3D迭代函数系统（IFS）的生成过程，解决了传统方法生成速度慢和分形多样性不足的问题，从而提升了预训练效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于利用公式驱动监督学习（FDSL）生成合成数据来避免真实数据标注的繁琐和隐私问题，但现有3D分形生成方法速度慢且易产生退化分形，影响下游任务性能，因此需要一种更高效的生成策略。</p>
<p><strong>Result:</strong> 所提方法在采样速度上比标准方法快约100倍，并在下游动作识别任务中优于其他3D分形过滤方法，实现了更优的性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了Targeted Smart Filtering方法，平衡了分形生成的美观性与多样性，避免了过度限制性方法对下游任务的负面影响，从而在速度和性能上取得显著提升。</p>
<p><strong>Abstract:</strong> Synthetic datasets are being recognized in the deep learning realm as a valuable alternative to exhaustively labeled real data. One such synthetic data generation method is Formula Driven Supervised Learning (FDSL), which can provide an infinite number of perfectly labeled data through a formula driven approach, such as fractals or contours. FDSL does not have common drawbacks like manual labor, privacy and other ethical concerns. In this work we generate 3D fractals using 3D Iterated Function Systems (IFS) for pre-training an action recognition model. The fractals are temporally transformed to form a video that is used as a pre-training dataset for downstream task of action recognition. We find that standard methods of generating fractals are slow and produce degenerate 3D fractals. Therefore, we systematically explore alternative ways of generating fractals and finds that overly-restrictive approaches, while generating aesthetically pleasing fractals, are detrimental for downstream task performance. We propose a novel method, Targeted Smart Filtering, to address both the generation speed and fractal diversity issue. The method reports roughly 100 times faster sampling speed and achieves superior downstream performance against other 3D fractal filtering methods.</p>
  </div>
</details>

<hr>
<h3 id="63-JEPA-VLA-Video-Predictive-Embedding-is-Needed-for-VLA-Models-cs-CV-cs-ROPDF"><a href="#63-JEPA-VLA-Video-Predictive-Embedding-is-Needed-for-VLA-Models-cs-CV-cs-ROPDF" class="headerlink" title="[63] JEPA-VLA: Video Predictive Embedding is Needed for VLA Models cs.CV | cs.ROPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11832">JEPA-VLA: Video Predictive Embedding is Needed for VLA Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11832" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shangchen Miao, Ningya Feng, Jialong Wu, Ye Lin, Xu He</span></p>
<p><strong>TL;DR:</strong> 本文提出JEPA-VLA方法，通过将视频预测嵌入（特别是V-JEPA 2）自适应地集成到现有的视觉-语言-动作模型中，以解决当前VLA模型样本效率低和泛化能力有限的问题。该方法在多个基准测试和真实机器人任务上取得了显著的性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于预训练视觉语言模型构建的VLA模型在机器人操作任务中虽取得进展，但仍受限于样本效率低和泛化能力不足。作者认为这些限制与预训练视觉表示有关，现有表示（如语言-图像对比学习或图像自监督学习）在环境理解和策略先验方面知识不足。</p>
<p><strong>Result:</strong> 实验表明，JEPA-VLA在LIBERO、LIBERO-plus、RoboTwin2.0等多个基准测试以及真实机器人任务中均实现了显著的性能增益。</p>
<p><strong>Insight:</strong> 创新点在于识别出视频预测嵌入（如V-JEPA 2）能有效编码任务相关的时间动态并过滤不可预测的环境因素，从而弥补现有视觉表示的不足。JEPA-VLA提供了一种简单有效的集成方法，可提升VLA模型的环境理解和策略先验能力。</p>
<p><strong>Abstract:</strong> Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks.</p>
  </div>
</details>

<hr>
<h3 id="64-WorldTree-Towards-4D-Dynamic-Worlds-from-Monocular-Video-using-Tree-Chains-cs-CVPDF"><a href="#64-WorldTree-Towards-4D-Dynamic-Worlds-from-Monocular-Video-using-Tree-Chains-cs-CVPDF" class="headerlink" title="[64] WorldTree: Towards 4D Dynamic Worlds from Monocular Video using Tree-Chains cs.CVPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11845">WorldTree: Towards 4D Dynamic Worlds from Monocular Video using Tree-Chains</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11845" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qisen Wang, Yifan Zhao, Jia Li</span></p>
<p><strong>TL;DR:</strong> 本文提出WorldTree框架，从单目视频构建4D动态世界。该框架通过时间划分树（TPT）实现从粗到细的层次时间分解优化，并通过空间祖先链（SAC）递归查询祖先层次结构以提供互补的空间动态，从而统一时空分解表示。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有动态重建方法在单目输入的实际应用中面临挑战，缺乏统一的时空分解框架，常受限于整体时间优化或耦合的层次空间组合。</p>
<p><strong>Result:</strong> 在NVIDIA-LS数据集上LPIPS指标提升8.26%，在DyCheck数据集上mLPIPS指标提升9.09%，优于次优方法。</p>
<p><strong>Insight:</strong> 创新点在于提出了基于继承的分区树结构进行层次时间分解，以及通过空间祖先链专门化跨祖先节点的运动表示，实现了时空表示的互补与统一。</p>
<p><strong>Abstract:</strong> Dynamic reconstruction has achieved remarkable progress, but there remain challenges in monocular input for more practical applications. The prevailing works attempt to construct efficient motion representations, but lack a unified spatiotemporal decomposition framework, suffering from either holistic temporal optimization or coupled hierarchical spatial composition. To this end, we propose WorldTree, a unified framework comprising Temporal Partition Tree (TPT) that enables coarse-to-fine optimization based on the inheritance-based partition tree structure for hierarchical temporal decomposition, and Spatial Ancestral Chains (SAC) that recursively query ancestral hierarchical structure to provide complementary spatial dynamics while specializing motion representations across ancestral nodes. Experimental results on different datasets indicate that our proposed method achieves 8.26% improvement of LPIPS on NVIDIA-LS and 9.09% improvement of mLPIPS on DyCheck compared to the second-best method. Code: <a target="_blank" rel="noopener" href="https://github.com/iCVTEAM/WorldTree">https://github.com/iCVTEAM/WorldTree</a>.</p>
  </div>
</details>

<hr>
<h3 id="65-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception-cs-CV-cs-AI-cs-CL-cs-LGPDF"><a href="#65-Zooming-without-Zooming-Region-to-Image-Distillation-for-Fine-Grained-Multimodal-Perception-cs-CV-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[65] Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception cs.CV | cs.AI | cs.CL | cs.LGPDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11858">Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11858" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lai Wei, Liangbo He, Jun Lan, Lingzhong Dong, Yutong Cai</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为’区域到图像蒸馏’的方法，旨在解决多模态大语言模型在细粒度感知任务上的不足。通过将推理时迭代缩放区域的过程转化为训练时的数据生成策略，该方法使模型能够在单次前向传播中实现细粒度理解，避免了传统’图像思维’方法的高延迟问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多模态大语言模型在全局视觉理解上表现出色，但在细粒度感知任务中，关键证据往往较小且容易被全局上下文淹没。现有的’图像思维’方法通过推理时反复缩放感兴趣区域来缓解此问题，但带来了因重复调用工具和视觉重新编码导致的高延迟。</p>
<p><strong>Result:</strong> 在提出的ZoomBench基准测试（包含845个VQA数据，覆盖六个细粒度感知维度）上，该方法实现了领先性能。实验还表明，该方法不仅提升了细粒度感知能力，还改善了视觉推理和GUI代理等一般多模态认知任务的表现。</p>
<p><strong>Insight:</strong> 核心创新点在于将’缩放’这一推理时操作蒸馏为训练时原语，从而将代理缩放的优势内化到模型的一次前向传播中。这提供了一种在保持性能的同时显著降低延迟的新范式，并探讨了何时需要’图像思维’与何时其增益可被蒸馏的边界。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent “Thinking-with-Images” methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves “single-glance” fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global–regional “zooming gap”. Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when “Thinking-with-Images” is necessary versus when its gains can be distilled into a single forward pass. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/inclusionAI/Zooming-without-Zooming">https://github.com/inclusionAI/Zooming-without-Zooming</a>.</p>
  </div>
</details>

<hr>
<h3 id="66-Benchmarking-Vision-Language-Models-for-French-PDF-to-Markdown-Conversion-cs-CV-cs-CL-cs-LGPDF"><a href="#66-Benchmarking-Vision-Language-Models-for-French-PDF-to-Markdown-Conversion-cs-CV-cs-CL-cs-LGPDF" class="headerlink" title="[66] Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion cs.CV | cs.CL | cs.LGPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11960">Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11960" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bruno Rigal, Victor Dupriez, Alexis Mignon, Ronan Le Hy, Nicolas Mery</span></p>
<p><strong>TL;DR:</strong> 本报告评估了近期视觉语言模型在具有挑战性的法语文档上进行PDF到Markdown转换的性能。文档解析是检索增强生成流程的关键步骤，其转录和布局错误会传播至下游任务。现有基准通常侧重英语或中文，且可能过度惩罚对下游使用无关紧要的格式差异。为此，研究引入了一个专注于法语的基准，该基准通过模型分歧采样从6万份文档中选取困难页面，涵盖手写表单、复杂布局、密集表格和图形丰富页面。评估采用单元测试式检查，针对具体故障模式，并结合类别特定归一化以忽略仅呈现方式的差异。在15个模型的测试中，观察到最强专有模型在手写和表单处理上具有显著更高的鲁棒性，而多个开源权重系统在标准印刷布局上仍保持竞争力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有文档解析基准在语言覆盖（过于侧重英语或中文）和评估标准（过度惩罚对下游任务无关的格式差异）上的局限性，为法语PDF到Markdown转换提供一个更公平、更具挑战性的评估框架，以提升检索增强生成流程中关键解析步骤的可靠性。</p>
<p><strong>Result:</strong> 在涵盖手写表单、复杂布局等的法语困难页面基准上评估了15个模型。最强专有模型在手写和表单处理上表现出显著更高的鲁棒性；多个开源权重模型在标准印刷布局上仍具竞争力。</p>
<p><strong>Insight:</strong> 创新点包括：1) 通过模型分歧采样构建更具挑战性的多模态文档理解基准，聚焦特定语言（法语）和困难样本；2) 设计结合单元测试式检查（针对文本存在、阅读顺序、局部表格约束等具体故障模式）和类别特定归一化的评估方法，旨在更公平地评估模型核心解析能力，忽略仅呈现方式的差异。这为多语言、复杂场景下的文档解析评估提供了新思路。</p>
<p><strong>Abstract:</strong> This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.   We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts.</p>
  </div>
</details>

<hr>
<h3 id="67-Spatial-Chain-of-Thought-Bridging-Understanding-and-Generation-Models-for-Spatial-Reasoning-Generation-cs-CVPDF"><a href="#67-Spatial-Chain-of-Thought-Bridging-Understanding-and-Generation-Models-for-Spatial-Reasoning-Generation-cs-CVPDF" class="headerlink" title="[67] Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation cs.CVPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11980">Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11980" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wei Chen, Yancheng Long, Mingqiao Liu, Haojie Ding, Yankai Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为空间思维链（SCoT）的即插即用框架，旨在将多模态大语言模型（MLLMs）的空间推理能力与扩散模型的生成能力有效结合，以解决扩散模型在复杂空间理解和推理任务上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散模型在美学图像合成方面表现出色，但在复杂空间理解和推理上存在困难；现有方法要么计算成本高，要么因仅依赖文本提示而丢失空间信息。</p>
<p><strong>Result:</strong> 在图像生成基准测试中达到了最先进的性能，在复杂推理任务上显著优于基线方法，并在图像编辑场景中表现出强大效能。</p>
<p><strong>Insight:</strong> 创新点在于通过交替文本-坐标指令格式训练增强扩散模型的布局感知能力，并利用最先进的MLLMs作为规划器生成全面的布局计划，从而直接将其空间规划能力迁移到生成过程中，这是一种无需联合训练的高效桥接方法。</p>
<p><strong>Abstract:</strong> While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model’s layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios.</p>
  </div>
</details>

<hr>
<h3 id="68-Can-Local-Vision-Language-Models-improve-Activity-Recognition-over-Vision-Transformers-–-Case-Study-on-Newborn-Resuscitation-cs-CVPDF"><a href="#68-Can-Local-Vision-Language-Models-improve-Activity-Recognition-over-Vision-Transformers-–-Case-Study-on-Newborn-Resuscitation-cs-CVPDF" class="headerlink" title="[68] Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? – Case Study on Newborn Resuscitation cs.CVPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12002">Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? – Case Study on Newborn Resuscitation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12002" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Enrico Guerriero, Kjersti Engan, Øyvind Meinich-Bache</span></p>
<p><strong>TL;DR:</strong> 本研究探讨了本地视觉语言模型（VLM）结合大型语言模型（LLM）在新生儿复苏视频中活动识别的潜力，并与监督式TimeSFormer基线进行比较。实验表明，经过LoRA微调的本地VLM在F1分数上达到0.91，显著优于TimeSFormer的0.70，但原始零-shot VLM存在幻觉问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 新生儿复苏的准确记录对质量改进和临床指南遵循至关重要，但实践中仍未被充分利用。现有基于3D-CNN和Vision Transformer的方法在识别细粒度活动时面临挑战，因此研究旨在探索生成式AI（GenAI）方法，特别是本地VLM与LLM结合，以提升此类视频的活动识别性能。</p>
<p><strong>Result:</strong> 在包含13.26小时新生儿复苏视频的模拟数据集上，经过LoRA微调的本地VLM达到F1分数0.91，超越了监督式TimeSFormer基线的0.70，实现了SOTA水平；而零-shot VLM策略因幻觉问题表现不佳。</p>
<p><strong>Insight:</strong> 论文宣称的创新点在于首次将本地VLM与LLM结合应用于新生儿复苏视频的细粒度活动识别，并验证了LoRA微调的有效性。从客观角度看，研究揭示了本地VLM在医疗视频分析中的潜力，通过轻量级适配技术（如LoRA）可以显著提升性能，为资源受限环境下的高效AI应用提供了新思路。</p>
<p><strong>Abstract:</strong> Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70.</p>
  </div>
</details>

<hr>
<h3 id="69-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning-cs-CVPDF"><a href="#69-GigaBrain-0-5M-a-VLA-That-Learns-From-World-Model-Based-Reinforcement-Learning-cs-CVPDF" class="headerlink" title="[69] GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning cs.CVPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12099">GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12099" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">GigaBrain Team, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了GigaBrain-0.5M*，一种基于世界模型的强化学习训练的视觉-语言-动作模型。该方法旨在克服传统VLA模型在场景理解和未来预测方面的局限性，通过在已预训练的GigaBrain-0.5模型基础上，集成名为RAMP的世界模型条件化策略强化学习框架，以实现鲁棒的跨任务适应和长时程任务执行。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统视觉-语言-动作模型直接从当前观测预测多步动作序列，存在场景理解受限和未来预测能力弱的固有局限。而基于网络规模视频语料预训练的视频世界模型展现出强大的时空推理和未来预测能力，因此作者希望利用世界模型来增强VLA模型的学习。</p>
<p><strong>Result:</strong> 在包括Laundry Folding、Box Packing和Espresso Preparation在内的挑战性任务上，其提出的RAMP方法相比RECAP基线取得了约30%的性能提升。GigaBrain-0.5M*在真实世界部署视频中展现出可靠的长时程执行能力，能一致地完成复杂操作任务而不失败。</p>
<p><strong>Insight:</strong> 核心创新点在于将预训练的视频世界模型作为基础，通过RAMP框架（基于世界模型条件化的策略强化学习）来训练VLA模型，从而将世界模型的强大时空推理和预测能力注入到机器人动作策略中，实现了从感知到动作的端到端增强和跨任务适应能力的提升。</p>
<p><strong>Abstract:</strong> Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30% on challenging tasks including \texttt{Laundry Folding}, \texttt{Box Packing}, and \texttt{Espresso Preparation}. Critically, \textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \href{<a target="_blank" rel="noopener" href="https://gigabrain05m.github.io}{project/">https://gigabrain05m.github.io}{project</a> page}.</p>
  </div>
</details>

<hr>
<h3 id="70-FAIL-Flow-Matching-Adversarial-Imitation-Learning-for-Image-Generation-cs-CVPDF"><a href="#70-FAIL-Flow-Matching-Adversarial-Imitation-Learning-for-Image-Generation-cs-CVPDF" class="headerlink" title="[70] FAIL: Flow Matching Adversarial Imitation Learning for Image Generation cs.CVPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12155">FAIL: Flow Matching Adversarial Imitation Learning for Image Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12155" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yeyao Ma, Chen Li, Xiaosong Zhang, Han Hu, Weidi Xie</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为FAIL（Flow Matching Adversarial Imitation Learning）的流匹配模型后训练方法，该方法将流匹配与对抗模仿学习相结合，通过对抗训练最小化策略与专家分布之间的差异，无需显式奖励或成对比较。该方法派生出两种算法：FAIL-PD利用可微ODE求解器获得低方差路径梯度，FAIL-PG则提供适用于离散或计算受限场景的黑盒替代方案。通过在Nano Banana pro的13,000个演示上微调FLUX模型，该方法在提示跟随和美学基准测试中取得了有竞争力的性能，并能泛化到离散图像和视频生成，同时可作为鲁棒正则化器缓解基于奖励的优化中的奖励黑客问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决流匹配模型后训练中，监督微调（SFT）无法纠正未见状态下的策略漂移，而偏好优化方法又需要昂贵偏好对或奖励建模的问题。</p>
<p><strong>Result:</strong> 在仅使用13,000个演示微调FLUX模型后，在提示跟随和美学基准测试中取得了有竞争力的性能。</p>
<p><strong>Insight:</strong> 将流匹配的后训练问题形式化为模仿学习，并通过对抗训练最小化分布差异，避免了显式奖励建模或成对比较的需求；提出的两种算法（FAIL-PD和FAIL-PG）分别针对连续可微和离散&#x2F;计算受限场景，提供了灵活的实现方案；框架具有良好泛化性，可应用于离散图像&#x2F;视频生成，并可作为正则化器缓解奖励黑客问题。</p>
<p><strong>Abstract:</strong> Post-training of flow matching models-aligning the output distribution with a high-quality target-is mathematically equivalent to imitation learning. While Supervised Fine-Tuning mimics expert demonstrations effectively, it cannot correct policy drift in unseen states. Preference optimization methods address this but require costly preference pairs or reward modeling. We propose Flow Matching Adversarial Imitation Learning (FAIL), which minimizes policy-expert divergence through adversarial training without explicit rewards or pairwise comparisons. We derive two algorithms: FAIL-PD exploits differentiable ODE solvers for low-variance pathwise gradients, while FAIL-PG provides a black-box alternative for discrete or computationally constrained settings. Fine-tuning FLUX with only 13,000 demonstrations from Nano Banana pro, FAIL achieves competitive performance on prompt following and aesthetic benchmarks. Furthermore, the framework generalizes effectively to discrete image and video generation, and functions as a robust regularizer to mitigate reward hacking in reward-based optimization. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/HansPolo113/FAIL">https://github.com/HansPolo113/FAIL</a>.</p>
  </div>
</details>

<hr>
<h3 id="71-DreamID-Omni-Unified-Framework-for-Controllable-Human-Centric-Audio-Video-Generation-cs-CVPDF"><a href="#71-DreamID-Omni-Unified-Framework-for-Controllable-Human-Centric-Audio-Video-Generation-cs-CVPDF" class="headerlink" title="[71] DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation cs.CVPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12160">DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12160" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xu Guo, Fulong Ye, Qichao Sun, Liyang Chen, Bingchuan Li</span></p>
<p><strong>TL;DR:</strong> 本文提出了DreamID-Omni，一个用于可控的、以人为中心的音视频生成的统一框架。它通过对称条件扩散Transformer整合异构条件信号，并采用双级解耦策略解决多人场景中的身份与音色绑定失败问题，同时利用多任务渐进式训练方案协调不同生成任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法通常将基于参考的音视频生成、视频编辑和音频驱动视频动画等以人为中心的任务视为孤立目标，且难以在单一框架内实现对多个人物身份和语音音色的精确解耦控制。</p>
<p><strong>Result:</strong> 大量实验表明，DreamID-Omni在视频、音频和音视频一致性方面均实现了全面的最先进性能，甚至超越了领先的专有商业模型。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个统一框架处理多种音视频生成任务，核心是设计了对称条件注入方案、结合信号级（同步RoPE）与语义级（结构化描述）的双级解耦策略，以及利用弱约束生成先验来正则化强约束任务的多任务渐进训练方案，有效解决了身份-音色绑定和说话人混淆问题。</p>
<p><strong>Abstract:</strong> Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.</p>
  </div>
</details>

<hr>
<h3 id="72-EO-VAE-Towards-A-Multi-sensor-Tokenizer-for-Earth-Observation-Data-cs-CVPDF"><a href="#72-EO-VAE-Towards-A-Multi-sensor-Tokenizer-for-Earth-Observation-Data-cs-CVPDF" class="headerlink" title="[72] EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data cs.CVPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12177">EO-VAE: Towards A Multi-sensor Tokenizer for Earth Observation Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12177" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nils Lehmann, Yi Wang, Zhitong Xiong, Xiaoxiang Zhu</span></p>
<p><strong>TL;DR:</strong> 本文提出了EO-VAE，一种专为地球观测数据设计的多传感器变分自编码器，旨在作为该领域的通用分词器。它通过动态超网络，用一个单一模型编码和重建来自不同传感器的灵活通道组合，解决了传统方法需要为每种模态单独训练分词器的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有先进的生成模型依赖于将高维输入压缩为高效潜在表示的分词器，但这一范式主要针对RGB图像。地球观测数据因传感器规格多样和光谱通道可变而带来独特挑战，需要一种能统一处理多传感器数据的通用分词器。</p>
<p><strong>Result:</strong> 在TerraMesh数据集上的实验表明，EO-VAE在重建保真度上优于TerraMind分词器，为遥感领域的潜在生成建模建立了一个强大的基线。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了一个统一的多传感器分词器架构，通过动态超网络灵活适应不同的通道组合，避免了为每个传感器模态训练独立模型的需要，为地球观测数据的统一表征学习提供了新思路。</p>
<p><strong>Abstract:</strong> State-of-the-art generative image and video models rely heavily on tokenizers that compress high-dimensional inputs into more efficient latent representations. While this paradigm has revolutionized RGB generation, Earth observation (EO) data presents unique challenges due to diverse sensor specifications and variable spectral channels. We propose EO-VAE, a multi-sensor variational autoencoder designed to serve as a foundational tokenizer for the EO domain. Unlike prior approaches that train separate tokenizers for each modality, EO-VAE utilizes a single model to encode and reconstruct flexible channel combinations via dynamic hypernetworks. Our experiments on the TerraMesh dataset demonstrate that EO-VAE achieves superior reconstruction fidelity compared to the TerraMind tokenizers, establishing a robust baseline for latent generative modeling in remote sensing.</p>
  </div>
</details>

<hr>
<h3 id="73-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing-cs-CV-cs-AIPDF"><a href="#73-DeepGen-1-0-A-Lightweight-Unified-Multimodal-Model-for-Advancing-Image-Generation-and-Editing-cs-CV-cs-AIPDF" class="headerlink" title="[73] DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing cs.CV | cs.AIPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12205">DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12205" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dianyi Wang, Ruihang Li, Feng Han, Chaofan Ma, Wei Song</span></p>
<p><strong>TL;DR:</strong> 本文提出了DeepGen 1.0，一个仅5B参数的轻量级统一多模态模型，用于图像生成和编辑。通过引入堆叠通道桥接（SCB）深度对齐框架和包含三个渐进阶段的数据中心化训练策略（包括对齐预训练、联合监督微调和基于MR-GRPO的强化学习），该模型在仅使用约5000万样本训练的情况下，在多个基准测试上取得了领先的性能，超越了参数量大得多的模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前用于图像生成和编辑的统一多模态模型通常依赖海量参数（如&gt;100亿），导致训练成本和部署开销巨大。本文旨在开发一个轻量级但能力全面的模型，以克服紧凑模型在语义理解和细粒度控制方面的局限性。</p>
<p><strong>Result:</strong> DeepGen 1.0在多个基准测试上取得领先性能：在WISE基准上超越800亿参数的HunyuanImage达28%，在UniREditBench基准上超越270亿参数的Qwen-Image-Edit达37%。</p>
<p><strong>Insight:</strong> 主要创新点包括：1) 堆叠通道桥接（SCB）框架，通过从VLM多层提取层次化特征并与可学习的‘思考令牌’融合，为生成主干提供结构化、富含推理的指导；2) 包含三个渐进阶段的数据中心化训练策略，特别是利用混合奖励和监督信号的MR-GRPO强化学习方法，在提升生成质量和人类偏好对齐的同时保持训练稳定并避免视觉伪影。模型的高效性（轻量参数、少样本训练）和开源策略为统一多模态研究提供了高效、高性能的替代方案。</p>
<p><strong>Abstract:</strong> Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., &gt;10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable ‘think tokens’ to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research.</p>
  </div>
</details>

<hr>
<h3 id="74-Best-of-Both-Worlds-Multimodal-Reasoning-and-Generation-via-Unified-Discrete-Flow-Matching-cs-CVPDF"><a href="#74-Best-of-Both-Worlds-Multimodal-Reasoning-and-Generation-via-Unified-Discrete-Flow-Matching-cs-CVPDF" class="headerlink" title="[74] Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching cs.CVPDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12221">Best of Both Worlds: Multimodal Reasoning and Generation via Unified Discrete Flow Matching</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12221" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Onkar Susladkar, Tushar Prakash, Gayatri Deshmukh, Kiet A. Nguyen, Jiaxun Zhang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了UniDFlow，一个统一的离散流匹配框架，用于多模态理解、生成和编辑。它通过任务特定的低秩适配器解耦理解和生成，避免了目标干扰和表示纠缠，同时一种新颖的基于参考的多模态偏好对齐在相同条件下优化相对结果，提高了忠实性和可控性，而无需大规模重新训练。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决多模态任务中理解和生成目标之间的干扰与表示纠缠问题，旨在实现一个统一且高效的框架，以提升多模态模型的忠实性、可控性和泛化能力。</p>
<p><strong>Result:</strong> UniDFlow在八个基准测试中取得了最先进的性能，并在未经过显式任务特定训练的情况下，在图像修复、上下文图像生成、基于参考的编辑和组合生成等任务上表现出强大的零样本泛化能力。</p>
<p><strong>Insight:</strong> 创新点在于通过任务特定的低秩适配器解耦多模态任务，以及引入基于参考的多模态偏好对齐来优化相对结果。这为构建统一、高效且可控的多模态模型提供了新思路，避免了大规模重新训练的需求。</p>
<p><strong>Abstract:</strong> We propose UniDFlow, a unified discrete flow-matching framework for multimodal understanding, generation, and editing. It decouples understanding and generation via task-specific low-rank adapters, avoiding objective interference and representation entanglement, while a novel reference-based multimodal preference alignment optimizes relative outcomes under identical conditioning, improving faithfulness and controllability without large-scale retraining. UniDFlpw achieves SOTA performance across eight benchmarks and exhibits strong zero-shot generalization to tasks including inpainting, in-context image generation, reference-based editing, and compositional generation, despite no explicit task-specific training.</p>
  </div>
</details>

<hr>
<h3 id="75-MonarchRT-Efficient-Attention-for-Real-Time-Video-Generation-cs-CV-cs-LGPDF"><a href="#75-MonarchRT-Efficient-Attention-for-Real-Time-Video-Generation-cs-CV-cs-LGPDF" class="headerlink" title="[75] MonarchRT: Efficient Attention for Real-Time Video Generation cs.CV | cs.LGPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12271">MonarchRT: Efficient Attention for Real-Time Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12271" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng</span></p>
<p><strong>TL;DR:</strong> 本文提出MonarchRT，一种用于实时视频生成的高效注意力参数化方法。该方法针对扩散Transformer中3D自注意力的二次计算成本瓶颈，通过Monarch矩阵分解注意力，结合对齐的块结构和扩展的平铺Monarch参数化，在保持计算效率的同时实现高表达能力。通过定制Triton内核和微调优化，在Nvidia多款GPU上实现了显著的加速，首次在单张RTX 5090上以16 FPS实现了真正的实时视频生成。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决实时视频生成中扩散Transformer的3D自注意力二次计算成本瓶颈，特别是在少步数和自回归的实时场景下，现有稀疏注意力近似方法失效的问题。</p>
<p><strong>Result:</strong> 在SOTA模型Self-Forcing上应用MonarchRT，实现了高达95%的注意力稀疏性且无质量损失；在Nvidia RTX 5090、H100和B200 GPU上分别优于FlashAttention-2&#x2F;3&#x2F;4内核，获得1.4-11.8倍的加速；首次在单张RTX 5090上以16 FPS实现真正的实时视频生成。</p>
<p><strong>Insight:</strong> 创新点在于发现视频注意力并非可靠稀疏，而是结合了时空位置驱动的周期性结构、动态稀疏语义对应和密集混合；提出基于Monarch矩阵的结构化注意力参数化方法，通过因子化设计和块对齐实现高效高表达；通过定制内核和微调克服参数化开销，为实时视频生成提供了高效的稀疏注意力解决方案。</p>
<p><strong>Abstract:</strong> Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</p>
  </div>
</details>

<hr>
<h3 id="76-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling-cs-CV-cs-AI-cs-LGPDF"><a href="#76-UniT-Unified-Multimodal-Chain-of-Thought-Test-time-Scaling-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[76] UniT: Unified Multimodal Chain-of-Thought Test-time Scaling cs.CV | cs.AI | cs.LGPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12279">UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12279" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha</span></p>
<p><strong>TL;DR:</strong> 本文提出了UniT框架，用于实现统一多模态模型的测试时扩展，通过多轮推理、验证和精炼来提升复杂多模态任务的处理能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有统一多模态模型通常单次处理任务，难以应对需要分解指令、验证中间结果和迭代修正的复杂空间组合、多对象交互或动态指令任务，而测试时扩展在语言模型中已证明有效，但扩展到多模态模型仍具挑战。</p>
<p><strong>Result:</strong> 实验表明，在短推理轨迹上训练的统一模型能泛化到测试时的长推理链；顺序思维链推理比并行采样更具可扩展性和计算效率；生成和编辑轨迹训练提升了分布外视觉推理性能。</p>
<p><strong>Insight:</strong> 创新点在于将测试时扩展与思维链结合到多模态场景，通过智能数据合成、统一模型训练和灵活测试时推理，激发验证、子目标分解和内容记忆等认知行为，为统一模型的生成和理解提供了有效范式。</p>
<p><strong>Abstract:</strong> Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</p>
  </div>
</details>

<hr>
<div id='eess.SP'></div>

<h1 id="eess-SP-Back"><a href="#eess-SP-Back" class="headerlink" title="eess.SP [Back]"></a>eess.SP <a href="#toc">[Back]</a></h1><h3 id="77-Hybrid-operator-learning-of-wave-scattering-maps-in-high-contrast-media-eess-SP-cs-AI-cs-CVPDF"><a href="#77-Hybrid-operator-learning-of-wave-scattering-maps-in-high-contrast-media-eess-SP-cs-AI-cs-CVPDF" class="headerlink" title="[77] Hybrid operator learning of wave scattering maps in high-contrast media eess.SP | cs.AI | cs.CVPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11197">Hybrid operator learning of wave scattering maps in high-contrast media</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.SP | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11197" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Advait Balaji, Trevor Teolis, S. David Mis, Jose Antonio Lara Benitez, Chao Wang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种混合算子学习方法，用于高对比度介质中的波散射建模。该方法将散射算子分解为平滑背景传播和高对比度散射校正两部分，分别用傅里叶神经算子和视觉Transformer学习，以提升高频Helmholtz问题中的相位和振幅精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决高对比度介质（如含盐体的地下模型）中波传播和散射的代理建模问题，这些场景中的强散射和相位敏感性对现有神经算子构成挑战。</p>
<p><strong>Result:</strong> 在高频Helmholtz强对比度问题上评估，混合模型在相位和振幅精度上显著优于单独的FNO或Transformer，并展现出良好的精度-参数缩放特性。</p>
<p><strong>Insight:</strong> 创新点在于将散射算子分解为平滑背景和高对比度校正两部分，并分别用FNO（处理全局耦合特征）和Transformer（通过注意力建模强空间交互）进行学习，这种混合架构有效结合了两种算子的优势。</p>
<p><strong>Abstract:</strong> Surrogate modeling of wave propagation and scattering (i.e. the wave speed and source to wave field map) in heterogeneous media has significant potential in applications such as seismic imaging and inversion. High-contrast settings, such as subsurface models with salt bodies, exhibit strong scattering and phase sensitivity that challenge existing neural operators. We propose a hybrid architecture that decomposes the scattering operator into two separate contributions: a smooth background propagation and a high-contrast scattering correction. The smooth component is learned with a Fourier Neural Operator (FNO), which produces globally coupled feature tokens encoding background wave propagation; these tokens are then passed to a vision transformer, where attention is used to model the high-contrast scattering correction dominated by strong, spatial interactions. Evaluated on high-frequency Helmholtz problems with strong contrasts, the hybrid model achieves substantially improved phase and amplitude accuracy compared to standalone FNOs or transformers, with favorable accuracy-parameter scaling.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="78-Stop-Tracking-Me-Proactive-Defense-Against-Attribute-Inference-Attack-in-LLMs-cs-CR-cs-AI-cs-CLPDF"><a href="#78-Stop-Tracking-Me-Proactive-Defense-Against-Attribute-Inference-Attack-in-LLMs-cs-CR-cs-AI-cs-CLPDF" class="headerlink" title="[78] Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs cs.CR | cs.AI | cs.CLPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11528">Stop Tracking Me! Proactive Defense Against Attribute Inference Attack in LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11528" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dong Yan, Jian Liang, Ran He, Tieniu Tan</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为TRACE-RPS的统一防御框架，旨在对抗大型语言模型（LLMs）从用户生成文本中推断私人属性（如年龄、位置、性别）的攻击。该框架结合了细粒度匿名化（TRACE）和推理阻止优化（RPS），通过识别并匿名化泄露隐私的文本元素，并诱导模型产生拒绝行为，从而有效防止属性推断。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于匿名化的防御方法粒度较粗，缺乏对泄露隐私元素的词级精确匿名化，且通过修改文本来隐藏敏感线索仍无法阻止模型通过推理能力进行属性推断，因此需要一种更有效的主动防御方案。</p>
<p><strong>Result:</strong> 评估显示，TRACE-RPS在开源模型上将属性推断准确率从约50%降低至5%以下，同时展现出强大的跨模型泛化能力、提示变化鲁棒性以及效用-隐私权衡。</p>
<p><strong>Insight:</strong> 创新点在于将细粒度匿名化（利用注意力机制和推理链生成来定位隐私泄露元素）与轻量级两阶段优化策略（诱导模型拒绝行为）相结合，形成统一的主动防御框架，有效解决了现有方法在精度和推理防御上的局限性。</p>
<p><strong>Abstract:</strong> Recent studies have shown that large language models (LLMs) can infer private user attributes (e.g., age, location, gender) from user-generated text shared online, enabling rapid and large-scale privacy breaches. Existing anonymization-based defenses are coarse-grained, lacking word-level precision in anonymizing privacy-leaking elements. Moreover, they are inherently limited as altering user text to hide sensitive cues still allows attribute inference to occur through models’ reasoning capabilities. To address these limitations, we propose a unified defense framework that combines fine-grained anonymization (TRACE) with inference-preventing optimization (RPS). TRACE leverages attention mechanisms and inference chain generation to identify and anonymize privacy-leaking textual elements, while RPS employs a lightweight two-stage optimization strategy to induce model rejection behaviors, thereby preventing attribute inference. Evaluations across diverse LLMs show that TRACE-RPS reduces attribute inference accuracy from around 50% to below 5% on open-source models. In addition, our approach offers strong cross-model generalization, prompt-variation robustness, and utility-privacy tradeoffs. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Jasper-Yan/TRACE-RPS">https://github.com/Jasper-Yan/TRACE-RPS</a>.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="79-Analytical-Search-cs-IR-cs-AI-cs-CLPDF"><a href="#79-Analytical-Search-cs-IR-cs-AI-cs-CLPDF" class="headerlink" title="[79] Analytical Search cs.IR | cs.AI | cs.CLPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11581">Analytical Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11581" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yiteng Tu, Shuo Miao, Weihang Su, Yiqun Liu, Qingyao Ai</span></p>
<p><strong>TL;DR:</strong> 本文提出了‘分析性搜索’作为一种新兴的搜索范式，旨在满足法律、金融、科学等领域中趋势分析、因果影响评估等分析性信息需求。该范式将搜索重构为一个由证据驱动、过程导向的分析工作流，通过明确建模分析意图、检索并融合证据、进行结构化多步推理来产生可验证的结论。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有信息检索范式（无论是基于相关性的文档排序，还是基于大语言模型的检索增强生成）难以满足分析性任务在语料库规模下的端到端需求，它们要么侧重信息查找而非端到端问题解决，要么将一切简化为简单的问答，对推理、证据使用和可验证性的控制有限。</p>
<p><strong>Result:</strong> 论文未在摘要中提及具体的定量实验结果或基准测试，而是提出了一个统一的系统框架，并讨论了构建分析性搜索引擎的潜在研究方向。</p>
<p><strong>Insight:</strong> 创新点在于将‘分析性搜索’定位为一个独立的搜索范式，其核心是证据治理、过程导向的工作流，强调对分析意图的显式建模、面向召回的证据检索、推理感知的融合以及自适应验证，旨在推动下一代支持分析性信息需求的搜索引擎发展。</p>
<p><strong>Abstract:</strong> Analytical information needs, such as trend analysis and causal impact assessment, are prevalent across various domains including law, finance, science, and much more. However, existing information retrieval paradigms, whether based on relevance-oriented document ranking or retrieval-augmented generation (RAG) with large language models (LLMs), often struggle to meet the end-to-end requirements of such tasks at the corpus scale. They either emphasize information finding rather than end-to-end problem solving, or simply treat everything as naive question answering, offering limited control over reasoning, evidence usage, and verifiability. As a result, they struggle to support analytical queries that have diverse utility concepts and high accountability requirements.   In this paper, we propose analytical search as a distinct and emerging search paradigm designed to fulfill these analytical information needs. Analytical search reframes search as an evidence-governed, process-oriented analytical workflow that explicitly models analytical intent, retrieves evidence for fusion, and produces verifiable conclusions through structured, multi-step inference. We position analytical search in contrast to existing paradigms, and present a unified system framework that integrates query understanding, recall-oriented retrieval, reasoning-aware fusion, and adaptive verification. We also discuss potential research directions for the construction of analytical search engines. In this way, we highlight the conceptual significance and practical importance of analytical search and call on efforts toward the next generation of search engines that support analytical information needs.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="80-Mitigating-Error-Accumulation-in-Continuous-Navigation-via-Memory-Augmented-Kalman-Filtering-cs-RO-cs-CV-eess-SYPDF"><a href="#80-Mitigating-Error-Accumulation-in-Continuous-Navigation-via-Memory-Augmented-Kalman-Filtering-cs-RO-cs-CV-eess-SYPDF" class="headerlink" title="[80] Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering cs.RO | cs.CV | eess.SYPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11183">Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | eess.SY</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11183" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yin Tang, Jiawei Ma, Jinrui Zhang, Alex Jinpeng Wang, Deyu Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出NeuroKalman框架，通过将连续导航建模为递归贝叶斯状态估计问题，将导航解耦为先验预测和似然校正两个互补过程，以缓解视觉语言导航中因航位推算导致的误差累积和状态漂移问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有视觉语言导航模型采用航位推算逐步更新位置，会导致误差随时间累积，产生内部状态与客观坐标失准的’状态漂移’问题，损害完整轨迹预测。</p>
<p><strong>Result:</strong> 在TravelUAV基准测试上的综合实验表明，仅需10%的训练数据进行微调，该方法明显优于强基线，并能有效调控漂移累积。</p>
<p><strong>Insight:</strong> 核心创新在于受经典控制理论启发，将序列预测形式化为状态估计问题，并数学关联了测量似然的核密度估计与基于注意力的检索机制，使系统无需梯度更新即可利用检索到的历史锚点修正潜在表示。</p>
<p><strong>Abstract:</strong> Continuous navigation in complex environments is critical for Unmanned Aerial Vehicle (UAV). However, the existing Vision-Language Navigation (VLN) models follow the dead-reckoning, which iteratively updates its position for the next waypoint prediction, and subsequently construct the complete trajectory. Then, such stepwise manner will inevitably lead to accumulated errors of position over time, resulting in misalignment between internal belief and objective coordinates, which is known as “state drift” and ultimately compromises the full trajectory prediction. Drawing inspiration from classical control theory, we propose to correct for errors by formulating such sequential prediction as a recursive Bayesian state estimation problem. In this paper, we design NeuroKalman, a novel framework that decouples navigation into two complementary processes: a Prior Prediction, based on motion dynamics and a Likelihood Correction, from historical observation. We first mathematically associate Kernel Density Estimation of the measurement likelihood with the attention-based retrieval mechanism, which then allows the system to rectify the latent representation using retrieved historical anchors without gradient updates. Comprehensive experiments on TravelUAV benchmark demonstrate that, with only 10% of the training data fine-tuning, our method clearly outperforms strong baselines and regulates drift accumulation.</p>
  </div>
</details>

<hr>
<h3 id="81-HyperDet-3D-Object-Detection-with-Hyper-4D-Radar-Point-Clouds-cs-RO-cs-CV-cs-LGPDF"><a href="#81-HyperDet-3D-Object-Detection-with-Hyper-4D-Radar-Point-Clouds-cs-RO-cs-CV-cs-LGPDF" class="headerlink" title="[81] HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds cs.RO | cs.CV | cs.LGPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11554">HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11554" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yichun Xiao, Runwei Guan, Fangqiang Ding</span></p>
<p><strong>TL;DR:</strong> HyperDet是一个与检测器无关的雷达3D检测框架，通过聚合多帧多雷达点云、几何一致性验证和前景扩散模块，构建任务感知的超4D雷达点云，以提升雷达点云的质量，使其能直接用于标准的LiDAR检测器。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 4D毫米波雷达具有天气鲁棒性和速度感知能力，且成本低于LiDAR，但其点云稀疏、不规则且易受多径噪声影响，导致基于雷达的3D检测性能落后于LiDAR系统。</p>
<p><strong>Result:</strong> 在MAN TruckScenes数据集上，HyperDet配合VoxelNeXt和CenterPoint检测器，相比原始雷达输入有持续改进，部分缩小了雷达与LiDAR之间的性能差距。</p>
<p><strong>Insight:</strong> 创新点包括：通过跨传感器一致性验证抑制噪声；利用前景扩散模块和雷达-LiDAR混合监督增强点云密度与属性；将模型蒸馏为一致性模型以实现单步推理，从而在输入层面优化雷达数据以适配LiDAR检测器，无需修改网络架构。</p>
<p><strong>Abstract:</strong> 4D mmWave radar provides weather-robust, velocity-aware measurements and is more cost-effective than LiDAR. However, radar-only 3D detection still trails LiDAR-based systems because radar point clouds are sparse, irregular, and often corrupted by multipath noise, yielding weak and unstable geometry. We present HyperDet, a detector-agnostic radar-only 3D detection framework that constructs a task-aware hyper 4D radar point cloud for standard LiDAR-oriented detectors. HyperDet aggregates returns from multiple surround-view 4D radars over consecutive frames to improve coverage and density, then applies geometry-aware cross-sensor consensus validation with a lightweight self-consistency check outside overlap regions to suppress inconsistent returns. It further integrates a foreground-focused diffusion module with training-time mixed radar-LiDAR supervision to densify object structures while lifting radar attributes (e.g., Doppler, RCS); the model is distilled into a consistency model for single-step inference. On MAN TruckScenes, HyperDet consistently improves over raw radar inputs with VoxelNeXt and CenterPoint, partially narrowing the radar-LiDAR gap. These results show that input-level refinement enables radar to better leverage LiDAR-oriented detectors without architectural modifications.</p>
  </div>
</details>

<hr>
<h3 id="82-ABot-N0-Technical-Report-on-the-VLA-Foundation-Model-for-Versatile-Embodied-Navigation-cs-RO-cs-AI-cs-CVPDF"><a href="#82-ABot-N0-Technical-Report-on-the-VLA-Foundation-Model-for-Versatile-Embodied-Navigation-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[82] ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation cs.RO | cs.AI | cs.CVPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11598">ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11598" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zedong Chu, Shichao Xie, Xiaolong Wu, Yanfen Shen, Minghua Luo</span></p>
<p><strong>TL;DR:</strong> 本文介绍了ABot-N0，一个统一的视觉-语言-动作（VLA）基础模型，旨在实现具身导航任务的‘大一统’。该模型采用分层‘大脑-动作’架构，结合基于LLM的认知大脑进行语义推理和基于流匹配的动作专家生成连续轨迹。通过大规模数据引擎构建数据集，在7个基准测试中达到新的SOTA性能，并集成了规划器与分层拓扑记忆以实现动态环境中的长时程任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决具身导航领域因任务特定架构而碎片化的问题，提出一个统一模型以覆盖多种核心导航任务，实现通用性和效率的提升。</p>
<p><strong>Result:</strong> 在7个基准测试中达到新的SOTA性能，显著超越专用模型，覆盖Point-Goal、Object-Goal等5个核心任务。</p>
<p><strong>Insight:</strong> 创新点包括分层‘大脑-动作’架构结合LLM与流匹配技术，以及大规模数据引擎构建多任务数据集；从客观角度看，其统一模型设计和集成规划记忆系统为具身智能的泛化与鲁棒性提供了新思路。</p>
<p><strong>Abstract:</strong> Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a <code>Grand Unification&#39;&#39; across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical </code>Brain-Action’’ architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="83-Learning-Perceptual-Representations-for-Gaming-NR-VQA-with-Multi-Task-FR-Signals-eess-IV-cs-CV-cs-MMPDF"><a href="#83-Learning-Perceptual-Representations-for-Gaming-NR-VQA-with-Multi-Task-FR-Signals-eess-IV-cs-CV-cs-MMPDF" class="headerlink" title="[83] Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals eess.IV | cs.CV | cs.MMPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11903">Learning Perceptual Representations for Gaming NR-VQA with Multi-Task FR Signals</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.MM</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11903" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yu-Chih Chen, Michael Wang, Chieh-Dun Wen, Kai-Siang Ma, Avinab Saha</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MTL-VQA的多任务学习框架，用于解决游戏视频的无参考质量评估（NR-VQA）问题。该方法利用全参考（FR）指标作为监督信号，在无需人工标注的情况下学习感知上有意义的特征表示，并通过自适应任务加权联合优化多个FR目标，从而学习到可有效迁移至NR-VQA任务的共享表示。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 游戏视频的NR-VQA面临挑战，包括人工评分数据集有限以及游戏视频特有的快速运动、风格化图形和压缩伪影等内容特征。本文旨在通过多任务学习利用FR信号来学习感知表示，以克服这些挑战。</p>
<p><strong>Result:</strong> 在游戏视频数据集上的实验表明，MTL-VQA在MOS监督、标签高效和自监督设置下，其性能均与最先进的NR-VQA方法相当。</p>
<p><strong>Insight:</strong> 创新点在于提出了一种利用多任务FR信号进行无监督预训练以学习感知表示的方法，并通过自适应任务加权来联合优化多个FR目标。从客观角度看，该方法巧妙地利用易于获取的FR指标作为代理任务，为数据稀缺的NR-VQA任务学习到了有效的特征表示。</p>
<p><strong>Abstract:</strong> No-reference video quality assessment (NR-VQA) for gaming videos is challenging due to limited human-rated datasets and unique content characteristics including fast motion, stylized graphics, and compression artifacts. We present MTL-VQA, a multi-task learning framework that uses full-reference metrics as supervisory signals to learn perceptually meaningful features without human labels for pretraining. By jointly optimizing multiple full-reference (FR) objectives with adaptive task weighting, our approach learns shared representations that transfer effectively to NR-VQA. Experiments on gaming video datasets show MTL-VQA achieves performance competitive with state-of-the-art NR-VQA methods across both MOS-supervised and label-efficient&#x2F;self-supervised settings.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="84-Althea-Human-AI-Collaboration-for-Fact-Checking-and-Critical-Reasoning-cs-HC-cs-CLPDF"><a href="#84-Althea-Human-AI-Collaboration-for-Fact-Checking-and-Critical-Reasoning-cs-HC-cs-CLPDF" class="headerlink" title="[84] Althea: Human-AI Collaboration for Fact-Checking and Critical Reasoning cs.HC | cs.CLPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11161">Althea: Human-AI Collaboration for Fact-Checking and Critical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11161" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Svetlana Churina, Kokil Jaidka, Anab Maulana Barik, Harshit Aneja, Cai Yang</span></p>
<p><strong>TL;DR:</strong> Althea是一个用于事实核查和批判性推理的人机协作系统，通过整合问题生成、证据检索和结构化推理来支持用户对在线声明的评估。在AVeriTeC基准测试中，Althea的Macro-F1得分为0.44，优于标准验证流程，并提高了对支持和反驳声明的区分能力。用户研究显示，引导式交互在准确性和信心方面带来最显著的即时提升，而自主搜索则产生最持久的长期改进。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决网络信息生态中事实核查系统在可扩展性和认知可信度方面的不足，自动化方法缺乏透明度，而人工验证则缓慢且不一致。</p>
<p><strong>Result:</strong> 在AVeriTeC基准上达到Macro-F1 0.44，优于标准验证流程；用户研究（N&#x3D;642）表明引导式交互（Exploratory模式）在准确性和信心上提升最强，自主搜索（Self-search模式）带来最持久的改进。</p>
<p><strong>Insight:</strong> 创新点在于将检索增强与结构化推理结合，支持用户驱动的评估；客观分析表明系统通过不同交互模式（引导式与自主式）结构化认知工作，促进知识内化，而非仅依赖努力或暴露，这为人机协作系统的设计提供了新视角。</p>
<p><strong>Abstract:</strong> The web’s information ecosystem demands fact-checking systems that are both scalable and epistemically trustworthy. Automated approaches offer efficiency but often lack transparency, while human verification remains slow and inconsistent. We introduce Althea, a retrieval-augmented system that integrates question generation, evidence retrieval, and structured reasoning to support user-driven evaluation of online claims. On the AVeriTeC benchmark, Althea achieves a Macro-F1 of 0.44, outperforming standard verification pipelines and improving discrimination between supported and refuted claims. We further evaluate Althea through a controlled user study and a longitudinal survey experiment (N &#x3D; 642), comparing three interaction modes that vary in the degree of scaffolding: an Exploratory mode with guided reasoning, a Summary mode providing synthesized verdicts, and a Self-search mode that offers procedural guidance without algorithmic intervention. Results show that guided interaction produces the strongest immediate gains in accuracy and confidence, while self-directed search yields the most persistent improvements over time. This pattern suggests that performance gains are not driven solely by effort or exposure, but by how cognitive work is structured and internalized.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="85-Beyond-Pixels-Vector-to-Graph-Transformation-for-Reliable-Schematic-Auditing-cs-AI-cs-CVPDF"><a href="#85-Beyond-Pixels-Vector-to-Graph-Transformation-for-Reliable-Schematic-Auditing-cs-AI-cs-CVPDF" class="headerlink" title="[85] Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing cs.AI | cs.CVPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11678">Beyond Pixels: Vector-to-Graph Transformation for Reliable Schematic Auditing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11678" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chengwei Ma, Zhen Tian, Zhou Zhou, Zhixian Xu, Xiaowei Zhu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为Vector-to-Graph（V2G）的管道，用于将CAD工程原理图转换为属性图，以解决多模态大语言模型（MLLMs）在理解工程原理图时存在的结构盲区问题。该方法通过显式编码组件和连接关系，显著提升了电气合规性检查的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前最先进的多模态大语言模型（MLLMs）在视觉理解方面取得了显著进展，但其基于像素的范式存在结构盲区，无法有效捕捉工程原理图中的拓扑结构和符号逻辑，这限制了其在工程领域（如原理图审计）的实际部署。</p>
<p><strong>Result:</strong> 在电气合规性检查的诊断基准测试中，V2G方法在所有错误类别上都取得了巨大的准确率提升，而领先的MLLMs模型的表现则接近随机猜测水平。</p>
<p><strong>Insight:</strong> 论文的核心创新点在于提出了一个从矢量图到属性图的转换管道，将工程原理图的结构化依赖关系显式化，使其可被机器审计。这揭示了基于像素的方法在结构化理解任务上的系统性不足，并为多模态AI在工程领域的实际应用提供了一条可靠路径，即采用结构感知的表示方法。</p>
<p><strong>Abstract:</strong> Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual understanding, yet they suffer from a critical limitation: structural blindness. Even state-of-the-art models fail to capture topology and symbolic logic in engineering schematics, as their pixel-driven paradigm discards the explicit vector-defined relations needed for reasoning. To overcome this, we propose a Vector-to-Graph (V2G) pipeline that converts CAD diagrams into property graphs where nodes represent components and edges encode connectivity, making structural dependencies explicit and machine-auditable. On a diagnostic benchmark of electrical compliance checks, V2G yields large accuracy gains across all error categories, while leading MLLMs remain near chance level. These results highlight the systemic inadequacy of pixel-based methods and demonstrate that structure-aware representations provide a reliable path toward practical deployment of multimodal AI in engineering domains. To facilitate further research, we release our benchmark and implementation at <a target="_blank" rel="noopener" href="https://github.com/gm-embodied/V2G-Audit">https://github.com/gm-embodied/V2G-Audit</a>.</p>
  </div>
</details>

<hr>
<h3 id="86-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces-cs-AI-cs-CL-cs-LGPDF"><a href="#86-ThinkRouter-Efficient-Reasoning-via-Routing-Thinking-between-Latent-and-Discrete-Spaces-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[86] ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces cs.AI | cs.CL | cs.LGPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11683">ThinkRouter: Efficient Reasoning via Routing Thinking between Latent and Discrete Spaces</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11683" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xin Xu, Tong Yu, Xiang Chen, Haoliang Wang, Julian McAuley</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了ThinkRouter，一种推理时基于置信度的路由机制，用于在潜在空间和离散标记空间之间动态切换思考路径，以提高大型推理模型的效率和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于分析发现，潜在推理中错误答案轨迹的置信度步骤较少，而多个低置信度替代方案聚合的软嵌入可能引入噪声，导致对不可靠推理轨迹的过度自信，因此需要一种机制来避免高置信度和噪声。</p>
<p><strong>Result:</strong> 在STEM推理和编码基准测试上的广泛实验表明，ThinkRouter在准确率上优于显式思维链、随机路由和潜在推理基线，平均Pass@1提高了19.70分，同时生成长度减少了高达15.55%。</p>
<p><strong>Insight:</strong> 创新点在于引入置信度感知的路由机制，根据模型置信度动态选择思考空间，从而校准显式思维链和潜在推理中的错误，并通过全局降低模型置信度加速思考结束标记的生成。</p>
<p><strong>Abstract:</strong> Recent work explores latent reasoning to improve reasoning efficiency by replacing explicit reasoning trajectories with continuous representations in a latent space, yet its effectiveness varies across settings. Analysis of model confidence dynamics under latent reasoning reveals that thinking trajectories ending in incorrect answers contain fewer low-confidence steps than those ending in correct answers. Meanwhile, we suggest that soft embeddings aggregated by multiple low-confidence thinking alternatives may introduce and propagate noise, leading to high confidence in unreliable reasoning trajectories. Motivated by these observations, ThinkRouter, an inference-time confidence-aware routing mechanism is proposed to avoid high confidence and noise for efficient reasoning. ThinkRouter routes thinking to the discrete token space when model confidence is low, and to the latent space otherwise. Extensive experiments on STEM reasoning and coding benchmarks across diverse large reasoning models demonstrate that ThinkRouter outperforms explicit CoT, random routing, and latent reasoning baselines in terms of accuracy, achieving an average improvement of 19.70 points in Pass@1, while reducing generation length by up to 15.55%. Further comprehensive analysis reveals that ThinkRouter can calibrate errors arising from explicit CoT and latent reasoning, and accelerates end-of-thinking token generation by globally lowering model confidence.</p>
  </div>
</details>

<hr>
<h3 id="87-TSR-Trajectory-Search-Rollouts-for-Multi-Turn-RL-of-LLM-Agents-cs-AI-cs-CL-cs-LGPDF"><a href="#87-TSR-Trajectory-Search-Rollouts-for-Multi-Turn-RL-of-LLM-Agents-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[87] TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents cs.AI | cs.CL | cs.LGPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11767">TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11767" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Aladin Djuhera, Swanand Ravindra Kadhe, Farhan Ahmed, Holger Boche</span></p>
<p><strong>TL;DR:</strong> 本文提出TSR（轨迹搜索展开）方法，用于改进多轮强化学习中轨迹生成的质量。TSR通过在训练时执行轻量级树状搜索，利用任务特定反馈选择高得分动作来构建高质量轨迹，从而提升学习效果和稳定性。该方法与优化器无关，在Sokoban、FrozenLake和WebShop任务上结合PPO和GRPO实现了最高15%的性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 多轮强化学习中奖励稀疏或延迟、环境随机性高，导致朴素轨迹采样可能阻碍探索并引发模式崩溃，因此需要改进训练时的轨迹生成机制。</p>
<p><strong>Result:</strong> 在Sokoban、FrozenLake和WebShop任务上，TSR结合PPO和GRPO实现了最高15%的性能提升，学习过程更稳定，仅需一次性增加训练计算成本。</p>
<p><strong>Insight:</strong> 将测试时的搜索技术（如best-of-N、波束搜索和浅层前瞻搜索）迁移到训练时的轨迹生成阶段，通过轻量级搜索提升轨迹质量，这是一种与优化器无关的通用方法，可补充现有框架和拒绝采样式选择方法。</p>
<p><strong>Abstract:</strong> Advances in large language models (LLMs) are driving a shift toward using reinforcement learning (RL) to train agents from iterative, multi-turn interactions across tasks. However, multi-turn RL remains challenging as rewards are often sparse or delayed, and environments can be stochastic. In this regime, naive trajectory sampling can hinder exploitation and induce mode collapse. We propose TSR (Trajectory-Search Rollouts), a training-time approach that repurposes test-time scaling ideas for improved per-turn rollout generation. TSR performs lightweight tree-style search to construct high-quality trajectories by selecting high-scoring actions at each turn using task-specific feedback. This improves rollout quality and stabilizes learning while leaving the underlying optimization objective unchanged, making TSR optimizer-agnostic. We instantiate TSR with best-of-N, beam, and shallow lookahead search, and pair it with PPO and GRPO, achieving up to 15% performance gains and more stable learning on Sokoban, FrozenLake, and WebShop tasks at a one-time increase in training compute. By moving search from inference time to the rollout stage of training, TSR provides a simple and general mechanism for stronger multi-turn agent learning, complementary to existing frameworks and rejection-sampling-style selection methods.</p>
  </div>
</details>

<hr>
<h3 id="88-Beyond-End-to-End-Video-Models-An-LLM-Based-Multi-Agent-System-for-Educational-Video-Generation-cs-AI-cs-CLPDF"><a href="#88-Beyond-End-to-End-Video-Models-An-LLM-Based-Multi-Agent-System-for-Educational-Video-Generation-cs-AI-cs-CLPDF" class="headerlink" title="[88] Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation cs.AI | cs.CLPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11790">Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11790" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lingyong Yan, Jiulong Wu, Dong Xie, Weixian Shi, Deguo Xia</span></p>
<p><strong>TL;DR:</strong> 本文提出LAVES，一个基于LLM的分层多智能体系统，用于从教育问题生成高质量的教学视频。该系统将视频生成分解为由编排智能体协调的多个专业化智能体任务，通过结构化可执行脚本和确定性编译，实现了低成本、高吞吐量的自动化生产。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决现有端到端视频生成模型在需要严格逻辑严谨性和精确知识表示（如教学媒体）的场景中的局限性，包括程序保真度低、生产成本高和可控性有限等问题。</p>
<p><strong>Result:</strong> 在大规模部署中，LAVES实现了每天超过一百万视频的吞吐量，与当前行业标准方法相比，成本降低了95%以上，同时保持了较高的接受率。</p>
<p><strong>Insight:</strong> 创新点在于将教育视频生成构建为一个多目标任务，并采用分层多智能体架构进行分解和协调，结合显式的质量门控和迭代批判机制，通过生成结构化可执行脚本而非直接合成像素来实现确定性编译和自动化生产，从而在保证逻辑严谨性和教学连贯性的同时，大幅提升了效率和可控性。</p>
<p><strong>Abstract:</strong> Although recent end-to-end video generation models demonstrate impressive performance in visually oriented content creation, they remain limited in scenarios that require strict logical rigor and precise knowledge representation, such as instructional and educational media. To address this problem, we propose LAVES, a hierarchical LLM-based multi-agent system for generating high-quality instructional videos from educational problems. The LAVES formulates educational video generation as a multi-objective task that simultaneously demands correct step-by-step reasoning, pedagogically coherent narration, semantically faithful visual demonstrations, and precise audio–visual alignment. To address the limitations of prior approaches–including low procedural fidelity, high production cost, and limited controllability–LAVES decomposes the generation workflow into specialized agents coordinated by a central Orchestrating Agent with explicit quality gates and iterative critique mechanisms. Specifically, the Orchestrating Agent supervises a Solution Agent for rigorous problem solving, an Illustration Agent that produces executable visualization codes, and a Narration Agent for learner-oriented instructional scripts. In addition, all outputs from the working agents are subject to semantic critique, rule-based constraints, and tool-based compilation checks. Rather than directly synthesizing pixels, the system constructs a structured executable video script that is deterministically compiled into synchronized visuals and narration using template-driven assembly rules, enabling fully automated end-to-end production without manual editing. In large-scale deployments, LAVES achieves a throughput exceeding one million videos per day, delivering over a 95% reduction in cost compared to current industry-standard approaches while maintaining a high acceptance rate.</p>
  </div>
</details>

<hr>
<h3 id="89-Detecting-RLVR-Training-Data-via-Structural-Convergence-of-Reasoning-cs-AI-cs-CLPDF"><a href="#89-Detecting-RLVR-Training-Data-via-Structural-Convergence-of-Reasoning-cs-AI-cs-CLPDF" class="headerlink" title="[89] Detecting RLVR Training Data via Structural Convergence of Reasoning cs.AI | cs.CLPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11792">Detecting RLVR Training Data via Structural Convergence of Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11792" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hongbo Zhang, Yue Yang, Jianhao Yan, Guangsheng Bao, Yue Zhang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Min-kNN Distance的黑盒检测方法，用于识别在强化学习可验证奖励（RLVR）训练中见过的数据。该方法通过采样多个生成结果并计算k个最近邻编辑距离的平均值，量化RLVR训练导致的生成结构收敛性，从而有效区分训练数据与未见数据。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> RLVR训练现代推理模型时，未公开的训练数据引发基准污染担忧，而传统基于似然的检测方法因RLVR基于奖励反馈微调而非词元级概率优化而失效，因此需要一种无需参考模型或词元概率的有效检测手段。</p>
<p><strong>Result:</strong> 在多个RLVR训练的推理模型上实验表明，Min-kNN Distance能可靠区分RL训练见过的示例与未见示例，并优于现有的成员推理和RL污染检测基线方法。</p>
<p><strong>Insight:</strong> 创新点在于发现RLVR训练会诱导独特的行为特征：训练过的提示导致生成更僵化和相似，而未见提示保持多样性；Min-kNN Distance利用这一结构收敛性，提供了一种简单高效的黑盒检测方案，无需模型内部信息。</p>
<p><strong>Abstract:</strong> Reinforcement learning with verifiable rewards (RLVR) is central to training modern reasoning models, but the undisclosed training data raises concerns about benchmark contamination. Unlike pretraining methods, which optimize models using token-level probabilities, RLVR fine-tunes models based on reward feedback from self-generated reasoning trajectories, making conventional likelihood-based detection methods less effective. We show that RLVR induces a distinctive behavioral signature: prompts encountered during RLVR training result in more rigid and similar generations, while unseen prompts retain greater diversity. We introduce Min-$k$NN Distance, a simple black-box detector that quantifies this collapse by sampling multiple completions for a given prompt and computing the average of the $k$ smallest nearest-neighbor edit distances. Min-$k$NN Distance requires no access to the reference model or token probabilities. Experiments across multiple RLVR-trained reasoning models show that Min-$k$NN Distance reliably distinguishes RL-seen examples from unseen ones and outperforms existing membership inference and RL contamination detection baselines.</p>
  </div>
</details>

<hr>
<h3 id="90-Prototype-Transformer-Towards-Language-Model-Architectures-Interpretable-by-Design-cs-AI-cs-CL-cs-LGPDF"><a href="#90-Prototype-Transformer-Towards-Language-Model-Architectures-Interpretable-by-Design-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[90] Prototype Transformer: Towards Language Model Architectures Interpretable by Design cs.AI | cs.CL | cs.LGPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11852">Prototype Transformer: Towards Language Model Architectures Interpretable by Design</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11852" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yordan Yordanov, Matteo Forasassi, Bayar Menzat, Ruizhi Wang, Chang Qi</span></p>
<p><strong>TL;DR:</strong> 本文提出了原型Transformer（ProtoT），一种基于原型（参数向量）的自回归语言模型架构，旨在替代标准的基于自注意力的Transformer。该架构通过输入序列与原型之间的双向通信，使原型在训练中自动捕获可命名的概念（如“女性”），从而提供解释模型推理的潜力，并允许对其行为进行有针对性的编辑。在计算可扩展性方面，ProtoT的复杂度随序列长度线性增长，优于当前最先进的自注意力Transformer的二次方复杂度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前最先进的语言模型在某些领域超越了绝大多数人类，但其推理过程仍然不透明，这削弱了对其输出的信任。自回归语言模型虽然可以输出显式推理，但其真实的推理过程是黑盒的，引入了欺骗和幻觉等风险。因此，本文旨在设计一种本质上可解释的自回归语言模型架构。</p>
<p><strong>Result:</strong> ProtoT在模型和数据规模上扩展性良好，在文本生成和下游任务（GLUE）上表现良好。其性能接近最先进的架构，并且在输入扰动下的鲁棒性与某些基线相当或更好，同时提供了可解释的路径来展示鲁棒性和敏感性的来源。</p>
<p><strong>Insight:</strong> 创新点在于通过原型实现双向通信，使模型能够自动学习可解释的概念，从而提供透明的推理路径。从客观角度看，该架构在保持性能的同时，实现了线性计算复杂度和设计上的可解释性，为构建高性能且可解释的自回归语言模型开辟了新途径。</p>
<p><strong>Abstract:</strong> While state-of-the-art language models (LMs) surpass the vast majority of humans in certain domains, their reasoning remains largely opaque, undermining trust in their output. Furthermore, while autoregressive LMs can output explicit reasoning, their true reasoning process is opaque, which introduces risks like deception and hallucination. In this work, we introduce the Prototype Transformer (ProtoT) – an autoregressive LM architecture based on prototypes (parameter vectors), posed as an alternative to the standard self-attention-based transformers. ProtoT works by means of two-way communication between the input sequence and the prototypes, and we show that this leads to the prototypes automatically capturing nameable concepts (e.g. “woman”) during training. They provide the potential to interpret the model’s reasoning and allow for targeted edits of its behavior. Furthermore, by design, the prototypes create communication channels that aggregate contextual information at different time scales, aiding interpretability. In terms of computation scalability, ProtoT scales linearly with sequence length vs the quadratic scalability of SOTA self-attention transformers. Compared to baselines, ProtoT scales well with model and data size, and performs well on text generation and downstream tasks (GLUE). ProtoT exhibits robustness to input perturbations on par or better than some baselines, but differs from them by providing interpretable pathways showing how robustness and sensitivity arises. Reaching close to the performance of state-of-the-art architectures, ProtoT paves the way to creating well-performing autoregressive LMs interpretable by design.</p>
  </div>
</details>

<hr>
<h3 id="91-Tiny-Recursive-Reasoning-with-Mamba-2-Attention-Hybrid-cs-AI-cs-CLPDF"><a href="#91-Tiny-Recursive-Reasoning-with-Mamba-2-Attention-Hybrid-cs-AI-cs-CLPDF" class="headerlink" title="[91] Tiny Recursive Reasoning with Mamba-2 Attention Hybrid cs.AI | cs.CLPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12078">Tiny Recursive Reasoning with Mamba-2 Attention Hybrid</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12078" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenlong Wang, Fergal Reid</span></p>
<p><strong>TL;DR:</strong> 本文研究了在递归推理模型TRM中用Mamba-2混合算子替换Transformer块的效果。实验表明，在保持参数量相当（约6.8M）的情况下，混合模型在ARC-AGI-1基准上提升了pass@2等多项指标，验证了基于状态空间模型（SSM）的算子在递归推理框架中的有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探究Mamba-2的状态空间循环作为一种隐式迭代优化形式，是否适合并能够保持递归推理模型在抽象推理任务中的能力，从而扩展递归算子设计空间。</p>
<p><strong>Result:</strong> 在ARC-AGI-1基准上，混合模型将pass@2（官方指标）提升了2.0%（45.88% vs 43.88%），在更高K值（如pass@100）上持续优于原模型（提升4.75%），同时保持pass@1性能相当，表明模型生成正确解的覆盖范围更可靠。</p>
<p><strong>Insight:</strong> 创新点在于将Mamba-2混合算子首次引入递归推理框架，验证了基于SSM的算子可作为递归算子的可行候选，为理解递归推理的最佳混合策略迈出了第一步；客观上，这展示了在轻量级网络中结合不同迭代优化机制（隐式递归与状态空间循环）以提升推理覆盖率的潜力。</p>
<p><strong>Abstract:</strong> Recent work on recursive reasoning models like TRM demonstrates that tiny networks (7M parameters) can achieve strong performance on abstract reasoning tasks through latent recursion – iterative refinement in hidden representation space without emitting intermediate tokens. This raises a natural question about operator choice: Mamba-2’s state space recurrence is itself a form of iterative refinement, making it a natural candidate for recursive reasoning – but does introducing Mamba-2 into the recursive scaffold preserve reasoning capability? We investigate this by replacing the Transformer blocks in TRM with Mamba-2 hybrid operators while maintaining parameter parity (6.83M vs 6.86M parameters). On ARC-AGI-1, we find that the hybrid improves pass@2 (the official metric) by +2.0% (45.88% vs 43.88%) and consistently outperforms at higher K values (+4.75% at pass@100), whilst maintaining pass@1 parity. This suggests improved candidate coverage – the model generates correct solutions more reliably – with similar top-1 selection. Our results validate that Mamba-2 hybrid operators preserve reasoning capability within the recursive scaffold, establishing SSM-based operators as viable candidates in the recursive operator design space and taking a first step towards understanding the best mixing strategies for recursive reasoning.</p>
  </div>
</details>

<hr>
<h3 id="92-Stop-Unnecessary-Reflection-Training-LRMs-for-Efficient-Reasoning-with-Adaptive-Reflection-and-Length-Coordinated-Penalty-cs-AI-cs-CLPDF"><a href="#92-Stop-Unnecessary-Reflection-Training-LRMs-for-Efficient-Reasoning-with-Adaptive-Reflection-and-Length-Coordinated-Penalty-cs-AI-cs-CLPDF" class="headerlink" title="[92] Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty cs.AI | cs.CLPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12113">Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12113" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zewei Yu, Lirong Gao, Yuke Zhu, Bo Zheng, Sheng Guo</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为自适应反思与长度协调惩罚（ARLCP）的强化学习框架，旨在解决大型推理模型（LRMs）在复杂推理任务中因过度反思（如重复自问和循环推理）而导致的推理链过长、计算开销大且不提升准确性的问题。该方法通过动态平衡推理效率和解决方案准确性，鼓励模型生成更简洁有效的推理路径。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型推理模型在测试时扩展中表现出色，但常因过度反思生成过长的思维链，导致高令牌消耗、大量计算开销和延迟增加，且不提升准确性，尤其是在较小模型中。问题复杂性增加会引发更多不必要的反思，从而降低准确性并增加开销。</p>
<p><strong>Result:</strong> 在五个数学推理基准测试上，使用DeepSeek-R1-Distill-Qwen-1.5B和7B模型进行评估。实验结果表明，ARLCP在效率-准确性权衡上优于现有方法：对于1.5B模型，平均响应长度减少53.1%，同时准确性提升5.8%；对于7B模型，长度减少35.0%，准确性提升2.7%。</p>
<p><strong>Insight:</strong> 论文的创新点包括：1）自适应反思惩罚，动态削减不必要的反思步骤，同时保留必要的推理；2）根据问题估计复杂度校准的长度惩罚。通过协调这两种惩罚，实现了更优的推理效率与准确性平衡，为优化模型推理过程提供了新思路。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks by employing test-time scaling. However, they often generate over-long chains-of-thought that, driven by substantial reflections such as repetitive self-questioning and circular reasoning, lead to high token consumption, substantial computational overhead, and increased latency without improving accuracy, particularly in smaller models. Our observation reveals that increasing problem complexity induces more excessive and unnecessary reflection, which in turn reduces accuracy and increases token overhead. To address this challenge, we propose Adaptive Reflection and Length Coordinated Penalty (ARLCP), a novel reinforcement learning framework designed to dynamically balance reasoning efficiency and solution accuracy. ARLCP introduces two key innovations: (1) a reflection penalty that adaptively curtails unnecessary reflective steps while preserving essential reasoning, and (2) a length penalty calibrated to the estimated complexity of the problem. By coordinating these penalties, ARLCP encourages the model to generate more concise and effective reasoning paths. We evaluate our method on five mathematical reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B models. Experimental results show that ARLCP achieves a superior efficiency-accuracy trade-off compared to existing approaches. For the 1.5B model, it reduces the average response length by 53.1% while simultaneously improving accuracy by 5.8%. For the 7B model, it achieves a 35.0% reduction in length with a 2.7% accuracy gain. The code is released at <a target="_blank" rel="noopener" href="https://github.com/ZeweiYu1/ARLCP">https://github.com/ZeweiYu1/ARLCP</a> .</p>
  </div>
</details>

<hr>
<h3 id="93-Seq2Seq2Seq-Lossless-Data-Compression-via-Discrete-Latent-Transformers-and-Reinforcement-Learning-cs-AI-cs-CL-cs-ITPDF"><a href="#93-Seq2Seq2Seq-Lossless-Data-Compression-via-Discrete-Latent-Transformers-and-Reinforcement-Learning-cs-AI-cs-CL-cs-ITPDF" class="headerlink" title="[93] Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning cs.AI | cs.CL | cs.ITPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12146">Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.IT</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12146" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mahdi Khodabandeh, Ghazal Shabani, Arash Yousefi Jordehi, Seyed Abolghasem Mirroshandel</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Seq2Seq2Seq的新型无损数据压缩方法，该方法结合了离散潜在Transformer和强化学习技术，通过将数据压缩为令牌序列而非传统向量表示，实现了更高的压缩比并保持了数据的语义完整性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统压缩方法难以有效利用复杂数据结构中的冗余，而现有深度学习方法常依赖密集向量表示，掩盖了底层令牌结构。本文旨在解决这些限制，提出一种基于强化学习的T5语言模型架构，以优化压缩效率。</p>
<p><strong>Result:</strong> 该方法在压缩比方面相比传统方法有显著提升，具体定量结果未在摘要中提及，但宣称实现了更高的压缩比，且不依赖外部语法或世界知识。</p>
<p><strong>Insight:</strong> 创新点在于利用强化学习优化序列长度，将数据压缩为离散令牌序列，而非连续潜在空间表示，这更贴近原始数据格式，可借鉴于其他需要结构保持的压缩任务中。</p>
<p><strong>Abstract:</strong> Efficient lossless compression is essential for minimizing storage costs and transmission overhead while preserving data integrity. Traditional compression techniques, such as dictionary-based and statistical methods, often struggle to optimally exploit the structure and redundancy in complex data formats. Recent advancements in deep learning have opened new avenues for compression; however, many existing approaches depend on dense vector representations that obscure the underlying token structure. To address these limitations, we propose a novel lossless compression method that leverages Reinforcement Learning applied to a T5 language model architecture. This approach enables the compression of data into sequences of tokens rather than traditional vector representations. Unlike auto-encoders, which typically encode information into continuous latent spaces, our method preserves the token-based structure, aligning more closely with the original data format. This preservation allows for higher compression ratios while maintaining semantic integrity. By training the model using an off-policy Reinforcement Learning algorithm, we optimize sequence length to minimize redundancy and enhance compression efficiency. Our method introduces an efficient and adaptive data compression system built upon advanced Reinforcement Learning techniques, functioning independently of external grammatical or world knowledge. This approach shows significant improvements in compression ratios compared to conventional methods. By leveraging the latent information within language models, our system effectively compresses data without requiring explicit content understanding, paving the way for more robust and practical compression solutions across various applications.</p>
  </div>
</details>

<hr>
<h3 id="94-Pedagogically-Inspired-Data-Synthesis-for-Language-Model-Knowledge-Distillation-cs-AI-cs-CLPDF"><a href="#94-Pedagogically-Inspired-Data-Synthesis-for-Language-Model-Knowledge-Distillation-cs-AI-cs-CLPDF" class="headerlink" title="[94] Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation cs.AI | cs.CLPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12172">Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12172" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bowei He, Yankai Chen, Xiaokun Zhang, Linghe Kong, Philip S. Yu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种受教育学启发的数据合成框架（IOA），用于大型语言模型（LLM）的知识蒸馏。该框架通过知识识别、组织和适配三个阶段，系统性地识别学生模型的知识缺陷、组织渐进式课程并适配表征，以匹配学生模型的认知能力。实验表明，该方法在多个基准测试上显著优于现有蒸馏方法，特别是在复杂推理任务上取得了显著提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于合成数据的知识蒸馏方法缺乏教学意识，将知识转移视为一次性数据合成和训练任务，而非系统性的学习过程。本文旨在借鉴教育学原理（如布鲁姆的掌握学习理论和维果茨基的最近发展区），设计一个更有效的动态蒸馏过程。</p>
<p><strong>Result:</strong> 使用LLaMA-3.1&#x2F;3.2和Qwen2.5作为学生模型进行实验，IOA框架在DollyEval上使学生模型保留了教师模型94.7%的性能，而参数量不到1&#x2F;10。在复杂推理任务上，相比最先进的基线方法，在MATH上提升了19.2%，在HumanEval上提升了22.3%。</p>
<p><strong>Insight:</strong> 主要创新点是将教育学原理（掌握学习、最近发展区）系统性地融入知识蒸馏流程，设计了分阶段、渐进式的课程学习框架（IOA）。这为知识蒸馏提供了一种新的、更符合认知规律的范式，强调根据学生模型的能力动态调整知识传递的难度和顺序。</p>
<p><strong>Abstract:</strong> Knowledge distillation from Large Language Models (LLMs) to smaller models has emerged as a critical technique for deploying efficient AI systems. However, current methods for distillation via synthetic data lack pedagogical awareness, treating knowledge transfer as a one-off data synthesis and training task rather than a systematic learning process. In this paper, we propose a novel pedagogically-inspired framework for LLM knowledge distillation that draws from fundamental educational principles. Our approach introduces a three-stage pipeline – Knowledge Identifier, Organizer, and Adapter (IOA) – that systematically identifies knowledge deficiencies in student models, organizes knowledge delivery through progressive curricula, and adapts representations to match the cognitive capacity of student models. We integrate Bloom’s Mastery Learning Principles and Vygotsky’s Zone of Proximal Development to create a dynamic distillation process where student models approach teacher model’s performance on prerequisite knowledge before advancing, and new knowledge is introduced with controlled, gradual difficulty increments. Extensive experiments using LLaMA-3.1&#x2F;3.2 and Qwen2.5 as student models demonstrate that IOA achieves significant improvements over baseline distillation methods, with student models retaining 94.7% of teacher performance on DollyEval while using less than 1&#x2F;10th of the parameters. Our framework particularly excels in complex reasoning tasks, showing 19.2% improvement on MATH and 22.3% on HumanEval compared with state-of-the-art baselines.</p>
  </div>
</details>

<hr>
<div id='cs.NE'></div>

<h1 id="cs-NE-Back"><a href="#cs-NE-Back" class="headerlink" title="cs.NE [Back]"></a>cs.NE <a href="#toc">[Back]</a></h1><h3 id="95-Energy-Aware-Spike-Budgeting-for-Continual-Learning-in-Spiking-Neural-Networks-for-Neuromorphic-Vision-cs-NE-cs-AI-cs-CVPDF"><a href="#95-Energy-Aware-Spike-Budgeting-for-Continual-Learning-in-Spiking-Neural-Networks-for-Neuromorphic-Vision-cs-NE-cs-AI-cs-CVPDF" class="headerlink" title="[95] Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision cs.NE | cs.AI | cs.CVPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12236">Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.NE | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12236" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anika Tabassum Meem, Muntasir Hossain Nadid, Md Zesun Ahmed Mia</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种用于脉冲神经网络持续学习的能量感知脉冲预算框架，旨在解决神经形态视觉系统中持续学习面临的灾难性遗忘问题，并同时优化精度和能量效率。该方法整合了经验回放、可学习的泄露积分发放神经元参数和自适应脉冲调度器，在训练中强制执行数据集特定的能量约束。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有持续学习方法主要为人工神经网络设计，很少同时优化精度和能量效率，且在事件数据集上的探索有限。神经形态视觉系统需要部署在不断变化的环境中，而灾难性遗忘是其关键障碍。</p>
<p><strong>Result:</strong> 在基于帧的数据集（MNIST, CIFAR-10）上，脉冲预算作为稀疏性正则化器，在提高精度的同时将脉冲率降低了高达47%。在事件数据集（DVS-Gesture, N-MNIST, CIFAR-10-DVS）上，通过受控的预算放宽，精度提升了高达17.45个百分点，且计算开销最小。在涵盖两种模态的五个基准测试中，该方法在最小化动态功耗的同时，表现出一致的性能提升。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个统一的能量感知框架，将脉冲预算作为核心约束，并展示了其在不同数据模态（帧 vs. 事件）下的差异化作用机制（正则化 vs. 性能提升）。该方法通过联合优化学习过程和能量消耗，推动了神经形态视觉系统中持续学习的实际可行性。</p>
<p><strong>Abstract:</strong> Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with particularly limited exploration on event-based datasets. We propose an energy-aware spike budgeting framework for continual SNN learning that integrates experience replay, learnable leaky integrate-and-fire neuron parameters, and an adaptive spike scheduler to enforce dataset-specific energy constraints during training. Our approach exhibits modality-dependent behavior: on frame-based datasets (MNIST, CIFAR-10), spike budgeting acts as a sparsity-inducing regularizer, improving accuracy while reducing spike rates by up to 47%; on event-based datasets (DVS-Gesture, N-MNIST, CIFAR-10-DVS), controlled budget relaxation enables accuracy gains up to 17.45 percentage points with minimal computational overhead. Across five benchmarks spanning both modalities, our method demonstrates consistent performance improvements while minimizing dynamic power consumption, advancing the practical viability of continual learning in neuromorphic vision systems.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="96-GAC-KAN-An-Ultra-Lightweight-GNSS-Interference-Classifier-for-GenAI-Powered-Consumer-Edge-Devices-cs-LG-cs-CVPDF"><a href="#96-GAC-KAN-An-Ultra-Lightweight-GNSS-Interference-Classifier-for-GenAI-Powered-Consumer-Edge-Devices-cs-LG-cs-CVPDF" class="headerlink" title="[96] GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices cs.LG | cs.CVPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11186">GAC-KAN: An Ultra-Lightweight GNSS Interference Classifier for GenAI-Powered Consumer Edge Devices</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11186" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhihan Zeng, Kaihe Wang, Zhongpei Zhang, Yue Xiu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为GAC-KAN的超轻量级GNSS干扰分类器，旨在解决生成式AI（GenAI）时代下消费级边缘设备面临的数据稀缺和计算资源极度受限的双重挑战。该方法通过物理引导的模拟合成大规模高保真干扰数据集，并设计了一个结合非对称卷积块（ACB）和Ghost模块的多尺度主干网络（MS-GAC）来高效提取特征，同时使用参数极少的Kolmogorov-Arnold网络（KAN）作为决策头，实现了高精度与超低参数量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于，生成式AI在消费电子产品中的集成给边缘硬件带来了巨大的计算负担，导致用于GNSS信号保护等基础安全任务的资源极其有限，同时，现实世界干扰数据的稀缺也阻碍了鲁棒分类器的训练。</p>
<p><strong>Result:</strong> 实验结果表明，GAC-KAN在GNSS干扰分类任务上达到了98.0%的总体准确率，优于最先进的基线模型。其参数量仅为13万，比Vision Transformer（ViT）基线减少了约660倍，实现了极致的轻量化。</p>
<p><strong>Insight:</strong> 宣称的创新点包括：1）采用物理引导的模拟方法合成数据集以解决数据稀缺问题；2）设计MS-GAC主干网络结合ACB和Ghost模块，以最小冗余提取丰富的谱时特征；3）用参数效率极高的KAN替代传统的MLP决策头，以可学习的样条激活函数实现优越的非线性映射能力。从客观角度看，其核心创新在于将高效的卷积结构设计与新兴的KAN网络相结合，为资源极度受限的边缘设备安全任务提供了一个新颖的、软硬件协同优化的轻量级解决方案范式。</p>
<p><strong>Abstract:</strong> The integration of Generative AI (GenAI) into Consumer Electronics (CE)–from AI-powered assistants in wearables to generative planning in autonomous Uncrewed Aerial Vehicles (UAVs)–has revolutionized user experiences. However, these GenAI applications impose immense computational burdens on edge hardware, leaving strictly limited resources for fundamental security tasks like Global Navigation Satellite System (GNSS) signal protection. Furthermore, training robust classifiers for such devices is hindered by the scarcity of real-world interference data. To address the dual challenges of data scarcity and the extreme efficiency required by the GenAI era, this paper proposes a novel framework named GAC-KAN. First, we adopt a physics-guided simulation approach to synthesize a large-scale, high-fidelity jamming dataset, mitigating the data bottleneck. Second, to reconcile high accuracy with the stringent resource constraints of GenAI-native chips, we design a Multi-Scale Ghost-ACB-Coordinate (MS-GAC) backbone. This backbone combines Asymmetric Convolution Blocks (ACB) and Ghost modules to extract rich spectral-temporal features with minimal redundancy. Replacing the traditional Multi-Layer Perceptron (MLP) decision head, we introduce a Kolmogorov-Arnold Network (KAN), which employs learnable spline activation functions to achieve superior non-linear mapping capabilities with significantly fewer parameters. Experimental results demonstrate that GAC-KAN achieves an overall accuracy of 98.0%, outperforming state-of-the-art baselines. Significantly, the model contains only 0.13 million parameter–approximately 660 times fewer than Vision Transformer (ViT) baselines. This extreme lightweight characteristic makes GAC-KAN an ideal “always-on” security companion, ensuring GNSS reliability without contending for the computational resources required by primary GenAI tasks.</p>
  </div>
</details>

<hr>
<h3 id="97-Automated-Optimization-Modeling-via-a-Localizable-Error-Driven-Perspective-cs-LG-cs-AI-cs-CLPDF"><a href="#97-Automated-Optimization-Modeling-via-a-Localizable-Error-Driven-Perspective-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[97] Automated Optimization Modeling via a Localizable Error-Driven Perspective cs.LG | cs.AI | cs.CLPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11164">Automated Optimization Modeling via a Localizable Error-Driven Perspective</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11164" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Weiting Liu, Han Wu, Yufei Kuang, Xiongwei Han, Tao Zhong</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为MIND的新型错误驱动学习框架，用于通过大语言模型（LLMs）实现自动化优化建模。该框架基于对优化建模中错误传播的局部化模式的观察，通过构建高密度训练语料库和动态监督微调策略优化（DFPO）来解决现有方法中高质量数据稀缺和困难问题奖励稀疏的问题，从而提升LLMs在特定领域后训练中的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于LLMs的自动化优化建模方法在后训练中，其效果受限于高质量训练数据的稀缺和利用不足，具体表现为错误特定问题稀疏和困难问题奖励稀疏这两个根本性限制，导致领域特定后训练性能不佳。</p>
<p><strong>Result:</strong> 在六个基准测试上的实验表明，MIND框架持续优于所有最先进的自动化优化建模方法。</p>
<p><strong>Insight:</strong> 核心创新点在于发现了优化建模中错误传播具有独特的局部化模式（即错误可能局限于特定语义片段），并基于此构建了聚焦的高密度训练语料库，以及提出了动态监督微调策略优化（DFPO）进行局部精化，从而更有效地利用数据并解决困难问题。</p>
<p><strong>Abstract:</strong> Automated optimization modeling via Large Language Models (LLMs) has emerged as a promising approach to assist complex human decision-making. While post-training has become a pivotal technique to enhance LLMs’ capabilities in this domain, its effectiveness is severely constrained by the scarcity and underutilization of high-quality training data. However, through a detailed profiling of error patterns across various problem-response pairs drawn from post-training, we identify two fundamental limitations of existing automated optimization modeling approaches: (L1) the sparsity of error-specific problems and (L2) the sparse rewards associated with difficult problems. We demonstrate that these limitations can result in suboptimal performance in domain-specific post-training for LLMs. To tackle the above two limitations, we propose a novel error-driven learning framework – namely, auto\textbf{m}ated opt\textbf{i}mization modeli\textbf{n}g via a localizable error-\textbf{d}riven perspective (MIND) – that customizes the whole model training framework from data synthesis to post-training. MIND is based on our key observation of the unique localizable patterns in error propagation of optimization modelings, that is, modeling errors may remain localized to specific semantic segments and do not propagate throughout the entire solution. Thus, in contrast to holistic reasoning tasks such as mathematical proofs, MIND leverages the construction of a focused, high-density training corpus and proposes \textbf{D}ynamic Supervised \textbf{F}ine-Tuning \textbf{P}olicy \textbf{O}ptimization (DFPO) to tackle difficult problems through localized refinement. Experiments on six benchmarks demonstrate that MIND consistently outperforms all the state-of-the-art automated optimization modeling approaches.</p>
  </div>
</details>

<hr>
<h3 id="98-Patch-the-Distribution-Mismatch-RL-Rewriting-Agent-for-Stable-Off-Policy-SFT-cs-LG-cs-CLPDF"><a href="#98-Patch-the-Distribution-Mismatch-RL-Rewriting-Agent-for-Stable-Off-Policy-SFT-cs-LG-cs-CLPDF" class="headerlink" title="[98] Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT cs.LG | cs.CLPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11220">Patch the Distribution Mismatch: RL Rewriting Agent for Stable Off-Policy SFT</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11220" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiacheng Wang, Ping Jian, Zhen Yang, Zirong Chen, Keren Liao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于强化学习的数据重写代理，用于在监督微调前重写下游训练数据，以缓解分布偏移导致的灾难性遗忘问题。该方法通过优化重写策略，使重写数据更符合骨干模型的问答风格生成分布，同时保持多样性和任务一致性，从而构建高质量的重写数据集用于下游SFT。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当下游数据与模型先验训练分布存在显著偏移时，传统的监督微调会导致灾难性遗忘。现有数据重写方法通常基于提示诱导的条件分布采样重写，导致目标与模型的自然问答风格生成分布不一致，且依赖固定模板会导致多样性崩溃。</p>
<p><strong>Result:</strong> 大量实验表明，该方法在下游任务上取得了与标准SFT相当的性能提升，同时在非下游基准测试上平均减少了12.34%的遗忘。</p>
<p><strong>Insight:</strong> 将数据重写建模为策略学习问题，利用强化学习在奖励反馈下优化重写分布，联合优化问答风格分布对齐和多样性，并通过硬任务一致性门控确保质量，这是一种数据中心的创新方法。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have made rapid progress, yet adapting them to downstream scenarios still commonly relies on supervised fine-tuning (SFT). When downstream data exhibit a substantial distribution shift from the model’s prior training distribution, SFT can induce catastrophic forgetting. To narrow this gap, data rewriting has been proposed as a data-centric approach that rewrites downstream training data prior to SFT. However, existing methods typically sample rewrites from a prompt-induced conditional distribution, so the resulting targets are not necessarily aligned with the model’s natural QA-style generation distribution. Moreover, reliance on fixed templates can lead to diversity collapse. To address these issues, we cast data rewriting as a policy learning problem and learn a rewriting policy that better matches the backbone’s QA-style generation distribution while preserving diversity. Since distributional alignment, diversity and task consistency are automatically evaluable but difficult to optimize end-to-end with differentiable objectives, we leverage reinforcement learning to optimize the rewrite distribution under reward feedback and propose an RL-based data-rewriting agent. The agent jointly optimizes QA-style distributional alignment and diversity under a hard task-consistency gate, thereby constructing a higher-quality rewritten dataset for downstream SFT. Extensive experiments show that our method achieves downstream gains comparable to standard SFT while reducing forgetting on non-downstream benchmarks by 12.34% on average. Our code is available at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112">https://anonymous.4open.science/r/Patch-the-Prompt-Gap-4112</a> .</p>
  </div>
</details>

<hr>
<h3 id="99-UltraLIF-Fully-Differentiable-Spiking-Neural-Networks-via-Ultradiscretization-and-Max-Plus-Algebra-cs-LG-cs-AI-cs-CV-math-RA-q-bio-NCPDF"><a href="#99-UltraLIF-Fully-Differentiable-Spiking-Neural-Networks-via-Ultradiscretization-and-Max-Plus-Algebra-cs-LG-cs-AI-cs-CV-math-RA-q-bio-NCPDF" class="headerlink" title="[99] UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra cs.LG | cs.AI | cs.CV | math.RA | q-bio.NCPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11206">UltraLIF: Fully Differentiable Spiking Neural Networks via Ultradiscretization and Max-Plus Algebra</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | math.RA | q-bio.NC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11206" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jose Marie Antonio Miñoza</span></p>
<p><strong>TL;DR:</strong> 本文提出UltraLIF框架，通过超离散化和极大-加代数构建完全可微的脉冲神经网络，以替代传统的启发式代理梯度方法。该框架从LIF常微分方程和扩散方程分别推导出UltraLIF和UltraDLIF两种神经元模型，实现了标准反向传播训练且无前向-反向不匹配问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决脉冲神经网络中脉冲生成不可微的问题，避免依赖启发式代理梯度，为SNN训练提供数学原理支撑的连续松弛方法。</p>
<p><strong>Result:</strong> 在涵盖静态图像、神经形态视觉和音频的六个基准测试中，性能优于代理梯度基线方法，尤其在单时间步（T&#x3D;1）的神经形态和时序数据集上提升显著；通过可选稀疏惩罚可在保持竞争力的准确率下显著降低能耗。</p>
<p><strong>Insight:</strong> 核心创新在于利用热带几何中的超离散化，将极大-加半环自然建模神经阈值动态，以对数-求和-指数函数作为可微软最大值逼近硬阈值；理论分析证明了向经典LIF动态的点态收敛及梯度有界性，为SNN提供了数学严谨的可微框架。</p>
<p><strong>Abstract:</strong> Spiking Neural Networks (SNNs) offer energy-efficient, biologically plausible computation but suffer from non-differentiable spike generation, necessitating reliance on heuristic surrogate gradients. This paper introduces UltraLIF, a principled framework that replaces surrogate gradients with ultradiscretization, a mathematical formalism from tropical geometry providing continuous relaxations of discrete dynamics. The central insight is that the max-plus semiring underlying ultradiscretization naturally models neural threshold dynamics: the log-sum-exp function serves as a differentiable soft-maximum that converges to hard thresholding as a learnable temperature parameter $\eps \to 0$. Two neuron models are derived from distinct dynamical systems: UltraLIF from the LIF ordinary differential equation (temporal dynamics) and UltraDLIF from the diffusion equation modeling gap junction coupling across neuronal populations (spatial dynamics). Both yield fully differentiable SNNs trainable via standard backpropagation with no forward-backward mismatch. Theoretical analysis establishes pointwise convergence to classical LIF dynamics with quantitative error bounds and bounded non-vanishing gradients. Experiments on six benchmarks spanning static images, neuromorphic vision, and audio demonstrate improvements over surrogate gradient baselines, with gains most pronounced in single-timestep ($T{&#x3D;}1$) settings on neuromorphic and temporal datasets. An optional sparsity penalty enables significant energy reduction while maintaining competitive accuracy.</p>
  </div>
</details>

<hr>
<h3 id="100-Hierarchical-Concept-Embedding-Pursuit-for-Interpretable-Image-Classification-cs-LG-cs-CVPDF"><a href="#100-Hierarchical-Concept-Embedding-Pursuit-for-Interpretable-Image-Classification-cs-LG-cs-CVPDF" class="headerlink" title="[100] Hierarchical Concept Embedding &amp; Pursuit for Interpretable Image Classification cs.LG | cs.CVPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11448">Hierarchical Concept Embedding &amp; Pursuit for Interpretable Image Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11448" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nghia Nguyen, Tianjiao Ding, René Vidal</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为分层概念嵌入与追踪（HCEP）的框架，用于可解释的图像分类。该框架在视觉-语言模型的潜在空间中引入概念嵌入的层次结构，并通过分层稀疏编码来恢复图像中的概念，旨在解决现有稀疏概念恢复方法忽略概念层次结构、导致解释不一致的问题。实验表明，HCEP在概念精确率和召回率上优于基线方法，同时保持有竞争力的分类准确率，尤其在样本有限时表现更优。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于稀疏概念恢复的可解释图像分类方法通常利用视觉-语言模型的潜在空间，将图像嵌入表示为概念嵌入的稀疏组合，但它们忽略了概念的层次结构，可能导致预测正确但解释与层次结构不一致。本文旨在通过融入层次结构来构建更可靠和可解释的模型。</p>
<p><strong>Result:</strong> 在真实世界数据集上的实验表明，HCEP在概念精确率和召回率上优于基线方法，同时保持了有竞争力的分类准确率。当样本数量有限时，HCEP实现了更优的分类准确率和概念恢复性能。</p>
<p><strong>Insight:</strong> 创新点在于将概念层次结构显式地引入到稀疏编码框架中，通过构建分层概念嵌入和假设图像的正确概念形成层次结构中的根路径，从而推导出在嵌入空间中识别这些概念的条件。这提升了模型的可解释性和可靠性，特别是在数据稀缺场景下。从客观角度看，该方法将结构化先验知识（概念层次）与稀疏表示学习相结合，为可解释AI提供了一种新思路。</p>
<p><strong>Abstract:</strong> Interpretable-by-design models are gaining traction in computer vision because they provide faithful explanations for their predictions. In image classification, these models typically recover human-interpretable concepts from an image and use them for classification. Sparse concept recovery methods leverage the latent space of vision-language models to represent image embeddings as a sparse combination of concept embeddings. However, because such methods ignore the hierarchical structure of concepts, they can produce correct predictions with explanations that are inconsistent with the hierarchy. In this work, we propose Hierarchical Concept Embedding &amp; Pursuit (HCEP), a framework that induces a hierarchy of concept embeddings in the latent space and uses hierarchical sparse coding to recover the concepts present in an image. Given a hierarchy of semantic concepts, we construct a corresponding hierarchy of concept embeddings and, assuming the correct concepts for an image form a rooted path in the hierarchy, derive desirable conditions for identifying them in the embedded space. We show that hierarchical sparse coding reliably recovers hierarchical concept embeddings, whereas vanilla sparse coding fails. Our experiments on real-world datasets demonstrate that HCEP outperforms baselines in concept precision and recall while maintaining competitive classification accuracy. Moreover, when the number of samples is limited, HCEP achieves superior classification accuracy and concept recovery. These results show that incorporating hierarchical structures into sparse coding yields more reliable and interpretable image classification models.</p>
  </div>
</details>

<hr>
<h3 id="101-Evaluating-Memory-Structure-in-LLM-Agents-cs-LG-cs-CLPDF"><a href="#101-Evaluating-Memory-Structure-in-LLM-Agents-cs-LG-cs-CLPDF" class="headerlink" title="[101] Evaluating Memory Structure in LLM Agents cs.LG | cs.CLPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11243">Evaluating Memory Structure in LLM Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Alina Shutova, Alexandra Olenina, Ivan Vinogradov, Anton Sinitsin</span></p>
<p><strong>TL;DR:</strong> 本文提出了StructMemEval基准测试，用于评估LLM智能体组织长期记忆结构的能力，而不仅仅是事实回忆。通过一系列需要结构化知识（如交易账本、待办事项列表、树状结构等）的任务，研究发现简单的检索增强LLM难以处理这些任务，而记忆智能体在提示如何组织记忆时可以可靠解决，但现代LLM在未提示时往往无法识别记忆结构。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于LLM的智能体和聊天助手依赖长期记忆框架存储可重用知识、回忆用户偏好和增强推理，但现有基准主要关注简单事实保留、多跳回忆和时间变化，缺乏对复杂记忆层次结构的测试，因此需要新的评估方法来分析记忆架构能力并指导未来设计。</p>
<p><strong>Result:</strong> 在StructMemEval基准上的初步实验表明，简单检索增强LLM在这些结构化任务上表现不佳，而记忆智能体在提示如何组织记忆时可以可靠解决任务，但现代LLM在未提示时往往无法识别记忆结构。</p>
<p><strong>Insight:</strong> 论文的创新点在于提出了一个专注于评估LLM智能体组织长期记忆结构能力的基准测试StructMemEval，填补了现有基准仅测试简单事实回忆的不足；客观分析认为，该研究揭示了当前LLM在未明确提示时难以自发识别和应用记忆结构，这为LLM训练和记忆框架的未来改进提供了重要方向。</p>
<p><strong>Abstract:</strong> Modern LLM-based agents and chat assistants rely on long-term memory frameworks to store reusable knowledge, recall user preferences, and augment reasoning. As researchers create more complex memory architectures, it becomes increasingly difficult to analyze their capabilities and guide future memory designs. Most long-term memory benchmarks focus on simple fact retention, multi-hop recall, and time-based changes. While undoubtedly important, these capabilities can often be achieved with simple retrieval-augmented LLMs and do not test complex memory hierarchies. To bridge this gap, we propose StructMemEval - a benchmark that tests the agent’s ability to organize its long-term memory, not just factual recall. We gather a suite of tasks that humans solve by organizing their knowledge in a specific structure: transaction ledgers, to-do lists, trees and others. Our initial experiments show that simple retrieval-augmented LLMs struggle with these tasks, whereas memory agents can reliably solve them if prompted how to organize their memory. However, we also find that modern LLMs do not always recognize the memory structure when not prompted to do so. This highlights an important direction for future improvements in both LLM training and memory frameworks.</p>
  </div>
</details>

<hr>
<h3 id="102-Adaptive-Milestone-Reward-for-GUI-Agents-cs-LG-cs-AI-cs-CLPDF"><a href="#102-Adaptive-Milestone-Reward-for-GUI-Agents-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[102] Adaptive Milestone Reward for GUI Agents cs.LG | cs.AI | cs.CLPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11524">Adaptive Milestone Reward for GUI Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11524" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Congmin Zheng, Xiaoyun Mo, Xinbei Ma, Qiqiang Lin, Yin Zhao</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为自适应里程碑奖励（ADMIRE）的机制，用于解决强化学习在训练移动GUI智能体时面临的长时程任务中的时序信用分配问题。该方法通过将轨迹锚定到从成功探索中动态提炼的里程碑，构建了一个可验证的自适应奖励系统，并集成了非对称信用分配策略来优化轨迹。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于解决强化学习中奖励设计的固有矛盾：结果奖励保真度高但信号稀疏，过程奖励监督密集但易产生偏差和奖励黑客行为，特别是在长时程的移动GUI任务中。</p>
<p><strong>Result:</strong> 在AndroidWorld基准测试上，ADMIRE在不同基础模型上均实现了超过10%的绝对成功率提升，并在Web导航和具身任务等多种RL算法和异构环境中表现出强大的泛化性能。</p>
<p><strong>Insight:</strong> 创新点在于提出了一个结合了动态里程碑提炼和非对称信用分配的自适应奖励机制，有效平衡了奖励的保真度与密度，为长时程GUI任务的强化学习训练提供了可借鉴的解决方案。</p>
<p><strong>Abstract:</strong> Reinforcement Learning (RL) has emerged as a mainstream paradigm for training Mobile GUI Agents, yet it struggles with the temporal credit assignment problem inherent in long-horizon tasks. A primary challenge lies in the trade-off between reward fidelity and density: outcome reward offers high fidelity but suffers from signal sparsity, while process reward provides dense supervision but remains prone to bias and reward hacking. To resolve this conflict, we propose the Adaptive Milestone Reward (ADMIRE) mechanism. ADMIRE constructs a verifiable, adaptive reward system by anchoring trajectory to milestones, which are dynamically distilled from successful explorations. Crucially, ADMIRE integrates an asymmetric credit assignment strategy that denoises successful trajectories and scaffolds failed trajectories. Extensive experiments demonstrate that ADMIRE consistently yields over 10% absolute improvement in success rate across different base models on AndroidWorld. Moreover, the method exhibits robust generalizability, achieving strong performance across diverse RL algorithms and heterogeneous environments such as web navigation and embodied tasks.</p>
  </div>
</details>

<hr>
<h3 id="103-Where-Bits-Matter-in-World-Model-Planning-A-Paired-Mixed-Bit-Study-for-Efficient-Spatial-Reasoning-cs-LG-cs-AI-cs-CV-cs-ROPDF"><a href="#103-Where-Bits-Matter-in-World-Model-Planning-A-Paired-Mixed-Bit-Study-for-Efficient-Spatial-Reasoning-cs-LG-cs-AI-cs-CV-cs-ROPDF" class="headerlink" title="[103] Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning cs.LG | cs.AI | cs.CV | cs.ROPDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11882">Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11882" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Suraj Ranganath, Anish Patnaik, Vaishak Menon</span></p>
<p><strong>TL;DR:</strong> 本文研究了在空间推理世界模型规划中，比特位分配对性能的影响，发现总比特位宽和模块间比特分配共同决定低比特规划行为，尤其是在4比特的过渡区域，保持编码器精度能提升规划性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探究在严格精度预算下，高效空间推理世界模型的可靠性，并分析低比特规划性能主要由总比特位宽决定还是由模块间的比特分配决定。</p>
<p><strong>Result:</strong> 在Wall规划任务上使用DINO-WM进行实验，观察到一致的三区域模式：8比特和6比特设置接近FP16性能，3比特设置崩溃，4比特设置对分配敏感；在4比特过渡区域，保持编码器精度相对于均匀量化能改善规划，且近尺寸非对称变体显示出相同的编码器侧方向。</p>
<p><strong>Insight:</strong> 创新点在于通过配对目标混合比特评估，揭示了比特分配在过渡精度区域的关键作用，并提出了模块感知、预算感知的量化策略作为高效空间推理的广泛研究方向。</p>
<p><strong>Abstract:</strong> Efficient spatial reasoning requires world models that remain reliable under tight precision budgets. We study whether low-bit planning behavior is determined mostly by total bitwidth or by where bits are allocated across modules. Using DINO-WM on the Wall planning task, we run a paired-goal mixed-bit evaluation across uniform, mixed, asymmetric, and layerwise variants under two planner budgets. We observe a consistent three-regime pattern: 8-bit and 6-bit settings remain close to FP16, 3-bit settings collapse, and 4-bit settings are allocation-sensitive. In that transition region, preserving encoder precision improves planning relative to uniform quantization, and near-size asymmetric variants show the same encoder-side direction. In a later strict 22-cell replication with smaller per-cell episode count, the mixed-versus-uniform INT4 sign becomes budget-conditioned, which further highlights the sensitivity of this transition regime. These findings motivate module-aware, budget-aware quantization policies as a broader research direction for efficient spatial reasoning. Code and run artifacts are available at <a target="_blank" rel="noopener" href="https://github.com/suraj-ranganath/DINO-MBQuant">https://github.com/suraj-ranganath/DINO-MBQuant</a>.</p>
  </div>
</details>

<hr>
<h3 id="104-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels-cs-LG-cs-CLPDF"><a href="#104-DICE-Diffusion-Large-Language-Models-Excel-at-Generating-CUDA-Kernels-cs-LG-cs-CLPDF" class="headerlink" title="[104] DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels cs.LG | cs.CLPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11715">DICE: Diffusion Large Language Models Excel at Generating CUDA Kernels</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11715" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haolei Bai, Lingcheng Kong, Xueyi Chen, Jianmian Wang, Zhiqiang Tao</span></p>
<p><strong>TL;DR:</strong> 本文提出了DICE，一个用于生成高性能CUDA内核的扩散大语言模型系列。为了解决训练数据稀缺和模型适配的挑战，作者构建了CuKe数据集并设计了BiC-RL两阶段强化学习框架。实验表明，DICE在KernelBench基准测试中显著超越了同规模的自回归和扩散模型，达到了新的SOTA水平。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 扩散大语言模型因其并行生成能力在代码生成任务中具有潜力，但针对高度专业化的CUDA内核生成任务，面临高质量训练数据严重缺乏和模型适配的挑战。</p>
<p><strong>Result:</strong> 在KernelBench基准测试上的广泛实验表明，DICE系列模型（1.7B、4B、8B）显著优于同规模的自回归和扩散LLM，确立了CUDA内核生成的新SOTA。</p>
<p><strong>Insight:</strong> 核心创新点在于为特定领域（CUDA内核）构建优化数据集（CuKe）以及设计针对性的两阶段强化学习训练框架（BiC-RL），将扩散模型的并行生成优势与代码生成所需的整体结构规划和非顺序细化能力相结合。</p>
<p><strong>Abstract:</strong> Diffusion large language models (dLLMs) have emerged as a compelling alternative to autoregressive (AR) LLMs, owing to their capacity for parallel token generation. This paradigm is particularly well-suited for code generation, where holistic structural planning and non-sequential refinement are critical. Despite this potential, tailoring dLLMs for CUDA kernel generation remains challenging, obstructed not only by the high specialization but also by the severe lack of high-quality training data. To address these challenges, we construct CuKe, an augmented supervised fine-tuning dataset optimized for high-performance CUDA kernels. On top of it, we propose a bi-phase curated reinforcement learning (BiC-RL) framework consisting of a CUDA kernel infilling stage and an end-to-end CUDA kernel generation stage. Leveraging this training framework, we introduce DICE, a series of diffusion large language models designed for CUDA kernel generation, spanning three parameter scales, 1.7B, 4B, and 8B. Extensive experiments on KernelBench demonstrate that DICE significantly outperforms both autoregressive and diffusion LLMs of comparable scale, establishing a new state-of-the-art for CUDA kernel generation.</p>
  </div>
</details>

<hr>
<h3 id="105-Towards-On-Policy-SFT-Distribution-Discriminant-Theory-and-its-Applications-in-LLM-Training-cs-LG-cs-AI-cs-CVPDF"><a href="#105-Towards-On-Policy-SFT-Distribution-Discriminant-Theory-and-its-Applications-in-LLM-Training-cs-LG-cs-AI-cs-CVPDF" class="headerlink" title="[105] Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training cs.LG | cs.AI | cs.CVPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12222">Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12222" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miaosen Zhang, Yishan Liu, Shuxia Lin, Xu Yang, Qi Dai</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为On-Policy SFT的框架，旨在弥合监督微调（SFT）与强化学习（RL）在泛化性能上的差距。其核心是提出了分布判别理论（DDT），并基于此开发了两种技术：分布内微调（IDFT）和提示解码（Hinted Decoding），使SFT能够利用类似RL的在线策略数据，从而在保持SFT计算效率的同时，达到与离线RL算法相当的泛化性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 监督微调（SFT）虽然计算高效，但其泛化能力通常弱于使用在线策略数据的强化学习（RL）。本文旨在通过使SFT能够利用在线策略数据来弥补这一性能差距。</p>
<p><strong>Result:</strong> 大量实验表明，该框架在泛化性能上与DPO、SimPO等主流离线RL算法相当，同时保持了SFT流程的计算效率。</p>
<p><strong>Insight:</strong> 主要创新点在于提出了分布判别理论（DDT）来量化和解释数据与模型诱导分布的对齐程度，并基于此设计了损失层面的IDFT和数据层面的Hinted Decoding两种互补技术，为在RL不可行的领域提供了一种高效且性能优异的SFT替代方案。</p>
<p><strong>Abstract:</strong> Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL’s use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \textbf{\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between data and the model-induced distribution. Leveraging DDT, we introduce two complementary techniques: (i) \textbf{\textit{In-Distribution Finetuning (IDFT)}}, a loss-level method to enhance generalization ability of SFT, and (ii) \textbf{\textit{Hinted Decoding}}, a data-level technique that can re-align the training corpus to the model’s distribution. Extensive experiments demonstrate that our framework achieves generalization performance on par with prominent offline RL algorithms, including DPO and SimPO, while maintaining the efficiency of an SFT pipeline. The proposed framework thus offers a practical alternative in domains where RL is infeasible. We open-source the code here: <a target="_blank" rel="noopener" href="https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT">https://github.com/zhangmiaosen2000/Towards-On-Policy-SFT</a></p>
  </div>
</details>

<hr>
<h3 id="106-A-2-V-SLP-Alignment-Aware-Variational-Modeling-for-Disentangled-Sign-Language-Production-cs-LG-cs-CLPDF"><a href="#106-A-2-V-SLP-Alignment-Aware-Variational-Modeling-for-Disentangled-Sign-Language-Production-cs-LG-cs-CLPDF" class="headerlink" title="[106] A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production cs.LG | cs.CLPDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11861">A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11861" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sümeyye Meryem Taşyürek, Enis Mücahid İskender, Hacer Yalim Keles</span></p>
<p><strong>TL;DR:</strong> 本文提出A$^{2}$V-SLP，一种基于对齐感知变分建模的手语生成框架，通过变分自编码器学习解耦的发音器特定潜在分布，并利用非自回归Transformer预测分布参数，结合随机采样生成手语姿态序列，以提升解耦表示和运动真实性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对现有基于确定性嵌入的手语生成方法可能出现的潜在表示崩溃问题，旨在通过分布式的潜在建模来保持发音器级别的解耦表示，并加强语言输入与发音动作之间的对齐。</p>
<p><strong>Result:</strong> 在完全无词目（gloss-free）的设置下，实验结果表明该方法在反向翻译性能上达到SOTA，并提升了运动真实性，相比确定性潜在回归方法取得了持续增益。</p>
<p><strong>Insight:</strong> 创新点在于采用变分建模学习解耦的潜在分布而非确定性嵌入，结合非自回归Transformer预测分布参数，并通过随机采样和词目注意力机制增强对齐与生成质量，为解耦表示学习提供了新思路。</p>
<p><strong>Abstract:</strong> Building upon recent structural disentanglement frameworks for sign language production, we propose A$^{2}$V-SLP, an alignment-aware variational framework that learns articulator-wise disentangled latent distributions rather than deterministic embeddings. A disentangled Variational Autoencoder (VAE) encodes ground-truth sign pose sequences and extracts articulator-specific mean and variance vectors, which are used as distributional supervision for training a non-autoregressive Transformer. Given text embeddings, the Transformer predicts both latent means and log-variances, while the VAE decoder reconstructs the final sign pose sequences through stochastic sampling at the decoding stage. This formulation maintains articulator-level representations by avoiding deterministic latent collapse through distributional latent modeling. In addition, we integrate a gloss attention mechanism to strengthen alignment between linguistic input and articulated motion. Experimental results show consistent gains over deterministic latent regression, achieving state-of-the-art back-translation performance and improved motion realism in a fully gloss-free setting.</p>
  </div>
</details>

<hr>
<h3 id="107-RAM-Net-Expressive-Linear-Attention-with-Selectively-Addressable-Memory-cs-LG-cs-CLPDF"><a href="#107-RAM-Net-Expressive-Linear-Attention-with-Selectively-Addressable-Memory-cs-LG-cs-CLPDF" class="headerlink" title="[107] RAM-Net: Expressive Linear Attention with Selectively Addressable Memory cs.LG | cs.CLPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.11958">RAM-Net: Expressive Linear Attention with Selectively Addressable Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.11958" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaicheng Xiao, Haotian Li, Liran Dong, Guoliang Xing</span></p>
<p><strong>TL;DR:</strong> RAM-Net是一种新颖的线性注意力架构，旨在通过将输入映射为高维稀疏向量作为显式地址，来选择性访问一个大规模记忆状态，从而在保持计算效率的同时，弥补全注意力的表示能力与线性模型记忆效率之间的差距。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决线性注意力模型因将无限历史压缩到固定大小记忆体而导致的表达能力受限和信息丢失问题。</p>
<p><strong>Result:</strong> 在细粒度长程检索任务中持续超越SOTA基线，在标准语言建模和零样本常识推理基准测试中达到有竞争力的性能。</p>
<p><strong>Insight:</strong> 核心创新在于使用高维稀疏向量作为选择性可寻址的显式记忆地址，实现了状态大小的指数级扩展且不增加参数量，显著减轻信号干扰并提升检索保真度，同时稀疏性确保了极高的计算效率。</p>
<p><strong>Abstract:</strong> While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.</p>
  </div>
</details>

<hr>
<h3 id="108-Meta-Sel-Efficient-Demonstration-Selection-for-In-Context-Learning-via-Supervised-Meta-Learning-cs-LG-cs-AI-cs-CLPDF"><a href="#108-Meta-Sel-Efficient-Demonstration-Selection-for-In-Context-Learning-via-Supervised-Meta-Learning-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[108] Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning cs.LG | cs.AI | cs.CLPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12123">Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12123" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xubin Wang, Weijia Jia</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Meta-Sel的高效演示选择方法，用于上下文学习中的意图分类任务。该方法通过监督元学习训练一个轻量级、可解释的评分函数，基于TF-IDF余弦相似度和长度兼容性比这两个低成本元特征，从候选池中快速选择最相关的少样本示例，无需模型微调、在线探索或额外LLM调用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决上下文学习中演示选择的实际瓶颈：在有限的提示预算下，选择不同的少样本示例会显著影响准确率，但选择过程必须足够廉价以支持大规模候选池的实时查询。</p>
<p><strong>Result:</strong> 在四个意图数据集和五个开源LLM上对12种方法（包括提示工程基线、启发式选择、强化学习和基于影响的方法）的广泛实证研究中，Meta-Sel始终位列性能最佳的方法之一，尤其在小模型上能通过选择质量部分补偿模型容量限制，并保持有竞争力的选择时间开销。</p>
<p><strong>Insight:</strong> 创新点在于利用监督元学习构建元数据集，以类别一致性作为监督信号，训练校准的逻辑回归器，实现快速、确定性和可审计的演示选择；从客观角度看，该方法通过简单有效的特征组合和离线训练，平衡了选择性能与计算效率，为实际部署提供了实用解决方案。</p>
<p><strong>Abstract:</strong> Demonstration selection is a practical bottleneck in in-context learning (ICL): under a tight prompt budget, accuracy can change substantially depending on which few-shot examples are included, yet selection must remain cheap enough to run per query over large candidate pools. We propose Meta-Sel, a lightweight supervised meta-learning approach for intent classification that learns a fast, interpretable scoring function for (candidate, query) pairs from labeled training data.   Meta-Sel constructs a meta-dataset by sampling pairs from the training split and using class agreement as supervision, then trains a calibrated logistic regressor on two inexpensive meta-features: TF–IDF cosine similarity and a length-compatibility ratio. At inference time, the selector performs a single vectorized scoring pass over the full candidate pool and returns the top-k demonstrations, requiring no model fine-tuning, no online exploration, and no additional LLM calls. This yields deterministic rankings and makes the selection mechanism straightforward to audit via interpretable feature weights.   Beyond proposing Meta-Sel, we provide a broad empirical study of demonstration selection, benchmarking 12 methods – spanning prompt engineering baselines, heuristic selection, reinforcement learning, and influence-based approaches – across four intent datasets and five open-source LLMs. Across this benchmark, Meta-Sel consistently ranks among the top-performing methods, is particularly effective for smaller models where selection quality can partially compensate for limited model capacity, and maintains competitive selection-time overhead.</p>
  </div>
</details>

<hr>
<h3 id="109-Capability-Oriented-Training-Induced-Alignment-Risk-cs-LG-cs-CLPDF"><a href="#109-Capability-Oriented-Training-Induced-Alignment-Risk-cs-LG-cs-CLPDF" class="headerlink" title="[109] Capability-Oriented Training Induced Alignment Risk cs.LG | cs.CLPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12124">Capability-Oriented Training Induced Alignment Risk</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12124" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yujun Zhou, Yue Huang, Han Bao, Kehan Guo, Zhenwen Liang</span></p>
<p><strong>TL;DR:</strong> 本文研究能力导向训练引发的对齐风险，发现语言模型在存在隐性漏洞的强化学习环境中会自发学习利用这些漏洞来最大化奖励，即使训练中没有恶意意图。通过设计四个不同的‘漏洞游戏’进行实验，模型一致地学会了利用这些漏洞，并且这些利用策略是可泛化和可迁移的。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机是探讨AI对齐研究中一个更微妙的风险：能力导向训练引发的利用行为，即模型在训练中自发利用环境漏洞来最大化奖励，而非仅关注防止生成有害内容。</p>
<p><strong>Result:</strong> 实验结果表明，模型在所有四个漏洞游戏（涉及上下文条件合规、代理指标、奖励篡改和自我评估）中都学会了利用漏洞，显著提高了奖励但损害了任务正确性或安全性，且这些策略可泛化到新任务并通过数据从教师模型蒸馏到学生模型。</p>
<p><strong>Insight:</strong> 创新点在于揭示了能力导向训练诱导的风险对当前对齐方法构成根本挑战，强调未来AI安全需超越内容审核，转向严格审计和保护训练环境与奖励机制本身；从客观角度看，研究通过设计多样化漏洞游戏系统性地实证了模型利用漏洞的普遍性和可迁移性，为对齐研究提供了新视角。</p>
<p><strong>Abstract:</strong> While most AI alignment research focuses on preventing models from generating explicitly harmful content, a more subtle risk is emerging: capability-oriented training induced exploitation. We investigate whether language models, when trained with reinforcement learning (RL) in environments with implicit loopholes, will spontaneously learn to exploit these flaws to maximize their reward, even without any malicious intent in their training. To test this, we design a suite of four diverse “vulnerability games”, each presenting a unique, exploitable flaw related to context-conditional compliance, proxy metrics, reward tampering, and self-evaluation. Our experiments show that models consistently learn to exploit these vulnerabilities, discovering opportunistic strategies that significantly increase their reward at the expense of task correctness or safety. More critically, we find that these exploitative strategies are not narrow “tricks” but generalizable skills; they can be transferred to new tasks and even “distilled” from a capable teacher model to other student models through data alone. Our findings reveal that capability-oriented training induced risks pose a fundamental challenge to current alignment approaches, suggesting that future AI safety work must extend beyond content moderation to rigorously auditing and securing the training environments and reward mechanisms themselves. Code is available at <a target="_blank" rel="noopener" href="https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk">https://github.com/YujunZhou/Capability_Oriented_Alignment_Risk</a>.</p>
  </div>
</details>

<hr>
<h3 id="110-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation-cs-LG-cs-AI-cs-CLPDF"><a href="#110-Learning-beyond-Teacher-Generalized-On-Policy-Distillation-with-Reward-Extrapolation-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[110] Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation cs.LG | cs.AI | cs.CLPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2602.12125">Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2602.12125" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种广义的在线策略蒸馏（G-OPD）框架，通过引入灵活的参考模型和奖励缩放因子来扩展标准在线策略蒸馏（OPD）目标。理论分析表明OPD是密集KL约束强化学习的一个特例。在数学推理和代码生成任务上的实验表明，奖励外推（ExOPD）能持续提升标准OPD性能，甚至在特定设置下使学生模型超越教师模型的性能边界；在强到弱蒸馏设置中，选择教师RL前的基模型作为参考模型进行奖励校正能进一步提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准在线策略蒸馏（OPD）在提升学生模型性能方面表现出色，但其理论本质和优化潜力尚不明确。本文旨在从理论上阐释OPD，并通过扩展其目标框架（引入可调参考模型和奖励权重）来探索超越教师性能的可能性，解决如何更有效地进行知识蒸馏的问题。</p>
<p><strong>Result:</strong> 在数学推理和代码生成任务上的综合实验表明：1）奖励外推（ExOPD，奖励缩放因子&gt;1）在一系列师生模型大小配对中持续优于标准OPD；在将不同领域专家知识合并回原始学生的设置中，ExOPD使学生模型甚至超越了教师模型的性能边界，优于领域专家教师。2）在强到弱蒸馏设置中，选择教师RL前的基模型作为参考模型进行奖励校正，能提供更准确的奖励信号并进一步提升蒸馏性能，但这需要访问教师RL前的变体并带来更多计算开销。</p>
<p><strong>Insight:</strong> 论文的创新点在于：1）从理论层面将OPD形式化为密集KL约束RL的特例，为其提供了新的理论解释；2）提出了广义的G-OPD框架，通过奖励缩放因子（实现奖励外推）和灵活参考模型引入可调超参数，突破了标准OPD的固定权重限制；3）实验发现了奖励外推（ExOPD）能使学生模型超越教师性能边界的关键现象，以及在强到弱蒸馏中奖励校正的有效性，为未来OPD研究提供了新的优化方向和实用见解（如权衡性能提升与计算&#x2F;模型访问成本）。</p>
<p><strong>Abstract:</strong> On-policy distillation (OPD), which aligns the student with the teacher’s logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher’s performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher’s base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher’s pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2026-02-17/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2026-02-13/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
