<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-06-11 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-06-11
            
            
        </div>
        <span class="post-date">
            Jun 11, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CL">cs.CL</a> [Total: 36]</li>
<li><a href="#cs.CV">cs.CV</a> [Total: 66]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 1]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 2]</li>
</ul>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="1-Conservative-Bias-in-Large-Language-Models-Measuring-Relation-Predictions"><a href="#1-Conservative-Bias-in-Large-Language-Models-Measuring-Relation-Predictions" class="headerlink" title="[1] Conservative Bias in Large Language Models: Measuring Relation Predictions"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08120">Conservative Bias in Large Language Models: Measuring Relation Predictions</a></h3><p><em>Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文讨论了大型语言模型（LLMs）在关系提取任务中的保守偏见，倾向于选择No_Relation标签而非错误标签，但导致信息丢失。通过多组实验，研究发现保守偏见比幻觉问题更常见，并提出了Hobson’s choice概念来量化这种现象。</p>
<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在关系提取任务中表现出的保守偏见及其对信息提取的影响。

<p>Method: 通过多组提示、数据集和关系类型的系统评估，使用SBERT和LLM提示量化保守偏见的语义相似性。</p>
<p>Result: 保守偏见的发生频率是幻觉问题的两倍，保守行为在受限提示和开放提示中表现出语义相似性。</p>
<p>Conclusion: 保守偏见可能导致信息丢失，但其避免错误标签的优势仍需权衡。研究为LLMs在关系提取任务中的行为提供了新视角。</p>
<p>Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation<br>extraction tasks, frequently defaulting to No_Relation label when an<br>appropriate option is unavailable. While this behavior helps prevent incorrect<br>relation assignments, our analysis reveals that it also leads to significant<br>information loss when reasoning is not explicitly included in the output. We<br>systematically evaluate this trade-off across multiple prompts, datasets, and<br>relation types, introducing the concept of Hobson’s choice to capture scenarios<br>where models opt for safe but uninformative labels over hallucinated ones. Our<br>findings suggest that conservative bias occurs twice as often as hallucination.<br>To quantify this effect, we use SBERT and LLM prompts to capture the semantic<br>similarity between conservative bias behaviors in constrained prompts and<br>labels generated from semi-constrained and open-ended prompts.</p>
</details>


<h3 id="2-EconWebArena-Benchmarking-Autonomous-Agents-on-Economic-Tasks-in-Realistic-Web-Environments"><a href="#2-EconWebArena-Benchmarking-Autonomous-Agents-on-Economic-Tasks-in-Realistic-Web-Environments" class="headerlink" title="[2] EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08136">EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments</a></h3><p><em>Zefang Liu,Yinzhu Quan</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: EconWebArena是一个评估自主代理在真实网页环境中处理复杂经济任务的基准，包含360个任务，强调权威数据源和多模态推理。</p>
<details>
  <summary>Details</summary>
Motivation: 填补现有研究在权威数据源和真实网页环境中的经济推理任务上的空白。

<p>Method: 通过LLMs生成候选任务，并人工筛选以确保质量，评估多模态LLMs在任务中的表现。</p>
<p>Result: 发现现有模型在多模态理解和任务执行上存在显著性能差距。</p>
<p>Conclusion: EconWebArena为经济智能任务提供了严格的测试平台，揭示了未来改进方向。</p>
<p>Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on<br>complex, multimodal economic tasks in realistic web environments. The benchmark<br>comprises 360 curated tasks from 82 authoritative websites spanning domains<br>such as macroeconomics, labor, finance, trade, and public policy. Each task<br>challenges agents to navigate live websites, interpret structured and visual<br>content, interact with real interfaces, and extract precise, time-sensitive<br>data through multi-step workflows. We construct the benchmark by prompting<br>multiple large language models (LLMs) to generate candidate tasks, followed by<br>rigorous human curation to ensure clarity, feasibility, and source reliability.<br>Unlike prior work, EconWebArena emphasizes fidelity to authoritative data<br>sources and the need for grounded web-based economic reasoning. We evaluate a<br>diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure<br>cases, and conduct ablation studies to assess the impact of visual grounding,<br>plan-based reasoning, and interaction design. Our results reveal substantial<br>performance gaps and highlight persistent challenges in grounding, navigation,<br>and multimodal understanding, positioning EconWebArena as a rigorous testbed<br>for economic web intelligence.</p>
</details>


<h3 id="3-Compound-AI-Systems-Optimization-A-Survey-of-Methods-Challenges-and-Future-Directions"><a href="#3-Compound-AI-Systems-Optimization-A-Survey-of-Methods-Challenges-and-Future-Directions" class="headerlink" title="[3] Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08234">Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions</a></h3><p><em>Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 大规模语言模型（LLM）和AI系统的进步推动了复杂AI工作流设计的变革，复合AI系统能更好地执行复杂任务。本文系统回顾了优化这些系统的最新进展，并提出未来研究挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 随着复合AI系统的复杂性增加，传统优化方法（如SFT和RL）已不足以应对组件间交互的挑战，基于自然语言反馈的新方法显示出潜力。

<p>Method: 系统回顾并分类现有优化方法，包括数值和语言技术，形式化定义了复合AI系统优化的概念。</p>
<p>Result: 总结了当前优化技术的进展，提出分类维度，并指出了未解决的研究挑战。</p>
<p>Conclusion: 复合AI系统优化是一个快速发展的领域，未来需要进一步探索非可微分系统的优化方法。</p>
<p>Abstract: Recent advancements in large language models (LLMs) and AI systems have led<br>to a paradigm shift in the design and optimization of complex AI workflows. By<br>integrating multiple components, compound AI systems have become increasingly<br>adept at performing sophisticated tasks. However, as these systems grow in<br>complexity, new challenges arise in optimizing not only individual components<br>but also their interactions. While traditional optimization methods such as<br>supervised fine-tuning (SFT) and reinforcement learning (RL) remain<br>foundational, the rise of natural language feedback introduces promising new<br>approaches, especially for optimizing non-differentiable systems. This paper<br>provides a systematic review of recent progress in optimizing compound AI<br>systems, encompassing both numerical and language-based techniques. We<br>formalize the notion of compound AI system optimization, classify existing<br>methods along several key dimensions, and highlight open research challenges<br>and future directions in this rapidly evolving field. A list of surveyed papers<br>is publicly available at <a target="_blank" rel="noopener" href="https://github.com/MiuLab/AISysOpt-Survey">https://github.com/MiuLab/AISysOpt-Survey</a>.</p>
</details>


<h3 id="4-Can-AI-Validate-Science-Benchmarking-LLMs-for-Accurate-Scientific-Claim-rightarrow-Evidence-Reasoning"><a href="#4-Can-AI-Validate-Science-Benchmarking-LLMs-for-Accurate-Scientific-Claim-rightarrow-Evidence-Reasoning" class="headerlink" title="[4] Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08235">Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning</a></h3><p><em>Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: CLAIM-BENCH是一个评估大语言模型在科学论文中提取和验证主张与证据能力的基准，揭示了闭源模型优于开源模型的性能，并为提升模型科学理解能力提供了方法。</p>
<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在科学论文中处理复杂逻辑关系的能力，尤其是主张与证据的关联性。

<p>Method: 利用CLAIM-BENCH基准，系统比较了三种分治法启发的策略在六个大语言模型上的表现。</p>
<p>Result: 闭源模型（如GPT-4和Claude）在任务中表现更优，且设计的三遍和逐条提示策略显著提高了准确率，但计算成本增加。</p>
<p>Conclusion: CLAIM-BENCH为评估和提升大语言模型的科学理解能力提供了新标准，为未来系统开发奠定了基础。</p>
<p>Abstract: Large language models (LLMs) are increasingly being used for complex research<br>tasks such as literature review, idea generation, and scientific paper<br>analysis, yet their ability to truly understand and process the intricate<br>relationships within complex research papers, such as the logical links between<br>claims and supporting evidence remains largely unexplored. In this study, we<br>present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs’<br>capabilities in scientific claim-evidence extraction and validation, a task<br>that reflects deeper comprehension of scientific argumentation. We<br>systematically compare three approaches which are inspired by divide and<br>conquer approaches, across six diverse LLMs, highlighting model-specific<br>strengths and weaknesses in scientific comprehension. Through evaluation<br>involving over 300 claim-evidence pairs across multiple research domains, we<br>reveal significant limitations in LLMs’ ability to process complex scientific<br>content. Our results demonstrate that closed-source models like GPT-4 and<br>Claude consistently outperform open-source counterparts in precision and recall<br>across claim-evidence identification tasks. Furthermore, strategically designed<br>three-pass and one-by-one prompting approaches significantly improve LLMs’<br>abilities to accurately link dispersed evidence with claims, although this<br>comes at increased computational cost. CLAIM-BENCH sets a new standard for<br>evaluating scientific comprehension in LLMs, offering both a diagnostic tool<br>and a path forward for building systems capable of deeper, more reliable<br>reasoning across full-length papers.</p>
</details>


<h3 id="5-Automatic-Generation-of-Inference-Making-Questions-for-Reading-Comprehension-Assessments"><a href="#5-Automatic-Generation-of-Inference-Making-Questions-for-Reading-Comprehension-Assessments" class="headerlink" title="[5] Automatic Generation of Inference Making Questions for Reading Comprehension Assessments"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08260">Automatic Generation of Inference Making Questions for Reading Comprehension Assessments</a></h3><p><em>Wanjing Anya Ma,Michael Flor,Zuowei Wang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文研究了阅读理解的推理技能，提出一种推理类型分类法，并利用GPT-4o生成诊断性阅读理解题目，验证了自动生成与人工审核相结合的有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 提升阅读理解能力需要有效的诊断性评估工具，而推理技能是理解文本的关键。

<p>Method: 提出推理类型分类法；使用GPT-4o通过少量示例生成题目，对比有无链式思考提示的效果。</p>
<p>Result: GPT-4o生成的问题93.8%质量良好，但仅42.6%准确匹配目标推理类型。</p>
<p>Conclusion: 自动生成与人工审核结合，可扩展高质量诊断性阅读理解评估。</p>
<p>Abstract: Inference making is an essential but complex skill in reading comprehension<br>(RC). Some inferences require resolving references across sentences, and some<br>rely on using prior knowledge to fill in the detail that is not explicitly<br>written in the text. Diagnostic RC questions can help educators provide more<br>effective and targeted reading instruction and interventions for school-age<br>students. We introduce a taxonomy of inference types for RC and use it to<br>analyze the distribution of items within a diagnostic RC item bank. Next, we<br>present experiments using GPT-4o to generate bridging-inference RC items for<br>given reading passages via few-shot prompting, comparing conditions with and<br>without chain-of-thought prompts. Generated items were evaluated on three<br>aspects: overall item quality, appropriate inference type, and LLM reasoning,<br>achieving high inter-rater agreements above 0.90. Our results show that GPT-4o<br>produced 93.8% good-quality questions suitable for operational use in grade<br>3-12 contexts; however, only 42.6% of the generated questions accurately<br>matched the targeted inference type. We conclude that combining automatic item<br>generation with human judgment offers a promising path toward scalable,<br>high-quality diagnostic RC assessments.</p>
</details>


<h3 id="6-Wait-We-Don’t-Need-to-“Wait”-Removing-Thinking-Tokens-Improves-Reasoning-Efficiency"><a href="#6-Wait-We-Don’t-Need-to-“Wait”-Removing-Thinking-Tokens-Improves-Reasoning-Efficiency" class="headerlink" title="[6] Wait, We Don’t Need to “Wait”! Removing Thinking Tokens Improves Reasoning Efficiency"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08343">Wait, We Don’t Need to “Wait”! Removing Thinking Tokens Improves Reasoning Efficiency</a></h3><p><em>Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: NoWait 方法通过禁用显式自我反思标记（如“Wait”和“Hmm”）来减少推理过程中的冗余输出，提高效率，且不影响模型性能。</p>
<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理中常因过度思考而产生冗余输出，影响了效率。本文研究了显式自我反思标记的必要性。

<p>Method: 提出了NoWait方法，通过抑制自我反思标记（如“Wait”和“Hmm”）来实现高效推理。</p>
<p>Result: 在多个基准测试中，NoWait 将推理轨迹长度减少了27%-51%，且模型性能未受影响。</p>
<p>Conclusion: NoWait 是一种即插即用的解决方案，适用于高效且不影响性能的多模态推理。</p>
<p>Abstract: Recent advances in large reasoning models have enabled complex, step-by-step<br>reasoning but often introduce significant overthinking, resulting in verbose<br>and redundant outputs that hinder efficiency. In this study, we examine whether<br>explicit self-reflection, signaled by tokens such as “Wait” and “Hmm”, is<br>necessary for advanced reasoning. We propose NoWait, a simple yet effective<br>approach that disables explicit self-reflection by suppressing these tokens<br>during inference. Extensive experiments on ten benchmarks across textual,<br>visual, and video reasoning tasks show that NoWait reduces chain-of-thought<br>trajectory length by up to 27%-51% in five R1-style model series, without<br>compromising model utility. NoWait thus offers a plug-and-play solution for<br>efficient and utility-preserving multimodal reasoning.</p>
</details>


<h3 id="7-Text-Embeddings-Should-Capture-Implicit-Semantics-Not-Just-Surface-Meaning"><a href="#7-Text-Embeddings-Should-Capture-Implicit-Semantics-Not-Just-Surface-Meaning" class="headerlink" title="[7] Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08354">Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning</a></h3><p><em>Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该立场论文主张文本嵌入研究应超越表面意义，以隐含语义为核心建模目标，并提出数据多样化、评估基准改进等建议。</p>
<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型主要关注表面语义，忽视了隐含语义（如语用、说话者意图和社会文化背景），导致其在需要深层理解的任务中表现不佳。

<p>Method: 通过试点研究展示现有模型在隐含语义任务上的局限性，并提出研究范式的转变，包括改进训练数据和评估基准。</p>
<p>Result: 试点研究表明，即使是先进模型在隐含语义任务上表现仅略优于简单基线，凸显研究差距。</p>
<p>Conclusion: 呼吁将隐含语义作为核心建模目标，并提出数据多样化、基准改进等建议，以更贴近现实语言的复杂性。</p>
<p>Abstract: This position paper argues that the text embedding research community should<br>move beyond surface meaning and embrace implicit semantics as a central<br>modeling goal. Text embedding models have become foundational in modern NLP,<br>powering a wide range of applications and drawing increasing research<br>attention. Yet, much of this progress remains narrowly focused on surface-level<br>semantics. In contrast, linguistic theory emphasizes that meaning is often<br>implicit, shaped by pragmatics, speaker intent, and sociocultural context.<br>Current embedding models are typically trained on data that lacks such depth<br>and evaluated on benchmarks that reward the capture of surface meaning. As a<br>result, they struggle with tasks requiring interpretive reasoning, speaker<br>stance, or social meaning. Our pilot study highlights this gap, showing that<br>even state-of-the-art models perform only marginally better than simplistic<br>baselines on implicit semantics tasks. To address this, we call for a paradigm<br>shift: embedding research should prioritize more diverse and linguistically<br>grounded training data, design benchmarks that evaluate deeper semantic<br>understanding, and explicitly frame implicit meaning as a core modeling<br>objective, better aligning embeddings with real-world language complexity.</p>
</details>


<h3 id="8-CC-RAG-Structured-Multi-Hop-Reasoning-via-Theme-Based-Causal-Graphs"><a href="#8-CC-RAG-Structured-Multi-Hop-Reasoning-via-Theme-Based-Causal-Graphs" class="headerlink" title="[8] CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08364">CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs</a></h3><p><em>Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: CC-RAG通过结合零样本三元组提取和主题感知图链，改进RAG管道以实现结构化多跳推理，显著提升LLMs在专业领域因果推理的准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 研究针对LLMs在专业领域因果推理中的不足，提出改进RAG管道以更好地建模因果依赖关系。

<p>Method: 开发CC-RAG方法，结合零样本三元组提取和主题感知图链生成DAG，通过前后链进行结构化推理。</p>
<p>Result: 在比特币价格波动和高雪病两个真实领域实验中，CC-RAG在链相似性、信息密度和词汇多样性上优于标准RAG和零样本LLMs。</p>
<p>Conclusion: 明确建模因果结构能显著提升LLMs在专业领域生成答案的准确性和可解释性。</p>
<p>Abstract: Understanding cause and effect relationships remains a formidable challenge<br>for Large Language Models (LLMs), particularly in specialized domains where<br>reasoning requires more than surface-level correlations. Retrieval-Augmented<br>Generation (RAG) improves factual accuracy, but standard RAG pipelines treat<br>evidence as flat context, lacking the structure required to model true causal<br>dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that<br>integrates zero-shot triple extraction and theme-aware graph chaining into the<br>RAG pipeline, enabling structured multi-hop inference. Given a domain specific<br>corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of &lt;cause, relation,<br>effect&gt; triples and uses forward&#x2F;backward chaining to guide structured answer<br>generation. Experiments on two real-world domains: Bitcoin price fluctuations<br>and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot<br>LLMs in chain similarity, information density, and lexical diversity. Both<br>LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results<br>demonstrate that explicitly modeling causal structure enables LLMs to generate<br>more accurate and interpretable responses, especially in specialized domains<br>where flat retrieval fails.</p>
</details>


<h3 id="9-mSTEB-Massively-Multilingual-Evaluation-of-LLMs-on-Speech-and-Text-Tasks"><a href="#9-mSTEB-Massively-Multilingual-Evaluation-of-LLMs-on-Speech-and-Text-Tasks" class="headerlink" title="[9] mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08400">mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks</a></h3><p><em>Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文介绍了mSTEB基准，用于评估LLMs在多语言和多模态任务中的性能，揭示了高资源和低资源语言性能差距。</p>
<details>
  <summary>Details</summary>
Motivation: 现有LLMs评测主要集中在英语和高资源语言，缺乏对低资源语言的标准化评估。

<p>Method: 引入mSTEB基准，涵盖语音和文本模态的多种任务（如语言识别、翻译等），并评估多个主流和开源LLM。</p>
<p>Result: 评估显示高资源与低资源语言（尤其是非洲和美洲&#x2F;大洋洲语言）性能差距显著。</p>
<p>Conclusion: 需增加投资以解决LLMs在低资源语言中的覆盖率不足问题。</p>
<p>Abstract: Large Language models (LLMs) have demonstrated impressive performance on a<br>wide range of tasks, including in multimodal settings such as speech. However,<br>their evaluation is often limited to English and a few high-resource languages.<br>For low-resource languages, there is no standardized evaluation benchmark. In<br>this paper, we address this gap by introducing mSTEB, a new benchmark to<br>evaluate the performance of LLMs on a wide range of tasks covering language<br>identification, text classification, question answering, and translation tasks<br>on both speech and text modalities. We evaluated the performance of leading<br>LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open<br>models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in<br>performance between high-resource and low-resource languages, especially for<br>languages spoken in Africa and Americas&#x2F;Oceania. Our findings show that more<br>investment is needed to address their under-representation in LLMs coverage.</p>
</details>


<h3 id="10-TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration"><a href="#10-TACTIC-Translation-Agents-with-Cognitive-Theoretic-Interactive-Collaboration" class="headerlink" title="[10] TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08403">TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration</a></h3><p><em>Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: TACTIC是一个基于认知理论的多智能体翻译框架，通过模拟人类翻译的认知策略提升了翻译质量，实验表明其在多个基准测试中优于现有模型。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的多智能体翻译框架忽视了认知翻译研究的关键见解，未能完全发挥大语言模型的翻译潜力。

<p>Method: 提出TACTIC框架，包含六个功能不同的智能体，模拟人类翻译的认知过程，如草拟、精炼、评估等，实现高质量的翻译任务分解与协作。</p>
<p>Result: 在FLORES-200和WMT24基准测试中，TACTIC的平均表现超过了GPT-4.1和DeepSeek-R1，如XCOMET和COMETKIWI-23分数显著提升。</p>
<p>Conclusion: TACTIC通过认知理论驱动的智能体协作，显著提升了机器翻译质量，成为当前最先进的解决方案。</p>
<p>Abstract: Machine translation has long been a central task in natural language<br>processing. With the rapid advancement of large language models (LLMs), there<br>has been remarkable progress in translation quality. However, fully realizing<br>the translation potential of LLMs remains an open challenge. Recent studies<br>have explored multi-agent systems to decompose complex translation tasks into<br>collaborative subtasks, showing initial promise in enhancing translation<br>quality through agent cooperation and specialization. Nevertheless, existing<br>multi-agent translation frameworks largely neglect foundational insights from<br>cognitive translation studies. These insights emphasize how human translators<br>employ different cognitive strategies, such as balancing literal and free<br>translation, refining expressions based on context, and iteratively evaluating<br>outputs. To address this limitation, we propose a cognitively informed<br>multi-agent framework called TACTIC, which stands for T ranslation A gents with<br>Cognitive- T heoretic Interactive Collaboration. The framework comprises six<br>functionally distinct agents that mirror key cognitive processes observed in<br>human translation behavior. These include agents for drafting, refinement,<br>evaluation, scoring, context reasoning, and external knowledge gathering. By<br>simulating an interactive and theory-grounded translation workflow, TACTIC<br>effectively leverages the full capacity of LLMs for high-quality translation.<br>Experimental results on diverse language pairs from the FLORES-200 and WMT24<br>benchmarks show that our method consistently achieves state-of-the-art<br>performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by<br>an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it<br>further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at<br><a target="_blank" rel="noopener" href="https://github.com/weiyali126/TACTIC">https://github.com/weiyali126/TACTIC</a>.</p>
</details>


<h3 id="11-Large-Language-Models-Have-Intrinsic-Meta-Cognition-but-Need-a-Good-Lens"><a href="#11-Large-Language-Models-Have-Intrinsic-Meta-Cognition-but-Need-a-Good-Lens" class="headerlink" title="[11] Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08410">Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens</a></h3><p><em>Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 该论文提出AutoMeco框架和MIRA策略，用于评估和改进大语言模型（LLMs）的元认知能力，结果显示其有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs的认知错误检测能力，但忽略了其元认知能力（如对步骤错误的自我意识），这对模型可靠性至关重要。

<p>Method: 提出AutoMeco框架评估现有元认知指标，并设计训练免费的MIRA策略优化这些指标。</p>
<p>Result: 在三个数学推理数据集和三种LLMs上的实验表明，AutoMeco的合理性优于Best-of-N验证，且MIRA能更准确评估LLMs的元认知能力。</p>
<p>Conclusion: AutoMeco和MIRA为评估和改进LLMs的元认知能力提供了有效工具，有助于提升模型的可靠性。</p>
<p>Abstract: Previous research has primarily focused on the cognitive error detection<br>capabilities of Large Language Models (LLMs), often prompting them to analyze<br>mistakes in reasoning chains. However, few studies have examined the<br>meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),<br>which are crucial for their reliability. While studies on LLM self-evaluation<br>present some measures, such as perplexity, which can reflect the answer<br>correctness and be viewed as the lens of meta-cognition, they lack step-level<br>analysis and adaptation. This paper studies the evaluation of LLM<br>meta-cognition using the current lenses and how to improve these lenses.<br>Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation<br>framework for benchmarking the existing lenses. Furthermore, a training-free<br>Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost<br>current meta-cognition lenses. Experimental results on three mathematical<br>reasoning datasets and three LLMs show the reasonableness of AutoMeco by<br>comparing it with Best-of-N verification. Moreover, the meta-cognition ability<br>of LLMs can be better evaluated using MIRA.</p>
</details>


<h3 id="12-Know-MRI-A-Knowledge-Mechanisms-Revealer-Interpreter-for-Large-Language-Models"><a href="#12-Know-MRI-A-Knowledge-Mechanisms-Revealer-Interpreter-for-Large-Language-Models" class="headerlink" title="[12] Know-MRI: A Knowledge Mechanisms Revealer&amp;Interpreter for Large Language Models"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08427">Know-MRI: A Knowledge Mechanisms Revealer&amp;Interpreter for Large Language Models</a></h3><p><em>Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文介绍了Know-MRI，一个开源工具，旨在系统性分析大型语言模型的知识机制，解决现有方法输入和输出不一致的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的解释方法输入格式和输出不一致，限制了实际应用，需要一种系统性解决方案。

<p>Method: 开发了Know-MRI，包含可扩展的核心模块，能自动匹配输入数据与解释方法，并整合输出。</p>
<p>Result: 工具允许用户自由选择解释方法，基于输入数据全面诊断模型的知识机制。</p>
<p>Conclusion: Know-MRI为分析LLM知识机制提供了灵活且综合的解决方案，推动了模型可解释性研究。</p>
<p>Abstract: As large language models (LLMs) continue to advance, there is a growing<br>urgency to enhance the interpretability of their internal knowledge mechanisms.<br>Consequently, many interpretation methods have emerged, aiming to unravel the<br>knowledge mechanisms of LLMs from various perspectives. However, current<br>interpretation methods differ in input data formats and interpreting outputs.<br>The tools integrating these methods are only capable of supporting tasks with<br>specific inputs, significantly constraining their practical applications. To<br>address these challenges, we present an open-source Knowledge Mechanisms<br>Revealer&amp;Interpreter (Know-MRI) designed to analyze the knowledge mechanisms<br>within LLMs systematically. Specifically, we have developed an extensible core<br>module that can automatically match different input data with interpretation<br>methods and consolidate the interpreting outputs. It enables users to freely<br>choose appropriate interpretation methods based on the inputs, making it easier<br>to comprehensively diagnose the model’s internal knowledge mechanisms from<br>multiple perspectives. Our code is available at<br><a target="_blank" rel="noopener" href="https://github.com/nlpkeg/Know-MRI">https://github.com/nlpkeg/Know-MRI</a>. We also provide a demonstration video on<br><a target="_blank" rel="noopener" href="https://youtu.be/NVWZABJ43Bs">https://youtu.be/NVWZABJ43Bs</a>.</p>
</details>


<h3 id="13-CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models"><a href="#13-CAF-I-A-Collaborative-Multi-Agent-Framework-for-Enhanced-Irony-Detection-with-Large-Language-Models" class="headerlink" title="[13] CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08430">CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models</a></h3><p><em>Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文提出了一种名为CAF-I的多代理框架，用于解决现有大语言模型在反讽检测中的局限性，包括单视角限制、理解不足和缺乏可解释性。CAF-I通过多维分析和协作优化，显著提升了检测性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的反讽检测方法存在单视角限制、理解不足和缺乏可解释性等问题。本文旨在通过多代理协作框架CAF-I解决这些挑战。

<p>Method: CAF-I是一个多代理系统，包括上下文、语义和修辞代理进行多维分析，并通过交互协作优化。决策代理整合这些视角，优化反馈则由一个评估代理提供。</p>
<p>Result: 在基准数据集上的实验表明，CAF-I在零样本性能上达到了SOTA，平均Macro-F1为76.31，比之前最强的基线提高了4.98个百分点。</p>
<p>Conclusion: CAF-I通过模拟人类多视角分析，显著提升了反讽检测的准确性和可解释性，验证了其在解决现有问题中的有效性。</p>
<p>Abstract: Large language model (LLM) have become mainstream methods in the field of<br>sarcasm detection. However, existing LLM methods face challenges in irony<br>detection, including: 1. single-perspective limitations, 2. insufficient<br>comprehensive understanding, and 3. lack of interpretability. This paper<br>introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven<br>multi-agent system designed to overcome these issues. CAF-I employs specialized<br>agents for Context, Semantics, and Rhetoric, which perform multidimensional<br>analysis and engage in interactive collaborative optimization. A Decision Agent<br>then consolidates these perspectives, with a Refinement Evaluator Agent<br>providing conditional feedback for optimization. Experiments on benchmark<br>datasets establish CAF-I’s state-of-the-art zero-shot performance. Achieving<br>SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of<br>76.31, a 4.98 absolute improvement over the strongest prior baseline. This<br>success is attained by its effective simulation of human-like multi-perspective<br>analysis, enhancing detection accuracy and interpretability.</p>
</details>


<h3 id="14-Detecting-Harmful-Memes-with-Decoupled-Understanding-and-Guided-CoT-Reasoning"><a href="#14-Detecting-Harmful-Memes-with-Decoupled-Understanding-and-Guided-CoT-Reasoning" class="headerlink" title="[14] Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08477">Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning</a></h3><p><em>Fengjun Pan,Anh Tuan Luu,Xiaobao Wu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: U-CoT+ 是一种新型框架，通过将视觉模因转换为文本描述，并结合人工指导，实现了高效、灵活且可解释的有害模因检测。</p>
<details>
  <summary>Details</summary>
Motivation: 当前有害模因检测方法在资源效率、灵活性和可解释性方面存在不足，难以实际部署于内容审核系统。

<p>Method: 开发了 meme-to-text 管道，将模因转换为文本描述，并结合人工指导，使用零样本 CoT 提示引导模型推理。</p>
<p>Result: 在七个基准数据集上的实验验证了框架的有效性，展示了其在低资源和可解释性方面的潜力。</p>
<p>Conclusion: U-CoT+ 能够高效、灵活地检测有害模因，适合跨平台和跨区域的动态需求，为内容审核提供了新思路。</p>
<p>Abstract: Detecting harmful memes is essential for maintaining the integrity of online<br>environments. However, current approaches often struggle with resource<br>efficiency, flexibility, or explainability, limiting their practical deployment<br>in content moderation systems. To address these challenges, we introduce<br>U-CoT+, a novel framework for harmful meme detection. Instead of relying solely<br>on prompting or fine-tuning multimodal models, we first develop a high-fidelity<br>meme-to-text pipeline that converts visual memes into detail-preserving textual<br>descriptions. This design decouples meme interpretation from meme<br>classification, thus avoiding immediate reasoning over complex raw visual<br>content and enabling resource-efficient harmful meme detection with general<br>large language models (LLMs). Building on these textual descriptions, we<br>further incorporate targeted, interpretable human-crafted guidelines to guide<br>models’ reasoning under zero-shot CoT prompting. As such, this framework allows<br>for easy adaptation to different harmfulness detection criteria across<br>platforms, regions, and over time, offering high flexibility and<br>explainability. Extensive experiments on seven benchmark datasets validate the<br>effectiveness of our framework, highlighting its potential for explainable and<br>low-resource harmful meme detection using small-scale LLMs. Codes and data are<br>available at: <a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/HMC-AF2B/README.md">https://anonymous.4open.science/r/HMC-AF2B/README.md</a>.</p>
</details>


<h3 id="15-CoMuMDR-Code-mixed-Multi-modal-Multi-domain-corpus-for-Discourse-paRsing-in-conversations"><a href="#15-CoMuMDR-Code-mixed-Multi-modal-Multi-domain-corpus-for-Discourse-paRsing-in-conversations" class="headerlink" title="[15] CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08504">CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations</a></h3><p><em>Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: CoMuMDR是一个跨领域、多模态（音频和文本）的编码混合（印地语和英语）语料库，用于对话中的篇章解析，突出了当前模型在该领域的挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的篇章解析数据集通常限于单一领域的书面英语对话，不足以支持多领域、编码混合的现实场景。

<p>Method: 介绍了CoMuMDR语料库，包含印地语和英语的编码混合数据，并进行了九种篇章关系标注，测试了多种先进基线模型。</p>
<p>Result: 先进基线模型在CoMuMDR上的表现不佳，表明多领域、编码混合数据的复杂性对现有模型构成挑战。</p>
<p>Conclusion: 需要开发更好的模型以处理多领域、编码混合的篇章解析任务。</p>
<p>Abstract: Discourse parsing is an important task useful for NLU applications such as<br>summarization, machine comprehension, and emotion recognition. The current<br>discourse parsing datasets based on conversations consists of written English<br>dialogues restricted to a single domain. In this resource paper, we introduce<br>CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in<br>conversations. The corpus (code-mixed in Hindi and English) has both audio and<br>transcribed text and is annotated with nine discourse relations. We experiment<br>with various SoTA baseline models; the poor performance of SoTA models<br>highlights the challenges of multi-domain code-mixed corpus, pointing towards<br>the need for developing better models for such realistic settings.</p>
</details>


<h3 id="16-Efficient-Post-Training-Refinement-of-Latent-Reasoning-in-Large-Language-Models"><a href="#16-Efficient-Post-Training-Refinement-of-Latent-Reasoning-in-Large-Language-Models" class="headerlink" title="[16] Efficient Post-Training Refinement of Latent Reasoning in Large Language Models"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08552">Efficient Post-Training Refinement of Latent Reasoning in Large Language Models</a></h3><p><em>Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种轻量级的后训练框架，通过对比推理反馈和残差嵌入细化来优化潜在推理轨迹，显著提升了推理任务的准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中推理过程的局限性，特别是后训练阶段如何有效更新推理嵌入以提高准确性。

<p>Method: 使用对比推理反馈和残差嵌入细化两种策略，优化潜在推理轨迹。</p>
<p>Result: 在五个推理基准测试中验证了框架的有效性，尤其在MathQA上实现了5%的准确率提升。</p>
<p>Conclusion: 提出的框架能够在不增加训练成本的情况下显著提升推理任务的性能。</p>
<p>Abstract: Reasoning is a key component of language understanding in Large Language<br>Models. While Chain-of-Thought prompting enhances performance via explicit<br>intermediate steps, it suffers from sufficient token overhead and a fixed<br>reasoning trajectory, preventing step-wise refinement. Recent advances in<br>latent reasoning address these limitations by refining internal reasoning<br>processes directly in the model’s latent space, without producing explicit<br>outputs. However, a key challenge remains: how to effectively update reasoning<br>embeddings during post-training to guide the model toward more accurate<br>solutions. To overcome this challenge, we propose a lightweight post-training<br>framework that refines latent reasoning trajectories using two novel<br>strategies: 1) Contrastive reasoning feedback, which compares reasoning<br>embeddings against strong and weak baselines to infer effective update<br>directions via embedding enhancement; 2) Residual embedding refinement, which<br>stabilizes updates by progressively integrating current and historical<br>gradients, enabling fast yet controlled convergence. Extensive experiments and<br>case studies are conducted on five reasoning benchmarks to demonstrate the<br>effectiveness of the proposed framework. Notably, a 5% accuracy gain on MathQA<br>without additional training.</p>
</details>


<h3 id="17-RAISE-Enhancing-Scientific-Reasoning-in-LLMs-via-Step-by-Step-Retrieval"><a href="#17-RAISE-Enhancing-Scientific-Reasoning-in-LLMs-via-Step-by-Step-Retrieval" class="headerlink" title="[17] RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08625">RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval</a></h3><p><em>Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: RAISE是一个分三步的检索增强框架，通过逐步检索逻辑相关文档来提升科学推理能力。</p>
<details>
  <summary>Details</summary>
Motivation: 解决科学推理中的长链推理、领域术语知识和适应更新发现等挑战。

<p>Method: 分为问题分解、逻辑查询生成和逻辑检索三步。</p>
<p>Result: 在科学推理基准测试中表现优于其他基线方法。</p>
<p>Conclusion: RAISE不仅能检索领域知识相似的文档，还能找到逻辑相关性更高的文档。</p>
<p>Abstract: Scientific reasoning requires not only long-chain reasoning processes, but<br>also knowledge of domain-specific terminologies and adaptation to updated<br>findings. To deal with these challenges for scientific reasoning, we introduce<br>RAISE, a step-by-step retrieval-augmented framework which retrieves logically<br>relevant documents from in-the-wild corpus. RAISE is divided into three steps:<br>problem decomposition, logical query generation, and logical retrieval. We<br>observe that RAISE consistently outperforms other baselines on scientific<br>reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves<br>documents that are not only similar in terms of the domain knowledge, but also<br>documents logically more relevant.</p>
</details>


<h3 id="18-MEMETRON-Metaheuristic-Mechanisms-for-Test-time-Response-Optimization-of-Large-Language-Models"><a href="#18-MEMETRON-Metaheuristic-Mechanisms-for-Test-time-Response-Optimization-of-Large-Language-Models" class="headerlink" title="[18] MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08643">MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models</a></h3><p><em>Son The Nguyen,Theja Tulabandhula</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: MEMETRON是一个任务无关的框架，将大型语言模型的解码视为离散黑盒优化问题，利用混合元启发式算法（GENETRON和ANNETRON）搜索响应空间，显著优于标准解码和重排序方法。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型解码策略（如贪婪搜索或采样）缺乏对任务特定目标的明确优化，且控制能力有限。

<p>Method: MEMETRON通过混合元启发式算法（GENETRON和ANNETRON）和奖励模型，在不需模型重新训练或梯度访问的情况下，高效地发现高奖励响应。</p>
<p>Result: 在人类偏好对齐任务上，MEMETRON显著优于标准解码和重排序方法。</p>
<p>Conclusion: MEMETRON展示了在不重新训练模型的情况下改善对齐的潜力，是一种模块化且通用的框架。</p>
<p>Abstract: Large language models (LLMs) are increasingly used for both open-ended and<br>structured tasks, yet their inference-time behavior is still largely dictated<br>by heuristic decoding strategies such as greedy search, sampling, or reranking.<br>These methods provide limited control and do not explicitly optimize for<br>task-specific objectives. We introduce MEMETRON, a task-agnostic framework that<br>formulates LLM decoding as a discrete black-box optimization problem. MEMETRON<br>leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the<br>response space, guided by reward models and contextual operations performed by<br>the LLM itself. This approach enables efficient discovery of high-reward<br>responses without requiring model retraining or gradient access. The framework<br>is modular and generalizes across diverse tasks, requiring only a reward<br>function and lightweight prompt templates. We evaluate our framework on the<br>critical human preference alignment task and demonstrate that it significantly<br>outperforms standard decoding and reranking methods, highlighting its potential<br>to improve alignment without model retraining.</p>
</details>


<h3 id="19-TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning"><a href="#19-TableDreamer-Progressive-and-Weakness-guided-Data-Synthesis-from-Scratch-for-Table-Instruction-Tuning" class="headerlink" title="[19] TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08646">TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning</a></h3><p><em>Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: TableDreamer提出了一种渐进式、弱点导向的数据合成框架，用于生成表格指令调优数据，解决了现有方法在数据多样性和效率上的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 现有LLM-based数据合成方法在生成表格指令调优数据时存在两个主要问题：无法充分探索输入空间导致数据多样性不足，以及盲目追求数据量而忽视目标LLM的表格理解弱点。

<p>Method: TableDreamer通过生成多样化表格和指令作为种子数据，并在新识别的弱点数据指导下迭代探索输入空间，最终生成用于微调目标LLM的训练数据。</p>
<p>Result: 在10个表格基准测试中，TableDreamer将Llama3.1-8B-instruct的平均准确率从49.07%提升至60.69%，优于使用更多训练数据的现有方法。</p>
<p>Conclusion: TableDreamer通过渐进式和弱点导向的数据合成策略，显著提升了表格指令调优的效果，为相关任务提供了高效的数据生成框架。</p>
<p>Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,<br>they face two limitations in generating table instruction tuning data. First,<br>they can not thoroughly explore the vast input space of table understanding<br>tasks, leading to limited data diversity. Second, they ignore the weaknesses in<br>table understanding ability of the target LLM and blindly pursue the increase<br>of data quantity, resulting in suboptimal data efficiency. In this paper, we<br>introduce a progressive and weakness-guided data synthesis framework tailored<br>for table instruction tuning, named TableDreamer, to mitigate the above issues.<br>Specifically, we first synthesize diverse tables and related instructions as<br>seed data, and then perform an iterative exploration of the input space under<br>the guidance of the newly identified weakness data, which eventually serve as<br>the final training data for fine-tuning the target LLM. Extensive experiments<br>on 10 tabular benchmarks demonstrate the effectiveness of the proposed<br>framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%<br>(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms<br>state-of-the-art data synthesis baselines which use more training data. The<br>code and data is available at <a target="_blank" rel="noopener" href="https://github.com/SpursGoZmy/TableDreamer">https://github.com/SpursGoZmy/TableDreamer</a></p>
</details>


<h3 id="20-RuleReasoner-Reinforced-Rule-based-Reasoning-via-Domain-aware-Dynamic-Sampling"><a href="#20-RuleReasoner-Reinforced-Rule-based-Reasoning-via-Domain-aware-Dynamic-Sampling" class="headerlink" title="[20] RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08672">RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling</a></h3><p><em>Yang Liu,Jiaqi Li,Zilong Zheng</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: RuleReasoner是一种通过动态采样增强的强化学习方法，能够高效进行规则推理，且在分布内外任务中表现优于大型推理模型。</p>
<details>
  <summary>Details</summary>
Motivation: 探索小型推理模型是否能够通过强化学习有效学习规则推理，并具有跨任务和领域的鲁棒泛化能力。

<p>Method: 引入了RuleReasoner方法，通过领域感知的动态采样策略和多样化的任务集合进行强化学习。</p>
<p>Result: 在分布内外任务中，RuleReasoner显著优于前沿大型推理模型（ID任务平均提升4.1%，OOD任务提升10.4%），且计算效率更高。</p>
<p>Conclusion: RuleReasoner证明小型推理模型可以通过动态采样和强化学习实现高效的规则推理，同时具有更好的泛化能力和计算效率。</p>
<p>Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems<br>in reasoning, while deviations in rule formats, types, and complexity in<br>real-world applications pose severe challenges. Recent studies have shown that<br>large reasoning models (LRMs) have remarkable reasoning capabilities, and their<br>performance is substantially enhanced by reinforcement learning (RL). However,<br>it remains an open question whether small reasoning models (SRMs) can learn<br>rule-based reasoning effectively with robust generalization across diverse<br>tasks and domains. To address this, we introduce Reinforced Rule-based<br>Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct<br>rule-based reasoning via a wide collection of curated tasks and a novel<br>domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples<br>each training batch by updating the sampling weights of different domains based<br>on historical rewards. This facilitates domain augmentation and flexible online<br>learning schedules for RL, obviating the need for pre-hoc human-engineered<br>mix-training recipes used in existing methods. Empirical evaluations on<br>in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that<br>RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%<br>average points on eight ID tasks and $\Delta$10.4% average points on three OOD<br>tasks over OpenAI-o1). Notably, our approach also exhibits higher computational<br>efficiency compared to prior dynamic sampling methods for RL.</p>
</details>


<h3 id="21-Brevity-is-the-soul-of-sustainability-Characterizing-LLM-response-lengths"><a href="#21-Brevity-is-the-soul-of-sustainability-Characterizing-LLM-response-lengths" class="headerlink" title="[21] Brevity is the soul of sustainability: Characterizing LLM response lengths"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08686">Brevity is the soul of sustainability: Characterizing LLM response lengths</a></h3><p><em>Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文研究了大型语言模型（LLMs）推理过程中的能源消耗问题，提出通过输出压缩和提示工程策略减少响应长度，实现25-60%的能源优化。</p>
<details>
  <summary>Details</summary>
Motivation: LLMs的推理过程消耗大量能源，而现有研究中输出压缩优化方法较少，亟需开发能源高效的推理方法。

<p>Method: 通过基准测试12个仅解码器LLMs，分析了其响应的冗余信息，并提出提示工程策略以减少响应长度。</p>
<p>Result: 实验表明，针对长度减少和信息内容控制的提示策略能显著降低能源消耗（25-60%），同时保持响应质量。</p>
<p>Conclusion: 提示工程是优化LLM能源效率的有效手段，未来研究可进一步探索输出压缩领域。</p>
<p>Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)<br>arises from their inference processes; hence developing energy-efficient<br>methods for inference is crucial. While several techniques exist for inference<br>optimization, output compression remains relatively unexplored, with only a few<br>preliminary efforts addressing this aspect. In this work, we first benchmark 12<br>decoder-only LLMs across 5 datasets, revealing that these models often produce<br>responses that are substantially longer than necessary. We then conduct a<br>comprehensive quality assessment of LLM responses, formally defining six<br>information categories present in LLM responses. We show that LLMs often tend<br>to include redundant or additional information besides the minimal answer. To<br>address this issue of long responses by LLMs, we explore several simple and<br>intuitive prompt-engineering strategies. Empirical evaluation shows that<br>appropriate prompts targeting length reduction and controlling information<br>content can achieve significant energy optimization between 25-60% by reducing<br>the response length while preserving the quality of LLM responses.</p>
</details>


<h3 id="22-ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts"><a href="#22-ClimateViz-A-Benchmark-for-Statistical-Reasoning-and-Fact-Verification-on-Scientific-Charts" class="headerlink" title="[22] ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08700">ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts</a></h3><p><em>Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: ClimateViz是首个用于科学事实核查的大规模科学图表基准数据集，包含4.9万条标注，评估发现当前多模态模型在图表推理上表现远低于人类。</p>
<details>
  <summary>Details</summary>
Motivation: 科学图表在定量证据展示中至关重要，但现有的科学事实核查大多忽略了图表。

<p>Method: 构建ClimateViz数据集，包含专家标注的图表和知识图谱解释，评估多种多模态模型。</p>
<p>Result: 最佳模型准确率为76.2-77.8%，远低于人类表现（89.3-92.7%），解释增强可提升部分模型。</p>
<p>Conclusion: 图表推理现有模型能力不足，提供解释可辅助提升性能。</p>
<p>Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking<br>scientific charts, which are key for presenting quantitative evidence and<br>statistical reasoning. We introduce ClimateViz, the first large-scale benchmark<br>for scientific fact-checking using expert-curated scientific charts. ClimateViz<br>contains 49,862 claims linked to 2,896 visualizations, each labeled as support,<br>refute, or not enough information. To improve interpretability, each example<br>includes structured knowledge graph explanations covering trends, comparisons,<br>and causal relations. We evaluate state-of-the-art multimodal language models,<br>including both proprietary and open-source systems, in zero-shot and few-shot<br>settings. Results show that current models struggle with chart-based reasoning:<br>even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to<br>77.8 percent accuracy in label-only settings, far below human performance (89.3<br>and 92.7 percent). Explanation-augmented outputs improve performance in some<br>models. We released our dataset and code alongside the paper.</p>
</details>


<h3 id="23-Explainable-Compliance-Detection-with-Multi-Hop-Natural-Language-Inference-on-Assurance-Case-Structure"><a href="#23-Explainable-Compliance-Detection-with-Multi-Hop-Natural-Language-Inference-on-Assurance-Case-Structure" class="headerlink" title="[23] Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08713">Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure</a></h3><p><em>Fariz Ikhwantri,Dusica Marijan</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了一种基于自然语言推理（NLI）的合规性检测方法EXCLAIM，通过多跳推理解释保证案例的合规性，并利用大型语言模型生成保证案例以解决数据不足的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 保证案例的合规性检测面临法律与技术文本复杂、模型解释需求高以及数据有限等挑战。

<p>Method: 采用自然语言推理（NLI）和多跳推理框架，结合大型语言模型生成保证案例，并引入覆盖率和结构一致性指标。</p>
<p>Result: 通过GDPR要求的案例研究展示了方法的有效性，表明NLI方法在自动化合规过程中的潜力。</p>
<p>Conclusion: EXCLAIM方法为自动化合规性检测提供了可解释且可追溯的解决方案。</p>
<p>Abstract: Ensuring complex systems meet regulations typically requires checking the<br>validity of assurance cases through a claim-argument-evidence framework. Some<br>challenges in this process include the complicated nature of legal and<br>technical texts, the need for model explanations, and limited access to<br>assurance case data. We propose a compliance detection approach based on<br>Natural Language Inference (NLI): EXplainable CompLiance detection with<br>Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the<br>claim-argument-evidence structure of an assurance case as a multi-hop inference<br>for explainable and traceable compliance detection. We address the limited<br>number of assurance cases by generating them using large language models<br>(LLMs). We introduce metrics that measure the coverage and structural<br>consistency. We demonstrate the effectiveness of the generated assurance case<br>from GDPR requirements in a multi-hop inference task as a case study. Our<br>results highlight the potential of NLI-based approaches in automating the<br>regulatory compliance process.</p>
</details>


<h3 id="24-Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition"><a href="#24-Multi-Teacher-Language-Aware-Knowledge-Distillation-for-Multilingual-Speech-Emotion-Recognition" class="headerlink" title="[24] Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08717">Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition</a></h3><p><em>Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 提出了一种多教师知识蒸馏方法，用于提升英语、芬兰语和法语的多语言语音情感识别性能，取得了优于基准的成果。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管单语言语音情感识别（SER）取得进展，多语言SER仍然是个挑战，目标是训练一个能兼容多语言的单一模型。

<p>Method: 使用Wav2Vec2.0作为单语言教师模型的基础，通过语言感知的多教师知识蒸馏方法，将知识蒸馏到一个多语言学生模型中。</p>
<p>Result: 学生模型表现优异，英语数据集的加权召回率为72.9，芬兰语数据集的未加权召回率为63.4，优于微调和知识蒸馏基准。</p>
<p>Conclusion: 该方法在提升悲伤和中性情绪的识别上表现突出，但对愤怒和快乐的识别仍有挑战。</p>
<p>Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer<br>interaction. Despite strides in monolingual SER, extending them to build a<br>multilingual system remains challenging. Our goal is to train a single model<br>capable of multilingual SER by distilling knowledge from multiple teacher<br>models. To address this, we introduce a novel language-aware multi-teacher<br>knowledge distillation method to advance SER in English, Finnish, and French.<br>It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and<br>then distills their knowledge into a single multilingual student model. The<br>student model demonstrates state-of-the-art performance, with a weighted recall<br>of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish<br>dataset, surpassing fine-tuning and knowledge distillation baselines. Our<br>method excels in improving recall for sad and neutral emotions, although it<br>still faces challenges in recognizing anger and happiness.</p>
</details>


<h3 id="25-AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP"><a href="#25-AraReasoner-Evaluating-Reasoning-Based-LLMs-for-Arabic-NLP" class="headerlink" title="[25] AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08768">AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP</a></h3><p><em>Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文全面评估了多种专注于推理的大语言模型（LLMs）在阿拉伯语NLP任务上的表现，特别关注DeepSeek模型，通过零样本、少样本和微调等策略，揭示了关键发现，如少样本示例的显著提升。</p>
<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和通用NLP任务上表现出色，但其在阿拉伯语数据（以丰富的形态、多样的方言和复杂文字为特点）上的性能尚未充分探索。

<p>Method: 通过零样本、少样本和微调（包括LoRA）等多种策略，对多种LLMs在15项阿拉伯语NLP任务上进行系统评估。</p>
<p>Result: 精选三例少样本可将分类任务F1平均提升13点；DeepSeek在零样本下表现优于GPT o4-mini 12点；LoRA微调比模型规模提升带来额外8点F1和BLEU增益。</p>
<p>Conclusion: 研究展示了LLMs在复杂阿拉伯语任务中的潜力，并为提升性能和效率提供了实用策略。</p>
<p>Abstract: Large language models (LLMs) have shown remarkable progress in reasoning<br>abilities and general natural language processing (NLP) tasks, yet their<br>performance on Arabic data, characterized by rich morphology, diverse dialects,<br>and complex script, remains underexplored. This paper presents a comprehensive<br>benchmarking study of multiple reasoning-focused LLMs, with a special emphasis<br>on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP<br>tasks. We experiment with various strategies, including zero-shot, few-shot,<br>and fine-tuning. This allows us to systematically evaluate performance on<br>datasets covering a range of applications to examine their capacity for<br>linguistic reasoning under different levels of complexity. Our experiments<br>reveal several key findings. First, carefully selecting just three in-context<br>examples delivers an average uplift of over 13 F1 points on classification<br>tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection<br>from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures<br>outperform a strong GPT o4-mini baseline by an average of 12 F1 points on<br>complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning<br>yields up to an additional 8 points in F1 and BLEU compared to equivalent<br>increases in model scale. The code is available at<br><a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/AraReasoner41299">https://anonymous.4open.science/r/AraReasoner41299</a></p>
</details>


<h3 id="26-The-impact-of-fine-tuning-in-LLaMA-on-hallucinations-for-named-entity-extraction-in-legal-documentation"><a href="#26-The-impact-of-fine-tuning-in-LLaMA-on-hallucinations-for-named-entity-extraction-in-legal-documentation" class="headerlink" title="[26] The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08827">The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation</a></h3><p><em>Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出两步法从法律文档提取交通事故信息：先文本分割再实体提取，相比传统方法显著提升准确率。</p>
<details>
  <summary>Details</summary>
Motivation: 法律文档中交通事故信息的提取对保险公司成本量化至关重要，但传统方法效率低且易出错。

<p>Method: 采用两步法：文本分割（正则表达式或向量化）和基于LLM的实体提取，并对LLaMA模型进行微调。</p>
<p>Result: 微调后的LLaMA-2 70B准确率达79.4%，LLaMA-3 8B表现接近76.6%，GPT-4 Turbo最高86.1%。</p>
<p>Conclusion: 向量化分割结合LLM显著优于传统方法，模型迭代迅速，GPT-4 Turbo表现最佳。</p>
<p>Abstract: The extraction of information about traffic accidents from legal documents is<br>crucial for quantifying insurance company costs. Extracting entities such as<br>percentages of physical and&#x2F;or psychological disability and the involved<br>compensation amounts is a challenging process, even for experts, due to the<br>subtle arguments and reasoning in the court decision. A two-step procedure is<br>proposed: first, segmenting the document identifying the most relevant<br>segments, and then extracting the entities. For text segmentation, two<br>methodologies are compared: a classic method based on regular expressions and a<br>second approach that divides the document into blocks of n-tokens, which are<br>then vectorized using multilingual models for semantic searches<br>(text-embedding-ada-002&#x2F;MiniLM-L12-v2 ). Subsequently, large language models<br>(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to<br>the selected segments for entity extraction. For the LLaMA models, fine-tuning<br>is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a<br>significant number of hallucinations in extractions which are an important<br>contention point for named entity extraction. This work shows that these<br>hallucinations are substantially reduced after finetuning the model. The<br>performance of the methodology based on segment vectorization and subsequent<br>use of LLMs significantly surpasses the classic method which achieves an<br>accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning<br>achieves the highest accuracy 79.4%, surpassing its base version 61.7%.<br>Notably, the base LLaMA-3 8B model already performs comparably to the finetuned<br>LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model<br>development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.</p>
</details>


<h3 id="27-PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs"><a href="#27-PropMEND-Hypernetworks-for-Knowledge-Propagation-in-LLMs" class="headerlink" title="[27] PropMEND: Hypernetworks for Knowledge Propagation in LLMs"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08920">PropMEND: Hypernetworks for Knowledge Propagation in LLMs</a></h3><p><em>Zeyu Leo Liu,Greg Durrett,Eunsol Choi</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: PropMEND是一种基于超网络的知识传播方法，通过修改梯度以促进知识的传播，解决了传统知识编辑技术在多跳推理问题上的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 传统知识编辑技术虽能注入知识，但无法支持基于这些知识的推理。PropMEND旨在通过超网络学习如何调整梯度，以传播知识并支持多跳推理。

<p>Method: PropMEND扩展了MEND的元目标，通过超网络学习修改语言建模损失的梯度，使注入的知识能用于多跳问题回答。</p>
<p>Result: 在RippleEdit数据集上，PropMEND的性能显著提升，多跳问题准确率几乎翻倍。在未见过的实体-关系对上，PropMEND仍优于现有方法，但性能差距缩小。</p>
<p>Conclusion: PropMEND在知识传播方面表现优异，尤其在多跳推理任务上，但仍有改进空间，尤其是在泛化到更多关系上。</p>
<p>Abstract: Knowledge editing techniques for large language models (LLMs) can inject<br>knowledge that is later reproducible verbatim, but they fall short on<br>propagating that knowledge: models cannot answer questions that require<br>reasoning with the injected knowledge. We present a hypernetwork-based approach<br>for knowledge propagation, named PropMEND, where we meta-learn how to modify<br>gradients of a language modeling loss to encourage injected information to<br>propagate. Our approach extends the meta-objective of MEND [29] so that<br>gradient updates on knowledge are transformed to enable answering multi-hop<br>questions involving that knowledge. We show improved performance on the<br>RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop<br>questions whose answers are not explicitly stated in the injected fact. We<br>further introduce a new dataset, Controlled RippleEdit, to evaluate the<br>generalization of our hypernetwork, testing knowledge propagation along<br>relations and entities unseen during hypernetwork training. PropMEND still<br>outperforms existing approaches in unseen entity-relation pairs, yet the<br>performance gap decreases substantially, suggesting future work in propagating<br>knowledge to a wide range of relations.</p>
</details>


<h3 id="28-Can-A-Gamer-Train-A-Mathematical-Reasoning-Model"><a href="#28-Can-A-Gamer-Train-A-Mathematical-Reasoning-Model" class="headerlink" title="[28] Can A Gamer Train A Mathematical Reasoning Model?"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08935">Can A Gamer Train A Mathematical Reasoning Model?</a></h3><p><em>Andrew Shin</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 本文展示了如何在单个普通游戏GPU（RTX 3080 Ti）上训练一个高性能的数学推理模型，通过结合强化学习和内存优化技术，挑战了传统认为高性能AI需要大规模基础设施的观点。</p>
<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）的开发需要昂贵的计算资源，本文旨在通过技术创新降低训练成本，使高性能AI研究更加普及。

<p>Method: 采用强化学习和内存优化技术，训练一个1.5B参数的数学推理模型。</p>
<p>Result: 在资源受限环境下，该模型在数学推理基准测试中表现优于或可比拟更大规模的模型。</p>
<p>Conclusion: 研究结果表明，高性能数学推理模型并非必须依赖大规模基础设施，为资源有限的研究者提供了可行的解决方案。</p>
<p>Abstract: While large language models (LLMs) have achieved remarkable performance in<br>various tasks including mathematical reasoning, their development typically<br>demands prohibitive computational resources. Recent advancements have reduced<br>costs for training capable models, yet even these approaches rely on high-end<br>hardware clusters. In this paper, we demonstrate that a single average gaming<br>GPU can train a solid mathematical reasoning model, by integrating<br>reinforcement learning and memory optimization techniques. Specifically, we<br>train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB<br>memory that achieves comparable or better performance on mathematical reasoning<br>benchmarks than models several times larger, in resource-constrained<br>environments. Our results challenge the paradigm that state-of-the-art<br>mathematical reasoning necessitates massive infrastructure, democratizing<br>access to high-performance AI research.<br><a target="_blank" rel="noopener" href="https://github.com/shinandrew/YouronMath">https://github.com/shinandrew/YouronMath</a>.</p>
</details>


<h3 id="29-FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation"><a href="#29-FaithfulRAG-Fact-Level-Conflict-Modeling-for-Context-Faithful-Retrieval-Augmented-Generation" class="headerlink" title="[29] FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08938">FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation</a></h3><p><em>Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 论文提出了FaithfulRAG框架，通过显式建模检索上下文与模型参数知识之间的冲突，解决大型语言模型在知识密集任务中的不忠实问题。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法通过强制抑制模型参数知识实现忠实性，但这损害了模型内部知识结构并增加误解上下文的风险。

<p>Method: FaithfulRAG在事实层面识别知识冲突，并设计自思考过程，让模型在生成响应前对冲突事实进行推理与整合。</p>
<p>Result: 实验表明，该方法优于现有最佳方法。</p>
<p>Conclusion: FaithfulRAG有效解决了知识冲突问题，提升了模型的忠实性与性能。</p>
<p>Abstract: Large language models (LLMs) augmented with retrieval systems have<br>demonstrated significant potential in handling knowledge-intensive tasks.<br>However, these models often struggle with unfaithfulness issues, generating<br>outputs that either ignore the retrieved context or inconsistently blend it<br>with the LLM<code>s parametric knowledge. This issue is particularly severe in cases of knowledge conflict, where the retrieved context conflicts with the model</code>s<br>parametric knowledge. While existing faithful RAG approaches enforce strict<br>context adherence through well-designed prompts or modified decoding<br>strategies, our analysis reveals a critical limitation: they achieve<br>faithfulness by forcibly suppressing the model<code>s parametric knowledge, which undermines the model</code>s internal knowledge structure and increases the risk of<br>misinterpreting the context. To this end, this paper proposes FaithfulRAG, a<br>novel framework that resolves knowledge conflicts by explicitly modeling<br>discrepancies between the model&#96;s parametric knowledge and retrieved context.<br>Specifically, FaithfulRAG identifies conflicting knowledge at the fact level<br>and designs a self-thinking process, allowing LLMs to reason about and<br>integrate conflicting facts before generating responses. Extensive experiments<br>demonstrate that our method outperforms state-of-the-art methods. The code is<br>available at https:&#x2F;&#x2F; github.com&#x2F;DeepLearnXMU&#x2F;Faithful-RAG</p>
</details>


<h3 id="30-Can-LLMs-Ground-when-they-Don’t-Know-A-Study-on-Direct-and-Loaded-Political-Questions"><a href="#30-Can-LLMs-Ground-when-they-Don’t-Know-A-Study-on-Direct-and-Loaded-Political-Questions" class="headerlink" title="[30] Can LLMs Ground when they (Don’t) Know: A Study on Direct and Loaded Political Questions"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08952">Can LLMs Ground when they (Don’t) Know: A Study on Direct and Loaded Political Questions</a></h3><p><em>Clara Lachenmaier,Judith Sieker,Sina Zarrieß</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 研究发现大型语言模型在处理政治领域的事实信息时，难以有效纠正用户的错误信念，其与用户建立共同理解的能力存在显著挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在拥有或不拥有知识的情况下，如何处理共同基础，尤其是在政治领域中存在高风险错误信息的情况。

<p>Method: 通过直接知识问题和预设错误信息的诱导性问题，评估大型语言模型的回答能力及其是否主动纠正用户错误信念。</p>
<p>Result: 大型语言模型在纠正用户错误信念和建立共同理解方面表现不佳，尤其是在政治偏见和知识水平的影响下。</p>
<p>Conclusion: 研究强调大型语言模型在政治讨论中纠错和减轻错误信息传播的能力有限，引发对其实际应用的担忧。</p>
<p>Abstract: Communication among humans relies on conversational grounding, allowing<br>interlocutors to reach mutual understanding even when they do not have perfect<br>knowledge and must resolve discrepancies in each other’s beliefs. This paper<br>investigates how large language models (LLMs) manage common ground in cases<br>where they (don’t) possess knowledge, focusing on facts in the political domain<br>where the risk of misinformation and grounding failure is high. We examine the<br>ability of LLMs to answer direct knowledge questions and loaded questions that<br>presuppose misinformation. We evaluate whether loaded questions lead LLMs to<br>engage in active grounding and correct false user beliefs, in connection to<br>their level of knowledge and their political bias. Our findings highlight<br>significant challenges in LLMs’ ability to engage in grounding and reject false<br>user beliefs, raising concerns about their role in mitigating misinformation in<br>political discourse.</p>
</details>


<h3 id="31-Pre-trained-Language-Models-Learn-Remarkably-Accurate-Representations-of-Numbers"><a href="#31-Pre-trained-Language-Models-Learn-Remarkably-Accurate-Representations-of-Numbers" class="headerlink" title="[31] Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08966">Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers</a></h3><p><em>Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 预训练语言模型在算术上容易出错，现有方法未能有效探测数值嵌入的精确性。本文提出了一种新探测技术，能高精度解码数字，并证明模型在预训练后能精确表示数字。通过校准嵌入可减少算术错误。</p>
<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在算术任务中表现不佳，现有方法未能捕捉其数值嵌入的正弦模式结构，导致无法准确评估模型的数字表示能力。

<p>Method: 提出一种新的探测技术，能够从输入嵌入中高精度解码数值，揭示预训练语言模型中数字表示的精确性。</p>
<p>Result: 新探测技术证明模型在预训练后能高精度表示数字，且嵌入的精确性与算术错误相关，校准嵌入可减少错误。</p>
<p>Conclusion: 通过改进的探测技术揭示了语言模型中数字嵌入的精确性，校准嵌入能显著提升算术任务的性能。</p>
<p>Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing<br>work showed limited success in probing numeric values from models’<br>representations, indicating that these errors can be attributed to the inherent<br>unreliability of distributionally learned embeddings in representing exact<br>quantities. However, we observe that previous probing methods are inadequate<br>for the emergent structure of learned number embeddings with sinusoidal<br>patterns.<br>  In response, we propose a novel probing technique that decodes numeric values<br>from input embeddings with near-perfect accuracy across a range of open-source<br>LMs. This proves that after the sole pre-training, LMs represent numbers with<br>remarkable precision. Finally, we find that the embeddings’ preciseness judged<br>by our probe’s accuracy explains a large portion of LM’s errors in elementary<br>arithmetic, and show that aligning the embeddings with the pattern discovered<br>by our probe can mitigate these errors.</p>
</details>


<h3 id="32-Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System"><a href="#32-Atomic-to-Compositional-Generalization-for-Mobile-Agents-with-A-New-Benchmark-and-Scheduling-System" class="headerlink" title="[32] Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08972">Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System</a></h3><p><em>Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 摘要介绍了UI-NEXUS基准测试，用于评估移动代理在组合任务中的表现，并提出了AGENT-NEXUS调度系统以提升任务成功率。</p>
<details>
  <summary>Details</summary>
Motivation: 现有研究多关注原子任务，忽略了组合任务的重要性，而组合任务对实际应用至关重要。

<p>Method: 提出UI-NEXUS基准测试，涵盖三种组合操作类型，并在20个本地应用和30个在线服务应用上评估。随后提出AGENT-NEXUS调度系统，动态分解任务。</p>
<p>Result: 现有代理在组合任务中表现不佳，AGENT-NEXUS提升了24%至40%的任务成功率，且未显著增加开销。</p>
<p>Conclusion: UI-NEXUS揭示了组合任务的挑战，AGENT-NEXUS为移动代理提供了高效解决方案。</p>
<p>Abstract: Autonomous agents powered by multimodal large language models have been<br>developed to facilitate task execution on mobile devices. However, prior work<br>has predominantly focused on atomic tasks – such as shot-chain execution tasks<br>and single-screen grounding tasks – while overlooking the generalization to<br>compositional tasks, which are indispensable for real-world applications. This<br>work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile<br>agents on three categories of compositional operations: Simple Concatenation,<br>Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in<br>20 fully controllable local utility app environments, as well as 30 online<br>Chinese and English service apps. It comprises 100 interactive task templates<br>with an average optimal step count of 14.05. Experimental results across a<br>range of mobile agents with agentic workflow or agent-as-a-model show that<br>UI-NEXUS presents significant challenges. Specifically, existing agents<br>generally struggle to balance performance and efficiency, exhibiting<br>representative failure modes such as under-execution, over-execution, and<br>attention drift, causing visible atomic-to-compositional generalization gap.<br>Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient<br>scheduling system to tackle compositional mobile tasks. AGENT-NEXUS<br>extrapolates the abilities of existing mobile agents by dynamically decomposing<br>long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS<br>achieves 24% to 40% task success rate improvement for existing mobile agents on<br>compositional operation tasks within the UI-NEXUS benchmark without<br>significantly sacrificing inference overhead. The demo video, dataset, and code<br>are available on the project page at <a target="_blank" rel="noopener" href="https://ui-nexus.github.io/">https://ui-nexus.github.io</a>.</p>
</details>


<h3 id="33-FROST-EMA-Finnish-and-Russian-Oral-Speech-Dataset-of-Electromagnetic-Articulography-Measurements-with-L1-L2-and-Imitated-L2-Accents"><a href="#33-FROST-EMA-Finnish-and-Russian-Oral-Speech-Dataset-of-Electromagnetic-Articulography-Measurements-with-L1-L2-and-Imitated-L2-Accents" class="headerlink" title="[33] FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08981">FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents</a></h3><p><em>Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 介绍了一个新的双语语音数据集FROST-EMA，可用于语音学和自动语音验证系统的研究。</p>
<details>
  <summary>Details</summary>
Motivation: 研究语言变异性，尤其是二语及模仿二语对语音学和语音技术的影响。

<p>Method: 收集18名双语者的语音数据，包括母语、二语及模仿二语，并通过两个案例研究展示数据集的应用。</p>
<p>Result: 初步研究表明，二语和模仿二语对自动说话人验证系统性能有影响，同时也揭示了发音模式的差异。</p>
<p>Conclusion: FROST-EMA数据集为语言变异性研究提供了新工具，展示了其在语音学和语音技术领域的潜力。</p>
<p>Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of<br>Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,<br>who produced speech in their native language (L1), second language (L2), and<br>imitated L2 (fake foreign accent). The new corpus enables research into<br>language variability from phonetic and technological points of view.<br>Accordingly, we include two preliminary case studies to demonstrate both<br>perspectives. The first case study explores the impact of L2 and imitated L2 on<br>the performance of an automatic speaker verification system, while the second<br>illustrates the articulatory patterns of one speaker in L1, L2, and a fake<br>accent.</p>
</details>


<h3 id="34-Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning"><a href="#34-Learning-to-Reason-Across-Parallel-Samples-for-LLM-Reasoning" class="headerlink" title="[34] Learning to Reason Across Parallel Samples for LLM Reasoning"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09014">Learning to Reason Across Parallel Samples for LLM Reasoning</a></h3><p><em>Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 通过训练一个紧凑的LLM（SSA）来聚合多个样本集的答案，该方法在数学领域表现优于其他测试时扩展方法，并展现良好的泛化能力。</p>
<details>
  <summary>Details</summary>
Motivation: 提出SSA方法是为了更高效地利用多样本集的输出，提升大语言模型在推理任务中的表现。

<p>Method: 训练一个紧凑的SSA模型，通过强化学习优化其对多个样本的聚合能力，以提升答案准确性。</p>
<p>Result: 在多个推理数据集上，SSA优于其他测试时扩展方法（如基于奖励模型的重新排序）。</p>
<p>Conclusion: SSA展现了跨样本集大小、基础模型、任务等的泛化能力，并能高效与黑盒模型协同工作。</p>
<p>Abstract: Scaling test-time compute brings substantial performance gains for large<br>language models (LLMs). By sampling multiple answers and heuristically<br>aggregate their answers (e.g., either through majority voting or using<br>verifiers to rank the answers), one can achieve consistent performance gains in<br>math domains. In this paper, we propose a new way to leverage such multiple<br>sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that<br>takes a concatenated sequence of multiple samples and output the final answer,<br>optimizing it for the answer accuracy with reinforcement learning. Experiments<br>on multiple reasoning datasets show that SSA outperforms other test-time<br>scaling methods such as reward model-based re-ranking. Our approach also shows<br>a promising generalization ability, across sample set sizes, base model<br>families and scales, and tasks. By separating LLMs to generate answers and LLMs<br>to analyze and aggregate sampled answers, our approach can work with the<br>outputs from premier black box models easily and efficiently.</p>
</details>


<h3 id="35-Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning"><a href="#35-Router-R1-Teaching-LLMs-Multi-Round-Routing-and-Aggregation-via-Reinforcement-Learning" class="headerlink" title="[35] Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09033">Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning</a></h3><p><em>Haozhen Zhang,Tao Feng,Jiaxuan You</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 提出了基于强化学习的多LLM路由框架Router-R1，通过动态模型调用与响应集成，优化性能与成本的权衡。</p>
<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器仅支持单轮一对一映射，无法利用多模型的互补优势应对复杂任务。

<p>Method: 采用强化学习，将路由和聚合建模为序列决策过程；路由器本身为LLM，交替进行内部推理与动态模型调用。</p>
<p>Result: 在7个基准测试中，Router-R1表现优于基线，兼顾性能与成本控制。</p>
<p>Conclusion: Router-R1通过强化学习实现高效路由，能够泛化到未见过的模型选择，为性能-成本权衡优化提供新途径。</p>
<p>Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the<br>development of LLM routers that assign user queries to the most suitable model.<br>However, existing LLM routers typically perform a single-round, one-to-one<br>mapping (\textit{i.e.}, assigning each query to a single model in isolation),<br>which limits their capability to tackle complex tasks that demand the<br>complementary strengths of multiple LLMs. In this paper, we present<br>\textbf{Router-R1}, a reinforcement learning (RL)-based framework that<br>formulates multi-LLM routing and aggregation as a sequential decision process.<br>Router-R1 instantiates the router itself as a capable LLM, leveraging its<br>reasoning ability to interleave “think” actions (internal deliberation) with<br>“route” actions (dynamic model invocation), and integrates each response into<br>its evolving context. To guide learning, we employ a lightweight rule-based<br>reward comprising format rewards, final outcome rewards, and a novel cost<br>reward for performance and cost trade-off optimization, opening a pathway<br>toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions<br>only on simple model descriptors such as pricing, latency, and example<br>performance, enabling strong generalization to unseen model selection.<br>Experiments on seven general and multi-hop QA benchmarks show that Router-R1<br>outperforms over several strong baselines, achieving superior performance while<br>maintaining robust generalization and cost management.Code is available at<br><a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/Router-R1">https://github.com/ulab-uiuc/Router-R1</a>.</p>
</details>


<h3 id="36-Same-Task-Different-Circuits-Disentangling-Modality-Specific-Mechanisms-in-VLMs"><a href="#36-Same-Task-Different-Circuits-Disentangling-Modality-Specific-Mechanisms-in-VLMs" class="headerlink" title="[36] Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.09047">Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs</a></h3><p><em>Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov</em></p>
<p>Main category: cs.CL</p>
<p>TL;DR: 研究了视觉语言模型（VLMs）在视觉和文本任务中的性能差异，通过分析计算子图（circuits）和提出一种无需训练的方法来缩小差距。</p>
<details>
  <summary>Details</summary>
Motivation: 探索为什么VLMs在文本任务上表现优于视觉任务，并尝试缩小这种性能差距。

<p>Method: 比较视觉和文本任务的计算子图，发现它们在后期层才能对齐，提出通过将视觉数据令牌从后期层修补回早期层来优化表现。</p>
<p>Result: 实验表明这种方法平均能缩小两种模态之间性能差距的三分之一。</p>
<p>Conclusion: 揭示了多模态性能差距的原因，并提出了一种无需训练即可部分解决该问题的方法。</p>
<p>Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions<br>on visual inputs (e.g., counting objects in an image), yet demonstrate higher<br>accuracies when performing an analogous task on text (e.g., counting words in a<br>text). We investigate this accuracy gap by identifying and comparing the<br>\textit{circuits} - the task-specific computational sub-graphs - in different<br>modalities. We show that while circuits are largely disjoint between<br>modalities, they implement relatively similar functionalities: the differences<br>lie primarily in processing modality-specific data positions (an image or a<br>text sequence). Zooming in on the image data representations, we observe they<br>become aligned with the higher-performing analogous textual representations<br>only towards later layers, too late in processing to effectively influence<br>subsequent positions. To overcome this, we patch the representations of visual<br>data tokens from later layers back into earlier layers. In experiments with<br>multiple tasks and models, this simple intervention closes a third of the<br>performance gap between the modalities, on average. Our analysis sheds light on<br>the multi-modal performance gap in VLMs and suggests a training-free approach<br>for reducing it.</p>
</details>


<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="37-Towards-Reliable-AR-Guided-Surgical-Navigation-Interactive-Deformation-Modeling-with-Data-Driven-Biomechanics-and-Prompts"><a href="#37-Towards-Reliable-AR-Guided-Surgical-Navigation-Interactive-Deformation-Modeling-with-Data-Driven-Biomechanics-and-Prompts" class="headerlink" title="[37] Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08048">Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts</a></h3><p><em>Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种数据驱动的生物力学算法，结合人机交互机制，提高了AR引导手术中变形建模的效率和准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在术中变形建模中存在计算成本高和难以处理大范围解剖变化的问题，影响AR手术导航的可靠性。

<p>Method: 采用数据驱动的生物力学算法，结合人机交互机制，允许医生动态纠正解剖对准错误。</p>
<p>Result: 算法在公开数据集上的平均目标配准误差为3.42 mm，结合交互框架后降至2.78 mm，优于现有方法。</p>
<p>Conclusion: 该框架实现了高效准确的变形建模，提升了医生与算法的协作，为安全的计算机辅助手术奠定了基础。</p>
<p>Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ<br>models are superimposed onto the patient’s intraoperative anatomy to visualize<br>critical structures such as vessels and tumors. Accurate deformation modeling<br>is essential to maintain the reliability of AR overlays by ensuring alignment<br>between preoperative models and the dynamically changing anatomy. Although the<br>finite element method (FEM) offers physically plausible modeling, its high<br>computational cost limits intraoperative applicability. Moreover, existing<br>algorithms often fail to handle large anatomical changes, such as those induced<br>by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical<br>correspondences and compromised AR guidance. To address these challenges, we<br>propose a data-driven biomechanics algorithm that preserves FEM-level accuracy<br>while improving computational efficiency. In addition, we introduce a novel<br>human-in-the-loop mechanism into the deformation modeling process. This enables<br>surgeons to interactively provide prompts to correct anatomical misalignments,<br>thereby incorporating clinical expertise and allowing the model to adapt<br>dynamically to complex surgical scenarios. Experiments on a publicly available<br>dataset demonstrate that our algorithm achieves a mean target registration<br>error of 3.42 mm. Incorporating surgeon prompts through the interactive<br>framework further reduces the error to 2.78 mm, surpassing state-of-the-art<br>methods in volumetric accuracy. These results highlight the ability of our<br>framework to deliver efficient and accurate deformation modeling while<br>enhancing surgeon-algorithm collaboration, paving the way for safer and more<br>reliable computer-assisted surgeries.</p>
</details>


<h3 id="38-ReCogDrive-A-Reinforced-Cognitive-Framework-for-End-to-End-Autonomous-Driving"><a href="#38-ReCogDrive-A-Reinforced-Cognitive-Framework-for-End-to-End-Autonomous-Driving" class="headerlink" title="[38] ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08052">ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving</a></h3><p><em>Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: ReCogDrive通过结合视觉语言模型(VLMs)和扩散规划器，解决了自动驾驶在长尾场景中的性能问题，并显著提升了性能指标。</p>
<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶在罕见和长尾场景中表现不佳，且现有方法存在领域差异、维度不匹配和模仿学习的局限性。

<p>Method: 采用三阶段训练：1) 用驾驶问答数据集训练VLMs；2) 用扩散规划器进行模仿学习；3) 通过强化学习微调规划器。</p>
<p>Result: 在NAVSIM基准测试中，PDMS达到89.6，超越了之前的最佳性能5.6分。</p>
<p>Conclusion: ReCogDrive通过整合VLMs和扩散规划器，显著提升了自动驾驶在复杂场景中的表现，并成为新的SOTA。</p>
<p>Abstract: Although end-to-end autonomous driving has made remarkable progress, its<br>performance degrades significantly in rare and long-tail scenarios. Recent<br>approaches attempt to address this challenge by leveraging the rich world<br>knowledge of Vision-Language Models (VLMs), but these methods suffer from<br>several limitations: (1) a significant domain gap between the pre-training data<br>of VLMs and real-world driving data, (2) a dimensionality mismatch between the<br>discrete language space and the continuous action space, and (3) imitation<br>learning tends to capture the average behavior present in the dataset, which<br>may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an<br>autonomous driving system that integrates VLMs with diffusion planner, which<br>adopts a three-stage paradigm for training. In the first stage, we use a<br>large-scale driving question-answering datasets to train the VLMs, mitigating<br>the domain discrepancy between generic content and real-world driving<br>scenarios. In the second stage, we employ a diffusion-based planner to perform<br>imitation learning, mapping representations from the latent language space to<br>continuous driving actions. Finally, we fine-tune the diffusion planner using<br>reinforcement learning with NAVSIM non-reactive simulator, enabling the model<br>to generate safer, more human-like driving trajectories. We evaluate our<br>approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6<br>and setting a new state-of-the-art that surpasses the previous vision-only SOTA<br>by 5.6 PDMS.</p>
</details>


<h3 id="39-CuRe-Cultural-Gaps-in-the-Long-Tail-of-Text-to-Image-Systems"><a href="#39-CuRe-Cultural-Gaps-in-the-Long-Tail-of-Text-to-Image-Systems" class="headerlink" title="[39] CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08071">CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems</a></h3><p><em>Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文分析了文本到图像（T2I）系统在训练数据中对全球南方文化代表性不足的问题，提出了一种名为CuRe的新型、可扩展的评测和评分套件。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的T2I系统主要基于欧美中心的数据训练，忽略了全球南方文化的多样性。为了解决这一问题，研究团队开发了CuRe评测套件，以量化文化代表性。

<p>Method: CuRe利用Wikimedia知识图谱构建了一个分类层级数据集，包含32个子类别的300个文化要素，并通过属性规格的边际效用作为代理人类评分的指标。</p>
<p>Result: 研究显示，CuRe评分与人类对感知相似性、图文对齐性和文化多样性的评分具有强相关性，并在多种T2I系统和视觉语言模型中验证了其有效性。</p>
<p>Conclusion: CuRe为评测和改进T2I系统的文化代表性提供了一种有效工具，相关代码和数据集已开源。</p>
<p>Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is<br>heavily Amero and Euro-centric, underrepresenting the cultures of the Global<br>South. To analyze these biases, we introduce CuRe, a novel and scalable<br>benchmarking and scoring suite for cultural representativeness that leverages<br>the marginal utility of attribute specification to T2I systems as a proxy for<br>human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy<br>built from the crowdsourced Wikimedia knowledge graph, with 300 cultural<br>artifacts across 32 cultural subcategories grouped into six broad cultural axes<br>(food, art, fashion, architecture, celebrations, and people). Our dataset’s<br>categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing<br>their response to increasing the informativeness of text conditioning, enabling<br>fine-grained cultural comparisons. We empirically observe much stronger<br>correlations of our class of scorers to human judgments of perceptual<br>similarity, image-text alignment, and cultural diversity across image encoders<br>(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,<br>Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three<br>variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,<br>and DALL-E 3. The code and dataset is open-sourced and available at<br><a target="_blank" rel="noopener" href="https://aniketrege.github.io/cure/">https://aniketrege.github.io/cure/</a>.</p>
</details>


<h3 id="40-IGraSS-Learning-to-Identify-Infrastructure-Networks-from-Satellite-Imagery-by-Iterative-Graph-constrained-Semantic-Segmentation"><a href="#40-IGraSS-Learning-to-Identify-Infrastructure-Networks-from-Satellite-Imagery-by-Iterative-Graph-constrained-Semantic-Segmentation" class="headerlink" title="[40] IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08137">IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation</a></h3><p><em>Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: IGraSS是一种结合语义分割和图基优化的新型框架，显著提升了渠道网络映射的准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型依赖于大量标注数据，但标注不完整会影响效果。IGraSS旨在利用图级特性（如可达性）优化标注数据。

<p>Method: 融合RGB、NDWI和DEM的语义分割模块与基于图的标注优化模块，迭代优化结果。</p>
<p>Result: IGraSS将不可达渠道片段从18%降至3%，并显著提升渠道识别效果。还可推广至路网等其他基础设施。</p>
<p>Conclusion: IGraSS为噪声标注优化和遥感图像映射提供了一种通用且高效的解决方案。</p>
<p>Abstract: Accurate canal network mapping is essential for water management, including<br>irrigation planning and infrastructure maintenance. State-of-the-art semantic<br>segmentation models for infrastructure mapping, such as roads, rely on large,<br>well-annotated remote sensing datasets. However, incomplete or inadequate<br>ground truth can hinder these learning approaches. Many infrastructure networks<br>have graph-level properties such as reachability to a source (like canals) or<br>connectivity (roads) that can be leveraged to improve these existing ground<br>truth. This paper develops a novel iterative framework IGraSS, combining a<br>semantic segmentation module-incorporating RGB and additional modalities (NDWI,<br>DEM)-with a graph-based ground-truth refinement module. The segmentation module<br>processes satellite imagery patches, while the refinement module operates on<br>the entire data viewing the infrastructure network as a graph. Experiments show<br>that IGraSS reduces unreachable canal segments from around 18% to 3%, and<br>training with refined ground truth significantly improves canal identification.<br>IGraSS serves as a robust framework for both refining noisy ground truth and<br>mapping canal networks from remote sensing imagery. We also demonstrate the<br>effectiveness and generalizability of IGraSS using road networks as an example,<br>applying a different graph-theoretic constraint to complete road networks.</p>
</details>


<h3 id="41-Spectral-Domain-Neural-Reconstruction-for-Passband-FMCW-Radars"><a href="#41-Spectral-Domain-Neural-Reconstruction-for-Passband-FMCW-Radars" class="headerlink" title="[41] Spectral Domain Neural Reconstruction for Passband FMCW Radars"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08163">Spectral Domain Neural Reconstruction for Passband FMCW Radars</a></h3><p><em>Harshvardhan Takawale,Nirupam Roy</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: SpINRv2是一个基于神经网络的框架，用于通过FMCW雷达实现高保真度的体积重建，通过改进解决了高频下的相位混叠和子区间模糊问题。</p>
<details>
  <summary>Details</summary>
Motivation: 高频雷达环境下，相位混叠和子区间模糊问题限制了重建精度，SpINRv2旨在解决这些问题。

<p>Method: 提出一种完全可微的频率域前向模型，结合隐式神经表示（INR），并引入稀疏性和平滑性正则化。</p>
<p>Result: SpINRv2在高频环境下显著优于传统和学习基线方法，成为神经雷达3D成像的新标杆。</p>
<p>Conclusion: SpINRv2通过改进的模型和正则化方法，在高频雷达场景中实现了更精确的体积重建。</p>
<p>Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric<br>reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.<br>Extending our prior work (SpINR), this version introduces enhancements that<br>allow accurate learning under high start frequencies-where phase aliasing and<br>sub-bin ambiguity become prominent. Our core contribution is a fully<br>differentiable frequency-domain forward model that captures the complex radar<br>response using closed-form synthesis, paired with an implicit neural<br>representation (INR) for continuous volumetric scene modeling. Unlike<br>time-domain baselines, SpINRv2 directly supervises the complex frequency<br>spectrum, preserving spectral fidelity while drastically reducing computational<br>overhead. Additionally, we introduce sparsity and smoothness regularization to<br>disambiguate sub-bin ambiguities that arise at fine range resolutions.<br>Experimental results show that SpINRv2 significantly outperforms both classical<br>and learning-based baselines, especially under high-frequency regimes,<br>establishing a new benchmark for neural radar-based 3D imaging.</p>
</details>


<h3 id="42-Surgeon-Style-Fingerprinting-and-Privacy-Risk-Quantification-via-Discrete-Diffusion-Models-in-a-Vision-Language-Action-Framework"><a href="#42-Surgeon-Style-Fingerprinting-and-Privacy-Risk-Quantification-via-Discrete-Diffusion-Models-in-a-Vision-Language-Action-Framework" class="headerlink" title="[42] Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08185">Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework</a></h3><p><em>Huixin Zhan,Jason H. Moore</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出一种结合离散扩散框架与视觉-语言-动作（VLA）管道的个性化手术指纹建模方法，通过学习独特的手势序列平衡性能与隐私风险。</p>
<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生因训练、经验和动作行为差异导致的个性化信号，需要一种方法在建模中兼顾个性化与隐私保护。

<p>Method: 采用离散扩散框架与VLA管道，通过视频、手术意图语言和隐私感知的医生身份嵌入，将手势预测转化为结构化序列去噪任务。</p>
<p>Result: 在JIGSAWS数据集上验证方法能准确重构手势序列并学习独特运动指纹，但更个性化的嵌入会增加身份泄露风险。</p>
<p>Conclusion: 个性化嵌入虽提升性能，但也增加隐私风险，需在手术建模中平衡两者。</p>
<p>Abstract: Surgeons exhibit distinct operating styles due to differences in training,<br>experience, and motor behavior - yet current AI systems often ignore this<br>personalization signal. We propose a novel approach to model fine-grained,<br>surgeon-specific fingerprinting in robotic surgery using a discrete diffusion<br>framework integrated with a vision-language-action (VLA) pipeline. Our method<br>formulates gesture prediction as a structured sequence denoising task,<br>conditioned on multimodal inputs including endoscopic video, surgical intent<br>language, and a privacy-aware embedding of surgeon identity and skill.<br>Personalized surgeon fingerprinting is encoded through natural language prompts<br>using third-party language models, allowing the model to retain individual<br>behavioral style without exposing explicit identity. We evaluate our method on<br>the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture<br>sequences while learning meaningful motion fingerprints unique to each surgeon.<br>To quantify the privacy implications of personalization, we perform membership<br>inference attacks and find that more expressive embeddings improve task<br>performance but simultaneously increase susceptibility to identity leakage.<br>These findings demonstrate that while personalized embeddings improve<br>performance, they also increase vulnerability to identity leakage, revealing<br>the importance of balancing personalization with privacy risk in surgical<br>modeling. Code is available at:<br><a target="_blank" rel="noopener" href="https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting">https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting</a>.</p>
</details>


<h3 id="43-Open-World-Scene-Graph-Generation-using-Vision-Language-Models"><a href="#43-Open-World-Scene-Graph-Generation-using-Vision-Language-Models" class="headerlink" title="[43] Open World Scene Graph Generation using Vision Language Models"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08189">Open World Scene Graph Generation using Vision Language Models</a></h3><p><em>Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一个无需训练、模型无关的框架Open-World SGG，利用预训练的视觉语言模型（VLMs）直接生成场景图，无需额外学习。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的场景图生成（SGG）方法需要数据集特定的监督学习，限制了其在开放世界中的应用。而现有基于VLMs的方法仍需微调。

<p>Method: 将SGG视为零样本结构化推理问题，结合多模态提示、嵌入对齐和轻量级对优化策略。</p>
<p>Result: 在Visual Genome、Open Images V6和Panoptic Scene Graph数据集上的实验表明，预训练VLMs具备无需任务级训练的关系理解能力。</p>
<p>Conclusion: 该方法为开放世界中的场景图生成提供了高效、无需训练的解决方案。</p>
<p>Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and<br>distill their salient pairwise relationships. Most methods depend on<br>dataset-specific supervision to learn the variety of interactions, restricting<br>their usefulness in open-world settings, involving novel objects and&#x2F;or<br>relations. Even methods that leverage large Vision Language Models (VLMs)<br>typically require benchmark-specific fine-tuning. We introduce Open-World SGG,<br>a training-free, efficient, model-agnostic framework that taps directly into<br>the pretrained knowledge of VLMs to produce scene graphs with zero additional<br>learning. Casting SGG as a zero-shot structured-reasoning problem, our method<br>combines multimodal prompting, embedding alignment, and a lightweight<br>pair-refinement strategy, enabling inference over unseen object vocabularies<br>and relation sets. To assess this setting, we formalize an Open-World<br>evaluation protocol that measures performance when no SGG-specific data have<br>been observed either in terms of objects and relations. Experiments on Visual<br>Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate<br>the capacity of pretrained VLMs to perform relational understanding without<br>task-level training.</p>
</details>


<h3 id="44-GIQ-Benchmarking-3D-Geometric-Reasoning-of-Vision-Foundation-Models-with-Simulated-and-Real-Polyhedra"><a href="#44-GIQ-Benchmarking-3D-Geometric-Reasoning-of-Vision-Foundation-Models-with-Simulated-and-Real-Polyhedra" class="headerlink" title="[44] GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08194">GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra</a></h3><p><em>Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: GIQ是一个评估视觉和视觉语言基础模型几何推理能力的基准测试，揭示了当前模型在几何理解上的不足。</p>
<details>
  <summary>Details</summary>
Motivation: 评估模型对几何属性的真实理解能力。

<p>Method: 通过合成和真实图像，以及多种几何形状（如柏拉图立体、阿基米德立体等）进行测试。</p>
<p>Result: 现有模型在3D重建、对称性检测和几何分类等任务中表现不佳。</p>
<p>Conclusion: GIQ为未来的几何智能研究提供了重要基准。</p>
<p>Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)<br>demonstrate impressive results on standard benchmarks, yet their true<br>understanding of geometric properties remains unclear. We introduce GIQ , a<br>comprehensive benchmark specifically designed to evaluate the geometric<br>reasoning capabilities of vision and vision-language foundation models. GIQ<br>comprises synthetic and real-world images of 224 diverse polyhedra - including<br>Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and<br>compound shapes - covering varying levels of complexity and symmetry. Through<br>systematic experiments involving monocular 3D reconstruction, 3D symmetry<br>detection, mental rotation tests, and zero-shot shape classification tasks, we<br>reveal significant shortcomings in current models. State-of-the-art<br>reconstruction algorithms trained on extensive 3D datasets struggle to<br>reconstruct even basic geometric forms accurately. While foundation models<br>effectively detect specific 3D symmetry elements via linear probing, they<br>falter significantly in tasks requiring detailed geometric differentiation,<br>such as mental rotation. Moreover, advanced vision-language assistants exhibit<br>remarkably low accuracy on complex polyhedra, systematically misinterpreting<br>basic properties like face geometry, convexity, and compound structures. GIQ is<br>publicly available, providing a structured platform to highlight and address<br>critical gaps in geometric intelligence, facilitating future progress in<br>robust, geometry-aware representation learning.</p>
</details>


<h3 id="45-A-Comprehensive-Study-of-Decoder-Only-LLMs-for-Text-to-Image-Generation"><a href="#45-A-Comprehensive-Study-of-Decoder-Only-LLMs-for-Text-to-Image-Generation" class="headerlink" title="[45] A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08210">A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation</a></h3><p><em>Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 研究了使用现代仅解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现层归一化平均嵌入优于T5基线。</p>
<details>
  <summary>Details</summary>
Motivation: 当前许多文本到图像模型仍使用过时的T5和CLIP作为文本编码器，希望通过现代LLM提升性能。

<p>Method: 构建标准化训练和评估流程，训练27个模型，分析LLM嵌入提取方法、变体和大小对生成的影响。</p>
<p>Result: 使用层归一化平均嵌入优于传统最后层嵌入，LLM在复杂提示对齐和视觉语言推理上表现更好。</p>
<p>Conclusion: 现代LLM作为文本编码器在文本到图像生成中表现优于传统方法。</p>
<p>Abstract: Both text-to-image generation and large language models (LLMs) have made<br>significant advancements. However, many text-to-image models still employ the<br>somewhat outdated T5 and CLIP as their text encoders. In this work, we<br>investigate the effectiveness of using modern decoder-only LLMs as text<br>encoders for text-to-image diffusion models. We build a standardized training<br>and evaluation pipeline that allows us to isolate and evaluate the effect of<br>different text embeddings. We train a total of 27 text-to-image models with 12<br>different text encoders to analyze the critical aspects of LLMs that could<br>impact text-to-image generation, including the approaches to extract<br>embeddings, different LLMs variants, and model sizes. Our experiments reveal<br>that the de facto way of using last-layer embeddings as conditioning leads to<br>inferior performance. Instead, we explore embeddings from various layers and<br>find that using layer-normalized averaging across all layers significantly<br>improves alignment with complex prompts. Most LLMs with this conditioning<br>outperform the baseline T5 model, showing enhanced performance in advanced<br>visio-linguistic reasoning skills.</p>
</details>


<h3 id="46-Using-Satellite-Images-And-Self-supervised-Machine-Learning-Networks-To-Detect-Water-Hidden-Under-Vegetation"><a href="#46-Using-Satellite-Images-And-Self-supervised-Machine-Learning-Networks-To-Detect-Water-Hidden-Under-Vegetation" class="headerlink" title="[46] Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08214">Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation</a></h3><p><em>Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出一种自监督学习方法，通过深度聚类和负采样训练模型，无需人工标注即可分割雷达卫星图像中的水域和陆地，并采用集成模型提高性能。</p>
<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量人工标注的高分辨率雷达卫星图像，成本高且耗时。自监督学习可以解决这一问题。

<p>Method: 结合深度聚类和负采样训练自监督模型，并采用集成模型降低方差。</p>
<p>Result: 与全监督模型相比，自监督集成模型的IOU指标提高了0.02。</p>
<p>Conclusion: 自监督学习方法在湿地监测中具有潜力，能显著减少对人工标注的依赖。</p>
<p>Abstract: In recent years the wide availability of high-resolution radar satellite<br>images along with the advancement of computer vision models have enabled the<br>remote monitoring of the surface area of wetlands. However, these models<br>require large amounts of manually annotated satellite images, which are slow<br>and expensive to produce. To overcome this problem, self-supervised training<br>methods have been deployed to train models without using annotated data. In<br>this paper we use a combination of deep clustering and negative sampling to<br>train a model to segment radar satellite images into areas that separate water<br>from land without the use of any manual annotations. Furthermore, we implement<br>an ensemble version of the model to reduce variance and improve performance.<br>Compared to a single fully-supervised model using the same architecture, our<br>ensemble of self-supervised models achieves a 0.02 improvement in the<br>Intersection Over Union metric over our test dataset.</p>
</details>


<h3 id="47-Jamais-Vu-Exposing-the-Generalization-Gap-in-Supervised-Semantic-Correspondence"><a href="#47-Jamais-Vu-Exposing-the-Generalization-Gap-in-Supervised-Semantic-Correspondence" class="headerlink" title="[47] Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08220">Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence</a></h3><p><em>Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出一种通过单目深度估计将2D关键点提升到3D空间的新方法，学习密集对应关系，并在未见关键点上显著优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 现有监督语义对应方法泛化能力有限，仅稀疏标注的关键点难以学习密集对应，需改进。

<p>Method: 利用单目深度估计将2D关键点映射到3D规范空间，构建连续流形，无需显式3D监督或相机标注。</p>
<p>Result: 在未见关键点上显著优于监督基线，且无监督方法在跨数据集泛化中表现更好。</p>
<p>Conclusion: 提出的方法能学习更具鲁棒性的对应关系，展示了无监督方法在泛化中的潜力。</p>
<p>Abstract: Semantic correspondence (SC) aims to establish semantically meaningful<br>matches across different instances of an object category. We illustrate how<br>recent supervised SC methods remain limited in their ability to generalize<br>beyond sparsely annotated training keypoints, effectively acting as keypoint<br>detectors. To address this, we propose a novel approach for learning dense<br>correspondences by lifting 2D keypoints into a canonical 3D space using<br>monocular depth estimation. Our method constructs a continuous canonical<br>manifold that captures object geometry without requiring explicit 3D<br>supervision or camera annotations. Additionally, we introduce SPair-U, an<br>extension of SPair-71k with novel keypoint annotations, to better assess<br>generalization. Experiments not only demonstrate that our model significantly<br>outperforms supervised baselines on unseen keypoints, highlighting its<br>effectiveness in learning robust correspondences, but that unsupervised<br>baselines outperform supervised counterparts when generalized across different<br>datasets.</p>
</details>


<h3 id="48-A-Good-CREPE-needs-more-than-just-Sugar-Investigating-Biases-in-Compositional-Vision-Language-Benchmarks"><a href="#48-A-Good-CREPE-needs-more-than-just-Sugar-Investigating-Biases-in-Compositional-Vision-Language-Benchmarks" class="headerlink" title="[48] A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08227">A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks</a></h3><p><em>Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文研究了17个常用于评估视觉-语言模型（VLM）组合理解能力的基准（如SugarCREPE、VALSE），揭示了其中普遍存在的设计偏差，并提出改进建议。</p>
<details>
  <summary>Details</summary>
Motivation: 研究动机在于发现现有基准在设计上的不足（如数据来源和负样本构造方式），导致其无法有效衡量模型的组合理解能力。

<p>Method: 方法包括分析基准的设计选择（如数据源和负样本构造），并使用简单启发式方法（如标记长度、语言模型对数似然）与CLIP模型对比。</p>
<p>Result: 结果表明，基准因设计不对称性（正负样本分布不均）易被简单启发式攻击，无法有效测试组合理解能力。</p>
<p>Conclusion: 结论是需改进基准设计以减少偏差，并提出了一些关键建议。</p>
<p>Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for<br>measuring compositional understanding capabilities of vision-language models<br>(VLMs). We scrutinize design choices in their construction, including data<br>source (e.g. MS-COCO) and curation procedures (e.g. constructing negative<br>images&#x2F;captions), uncovering several inherent biases across most benchmarks. We<br>find that blind heuristics (e.g. token-length, log-likelihood under a language<br>model) perform on par with CLIP models, indicating that these benchmarks do not<br>effectively measure compositional understanding. We demonstrate that the<br>underlying factor is a distribution asymmetry between positive and negative<br>images&#x2F;captions, induced by the benchmark construction procedures. To mitigate<br>these issues, we provide a few key recommendations for constructing more robust<br>vision-language compositional understanding benchmarks, that would be less<br>prone to such simple attacks.</p>
</details>


<h3 id="49-Highly-Compressed-Tokenizer-Can-Generate-Without-Training"><a href="#49-Highly-Compressed-Tokenizer-Can-Generate-Without-Training" class="headerlink" title="[49] Highly Compressed Tokenizer Can Generate Without Training"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08257">Highly Compressed Tokenizer Can Generate Without Training</a></h3><p><em>L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文探讨了1D图像标记化器在高度压缩图像为一维序列时的表现，并通过启发式操作展示了其编辑和生成能力。</p>
<details>
  <summary>Details</summary>
Motivation: 研究1D图像标记化器的高压缩特性及其在图像编辑和生成中的潜力。

<p>Method: 使用基于梯度的测试时优化和即插即用的损失函数（如重建或CLIP相似性）构建图像生成流程。</p>
<p>Result: 1D标记化器能够通过简单的操作（如复制和替换标记）实现细粒度图像编辑，并生成多样且真实的样本。</p>
<p>Conclusion: 1D标记化器的潜在空间表达能力强大，无需训练生成模型即可实现高效的图像编辑和生成。</p>
<p>Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged<br>tokens. In contrast, so-called 1D image tokenizers represent images as highly<br>compressed one-dimensional sequences of as few as 32 discrete tokens. We find<br>that the high degree of compression achieved by a 1D tokenizer with vector<br>quantization enables image editing and generative capabilities through<br>heuristic manipulation of tokens, demonstrating that even very crude<br>manipulations – such as copying and replacing tokens between latent<br>representations of images – enable fine-grained image editing by transferring<br>appearance and semantic attributes. Motivated by the expressivity of the 1D<br>tokenizer’s latent space, we construct an image generation pipeline leveraging<br>gradient-based test-time optimization of tokens with plug-and-play loss<br>functions such as reconstruction or CLIP similarity. Our approach is<br>demonstrated for inpainting and text-guided image editing use cases, and can<br>generate diverse and realistic samples without requiring training of any<br>generative model.</p>
</details>


<h3 id="50-Seeing-Voices-Generating-A-Roll-Video-from-Audio-with-Mirage"><a href="#50-Seeing-Voices-Generating-A-Roll-Video-from-Audio-with-Mirage" class="headerlink" title="[50] Seeing Voices: Generating A-Roll Video from Audio with Mirage"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08279">Seeing Voices: Generating A-Roll Video from Audio with Mirage</a></h3><p><em>Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Mirage是一个音频到视频的基础模型，能够根据音频输入生成逼真且富有表现力的视频。</p>
<details>
  <summary>Details</summary>
Motivation: 视频的力量在于音频与视觉的和谐结合，但现有方法要么忽略声音，要么局限于特定领域（如重新配音）。

<p>Method: 介绍Mirage，利用自注意力机制的统一训练方法，从零开始或基于现有权重训练音频到视频生成模型。</p>
<p>Result: Mirage生成的视频在主观质量上优于其他方法，并能与语音合成技术结合，生成多模态视频。</p>
<p>Conclusion: Mirage为音频到视频生成提供了一种通用方法，效果优于现有技术。</p>
<p>Abstract: From professional filmmaking to user-generated content, creators and<br>consumers have long recognized that the power of video depends on the<br>harmonious integration of what we hear (the video’s audio track) with what we<br>see (the video’s image sequence). Current approaches to video generation either<br>ignore sound to focus on general-purpose but silent image sequence generation<br>or address both visual and audio elements but focus on restricted application<br>domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation<br>model that excels at generating realistic, expressive output imagery from<br>scratch given an audio input. When integrated with existing methods for speech<br>synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal<br>video. When trained on audio-video footage of people talking (A-roll) and<br>conditioned on audio containing speech, Mirage generates video of people<br>delivering a believable interpretation of the performance implicit in input<br>audio. Our central technical contribution is a unified method for training<br>self-attention-based audio-to-video generation models, either from scratch or<br>given existing weights. This methodology allows Mirage to retain generality as<br>an approach to audio-to-video generation while producing outputs of superior<br>subjective quality to methods that incorporate audio-specific architectures or<br>loss components specific to people, speech, or details of how images or audio<br>are captured. We encourage readers to watch and listen to the results of Mirage<br>for themselves (see paper and comments for links).</p>
</details>


<h3 id="51-SEMA-a-Scalable-and-Efficient-Mamba-like-Attention-via-Token-Localization-and-Averaging"><a href="#51-SEMA-a-Scalable-and-Efficient-Mamba-like-Attention-via-Token-Localization-and-Averaging" class="headerlink" title="[51] SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08297">SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging</a></h3><p><em>Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了广义注意力的数学定义，并设计了SEMA方法，解决了注意力的二次计算复杂性和分散问题，在图像分类任务中优于线性注意力和Mamba模型。</p>
<details>
  <summary>Details</summary>
Motivation: 解决传统注意力机制在计算复杂度高和线性注意力无法聚焦的问题，提出了广义注意力的概念。

<p>Method: 利用令牌定位和算术平均设计SEMA方法，避免分散并保持聚焦。</p>
<p>Result: 在Imagenet-1k上，SEMA在更大规模的图像上表现优于线性注意力和Mamba模型。</p>
<p>Conclusion: SEMA是一种高效且可扩展的注意力替代方案，适用于视觉任务。</p>
<p>Abstract: Attention is the critical component of a transformer. Yet the quadratic<br>computational complexity of vanilla full attention in the input size and the<br>inability of its linear attention variant to focus have been challenges for<br>computer vision tasks. We provide a mathematical definition of generalized<br>attention and formulate both vanilla softmax attention and linear attention<br>within the general framework. We prove that generalized attention disperses,<br>that is, as the number of keys tends to infinity, the query assigns equal<br>weights to all keys. Motivated by the dispersion property and recent<br>development of Mamba form of attention, we design Scalable and Efficient Mamba<br>like Attention (SEMA) which utilizes token localization to avoid dispersion and<br>maintain focusing, complemented by theoretically consistent arithmetic<br>averaging to capture global aspect of attention. We support our approach on<br>Imagenet-1k where classification results show that SEMA is a scalable and<br>effective alternative beyond linear attention, outperforming recent vision<br>Mamba models on increasingly larger scales of images at similar model parameter<br>sizes.</p>
</details>


<h3 id="52-OpenRR-1k-A-Scalable-Dataset-for-Real-World-Reflection-Removal"><a href="#52-OpenRR-1k-A-Scalable-Dataset-for-Real-World-Reflection-Removal" class="headerlink" title="[52] OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08299">OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal</a></h3><p><em>Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种新的反射数据集收集方法，构建了高质量、多样化的OpenRR-1k数据集，提升了反射去除技术的鲁棒性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有反射去除技术因缺乏高质量的真实数据集而受限。

<p>Method: 提出了一种便捷、低成本、可扩展的数据收集范式，构建了OpenRR-1k数据集。</p>
<p>Result: 数据集在真实环境中显著提升了反射去除方法的鲁棒性。</p>
<p>Conclusion: OpenRR-1k数据集为反射去除研究提供了有效的支持，推动了技术进步。</p>
<p>Abstract: Reflection removal technology plays a crucial role in photography and<br>computer vision applications. However, existing techniques are hindered by the<br>lack of high-quality in-the-wild datasets. In this paper, we propose a novel<br>paradigm for collecting reflection datasets from a fresh perspective. Our<br>approach is convenient, cost-effective, and scalable, while ensuring that the<br>collected data pairs are of high quality, perfectly aligned, and represent<br>natural and diverse scenarios. Following this paradigm, we collect a<br>Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which<br>contains 1,000 high-quality transmission-reflection image pairs collected in<br>the wild. Through the analysis of several reflection removal methods and<br>benchmark evaluation experiments on our dataset, we demonstrate its<br>effectiveness in improving robustness in challenging real-world environments.<br>Our dataset is available at <a target="_blank" rel="noopener" href="https://github.com/caijie0620/OpenRR-1k">https://github.com/caijie0620/OpenRR-1k</a>.</p>
</details>


<h3 id="53-Hyperspectral-Image-Classification-via-Transformer-based-Spectral-Spatial-Attention-Decoupling-and-Adaptive-Gating"><a href="#53-Hyperspectral-Image-Classification-via-Transformer-based-Spectral-Spatial-Attention-Decoupling-and-Adaptive-Gating" class="headerlink" title="[53] Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08324">Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating</a></h3><p><em>Guandong Li,Mengxia Ye</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: STNet是一种新型网络架构，通过空间-光谱Transformer模块的创新设计，有效解决高光谱图像分类中的过拟合和泛化能力问题。</p>
<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类中存在高维数据、地物稀疏分布和光谱冗余等问题，导致分类过拟合和泛化能力受限。

<p>Method: 提出STNet，其核心是通过空间-光谱Transformer模块的解耦设计和双门控机制，实现高效的空间与光谱信息融合。</p>
<p>Result: STNet在IN、UP和KSC数据集上表现优异，优于主流高光谱分类方法，且未增加网络深度或宽度。</p>
<p>Conclusion: STNet通过创新的模块设计和门控机制，显著提升了特征提取和融合能力，减少了小样本和高噪声场景下的过拟合风险。</p>
<p>Abstract: Deep neural networks face several challenges in hyperspectral image<br>classification, including high-dimensional data, sparse distribution of ground<br>objects, and spectral redundancy, which often lead to classification<br>overfitting and limited generalization capability. To more effectively extract<br>and fuse spatial context with fine spectral information in hyperspectral image<br>(HSI) classification, this paper proposes a novel network architecture called<br>STNet. The core advantage of STNet stems from the dual innovative design of its<br>Spatial-Spectral Transformer module: first, the fundamental explicit decoupling<br>of spatial and spectral attention ensures targeted capture of key information<br>in HSI; second, two functionally distinct gating mechanisms perform intelligent<br>regulation at both the fusion level of attention flows (adaptive attention<br>fusion gating) and the internal level of feature transformation (GFFN). This<br>characteristic demonstrates superior feature extraction and fusion capabilities<br>compared to traditional convolutional neural networks, while reducing<br>overfitting risks in small-sample and high-noise scenarios. STNet enhances<br>model representation capability without increasing network depth or width. The<br>proposed method demonstrates superior performance on IN, UP, and KSC datasets,<br>outperforming mainstream hyperspectral image classification approaches.</p>
</details>


<h3 id="54-Locating-Tennis-Ball-Impact-on-the-Racket-in-Real-Time-Using-an-Event-Camera"><a href="#54-Locating-Tennis-Ball-Impact-on-the-Racket-in-Real-Time-Using-an-Event-Camera" class="headerlink" title="[54] Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08327">Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera</a></h3><p><em>Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出一种使用事件相机实时定位网球击球位置的方法，解决了高速相机内存消耗大和人工数字化耗时的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 在网球等球拍运动中，击球位置定位对分析球员和装备特性至关重要，但高速相机内存消耗大且人工处理耗时易错。

<p>Method: 利用事件相机高效捕捉亮度变化，结合传统计算机视觉技术和原创的事件处理方法（PATS），通过三个识别步骤实现实时定位。</p>
<p>Result: 实验结果在测量网球运动员性能的可接受范围内，计算时间足够短，适用于实时应用。</p>
<p>Conclusion: 该方法能高效实时定位击球位置，为球员表现分析提供支持。</p>
<p>Abstract: In racket sports, such as tennis, locating the ball’s position at impact is<br>important in clarifying player and equipment characteristics, thereby aiding in<br>personalized equipment design. High-speed cameras are used to measure the<br>impact location; however, their excessive memory consumption limits prolonged<br>scene capture, and manual digitization for position detection is time-consuming<br>and prone to human error. These limitations make it difficult to effectively<br>capture the entire playing scene, hindering the ability to analyze the player’s<br>performance. We propose a method for locating the tennis ball impact on the<br>racket in real time using an event camera. Event cameras efficiently measure<br>brightness changes (called &#96;events’) with microsecond accuracy under high-speed<br>motion while using lower memory consumption. These cameras enable users to<br>continuously monitor their performance over extended periods. Our method<br>consists of three identification steps: time range of swing, timing at impact,<br>and contours of ball and racket. Conventional computer vision techniques are<br>utilized along with an original event-based processing to detect the timing at<br>impact (PATS: the amount of polarity asymmetry in time symmetry). The results<br>of the experiments were within the permissible range for measuring tennis<br>players’ performance. Moreover, the computation time was sufficiently short for<br>real-time applications.</p>
</details>


<h3 id="55-How-Much-To-Guide-Revisiting-Adaptive-Guidance-in-Classifier-Free-Guidance-Text-to-Vision-Diffusion-Models"><a href="#55-How-Much-To-Guide-Revisiting-Adaptive-Guidance-in-Classifier-Free-Guidance-Text-to-Vision-Diffusion-Models" class="headerlink" title="[55] How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08351">How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models</a></h3><p><em>Huixuan Zhang,Junzhe Zhang,Xiaojun Wan</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种名为Step AG的自适应引导策略，通过仅在前几个去噪步骤中使用分类器自由引导，显著提高了生成速度（20%至30%），同时保持了图像质量和文本对齐性。</p>
<details>
  <summary>Details</summary>
Motivation: 由于分类器自由引导方法在文本到视觉生成扩散模型中需要双倍的计算步骤，导致成本显著增加。尽管已有研究提出自适应引导概念，但缺乏充分分析和实用性。本文旨在提出一种通用的自适应引导策略以解决这一问题。

<p>Method: 提出Step AG策略，仅在前几个去噪步骤中应用分类器自由引导，从而减少计算成本。</p>
<p>Result: 实验结果表明，Step AG在保持图像质量和文本对齐性的同时，平均提高了20%至30%的速度，且适用于不同模型和设置。</p>
<p>Conclusion: Step AG是一种简单且通用的自适应引导策略，有效提升生成效率，适用于广泛的扩散模型。</p>
<p>Abstract: With the rapid development of text-to-vision generation diffusion models,<br>classifier-free guidance has emerged as the most prevalent method for<br>conditioning. However, this approach inherently requires twice as many steps<br>for model forwarding compared to unconditional generation, resulting in<br>significantly higher costs. While previous study has introduced the concept of<br>adaptive guidance, it lacks solid analysis and empirical results, making<br>previous method unable to be applied to general diffusion models. In this work,<br>we present another perspective of applying adaptive guidance and propose Step<br>AG, which is a simple, universally applicable adaptive guidance strategy. Our<br>evaluations focus on both image quality and image-text alignment. whose results<br>indicate that restricting classifier-free guidance to the first several<br>denoising steps is sufficient for generating high-quality, well-conditioned<br>images, achieving an average speedup of 20% to 30%. Such improvement is<br>consistent across different settings such as inference steps, and various<br>models including video generation models, highlighting the superiority of our<br>method.</p>
</details>


<h3 id="56-MedMoE-Modality-Specialized-Mixture-of-Experts-for-Medical-Vision-Language-Understanding"><a href="#56-MedMoE-Modality-Specialized-Mixture-of-Experts-for-Medical-Vision-Language-Understanding" class="headerlink" title="[56] MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08356">MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding</a></h3><p><em>Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: MedMoE是一种动态适应医学成像模态的视觉语言处理框架，通过MoE模块和多尺度特征提取优化诊断性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言框架采用统一的局部特征提取策略，忽略了不同模态的特定需求，因此需要一种适应多模态的动态方法。

<p>Method: MedMoE基于Swin Transformer主干网，利用MoE模块（根据报告类型路由特征）和多尺度专家分支提取模态特定的视觉语义。</p>
<p>Result: 在多种医学基准测试中，MedMoE提升了跨模态的对齐和检索性能。</p>
<p>Conclusion: MedMoE证明了模态专用视觉表征在临床视觉语言系统中的价值，无需推断时的模态特定监督。</p>
<p>Abstract: Different medical imaging modalities capture diagnostic information at<br>varying spatial resolutions, from coarse global patterns to fine-grained<br>localized structures. However, most existing vision-language frameworks in the<br>medical domain apply a uniform strategy for local feature extraction,<br>overlooking the modality-specific demands. In this work, we present MedMoE, a<br>modular and extensible vision-language processing framework that dynamically<br>adapts visual representation based on the diagnostic context. MedMoE<br>incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,<br>which routes multi-scale image features through specialized expert branches<br>trained to capture modality-specific visual semantics. These experts operate<br>over feature pyramids derived from a Swin Transformer backbone, enabling<br>spatially adaptive attention to clinically relevant regions. This framework<br>produces localized visual representations aligned with textual descriptions,<br>without requiring modality-specific supervision at inference. Empirical results<br>on diverse medical benchmarks demonstrate that MedMoE improves alignment and<br>retrieval performance across imaging modalities, underscoring the value of<br>modality-specialized visual representations in clinical vision-language<br>systems.</p>
</details>


<h3 id="57-SECOND-Mitigating-Perceptual-Hallucination-in-Vision-Language-Models-via-Selective-and-Contrastive-Decoding"><a href="#57-SECOND-Mitigating-Perceptual-Hallucination-in-Vision-Language-Models-via-Selective-and-Contrastive-Decoding" class="headerlink" title="[57] SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08391">SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding</a></h3><p><em>Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了SECOND方法，通过选择性对比解码解决VLMs中的物体幻觉问题，提升视觉理解准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 现有VLMs因物体幻觉问题受限，需要更精准的视觉理解方法。

<p>Method: SECOND方法以对象为中心，逐步选择和整合多尺度视觉信息，并通过对比减少幻觉。</p>
<p>Result: SECOND显著减少幻觉，在多基准测试中表现优异。</p>
<p>Conclusion: 多尺度应用在VLMs中潜力巨大，SECOND方法优于现有技术。</p>
<p>Abstract: Despite significant advancements in Vision-Language Models (VLMs), the<br>performance of existing VLMs remains hindered by object hallucination, a<br>critical challenge to achieving accurate visual understanding. To address this<br>issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach<br>that enables VLMs to effectively leverage multi-scale visual information with<br>an object-centric manner, closely aligning with human visual perception. SECOND<br>progressively selects and integrates multi-scale visual information,<br>facilitating a more precise interpretation of images. By contrasting these<br>visual information iteratively, SECOND significantly reduces perceptual<br>hallucinations and outperforms a wide range of benchmarks. Our theoretical<br>analysis and experiments highlight the largely unexplored potential of<br>multi-scale application in VLMs, showing that prioritizing and contrasting<br>across scales outperforms existing methods.</p>
</details>


<h3 id="58-RadioDUN-A-Physics-Inspired-Deep-Unfolding-Network-for-Radio-Map-Estimation"><a href="#58-RadioDUN-A-Physics-Inspired-Deep-Unfolding-Network-for-Radio-Map-Estimation" class="headerlink" title="[58] RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08418">RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation</a></h3><p><em>Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出RadioDUN方法，通过稀疏信号恢复问题估计密集无线电地图，结合物理传播模型和动态重加权模块，提升估计性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以结合无线电地图的物理特性，导致稀疏样本的密集地图估计效果不佳。

<p>Method: 将无线电地图估计转化为稀疏信号恢复问题，结合物理传播模型分解优化子问题，提出RadioDUN网络和动态重加权模块，并设计阴影损失。</p>
<p>Result: 实验表明，RadioDUN优于现有方法。</p>
<p>Conclusion: 结合物理特性和深度学习优化，RadioDUN能有效提升无线电地图估计性能，未来将公开代码。</p>
<p>Abstract: The radio map represents the spatial distribution of spectrum resources<br>within a region, supporting efficient resource allocation and interference<br>mitigation. However, it is difficult to construct a dense radio map as a<br>limited number of samples can be measured in practical scenarios. While<br>existing works have used deep learning to estimate dense radio maps from sparse<br>samples, they are hard to integrate with the physical characteristics of the<br>radio map. To address this challenge, we cast radio map estimation as the<br>sparse signal recovery problem. A physical propagation model is further<br>incorporated to decompose the problem into multiple factor optimization<br>sub-problems, thereby reducing recovery complexity. Inspired by the existing<br>compressive sensing methods, we propose the Radio Deep Unfolding Network<br>(RadioDUN) to unfold the optimization process, achieving adaptive parameter<br>adjusting and prior fitting in a learnable manner. To account for the radio<br>propagation characteristics, we develop a dynamic reweighting module (DRM) to<br>adaptively model the importance of each factor for the radio map. Inspired by<br>the shadowing factor in the physical propagation model, we integrate<br>obstacle-related factors to express the obstacle-induced signal stochastic<br>decay. The shadowing loss is further designed to constrain the factor<br>prediction and act as a supplementary supervised objective, which enhances the<br>performance of RadioDUN. Extensive experiments have been conducted to<br>demonstrate that the proposed method outperforms the state-of-the-art methods.<br>Our code will be made publicly available upon publication.</p>
</details>


<h3 id="59-Better-Reasoning-with-Less-Data-Enhancing-VLMs-Through-Unified-Modality-Scoring"><a href="#59-Better-Reasoning-with-Less-Data-Enhancing-VLMs-Through-Unified-Modality-Scoring" class="headerlink" title="[59] Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08429">Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring</a></h3><p><em>Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了SCALE方法，用于解决视觉语言模型（VLM）中数据质量和对齐问题，通过跨模态评估框架提升数据选择效果。</p>
<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的性能依赖于高质量数据集，但现有数据集存在图像与文本不对齐和文本模糊等问题，限制了模型表现。

<p>Method: 提出SCALE管道，集成跨模态评估框架，通过生成任务特定描述并评估对齐、清晰度等指标来选择高质量数据。</p>
<p>Result: 研究发现现有单模态评估方法低估了对某些任务重要的样本，并证明适当生成的图像描述有助于统一多模态任务。</p>
<p>Conclusion: SCALE通过综合评估数据质量和对齐性，为视觉语言模型的指令调优提供了更高质量的数据选择方法。</p>
<p>Abstract: The application of visual instruction tuning and other post-training<br>techniques has significantly enhanced the capabilities of Large Language Models<br>(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with<br>more comprehensive visual language datasets. However, the effectiveness of VLMs<br>is highly dependent on large-scale, high-quality datasets that ensure precise<br>recognition and accurate reasoning. Two key challenges hinder progress: (1)<br>noisy alignments between images and the corresponding text, which leads to<br>misinterpretation, and (2) ambiguous or misleading text, which obscures visual<br>content. To address these challenges, we propose SCALE (Single modality data<br>quality and Cross modality Alignment Evaluation), a novel quality-driven data<br>selection pipeline for VLM instruction tuning datasets. Specifically, SCALE<br>integrates a cross-modality assessment framework that first assigns each data<br>entry to its appropriate vision-language task, generates general and<br>task-specific captions (covering scenes, objects, style, etc.), and evaluates<br>the alignment, clarity, task rarity, text coherence, and image clarity of each<br>entry based on the generated captions. We reveal that: (1) current unimodal<br>quality assessment methods evaluate one modality while overlooking the rest,<br>which can underestimate samples essential for specific tasks and discard the<br>lower-quality instances that help build model robustness; and (2) appropriately<br>generated image captions provide an efficient way to transfer the image-text<br>multimodal task into a unified text modality.</p>
</details>


<h3 id="60-Enhancing-Motion-Dynamics-of-Image-to-Video-Models-via-Adaptive-Low-Pass-Guidance"><a href="#60-Enhancing-Motion-Dynamics-of-Image-to-Video-Models-via-Adaptive-Low-Pass-Guidance" class="headerlink" title="[60] Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08456">Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance</a></h3><p><em>June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种自适应低通引导（ALG）方法，用于改善图像到视频（I2V）生成中因高频细节导致视频动态性不足的问题，显著提升了动态性而不损失画质。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法通过微调预训练的文本到视频（T2V）模型支持I2V生成，但会导致生成的视频动态性不足，原因是输入图像的高频细节过早影响了采样过程。

<p>Method: 提出自适应低通引导（ALG），通过在去噪早期阶段自适应调节输入图像的频率内容（低通滤波），避免模型过度拟合静态外观。</p>
<p>Result: 实验表明，ALG显著提升了生成视频的动态性（VBench-I2V测试中动态性平均提升36%），同时保持了图像质量和文本对齐。</p>
<p>Conclusion: ALG是一种简单有效的改进方法，解决了I2V生成中视频动态性不足的问题，且无需牺牲画质。</p>
<p>Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in<br>producing high-quality, dynamic videos. To improve the visual controllability,<br>recent works have considered fine-tuning pre-trained T2V models to support<br>image-to-video (I2V) generation. However, such adaptation frequently suppresses<br>motion dynamics of generated outputs, resulting in more static videos compared<br>to their T2V counterparts. In this work, we analyze this phenomenon and<br>identify that it stems from the premature exposure to high-frequency details in<br>the input image, which biases the sampling process toward a shortcut trajectory<br>that overfits to the static appearance of the reference image. To address this,<br>we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model<br>sampling procedure to generate more dynamic videos without compromising<br>per-frame image quality. Specifically, ALG adaptively modulates the frequency<br>content of the conditioning image by applying low-pass filtering at the early<br>stage of denoising. Extensive experiments demonstrate that ALG significantly<br>improves the temporal dynamics of generated videos, while preserving image<br>fidelity and text alignment. Especially, under VBench-I2V test suite, ALG<br>achieves an average improvement of 36% in dynamic degree without a significant<br>drop in video quality or image fidelity.</p>
</details>


<h3 id="61-MARMOT-Masked-Autoencoder-for-Modeling-Transient-Imaging"><a href="#61-MARMOT-Masked-Autoencoder-for-Modeling-Transient-Imaging" class="headerlink" title="[61] MARMOT: Masked Autoencoder for Modeling Transient Imaging"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08470">MARMOT: Masked Autoencoder for Modeling Transient Imaging</a></h3><p><em>Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了MARMOT，一种基于掩码自编码器的预训练模型，用于非视距（NLOS）瞬态成像任务，通过自监督学习从大规模数据中提取特征，并在下游任务中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 预训练模型在语言和视觉领域取得了显著成功，但在瞬态成像领域尚未广泛应用。本研究旨在将预训练范式引入NLOS瞬态成像，以弥补现有方法在数据驱动先验学习上的不足。

<p>Method: 作者提出了MARMOT模型，采用基于Transformer的编码器-解码器结构，通过扫描模式掩码（SPM）自监督学习部分掩码的瞬态数据特征，并预测完整测量结果。模型在合成的500K 3D模型数据集TransVerse上进行预训练。</p>
<p>Result: 通过定量和定性实验对比，MARMOT在NLOS瞬态成像任务中表现出高效性，优于现有方法。</p>
<p>Conclusion: MARMOT通过自监督预训练和特征迁移，为NLOS瞬态成像任务提供了一种高效解决方案，验证了预训练模型在该领域的潜力。</p>
<p>Abstract: Pretrained models have demonstrated impressive success in many modalities<br>such as language and vision. Recent works facilitate the pretraining paradigm<br>in imaging research. Transients are a novel modality, which are captured for an<br>object as photon counts versus arrival times using a precisely time-resolved<br>sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of<br>hidden objects are measured beyond the sensor’s direct line of sight. Using<br>NLOS transients, the majority of previous works optimize volume density or<br>surfaces to reconstruct the hidden objects and do not transfer priors learned<br>from datasets. In this work, we present a masked autoencoder for modeling<br>transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a<br>self-supervised model pretrianed on massive and diverse NLOS transient<br>datasets. Using a Transformer-based encoder-decoder, MARMOT learns features<br>from partially masked transients via a scanning pattern mask (SPM), where the<br>unmasked subset is functionally equivalent to arbitrary sampling, and predicts<br>full measurements. Pretrained on TransVerse-a synthesized transient dataset of<br>500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature<br>transfer or decoder finetuning. Comprehensive experiments are carried out in<br>comparisons with state-of-the-art methods. Quantitative and qualitative results<br>demonstrate the efficiency of our MARMOT.</p>
</details>


<h3 id="62-Context-aware-TFL-A-Universal-Context-aware-Contrastive-Learning-Framework-for-Temporal-Forgery-Localization"><a href="#62-Context-aware-TFL-A-Universal-Context-aware-Contrastive-Learning-Framework-for-Temporal-Forgery-Localization" class="headerlink" title="[62] Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08493">Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization</a></h3><p><em>Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种通用的上下文感知对比学习框架（UniCaClF），用于解决多媒体取证领域中的时间伪造定位（TFL）问题，通过在异常检测中发现和识别伪造片段。</p>
<details>
  <summary>Details</summary>
Motivation: 当前多媒体取证研究主要集中于检测伪造音视频内容，但忽略了部分视频片段被篡改的情况。时间伪造定位（TFL）在实际应用中更具挑战性。

<p>Method: 提出了一种基于监督对比学习的框架，结合上下文感知感知层和自适应上下文更新器，构建上下文感知对比目标，增强伪造片段的特征区分性。</p>
<p>Result: 在五个公开数据集上的实验结果表明，UniCaClF显著优于现有的竞争算法。</p>
<p>Conclusion: UniCaClF为时间伪造定位提供了一种高效且通用的解决方案，能够精确定位伪造片段。</p>
<p>Abstract: Most research efforts in the multimedia forensics domain have focused on<br>detecting forgery audio-visual content and reached sound achievements. However,<br>these works only consider deepfake detection as a classification task and<br>ignore the case where partial segments of the video are tampered with. Temporal<br>forgery localization (TFL) of small fake audio-visual clips embedded in real<br>videos is still challenging and more in line with realistic application<br>scenarios. To resolve this issue, we propose a universal context-aware<br>contrastive learning framework (UniCaCLF) for TFL. Our approach leverages<br>supervised contrastive learning to discover and identify forged instants by<br>means of anomaly detection, allowing for the precise localization of temporal<br>forged segments. To this end, we propose a novel context-aware perception layer<br>that utilizes a heterogeneous activation operation and an adaptive context<br>updater to construct a context-aware contrastive objective, which enhances the<br>discriminability of forged instant features by contrasting them with genuine<br>instant features in terms of their distances to the global context. An<br>efficient context-aware contrastive coding is introduced to further push the<br>limit of instant feature distinguishability between genuine and forged instants<br>in a supervised sample-by-sample manner, suppressing the cross-sample influence<br>to improve temporal forgery localization performance. Extensive experimental<br>results over five public datasets demonstrate that our proposed UniCaCLF<br>significantly outperforms the state-of-the-art competing algorithms.</p>
</details>


<h3 id="63-MLVTG-Mamba-Based-Feature-Alignment-and-LLM-Driven-Purification-for-Multi-Modal-Video-Temporal-Grounding"><a href="#63-MLVTG-Mamba-Based-Feature-Alignment-and-LLM-Driven-Purification-for-Multi-Modal-Video-Temporal-Grounding" class="headerlink" title="[63] MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08512">MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding</a></h3><p><em>Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为MLVTG的新框架，通过MambaAligner和LLMRefiner两个模块，解决了现有Transformer方法在视频时序定位中的冗余注意力和多模态对齐问题。</p>
<details>
  <summary>Details</summary>
Motivation: 视频时序定位（VTG）是视频理解中的基础任务，但现有方法存在冗余注意力和多模态对齐不足的问题。

<p>Method: 采用MambaAligner模块（基于Vision Mamba块）建模时序依赖关系，以及LLMRefiner模块（利用预训练LLM的特定冻结层）增强多模态对齐。</p>
<p>Result: 在QVHighlights、Charades-STA和TVSum数据集上，MLVTG取得了最优性能，显著超越了现有基线。</p>
<p>Conclusion: MLVTG通过双重对齐策略（时序建模和语义净化）实现了更精确的定位，为VTG任务提供了新思路。</p>
<p>Abstract: Video Temporal Grounding (VTG), which aims to localize video clips<br>corresponding to natural language queries, is a fundamental yet challenging<br>task in video understanding. Existing Transformer-based methods often suffer<br>from redundant attention and suboptimal multi-modal alignment. To address these<br>limitations, we propose MLVTG, a novel framework that integrates two key<br>modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba<br>blocks as a backbone instead of Transformers to model temporal dependencies and<br>extract robust video representations for multi-modal alignment. LLMRefiner<br>leverages the specific frozen layer of a pre-trained Large Language Model (LLM)<br>to implicitly transfer semantic priors, enhancing multi-modal alignment without<br>fine-tuning. This dual alignment strategy, temporal modeling via structured<br>state-space dynamics and semantic purification via textual priors, enables more<br>precise localization. Extensive experiments on QVHighlights, Charades-STA, and<br>TVSum demonstrate that MLVTG achieves state-of-the-art performance and<br>significantly outperforms existing baselines.</p>
</details>


<h3 id="64-Robust-Visual-Localization-via-Semantic-Guided-Multi-Scale-Transformer"><a href="#64-Robust-Visual-Localization-via-Semantic-Guided-Multi-Scale-Transformer" class="headerlink" title="[64] Robust Visual Localization via Semantic-Guided Multi-Scale Transformer"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08526">Robust Visual Localization via Semantic-Guided Multi-Scale Transformer</a></h3><p><em>Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种结合多尺度特征学习与语义场景理解的框架，以提升动态环境中的视觉定位性能。</p>
<details>
  <summary>Details</summary>
Motivation: 动态环境中的光照变化、恶劣天气和移动物体会干扰视觉定位的外观线索，现有方法难以保持一致性能。

<p>Method: 采用分层Transformer与跨尺度注意力机制，融合几何细节与上下文信息，并通过语义监督学习视图不变特征。</p>
<p>Result: 在TartanAir数据集上，该方法在动态对象、光照变化和遮挡等挑战性场景中优于现有姿态回归方法。</p>
<p>Conclusion: 结合多尺度处理与语义指导，为现实动态环境中的鲁棒视觉定位提供了有效策略。</p>
<p>Abstract: Visual localization remains challenging in dynamic environments where<br>fluctuating lighting, adverse weather, and moving objects disrupt appearance<br>cues. Despite advances in feature representation, current absolute pose<br>regression methods struggle to maintain consistency under varying conditions.<br>To address this challenge, we propose a framework that synergistically combines<br>multi-scale feature learning with semantic scene understanding. Our approach<br>employs a hierarchical Transformer with cross-scale attention to fuse geometric<br>details and contextual cues, preserving spatial precision while adapting to<br>environmental changes. We improve the performance of this architecture with<br>semantic supervision via neural scene representation during training, guiding<br>the network to learn view-invariant features that encode persistent structural<br>information while suppressing complex environmental interference. Experiments<br>on TartanAir demonstrate that our approach outperforms existing pose regression<br>methods in challenging scenarios with dynamic objects, illumination changes,<br>and occlusions. Our findings show that integrating multi-scale processing with<br>semantic guidance offers a promising strategy for robust visual localization in<br>real-world dynamic environments.</p>
</details>


<h3 id="65-LiftVSR-Lifting-Image-Diffusion-to-Video-Super-Resolution-via-Hybrid-Temporal-Modeling-with-Only-4-times-RTX-4090s"><a href="#65-LiftVSR-Lifting-Image-Diffusion-to-Video-Super-Resolution-via-Hybrid-Temporal-Modeling-with-Only-4-times-RTX-4090s" class="headerlink" title="[65] LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08529">LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s</a></h3><p><em>Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出LiftVSR，通过动态时间注意力和注意力内存缓存优化视频超分辨率，显著降低计算成本。</p>
<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在视频超分辨率中虽提升了感知质量，但计算成本高且时间一致性不足。

<p>Method: 结合动态时间注意力（DTA）和注意力内存缓存（AMC），分解时间建模为短帧精细建模和长片段一致性建模。</p>
<p>Result: LiftVSR在多个基准测试中表现优异，计算成本显著降低。</p>
<p>Conclusion: LiftVSR在效率和一致性上取得平衡，为视频超分辨率提供更优解决方案。</p>
<p>Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by<br>enhancing perceptual quality, largely through elaborately designed temporal<br>modeling to ensure inter-frame consistency. However, existing methods usually<br>suffer from limited temporal coherence and prohibitively high computational<br>costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for<br>long videos. In this work, we propose LiftVSR, an efficient VSR framework that<br>leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,<br>achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To<br>balance long-term consistency and efficiency, we introduce a hybrid temporal<br>modeling mechanism that decomposes temporal learning into two complementary<br>components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal<br>modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)<br>Attention Memory Cache (AMC) for long-term temporal modeling across segments<br>($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token<br>flows across frames within multi-head query and key tokens to warp inter-frame<br>contexts in the value tokens. AMC adaptively aggregates historical segment<br>information via a cache unit, ensuring long-term coherence with minimal<br>overhead. To further stabilize the cache interaction during inference, we<br>introduce an asymmetric sampling strategy that mitigates feature mismatches<br>arising from different diffusion sampling steps. Extensive experiments on<br>several typical VSR benchmarks have demonstrated that LiftVSR achieves<br>impressive performance with significantly lower computational costs.</p>
</details>


<h3 id="66-TrajFlow-Multi-modal-Motion-Prediction-via-Flow-Matching"><a href="#66-TrajFlow-Multi-modal-Motion-Prediction-via-Flow-Matching" class="headerlink" title="[66] TrajFlow: Multi-modal Motion Prediction via Flow Matching"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08541">TrajFlow: Multi-modal Motion Prediction via Flow Matching</a></h3><p><em>Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: TrajFlow是一种基于流匹配的运动预测框架，通过单次推理预测多模态轨迹，解决现有方法的计算效率问题，并在Waymo数据集上表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 自动驾驶中高效准确的运动预测对安全和决策至关重要，尤其是在动态多模态场景下。现有泛化轨迹预测方法计算效率低，难以满足需求。

<p>Method: 提出TrajFlow框架，采用流匹配技术，单次推理即可预测多模态轨迹；引入Plackett-Luce排序损失改进不确定性估计；设计自条件训练技术提升泛化能力。</p>
<p>Result: 在Waymo Open Motion Dataset上实现最先进性能，显著降低计算开销并保持预测一致性。</p>
<p>Conclusion: TrajFlow为安全关键型自动驾驶提供高效、高性能的解决方案，代码已开源。</p>
<p>Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and<br>informed decision-making in autonomous driving, particularly under dynamic<br>real-world conditions that necessitate multi-modal forecasts. We introduce<br>TrajFlow, a novel flow matching-based motion prediction framework that<br>addresses the scalability and efficiency challenges of existing generative<br>trajectory prediction methods. Unlike conventional generative approaches that<br>employ i.i.d. sampling and require multiple inference passes to capture diverse<br>outcomes, TrajFlow predicts multiple plausible future trajectories in a single<br>pass, significantly reducing computational overhead while maintaining coherence<br>across predictions. Moreover, we propose a ranking loss based on the<br>Plackett-Luce distribution to improve uncertainty estimation of predicted<br>trajectories. Additionally, we design a self-conditioning training technique<br>that reuses the model’s own predictions to construct noisy inputs during a<br>second forward pass, thereby improving generalization and accelerating<br>inference. Extensive experiments on the large-scale Waymo Open Motion Dataset<br>(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across<br>various key metrics, underscoring its effectiveness for safety-critical<br>autonomous driving applications. The code and other details are available on<br>the project website <a target="_blank" rel="noopener" href="https://traj-flow.github.io/">https://traj-flow.github.io/</a>.</p>
</details>


<h3 id="67-Convergence-of-Spectral-Principal-Paths-How-Deep-Networks-Distill-Linear-Representations-from-Noisy-Inputs"><a href="#67-Convergence-of-Spectral-Principal-Paths-How-Deep-Networks-Distill-Linear-Representations-from-Noisy-Inputs" class="headerlink" title="[67] Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08543">Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs</a></h3><p><em>Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了输入空间线性假设（ISLH）和光谱主路径（SPP）框架，研究深度网络中线性表示的形成，并通过实验验证其在视觉语言模型中的多模态鲁棒性，从而推动AI的透明性和鲁棒性。</p>
<details>
  <summary>Details</summary>
Motivation: 研究如何通过高级表示提升AI的透明性和控制性，从单个神经元转向与人类可解释概念对齐的结构化语义方向。

<p>Method: 提出输入空间线性假设（ISLH）和光谱主路径（SPP）框架，分析深度网络中线性表示的逐步提取过程。</p>
<p>Result: 实验表明，视觉语言模型中这些表示具有多模态鲁棒性。</p>
<p>Conclusion: 该研究为深度网络中表示形成的结构化理论提供了基础，有望改善AI的鲁棒性、公平性和透明性。</p>
<p>Abstract: High-level representations have become a central focus in enhancing AI<br>transparency and control, shifting attention from individual neurons or<br>circuits to structured semantic directions that align with human-interpretable<br>concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose<br>the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned<br>directions originate in the input space and are selectively amplified with<br>increasing depth. We then introduce the Spectral Principal Path (SPP)<br>framework, which formalizes how deep networks progressively distill linear<br>representations along a small set of dominant spectral directions. Building on<br>this framework, we further demonstrate the multimodal robustness of these<br>representations in Vision-Language Models (VLMs). By bridging theoretical<br>insights with empirical validation, this work advances a structured theory of<br>representation formation in deep networks, paving the way for improving AI<br>robustness, fairness, and transparency.</p>
</details>


<h3 id="68-From-Pixels-to-Graphs-using-Scene-and-Knowledge-Graphs-for-HD-EPIC-VQA-Challenge"><a href="#68-From-Pixels-to-Graphs-using-Scene-and-Knowledge-Graphs-for-HD-EPIC-VQA-Challenge" class="headerlink" title="[68] From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08553">From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge</a></h3><p><em>Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: SceneNet和KnowledgeNet是用于HD-EPIC VQA挑战赛2025的两种方法，分别通过场景图和外部常识知识提升视觉问答性能，组合使用时准确率达到44.21%。</p>
<details>
  <summary>Details</summary>
Motivation: 解决复杂视觉问答任务中对象交互、空间关系及常识知识推理的需求。

<p>Method: SceneNet利用多模态大语言模型生成场景图捕捉对象交互；KnowledgeNet整合ConceptNet的常识知识用于高级语义推理。</p>
<p>Result: 在HD-EPIC基准测试的七个类别中展示了各自的优势，组合框架的准确率达到44.21%。</p>
<p>Conclusion: SceneNet和KnowledgeNet的组合框架在复杂视觉问答任务中表现有效。</p>
<p>Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for<br>the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with<br>a multi-modal large language model (MLLM) to capture fine-grained object<br>interactions, spatial relationships, and temporally grounded events. In<br>parallel, KnowledgeNet incorporates ConceptNet’s external commonsense knowledge<br>to introduce high-level semantic connections between entities, enabling<br>reasoning beyond directly observable visual evidence. Each method demonstrates<br>distinct strengths across the seven categories of the HD-EPIC benchmark, and<br>their combination within our framework results in an overall accuracy of 44.21%<br>on the challenge, highlighting its effectiveness for complex egocentric VQA<br>tasks.</p>
</details>


<h3 id="69-Towards-Cross-Subject-EMG-Pattern-Recognition-via-Dual-Branch-Adversarial-Feature-Disentanglement"><a href="#69-Towards-Cross-Subject-EMG-Pattern-Recognition-via-Dual-Branch-Adversarial-Feature-Disentanglement" class="headerlink" title="[69] Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08555">Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement</a></h3><p><em>Xinyue Niu,Akira Furui</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出一种通过特征解耦消除校准需求的方法，实现有效的跨被试EMG模式识别。</p>
<details>
  <summary>Details</summary>
Motivation: 解决跨被试EMG模式识别中因解剖结构、电极位置和信号特性差异导致的模型校准问题。

<p>Method: 采用双分支对抗神经网络，将EMG特征解耦为模式特异和个体特异两部分，实现无校准的跨被试识别。</p>
<p>Result: 模型在未见用户数据上表现稳健，优于多种基线方法。</p>
<p>Conclusion: 为无需校准的跨被试EMG识别提供了新思路，并展示了模型在生物识别等应用中的潜力。</p>
<p>Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant<br>challenges due to inter-subject variability in muscle anatomy, electrode<br>placement, and signal characteristics. Traditional methods rely on<br>subject-specific calibration data to adapt models to new users, an approach<br>that is both time-consuming and impractical for large-scale, real-world<br>deployment. This paper presents an approach to eliminate calibration<br>requirements through feature disentanglement, enabling effective cross-subject<br>generalization. We propose an end-to-end dual-branch adversarial neural network<br>that simultaneously performs pattern recognition and individual identification<br>by disentangling EMG features into pattern-specific and subject-specific<br>components. The pattern-specific components facilitate robust pattern<br>recognition for new users without model calibration, while the subject-specific<br>components enable downstream applications such as task-invariant biometric<br>identification. Experimental results demonstrate that the proposed model<br>achieves robust performance on data from unseen users, outperforming various<br>baseline methods in cross-subject scenarios. Overall, this study offers a new<br>perspective for cross-subject EMG pattern recognition without model calibration<br>and highlights the proposed model’s potential for broader applications, such as<br>task-independent biometric systems.</p>
</details>


<h3 id="70-Hierarchical-Neural-Collapse-Detection-Transformer-for-Class-Incremental-Object-Detection"><a href="#70-Hierarchical-Neural-Collapse-Detection-Transformer-for-Class-Incremental-Object-Detection" class="headerlink" title="[70] Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08562">Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection</a></h3><p><em>Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Hier-DETR 是一种新型的增量目标检测框架，结合神经崩溃和层级关系标签，提升效率和性能。</p>
<details>
  <summary>Details</summary>
Motivation: 解决现有增量目标检测模型性能不足和推理时间长的问题。

<p>Method: 利用神经崩溃处理数据集不平衡问题，并结合类别标签的层级关系。</p>
<p>Result: 实现了高效且具有竞争力的性能。</p>
<p>Conclusion: Hier-DETR 为增量目标检测提供了更实用的解决方案。</p>
<p>Abstract: Recently, object detection models have witnessed notable performance<br>improvements, particularly with transformer-based models. However, new objects<br>frequently appear in the real world, requiring detection models to continually<br>learn without suffering from catastrophic forgetting. Although Incremental<br>Object Detection (IOD) has emerged to address this challenge, these existing<br>models are still not practical due to their limited performance and prolonged<br>inference time. In this paper, we introduce a novel framework for IOD, called<br>Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both<br>efficiency and competitive performance by leveraging Neural Collapse for<br>imbalance dataset and Hierarchical relation of classes’ labels.</p>
</details>


<h3 id="71-Generating-Vision-Language-Navigation-Instructions-Incorporated-Fine-Grained-Alignment-Annotations"><a href="#71-Generating-Vision-Language-Navigation-Instructions-Incorporated-Fine-Grained-Alignment-Annotations" class="headerlink" title="[71] Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08566">Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations</a></h3><p><em>Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出FCA-NIG框架，自动生成带有细粒度跨模态标注的导航指令，并构建FCA-R2R数据集，显著提升多种VLN代理的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏细粒度的跨模态对齐标注，无法满足导航决策的需求，需生成更精细的标注数据。

<p>Method: 通过轨迹划分、地标检测、指令生成和实体选择等步骤，生成子指令-轨迹对和实体-地标标注，最终构建完整数据集。</p>
<p>Result: FCA-R2R数据集显著提升了SF、EnvDrop等VLN代理的性能，增强了状态感知和导航泛化能力。</p>
<p>Conclusion: FCA-NIG框架无需人工标注即可生成高质量训练数据，推动了复杂导航任务中的细粒度跨模态学习。</p>
<p>Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate<br>environments by integrating visual perception and natural language<br>instructions, yet faces significant challenges due to the scarcity of<br>fine-grained cross-modal alignment annotations. Existing datasets primarily<br>focus on global instruction-trajectory matching, neglecting<br>sub-instruction-level and entity-level alignments critical for accurate<br>navigation action decision-making. To address this limitation, we propose<br>FCA-NIG, a generative framework that automatically constructs navigation<br>instructions with dual-level fine-grained cross-modal annotations. In this<br>framework, an augmented trajectory is first divided into sub-trajectories,<br>which are then processed through GLIP-based landmark detection, crafted<br>instruction construction, OFA-Speaker based R2R-like instruction generation,<br>and CLIP-powered entity selection, generating sub-instruction-trajectory pairs<br>with entity-landmark annotations. Finally, these sub-pairs are aggregated to<br>form a complete instruction-trajectory pair. The framework generates the<br>FCA-R2R dataset, the first large-scale augmentation dataset featuring precise<br>sub-instruction-sub-trajectory and entity-landmark alignments. Extensive<br>experiments demonstrate that training with FCA-R2R significantly improves the<br>performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,<br>RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances<br>agents’ state awareness and decision accuracy, while entity-landmark alignment<br>further boosts navigation performance and generalization. These results<br>highlight the effectiveness of FCA-NIG in generating high-quality, scalable<br>training data without manual annotation, advancing fine-grained cross-modal<br>learning in complex navigation tasks.</p>
</details>


<h3 id="72-Diversity-Guided-MLP-Reduction-for-Efficient-Large-Vision-Transformers"><a href="#72-Diversity-Guided-MLP-Reduction-for-Efficient-Large-Vision-Transformers" class="headerlink" title="[72] Diversity-Guided MLP Reduction for Efficient Large Vision Transformers"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08591">Diversity-Guided MLP Reduction for Efficient Large Vision Transformers</a></h3><p><em>Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为DGMR（Diversity-Guided MLP Reduction）的方法，通过减少多层感知机（MLP）模块的参数来降低大规模视觉Transformer的计算和内存成本，几乎不影响性能。</p>
<details>
  <summary>Details</summary>
Motivation: 由于大规模Transformer模型参数过多导致计算和内存成本高昂，论文发现MLP模块占据了大部分参数，因此提出压缩方法以降低成本。

<p>Method: 采用基于Gram-Schmidt的权重剪枝策略，消除MLP隐藏层的冗余神经元，同时保留权重多样性以提高蒸馏时的性能恢复能力。</p>
<p>Result: 实验表明，该方法在大规模视觉Transformer上实现了超过57%的参数和FLOPs减少，几乎无损性能；在EVA-CLIP-E（4.4B）上甚至实现了71.5%的减少。</p>
<p>Conclusion: DGMR方法显著减少了Transformer模型的参数和计算成本，同时保持了性能，为大规模模型的实际应用提供了可行方案。</p>
<p>Abstract: Transformer models achieve excellent scaling property, where the performance<br>is improved with the increment of model capacity. However, large-scale model<br>parameters lead to an unaffordable cost of computing and memory. We analyze<br>popular transformer architectures and find that multilayer perceptron (MLP)<br>modules take up the majority of model parameters. To this end, we focus on the<br>recoverability of the compressed models and propose a Diversity-Guided MLP<br>Reduction (DGMR) method to significantly reduce the parameters of large vision<br>transformers with only negligible performance degradation. Specifically, we<br>conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons<br>of MLP hidden layer, while preserving weight diversity for better performance<br>recover during distillation. Compared to the model trained from scratch, our<br>pruned model only requires 0.06% data of LAION-2B (for the training of large<br>vision transformers) without labels (ImageNet-1K) to recover the original<br>performance. Experimental results on several state-of-the-art large vision<br>transformers demonstrate that our method achieves a more than 57.0% parameter<br>and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),<br>our method accomplishes a 71.5% parameter and FLOPs reduction without<br>performance degradation. The source code and trained weights are available at<br><a target="_blank" rel="noopener" href="https://github.com/visresearch/DGMR">https://github.com/visresearch/DGMR</a>.</p>
</details>


<h3 id="73-Data-Efficient-Challenges-in-Visual-Inductive-Priors-A-Retrospective"><a href="#73-Data-Efficient-Challenges-in-Visual-Inductive-Priors-A-Retrospective" class="headerlink" title="[73] Data-Efficient Challenges in Visual Inductive Priors: A Retrospective"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08612">Data-Efficient Challenges in Visual Inductive Priors: A Retrospective</a></h3><p><em>Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tömen,Hadi Jamali-Rad,Jan van Gemert</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文研究在数据稀缺情况下训练深度学习模型的有效方法，通过组织‘VIPriors’挑战赛，探索不使用迁移学习的情况下，如何结合先验知识提升数据效率。</p>
<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺时深度学习模型性能下降的问题，推动开发结合先验知识的新型方法。

<p>Method: 通过‘VIPriors’挑战赛限制参与者仅使用少量数据从头训练模型，禁止迁移学习，探索有效方法。</p>
<p>Result: 成功的参赛方案结合了Transformer和CNN的模型集成、大规模数据增强以及先验知识方法。</p>
<p>Conclusion: 结合模型集成、数据增强和先验知识可以有效提升数据稀缺情况下的深度学习性能。</p>
<p>Abstract: Deep Learning requires large amounts of data to train models that work well.<br>In data-deficient settings, performance can be degraded. We investigate which<br>Deep Learning methods benefit training models in a data-deficient setting, by<br>organizing the “VIPriors: Visual Inductive Priors for Data-Efficient Deep<br>Learning” workshop series, featuring four editions of data-impaired challenges.<br>These challenges address the problem of training deep learning models for<br>computer vision tasks with limited data. Participants are limited to training<br>models from scratch using a low number of training samples and are not allowed<br>to use any form of transfer learning. We aim to stimulate the development of<br>novel approaches that incorporate prior knowledge to improve the data<br>efficiency of deep learning models. Successful challenge entries make use of<br>large model ensembles that mix Transformers and CNNs, as well as heavy data<br>augmentation. Novel prior knowledge-based methods contribute to success in some<br>entries.</p>
</details>


<h3 id="74-SAMSelect-A-Spectral-Index-Search-for-Marine-Debris-Visualization-using-Segment-Anything"><a href="#74-SAMSelect-A-Spectral-Index-Search-for-Marine-Debris-Visualization-using-Segment-Anything" class="headerlink" title="[74] SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08613">SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything</a></h3><p><em>Joost van Dalen,Yuki M. Asano,Marc Russwurm</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: SAMSelect是一种用于从多光谱图像中选择最佳三通道可视化组合的算法，特别适用于海洋科学家解读海洋垃圾。通过Segment Anything Model在小规模标注数据上实现高分类准确率，SAMSelect帮助专家更直观地进行图像解译。</p>
<details>
  <summary>Details</summary>
Motivation: 多光谱图像中的海洋垃圾由于其成分多样性和中等分辨率的特点，难以直观解译。目前专家通常依靠经验和启发式方法手动选择波段或光谱指数，缺乏系统性优化。SAMSelect旨在通过算法优化这一过程。

<p>Method: SAMSelect利用Segment Anything Model在小规模标注数据上评估不同波段或光谱指数组合的分类准确率，并选择最佳的三通道可视化方案。其假设是分类准确率高的组合也能提供良好的视觉解译效果。</p>
<p>Result: 在加纳阿克拉和南非德班等地的Sentinel-2图像测试中，SAMSelect发现了一些未被文献记载的波段组合（如B8和B2的归一化差值指数），其性能优于传统方法。</p>
<p>Conclusion: SAMSelect为海洋科学家提供了一种高效且开源的工具，显著提升了多光谱图像中海洋垃圾的可视化解译效果。</p>
<p>Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel<br>visualization for multispectral images. We develop SAMSelect and show its use<br>for marine scientists visually interpreting floating marine debris in<br>Sentinel-2 imagery. These debris are notoriously difficult to visualize due to<br>their compositional heterogeneity in medium-resolution imagery. Out of these<br>difficulties, a visual interpretation of imagery showing marine debris remains<br>a common practice by domain experts, who select bands and spectral indices on a<br>case-by-case basis informed by common practices and heuristics. SAMSelect<br>selects the band or index combination that achieves the best classification<br>accuracy on a small annotated dataset through the Segment Anything Model. Its<br>central assumption is that the three-channel visualization achieves the most<br>accurate segmentation results also provide good visual information for<br>photo-interpretation.<br>  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine<br>debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets<br>from the Plastic Litter Project. This reveals the potential of new previously<br>unused band combinations (e.g., a normalized difference index of B8, B2), which<br>demonstrate improved performance compared to literature-based indices. We<br>describe the algorithm in this paper and provide an open-source code repository<br>that will be helpful for domain scientists doing visual photo interpretation,<br>especially in the marine field.</p>
</details>


<h3 id="75-ECMNet-Lightweight-Semantic-Segmentation-with-Efficient-CNN-Mamba-Network"><a href="#75-ECMNet-Lightweight-Semantic-Segmentation-with-Efficient-CNN-Mamba-Network" class="headerlink" title="[75] ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08629">ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network</a></h3><p><em>Feixiang Du,Shengkun Wu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种轻量级Efficient CNN-Mamba Network（ECMNet），结合CNN与Mamba优势，用于提升语义分割任务中的全局上下文建模能力。</p>
<details>
  <summary>Details</summary>
Motivation: 解决当前CNN与Transformer模型在语义分割中全局上下文建模不足的问题。

<p>Method: 设计Enhanced Dual-Attention Block（EDAB）轻量瓶颈、Multi-Scale Attention Unit（MSAU）多尺度特征聚合模块，以及Mamba增强的Feature Fusion Module（FFM）。</p>
<p>Result: 在Cityscapes和CamVid数据集上分别达到70.6%和73.6% mIoU，参数量0.87M，计算量8.27G FLOPs。</p>
<p>Conclusion: ECMNet在精度与效率的平衡上表现优异。</p>
<p>Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers<br>have achieved wide applicaiton in semantic segmentation tasks. Although CNNs<br>with Transformer models greatly improve performance, the global context<br>modeling remains inadequate. Recently, Mamba achieved great potential in vision<br>tasks, showing its advantages in modeling long-range dependency. In this paper,<br>we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,<br>dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based<br>framework to address their complementary weaknesses. Specifically, We design a<br>Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to<br>improve the representations ability of feature, We devise a Multi-Scale<br>Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial<br>aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion<br>Module (FFM) merges diverse level feature, significantly enhancing segmented<br>accuracy. Extensive experiments on two representative datasets demonstrate that<br>the proposed model excels in accuracy and efficiency balance, achieving 70.6%<br>mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M<br>parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.</p>
</details>


<h3 id="76-RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping"><a href="#76-RoboSwap-A-GAN-driven-Video-Diffusion-Framework-For-Unsupervised-Robot-Arm-Swapping" class="headerlink" title="[76] RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08632">RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping</a></h3><p><em>Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: RoboSwap结合GAN和扩散模型，利用非配对数据实现视频中机械臂的替换，提升跨平台机器人学习的数据生成质量。</p>
<details>
  <summary>Details</summary>
Motivation: 解决视频条件机器人学习中高质量多样化数据稀缺的问题，特别是跨平台学习中的机械臂替换挑战。

<p>Method: 通过分割机械臂与背景，用非配对GAN进行机械臂转换，再用扩散模型增强视频连贯性和运动真实性。</p>
<p>Result: 在三个基准测试中，RoboSwap在结构连贯性和运动一致性上优于现有方法。</p>
<p>Conclusion: RoboSwap为机器人学习提供了可靠、跨平台的数据生成解决方案。</p>
<p>Abstract: Recent advancements in generative models have revolutionized video synthesis<br>and editing. However, the scarcity of diverse, high-quality datasets continues<br>to hinder video-conditioned robotic learning, limiting cross-platform<br>generalization. In this work, we address the challenge of swapping a robotic<br>arm in one video with another: a key step for crossembodiment learning. Unlike<br>previous methods that depend on paired video demonstrations in the same<br>environmental settings, our proposed framework, RoboSwap, operates on unpaired<br>data from diverse environments, alleviating the data collection needs. RoboSwap<br>introduces a novel video editing pipeline integrating both GANs and diffusion<br>models, combining their isolated advantages. Specifically, we segment robotic<br>arms from their backgrounds and train an unpaired GAN model to translate one<br>robotic arm to another. The translated arm is blended with the original video<br>background and refined with a diffusion model to enhance coherence, motion<br>realism and object interaction. The GAN and diffusion stages are trained<br>independently. Our experiments demonstrate that RoboSwap outperforms<br>state-of-the-art video and image editing models on three benchmarks in terms of<br>both structural coherence and motion consistency, thereby offering a robust<br>solution for generating reliable, cross-embodiment data in robotic learning.</p>
</details>


<h3 id="77-SurfR-Surface-Reconstruction-with-Multi-scale-Attention"><a href="#77-SurfR-Surface-Reconstruction-with-Multi-scale-Attention" class="headerlink" title="[77] SurfR: Surface Reconstruction with Multi-scale Attention"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08635">SurfR: Surface Reconstruction with Multi-scale Attention</a></h3><p><em>Siddhant Ranade,Gonçalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种基于隐式表示的快速准确的无组织点云表面重建算法，通过延迟查询、多尺度网格表示和跨尺度注意力实现最佳精度与速度的平衡。</p>
<details>
  <summary>Details</summary>
Motivation: 解决现有学习方法在单对象表示和通用表示之间的权衡问题，即前者需要每对象训练且模型小，后者模型大且推理慢但通用性强。

<p>Method: 采用延迟查询策略、并行多尺度网格表示和跨尺度注意力机制，以优化隐式表示的速度和精度。</p>
<p>Result: 新方法在所有基线模型的最优分辨率下速度最快，性能仅略低于最先进方法，实现了最佳精度与速度的权衡。</p>
<p>Conclusion: 通过三种关键贡献，该方法在隐式表示中实现了高效且高精度的表面重建，适用于通用3D形状。</p>
<p>Abstract: We propose a fast and accurate surface reconstruction algorithm for<br>unorganized point clouds using an implicit representation. Recent learning<br>methods are either single-object representations with small neural models that<br>allow for high surface details but require per-object training or generalized<br>representations that require larger models and generalize to newer shapes but<br>lack details, and inference is slow. We propose a new implicit representation<br>for general 3D shapes that is faster than all the baselines at their optimum<br>resolution, with only a marginal loss in performance compared to the<br>state-of-the-art. We achieve the best accuracy-speed trade-off using three key<br>contributions. Many implicit methods extract features from the point cloud to<br>classify whether a query point is inside or outside the object. First, to speed<br>up the reconstruction, we show that this feature extraction does not need to<br>use the query point at an early stage (lazy query). Second, we use a parallel<br>multi-scale grid representation to develop robust features for different noise<br>levels and input resolutions. Finally, we show that attention across scales can<br>provide improved reconstruction results.</p>
</details>


<h3 id="78-Enhancing-Video-Memorability-Prediction-with-Text-Motion-Cross-modal-Contrastive-Loss-and-Its-Application-in-Video-Summarization"><a href="#78-Enhancing-Video-Memorability-Prediction-with-Text-Motion-Cross-modal-Contrastive-Loss-and-Its-Application-in-Video-Summarization" class="headerlink" title="[78] Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08649">Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization</a></h3><p><em>Zhiyi Zhu,Xiaoyu Wu,Youwei Lu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种通过文本-运动跨模态对比损失（TMCCL）来增强运动特征表示的视频记忆性预测模型，并利用视频记忆性预测减少视频摘要标签的主观性（MWCVS）。</p>
<details>
  <summary>Details</summary>
Motivation: 现有视频记忆性预测模型未能充分利用运动线索，且运动特征提取器在微调阶段因缺乏标记数据而导致特征表示不佳。此外，视频记忆性预测的应用潜力未被充分探索。

<p>Method: 提出TMCCL，通过文本描述相似性构建正负样本集，增强运动特征表示；并利用视频记忆性预测设计MWCVS，减少视频摘要的主观性。</p>
<p>Result: TMCCL在两个视频记忆性预测数据集上达到最优性能；MWCVS在两个视频摘要数据集上验证了其有效性。</p>
<p>Conclusion: 论文成功提升了视频记忆性预测的准确性和应用价值，展示了其在视频摘要中的潜力。</p>
<p>Abstract: Video memorability refers to the ability of videos to be recalled after<br>viewing, playing a crucial role in creating content that remains memorable.<br>Existing models typically focus on extracting multimodal features to predict<br>video memorability scores but often fail to fully utilize motion cues. The<br>representation of motion features is compromised during the fine-tuning phase<br>of the motion feature extractor due to a lack of labeled data. In this paper,<br>we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal<br>video memorability prediction model designed to enhance the representation of<br>motion features. We tackle the challenge of improving motion feature<br>representation by leveraging text description similarities across videos to<br>establish positive and negative motion sample sets for a given target. This<br>enhancement allows the model to learn similar feature representations for<br>semantically related motion content, resulting in more accurate memorability<br>predictions. Our model achieves state-of-the-art performance on two video<br>memorability prediction datasets. Moreover, the potential applications of video<br>memorability prediction have been underexplored. To address this gap, we<br>present Memorability Weighted Correction for Video Summarization (MWCVS), using<br>video memorability prediction to reduce subjectivity in video summarization<br>labels. Experimental results on two video summarization datasets demonstrate<br>the effectiveness of MWCVS, showcasing the promising applications of video<br>memorability prediction.</p>
</details>


<h3 id="79-Beyond-Calibration-Physically-Informed-Learning-for-Raw-to-Raw-Mapping"><a href="#79-Beyond-Calibration-Physically-Informed-Learning-for-Raw-to-Raw-Mapping" class="headerlink" title="[79] Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08650">Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping</a></h3><p><em>Peter Grönquist,Stepan Tulyakov,Dengxin Dai</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种名为神经物理模型（NPM）的轻量级物理信息方法，用于多相机系统中的颜色一致性重现。</p>
<details>
  <summary>Details</summary>
Motivation: 解决多相机系统中由于传感器和光学变化导致的颜色一致性挑战。

<p>Method: 引入NPM，模拟指定光照下的原始图像，估计设备间的转换，支持物理测量初始化和有无配对数据的训练。</p>
<p>Result: 在公开数据集NUS和BeyondRGB上，NPM表现优于现有方法，实现跨传感器和光学系统的稳健颜色一致性。</p>
<p>Conclusion: NPM是一种高效且适应性强的解决方案，适用于多相机系统的颜色一致性重现。</p>
<p>Abstract: Achieving consistent color reproduction across multiple cameras is essential<br>for seamless image fusion and Image Processing Pipeline (ISP) compatibility in<br>modern devices, but it is a challenging task due to variations in sensors and<br>optics. Existing raw-to-raw conversion methods face limitations such as poor<br>adaptability to changing illumination, high computational costs, or impractical<br>requirements such as simultaneous camera operation and overlapping<br>fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,<br>physically-informed approach that simulates raw images under specified<br>illumination to estimate transformations between devices. The NPM effectively<br>adapts to varying illumination conditions, can be initialized with physical<br>measurements, and supports training with or without paired data. Experiments on<br>public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent<br>state-of-the-art methods, providing robust chromatic consistency across<br>different sensors and optical systems.</p>
</details>


<h3 id="80-LLaVA-c-Continual-Improved-Visual-Instruction-Tuning"><a href="#80-LLaVA-c-Continual-Improved-Visual-Instruction-Tuning" class="headerlink" title="[80] LLaVA-c: Continual Improved Visual Instruction Tuning"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08666">LLaVA-c: Continual Improved Visual Instruction Tuning</a></h3><p><em>Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了LLaVA-c方法，通过在LLaVA-1.5基础上引入频谱感知巩固和无监督查询正则化，解决了多任务学习中的任务平衡和基础模型退化问题，实现了持续学习与多任务联合学习相媲美的效果。</p>
<details>
  <summary>Details</summary>
Motivation: 多任务学习存在任务平衡和数据扩展成本的挑战，持续学习虽能逐步获取新知识但可能忽视基础模型的通用能力退化。本文旨在解决这些问题。

<p>Method: 在LLaVA-1.5基础上，通过频谱感知巩固改进任务平衡，并引入无监督查询正则化防止基础模型退化。</p>
<p>Result: LLaVA-c在持续预训练和微调中均提升了基准性能，并保留了通用能力，首次证明了持续学习可以匹敌多任务联合学习。</p>
<p>Conclusion: LLaVA-c通过简单但有效的修改，解决了持续学习中的关键问题，为未来研究提供了新思路。</p>
<p>Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual<br>understanding through visual instruction tuning on multitask datasets, enabling<br>strong instruction-following and multimodal performance. However, multitask<br>learning faces challenges such as task balancing, requiring careful adjustment<br>of data proportions, and expansion costs, where new tasks risk catastrophic<br>forgetting and need costly retraining. Continual learning provides a promising<br>alternative to acquiring new knowledge incrementally while preserving existing<br>capabilities. However, current methods prioritize task-specific performance,<br>neglecting base model degradation from overfitting to specific instructions,<br>which undermines general capabilities. In this work, we propose a simple but<br>effective method with two modifications on LLaVA-1.5: spectral-aware<br>consolidation for improved task balance and unsupervised inquiry regularization<br>to prevent base model degradation. We evaluate both general and task-specific<br>performance across continual pretraining and fine-tuning. Experiments<br>demonstrate that LLaVA-c consistently enhances standard benchmark performance<br>and preserves general capabilities. For the first time, we show that<br>task-by-task continual learning can achieve results that match or surpass<br>multitask joint learning. The code will be publicly released.</p>
</details>


<h3 id="81-ATAS-Any-to-Any-Self-Distillation-for-Enhanced-Open-Vocabulary-Dense-Prediction"><a href="#81-ATAS-Any-to-Any-Self-Distillation-for-Enhanced-Open-Vocabulary-Dense-Prediction" class="headerlink" title="[81] ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08678">ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction</a></h3><p><em>Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为ATAS的方法，通过自蒸馏技术提升CLIP模型在细粒度视觉-语言对齐和语义一致性方面的表现，无需额外模块或有监督微调。</p>
<details>
  <summary>Details</summary>
Motivation: 当前CLIP模型在细粒度、区域级别的理解上表现不足，影响了其在密集预测任务中的效果。

<p>Method: 提出Any-to-Any Self-Distillation (ATAS)，利用无标注图像和内部自蒸馏过程优化CLIP视觉编码器的表示，保持语义一致性的同时提升细节识别能力。</p>
<p>Result: 在开放词汇目标检测和语义分割任务中，ATAS显著优于基线CLIP模型。</p>
<p>Conclusion: ATAS方法的有效性验证了同时保持语义一致性和细粒度对齐对提升开放词汇密集预测任务的重要性。</p>
<p>Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary<br>dense prediction tasks by enabling recognition of a broad range of visual<br>concepts. However, CLIP still struggles with fine-grained, region-level<br>understanding, hindering its effectiveness on these dense prediction tasks. We<br>identify two pivotal factors required to address this limitation: semantic<br>coherence and fine-grained vision-language alignment. Current adaptation<br>methods often improve fine-grained alignment at the expense of semantic<br>coherence, and often rely on extra modules or supervised fine-tuning. To<br>overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel<br>approach that simultaneously enhances semantic coherence and fine-grained<br>alignment by leveraging own knowledge of a model across all representation<br>levels. Unlike prior methods, ATAS uses only unlabeled images and an internal<br>self-distillation process to refine representations of CLIP vision encoders,<br>preserving local semantic consistency while sharpening local detail<br>recognition. On open-vocabulary object detection and semantic segmentation<br>benchmarks, ATAS achieves substantial performance gains, outperforming baseline<br>CLIP models. These results validate the effectiveness of our approach and<br>underscore the importance of jointly maintaining semantic coherence and<br>fine-grained alignment for advanced open-vocabulary dense prediction.</p>
</details>


<h3 id="82-CanadaFireSat-Toward-high-resolution-wildfire-forecasting-with-multiple-modalities"><a href="#82-CanadaFireSat-Toward-high-resolution-wildfire-forecasting-with-multiple-modalities" class="headerlink" title="[82] CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08690">CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities</a></h3><p><em>Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 加拿大2023年经历了近年来最严重的野火季节，凸显气候变化加剧火灾季节长度和严重性的问题。该论文提出高分辨率野火预测方法，利用多模态数据提升预测精度。</p>
<details>
  <summary>Details</summary>
Motivation: 气候变化导致野火季节延长和加剧，需为北方生态系统提供更好的野火管理工具。高分辨率野火概率图是重要工具，但现有方法依赖低分辨率数据。

<p>Method: 提出CanadaFireSat基准数据集和基线方法，利用Sentinel-2、MODIS和ERA5等多模态数据，测试两种深度学习架构进行高分辨率野火预测。</p>
<p>Result: 多模态输入在所有指标上优于单模态输入，2023年野火季节F1得分达60.3%，展示了高分辨率和大陆尺度野火预测潜力。</p>
<p>Conclusion: 多模态深度学习模型在高分辨率野火预测中表现优异，为未来野火管理提供了有力工具。</p>
<p>Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent<br>history, causing damage across ecosystems, destroying communities, and emitting<br>large quantities of CO2. This extreme wildfire season is symptomatic of a<br>climate-change-induced increase in the length and severity of the fire season<br>that affects the boreal ecosystem. Therefore, it is critical to empower<br>wildfire management in boreal communities with better mitigation solutions.<br>Wildfire probability maps represent an important tool for understanding the<br>likelihood of wildfire occurrence and the potential severity of future<br>wildfires. The massive increase in the availability of Earth observation data<br>has enabled the development of deep learning-based wildfire forecasting models,<br>aiming at providing precise wildfire probability maps at different spatial and<br>temporal scales. A main limitation of such methods is their reliance on<br>coarse-resolution environmental drivers and satellite products, leading to<br>wildfire occurrence prediction of reduced resolution, typically around $\sim<br>0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and<br>baseline methods for high-resolution: 100 m wildfire forecasting across Canada,<br>leveraging multi-modal data from high-resolution multi-spectral satellite<br>images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and<br>environmental factors (ERA5 reanalysis data). Our experiments consider two<br>major deep learning architectures. We observe that using multi-modal temporal<br>inputs outperforms single-modal temporal inputs across all metrics, achieving a<br>peak performance of 60.3% in F1 score for the 2023 wildfire season, a season<br>never seen during model training. This demonstrates the potential of<br>multi-modal deep learning models for wildfire forecasting at high-resolution<br>and continental scale.</p>
</details>


<h3 id="83-VReST-Enhancing-Reasoning-in-Large-Vision-Language-Models-through-Tree-Search-and-Self-Reward-Mechanism"><a href="#83-VReST-Enhancing-Reasoning-in-Large-Vision-Language-Models-through-Tree-Search-and-Self-Reward-Mechanism" class="headerlink" title="[83] VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08691">VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism</a></h3><p><em>Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: VReST提出了一种无需训练的方法，通过蒙特卡洛树搜索和自奖励机制，提升大型视觉语言模型在复杂视觉推理任务中的性能，并在多模态数学推理基准测试中取得最优表现。</p>
<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型在复杂视觉推理中的表现受限，尤其在链式思考提示技术上。

<p>Method: 采用蒙特卡洛树搜索和创新的多模态自奖励机制，评估推理步骤的质量，无需额外模型。</p>
<p>Result: 在多模态数学推理基准测试中超越现有提示方法，达到最优性能。</p>
<p>Conclusion: VReST为多模态任务的未来研究提供了有前景的方向，并验证了测试时间扩展定律的有效性。</p>
<p>Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in<br>multimodal tasks, but their effectiveness in complex visual reasoning is still<br>constrained, especially when employing Chain-of-Thought prompting techniques.<br>In this paper, we propose VReST, a novel training-free approach that enhances<br>Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.<br>VReST meticulously traverses the reasoning landscape by establishing a search<br>tree, where each node encapsulates a reasoning step, and each path delineates a<br>comprehensive reasoning sequence. Our innovative multimodal Self-Reward<br>mechanism assesses the quality of reasoning steps by integrating the utility of<br>sub-questions, answer correctness, and the relevance of vision-language clues,<br>all without the need for additional models. VReST surpasses current prompting<br>methods and secures state-of-the-art performance across three multimodal<br>mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy<br>of test-time scaling laws in multimodal tasks, offering a promising direction<br>for future research.</p>
</details>


<h3 id="84-MoSiC-Optimal-Transport-Motion-Trajectory-for-Dense-Self-Supervised-Learning"><a href="#84-MoSiC-Optimal-Transport-Motion-Trajectory-for-Dense-Self-Supervised-Learning" class="headerlink" title="[84] MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08694">MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning</a></h3><p><em>Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种基于运动引导的自监督学习框架，通过聚类密集点轨迹学习时空一致性特征，在动态场景中表现优异。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在处理视频动态时因静态增强失败而表现不佳，需要更好的时空一致性特征学习方法。

<p>Method: 利用现有点跟踪器提取运动轨迹，通过动量编码器优化运输机制，传播集群分配以保持特征一致性。</p>
<p>Result: 在六种图像和视频数据集上，性能提升1%至6%，优于现有技术。</p>
<p>Conclusion: 运动作为辅助信号，显著提升了动态场景下的特征学习效果。</p>
<p>Abstract: Dense self-supervised learning has shown great promise for learning pixel-<br>and patch-level representations, but extending it to videos remains challenging<br>due to the complexity of motion dynamics. Existing approaches struggle as they<br>rely on static augmentations that fail under object deformations, occlusions,<br>and camera movement, leading to inconsistent feature learning over time. We<br>propose a motion-guided self-supervised learning framework that clusters dense<br>point tracks to learn spatiotemporally consistent representations. By<br>leveraging an off-the-shelf point tracker, we extract long-range motion<br>trajectories and optimize feature clustering through a momentum-encoder-based<br>optimal transport mechanism. To ensure temporal coherence, we propagate cluster<br>assignments along tracked points, enforcing feature consistency across views<br>despite viewpoint changes. Integrating motion as an implicit supervisory<br>signal, our method learns representations that generalize across frames,<br>improving robustness in dynamic scenes and challenging occlusion scenarios. By<br>initializing from strong image-pretrained models and leveraging video data for<br>training, we improve state-of-the-art by 1% to 6% on six image and video<br>datasets and four evaluation benchmarks. The implementation is publicly<br>available at our GitHub repository: <a target="_blank" rel="noopener" href="https://github.com/SMSD75/MoSiC/tree/main">https://github.com/SMSD75/MoSiC/tree/main</a></p>
</details>


<h3 id="85-TraGraph-GS-Trajectory-Graph-based-Gaussian-Splatting-for-Arbitrary-Large-Scale-Scene-Rendering"><a href="#85-TraGraph-GS-Trajectory-Graph-based-Gaussian-Splatting-for-Arbitrary-Large-Scale-Scene-Rendering" class="headerlink" title="[85] TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08704">TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering</a></h3><p><em>Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出TraGraph-GS方法，通过轨迹图优化大场景视角合成中的空间分区和高斯重叠问题，显著提升渲染质量。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在大场景视角合成中存在分区不灵活和纹理失真的问题，TraGraph-GS旨在通过轨迹图和正则化约束解决这些问题。

<p>Method: 采用基于图的空间分区方法，结合正则化约束和渐进渲染策略，优化大场景的高斯渲染质量。</p>
<p>Result: 在多个数据集上性能显著优于现有方法，PSNR提升平均1.86 dB（航空）和1.62 dB（地面）。</p>
<p>Conclusion: TraGraph-GS通过创新的空间分区和渲染策略，为大场景视角合成提供了高效且高质量的解决方案。</p>
<p>Abstract: High-quality novel view synthesis for large-scale scenes presents a<br>challenging dilemma in 3D computer vision. Existing methods typically partition<br>large scenes into multiple regions, reconstruct a 3D representation using<br>Gaussian splatting for each region, and eventually merge them for novel view<br>rendering. They can accurately render specific scenes, yet they do not<br>generalize effectively for two reasons: (1) rigid spatial partition techniques<br>struggle with arbitrary camera trajectories, and (2) the merging of regions<br>results in Gaussian overlap to distort texture details. To address these<br>challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable<br>high-precision rendering for arbitrarily large-scale scenes. We present a<br>spatial partitioning method for large-scale scenes based on graphs, which<br>incorporates a regularization constraint to enhance the rendering of textures<br>and distant objects, as well as a progressive rendering strategy to mitigate<br>artifacts caused by Gaussian overlap. Experimental results demonstrate its<br>superior performance both on four aerial and four ground datasets and highlight<br>its remarkable efficiency: our method achieves an average improvement of 1.86<br>dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to<br>state-of-the-art approaches.</p>
</details>


<h3 id="86-SceneSplat-A-Large-Dataset-and-Comprehensive-Benchmark-for-Language-Gaussian-Splatting"><a href="#86-SceneSplat-A-Large-Dataset-and-Comprehensive-Benchmark-for-Language-Gaussian-Splatting" class="headerlink" title="[86] SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08710">SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting</a></h3><p><em>Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了第一个大规模基准测试，系统性评估了三种语言高斯泼溅方法在3D空间中的表现，并引入了一个包含49K场景的新数据集GaussianWorld-49K，展示了通用化方法的优势。</p>
<details>
  <summary>Details</summary>
Motivation: 当前的语言高斯泼溅方法仅在少量场景和视角的2D渲染视图上评估，限制了整体3D理解的深度。论文旨在填补这一空白，促进通用化3D场景理解的研究。

<p>Method: 论文提出一个大规模基准测试，评估三类语言高斯泼溅方法在超过1000个室内外场景中的表现，并引入新数据集GaussianWorld-49K。</p>
<p>Result: 通用化方法在放松场景限制、快速推理和分割性能方面表现优越，展示了利用数据的潜力。</p>
<p>Conclusion: 通过公开代码、基准测试和数据集，论文旨在加速通用化3D高斯泼溅场景理解的研究。</p>
<p>Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient<br>encoding of scene geometry, appearance, and semantics. Moreover, grounding<br>language in 3D scenes has proven to be an effective strategy for 3D scene<br>understanding. Current Language Gaussian Splatting line of work fall into three<br>main groups: (i) per-scene optimization-based, (ii) per-scene<br>optimization-free, and (iii) generalizable approach. However, most of them are<br>evaluated only on rendered 2D views of a handful of scenes and viewpoints close<br>to the training views, limiting ability and insight into holistic 3D<br>understanding. To address this gap, we propose the first large-scale benchmark<br>that systematically assesses these three groups of methods directly in 3D<br>space, evaluating on 1060 scenes across three indoor datasets and one outdoor<br>dataset. Benchmark results demonstrate a clear advantage of the generalizable<br>paradigm, particularly in relaxing the scene-specific limitation, enabling fast<br>feed-forward inference on novel scenes, and achieving superior segmentation<br>performance. We further introduce GaussianWorld-49K a carefully curated 3DGS<br>dataset comprising around 49K diverse indoor and outdoor scenes obtained from<br>multiple sources, with which we demonstrate the generalizable approach could<br>harness strong data priors. Our codes, benchmark, and datasets will be made<br>public to accelerate research in generalizable 3DGS scene understanding.</p>
</details>


<h3 id="87-Geometric-deep-learning-for-local-growth-prediction-on-abdominal-aortic-aneurysm-surfaces"><a href="#87-Geometric-deep-learning-for-local-growth-prediction-on-abdominal-aortic-aneurysm-surfaces" class="headerlink" title="[87] Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08729">Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces</a></h3><p><em>Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种基于SE(3)-对称变换器模型的方法，通过保留血管表面解剖结构和几何保真度，预测腹主动脉瘤(AAA)的局部生长，从而改进个性化监测策略。</p>
<details>
  <summary>Details</summary>
Motivation: 目前的AAA监测基于直径阈值，忽略了3D形状与生长的复杂关系，标准化监测间隔可能不适合。个性化生长预测能优化监测策略。

<p>Method: 使用SE(3)-对称变换器模型，直接利用血管表面模型和局部多物理特征预测AAA生长，训练数据为24名患者的113次CTA扫描。</p>
<p>Result: 模型预测下一扫描时刻的AAA生长时，中位直径误差为1.18毫米，且能准确识别两年内需手术的患者(准确率0.93)。外部验证集结果同样良好。</p>
<p>Conclusion: 基于血管表面的局部定向AAA生长预测可行，有望推动个性化监测策略的发展。</p>
<p>Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the<br>abdominal aorta. AAAs may rupture, with a survival rate of only 20%. Current<br>clinical guidelines recommend elective surgical repair when the maximum AAA<br>diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet<br>these criteria are periodically monitored, with surveillance intervals based on<br>the maximum AAA diameter. However, this diameter does not take into account the<br>complex relation between the 3D AAA shape and its growth, making standardized<br>intervals potentially unfit. Personalized AAA growth predictions could improve<br>monitoring strategies. We propose to use an SE(3)-symmetric transformer model<br>to predict AAA growth directly on the vascular model surface enriched with<br>local, multi-physical features. In contrast to other works which have<br>parameterized the AAA shape, this representation preserves the vascular<br>surface’s anatomical structure and geometric fidelity. We train our model using<br>a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24<br>AAA patients at irregularly sampled intervals. After training, our model<br>predicts AAA growth to the next scan moment with a median diameter error of<br>1.18 mm. We further demonstrate our model’s utility to identify whether a<br>patient will become eligible for elective repair within two years (acc &#x3D; 0.93).<br>Finally, we evaluate our model’s generalization on an external validation set<br>consisting of 25 CTAs from 7 AAA patients from a different hospital. Our<br>results show that local directional AAA growth prediction from the vascular<br>surface is feasible and may contribute to personalized surveillance strategies.</p>
</details>


<h3 id="88-InceptionMamba-An-Efficient-Hybrid-Network-with-Large-Band-Convolution-and-Bottleneck-Mamba"><a href="#88-InceptionMamba-An-Efficient-Hybrid-Network-with-Large-Band-Convolution-and-Bottleneck-Mamba" class="headerlink" title="[88] InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08735">InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba</a></h3><p><em>Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种名为InceptionMamba的新主干架构，通过正交带卷积和瓶颈Mamba模块解决InceptionNeXt在空间依赖性和全局上下文建模上的不足，实现了卓越的分类和下游任务性能。</p>
<details>
  <summary>Details</summary>
Motivation: InceptionNeXt在图像分类和下游任务中表现出色，但因其一维条带卷积的局限性，无法充分捕捉不同维度的空间依赖关系和局部邻域空间建模。此外，卷积操作的局部性约束也限制了全局上下文建模的效果。

<p>Method: 用正交带卷积替代传统一维条带卷积，实现更紧密的空间建模；引入瓶颈Mamba模块，增强跨通道信息融合和扩大感受野。</p>
<p>Result: 在分类和多个下游任务中，InceptionMamba表现出最佳性能，同时具备优越的参数量和计算效率。</p>
<p>Conclusion: InceptionMamba通过改进空间建模和全局上下文处理，显著提升了性能，为图像分类和下游任务提供了高效解决方案。</p>
<p>Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown<br>excellent competitiveness in image classification and a number of downstream<br>tasks. Built on parallel one-dimensional strip convolutions, however, it<br>suffers from limited ability of capturing spatial dependencies along different<br>dimensions and fails to fully explore spatial modeling in local neighborhood.<br>Besides, inherent locality constraints of convolution operations are<br>detrimental to effective global context modeling. To overcome these<br>limitations, we propose a novel backbone architecture termed InceptionMamba in<br>this study. More specifically, the traditional one-dimensional strip<br>convolutions are replaced by orthogonal band convolutions in our InceptionMamba<br>to achieve cohesive spatial modeling. Furthermore, global contextual modeling<br>can be achieved via a bottleneck Mamba module, facilitating enhanced<br>cross-channel information fusion and enlarged receptive field. Extensive<br>evaluations on classification and various downstream tasks demonstrate that the<br>proposed InceptionMamba achieves state-of-the-art performance with superior<br>parameter and computational efficiency. The source code will be available at<br><a target="_blank" rel="noopener" href="https://github.com/Wake1021/InceptionMamba">https://github.com/Wake1021/InceptionMamba</a>.</p>
</details>


<h3 id="89-RS-MTDF-Multi-Teacher-Distillation-and-Fusion-for-Remote-Sensing-Semi-Supervised-Semantic-Segmentation"><a href="#89-RS-MTDF-Multi-Teacher-Distillation-and-Fusion-for-Remote-Sensing-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="[89] RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08772">RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation</a></h3><p><em>Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文提出了一种名为RS-MTDF的半监督语义分割框架，利用预训练视觉基础模型（VFMs）的多教师知识蒸馏和融合，显著提升了遥感图像语义分割的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 遥感图像的语义标注成本高昂且耗时，现有半监督方法因标记与未标记数据分布不匹配导致泛化能力不足。

<p>Method: 采用多教师（如DINOv2和CLIP）特征级蒸馏，将学生模型特征与VFMs的鲁棒表示对齐，并在解码器中融合蒸馏知识。</p>
<p>Result: 在ISPRS Potsdam、LoveDA和DeepGlobe数据集上取得最优性能，尤其在LoveDA上不同标注比例下均优于现有方法。</p>
<p>Conclusion: 多教师VFM指导能显著提升遥感分割的泛化能力和语义理解，各模块均通过消融实验验证其贡献。</p>
<p>Abstract: Semantic segmentation in remote sensing images is crucial for various<br>applications, yet its performance is heavily reliant on large-scale,<br>high-quality pixel-wise annotations, which are notoriously expensive and<br>time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a<br>promising alternative to mitigate this data dependency. However, existing SSS<br>methods often struggle with the inherent distribution mismatch between limited<br>labeled data and abundant unlabeled data, leading to suboptimal generalization.<br>We propose that Vision Foundation Models (VFMs), pre-trained on vast and<br>diverse datasets, possess robust generalization capabilities that can<br>effectively bridge this distribution gap and provide strong semantic priors for<br>SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and<br>Fusion), a novel framework that leverages the powerful semantic knowledge<br>embedded in VFMs to guide semi-supervised learning in remote sensing.<br>Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and<br>CLIP) as expert teachers, utilizing feature-level distillation to align student<br>features with their robust representations. To further enhance discriminative<br>power, the distilled knowledge is seamlessly fused into the student decoder.<br>Extensive experiments on three challenging remote sensing datasets (ISPRS<br>Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves<br>state-of-the-art performance. Notably, our method outperforms existing<br>approaches across various label ratios on LoveDA and secures the highest IoU in<br>the majority of semantic categories. These results underscore the efficacy of<br>multi-teacher VFM guidance in significantly enhancing both generalization and<br>semantic understanding for remote sensing segmentation. Ablation studies<br>further validate the contribution of each proposed module.</p>
</details>


<h3 id="90-Gaussian2Scene-3D-Scene-Representation-Learning-via-Self-supervised-Learning-with-3D-Gaussian-Splatting"><a href="#90-Gaussian2Scene-3D-Scene-Representation-Learning-via-Self-supervised-Learning-with-3D-Gaussian-Splatting" class="headerlink" title="[90] Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08777">Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting</a></h3><p><em>Keyi Liu,Weidong Yang,Ben Fei,Ying He</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文提出了一种名为Gaussian2Scene的新型自监督学习框架，利用3D高斯泼溅（3DGS）进行点云预训练，解决了现有方法依赖隐式场景表示和高内存需求的问题，并在多个下游任务中表现出色。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法依赖RGB-D图像作为重建信号，但受限于计算负担和高内存需求，且难以捕捉3D几何结构。Gaussian2Scene通过3DGS的效率和显式特性，旨在提升几何理解和跨模态学习。

<p>Method: 采用渐进式两阶段训练策略：第一阶段使用双分支掩码自编码器学习2D和3D场景表示；第二阶段通过高斯基元的几何位置和渲染的RGB图像进行监督学习。</p>
<p>Result: 在多个下游3D物体检测任务中，Gaussian2Scene均优于现有预训练方法，验证了其有效性。</p>
<p>Conclusion: Gaussian2Scene通过结合3DGS的效率和显式特性，显著提升了点云预训练的性能，为3D视觉任务提供了新的解决方案。</p>
<p>Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a<br>cornerstone for many 3D vision tasks, enabling effective learning from<br>large-scale unannotated data. At the scene level, existing SSL methods often<br>incorporate volume rendering into the pre-training framework, using RGB-D<br>images as reconstruction signals to facilitate cross-modal learning. This<br>strategy promotes alignment between 2D and 3D modalities and enables the model<br>to benefit from rich visual cues in the RGB-D inputs. However, these approaches<br>are limited by their reliance on implicit scene representations and high memory<br>demands. Furthermore, since their reconstruction objectives are applied only in<br>2D space, they often fail to capture underlying 3D geometric structures. To<br>address these challenges, we propose Gaussian2Scene, a novel scene-level SSL<br>framework that leverages the efficiency and explicit nature of 3D Gaussian<br>Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the<br>computational burden associated with volume rendering but also supports direct<br>3D scene reconstruction, thereby enhancing the geometric understanding of the<br>backbone network. Our approach follows a progressive two-stage training<br>strategy. In the first stage, a dual-branch masked autoencoder learns both 2D<br>and 3D scene representations. In the second stage, we initialize training with<br>reconstructed point clouds and further supervise learning using the geometric<br>locations of Gaussian primitives and rendered RGB images. This process<br>reinforces both geometric and cross-modal learning. We demonstrate the<br>effectiveness of Gaussian2Scene across several downstream 3D object detection<br>tasks, showing consistent improvements over existing pre-training methods.</p>
</details>


<h3 id="91-HunyuanVideo-HOMA-Generic-Human-Object-Interaction-in-Multimodal-Driven-Human-Animation"><a href="#91-HunyuanVideo-HOMA-Generic-Human-Object-Interaction-in-Multimodal-Driven-Human-Animation" class="headerlink" title="[91] HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08797">HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation</a></h3><p><em>Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，用于解决人-物交互视频生成中的限制，如依赖精确数据、泛化性差和可访问性低的问题。</p>
<details>
  <summary>Details</summary>
Motivation: 解决人-物交互视频生成中依赖精确数据、泛化性有限和可访问性低的问题。

<p>Method: 采用稀疏解耦运动引导和双模态扩散Transformer，结合HOI适配器和面部交叉注意力适配器。</p>
<p>Result: 在弱监督下实现了交互自然性和泛化性的最先进表现，并展示了文本条件生成和交互式对象操作的多样性。</p>
<p>Conclusion: HunyuanVideo-HOMA在提升可控性和减少依赖精确输入的同时，展示了广泛的应用潜力。</p>
<p>Abstract: To address key limitations in human-object interaction (HOI) video generation<br>– specifically the reliance on curated motion data, limited generalization to<br>novel objects&#x2F;scenarios, and restricted accessibility – we introduce<br>HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.<br>HunyuanVideo-HOMA enhances controllability and reduces dependency on precise<br>inputs through sparse, decoupled motion guidance. It encodes appearance and<br>motion signals into the dual input space of a multimodal diffusion transformer<br>(MMDiT), fusing them within a shared context space to synthesize temporally<br>consistent and physically plausible interactions. To optimize training, we<br>integrate a parameter-space HOI adapter initialized from pretrained MMDiT<br>weights, preserving prior knowledge while enabling efficient adaptation, and a<br>facial cross-attention adapter for anatomically accurate audio-driven lip<br>synchronization. Extensive experiments confirm state-of-the-art performance in<br>interaction naturalness and generalization under weak supervision. Finally,<br>HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and<br>interactive object manipulation, supported by a user-friendly demo interface.<br>The project page is at <a target="_blank" rel="noopener" href="https://anonymous.4open.science/w/homa-page-0FBE/">https://anonymous.4open.science/w/homa-page-0FBE/</a>.</p>
</details>


<h3 id="92-Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought"><a href="#92-Video-CoT-A-Comprehensive-Dataset-for-Spatiotemporal-Understanding-of-Videos-Based-on-Chain-of-Thought" class="headerlink" title="[92] Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08817">Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought</a></h3><p><em>Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: Video-CoT是一个创新的数据集，旨在通过链式思考方法提升视频理解中的时空细节捕捉能力，包含大量问题和答案对，为研究提供新方向。</p>
<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉语言模型在捕捉视频内容中的时空细节方面表现不足，这阻碍了深入视频分析。

<p>Method: 引入Video-CoT数据集，包含192,000个细粒度时空问题和23,000个链式思考标注样本，并提供基准测试。</p>
<p>Result: 实验表明现有模型在时空理解任务中表现不佳，突显了该任务的挑战性。</p>
<p>Conclusion: Video-CoT为多媒体理解研究开辟了新途径，并支持需要高级视频分析的智能系统创新。</p>
<p>Abstract: Video content comprehension is essential for various applications, ranging<br>from video analysis to interactive systems. Despite advancements in large-scale<br>vision-language models (VLMs), these models often struggle to capture the<br>nuanced, spatiotemporal details essential for thorough video analysis. To<br>address this gap, we introduce Video-CoT, a groundbreaking dataset designed to<br>enhance spatiotemporal understanding using Chain-of-Thought (CoT)<br>methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal<br>question-answer pairs and 23,000 high-quality CoT-annotated samples, providing<br>a solid foundation for evaluating spatiotemporal understanding in video<br>comprehension. Additionally, we provide a comprehensive benchmark for assessing<br>these tasks, with each task featuring 750 images and tailored evaluation<br>metrics. Our extensive experiments reveal that current VLMs face significant<br>challenges in achieving satisfactory performance, high-lighting the<br>difficulties of effective spatiotemporal understanding. Overall, the Video-CoT<br>dataset and benchmark open new avenues for research in multimedia understanding<br>and support future innovations in intelligent systems requiring advanced video<br>analysis capabilities. By making these resources publicly available, we aim to<br>encourage further exploration in this critical area. Project<br>website:<a target="_blank" rel="noopener" href="https://video-cot.github.io/">https://video-cot.github.io/</a> .</p>
</details>


<h3 id="93-CulturalFrames-Assessing-Cultural-Expectation-Alignment-in-Text-to-Image-Models-and-Evaluation-Metrics"><a href="#93-CulturalFrames-Assessing-Cultural-Expectation-Alignment-in-Text-to-Image-Models-and-Evaluation-Metrics" class="headerlink" title="[93] CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08835">CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics</a></h3><p><em>Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文通过CulturalFrames基准系统评估了T2I模型在文化表现上的不足，发现模型在显性和隐性文化期望上的失败率分别为68%和49%，现有评估指标与人类判断相关性差。</p>
<details>
  <summary>Details</summary>
Motivation: 研究T2I模型在多样化文化背景中的表现偏差，填补系统性量化文化对齐的研究空白。

<p>Method: 开发CulturalFrames基准，涵盖10国、5个社会文化领域，生成3637张图像并收集10k+人工标注，评估4种T2I模型。</p>
<p>Result: T2I模型在显性（68%）和隐性（49%）文化期望上的失败率高，现有评估指标与人类判断相关性低。</p>
<p>Conclusion: 揭示了T2I模型的文化表现缺陷，提出了改进的文化对齐建模与评估方向。</p>
<p>Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual<br>content generation raises concerns about their ability to accurately represent<br>diverse cultural contexts. In this work, we present the first study to<br>systematically quantify the alignment of T2I models and evaluation metrics with<br>respect to both explicit as well as implicit cultural expectations. To this<br>end, we introduce CulturalFrames, a novel benchmark designed for rigorous human<br>evaluation of cultural representation in visual generations. Spanning 10<br>countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,<br>3637 corresponding images generated by 4 state-of-the-art T2I models, and over<br>10k detailed human annotations. We find that T2I models not only fail to meet<br>the more challenging implicit expectations but also the less challenging<br>explicit expectations. Across models and countries, cultural expectations are<br>missed an average of 44% of the time. Among these failures, explicit<br>expectations are missed at a surprisingly high average rate of 68%, while<br>implicit expectation failures are also significant, averaging 49%. Furthermore,<br>we demonstrate that existing T2I evaluation metrics correlate poorly with human<br>judgments of cultural alignment, irrespective of their internal reasoning.<br>Collectively, our findings expose critical gaps, providing actionable<br>directions for developing more culturally informed T2I models and evaluation<br>methodologies.</p>
</details>


<h3 id="94-Adapting-Vision-Language-Foundation-Model-for-Next-Generation-Medical-Ultrasound-Image-Analysis"><a href="#94-Adapting-Vision-Language-Foundation-Model-for-Next-Generation-Medical-Ultrasound-Image-Analysis" class="headerlink" title="[94] Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08849">Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis</a></h3><p><em>Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingınst</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该论文探讨了如何通过领域适应方法优化视觉语言基础模型，以提升超声图像分析的性能，并展示了在分割和分类任务中的优异表现。</p>
<details>
  <summary>Details</summary>
Motivation: 超声图像分析中手动标注工作繁琐且主观性强，视觉语言基础模型虽适用但受限于领域差异，因此需要通过领域适应方法改进。

<p>Method: 利用大语言模型作为文本精炼器，结合特殊设计的适应策略和任务驱动头，优化视觉语言基础模型的微调流程。</p>
<p>Result: 在六个超声数据集和两项任务上的实验表明，该方法显著提升了模型性能，超越了现有最先进的视觉语言和纯粹基础模型。</p>
<p>Conclusion: 该研究成功开发了领域适应方法，有效提升了视觉语言基础模型在超声图像分析中的应用效果。</p>
<p>Abstract: Medical ultrasonography is an essential imaging technique for examining<br>superficial organs and tissues, including lymph nodes, breast, and thyroid. It<br>employs high-frequency ultrasound waves to generate detailed images of the<br>internal structures of the human body. However, manually contouring regions of<br>interest in these images is a labor-intensive task that demands expertise and<br>often results in inconsistent interpretations among individuals.<br>Vision-language foundation models, which have excelled in various computer<br>vision applications, present new opportunities for enhancing ultrasound image<br>analysis. Yet, their performance is hindered by the significant differences<br>between natural and medical imaging domains. This research seeks to overcome<br>these challenges by developing domain adaptation methods for vision-language<br>foundation models. In this study, we explore the fine-tuning pipeline for<br>vision-language foundation models by utilizing large language model as text<br>refiner with special-designed adaptation strategies and task-driven heads. Our<br>approach has been extensively evaluated on six ultrasound datasets and two<br>tasks: segmentation and classification. The experimental results show that our<br>method can effectively improve the performance of vision-language foundation<br>models for ultrasound image analysis, and outperform the existing<br>state-of-the-art vision-language and pure foundation models. The source code of<br>this study is available at<br>\href{<a target="_blank" rel="noopener" href="https://github.com/jinggqu/NextGen-UIA%7D%7BGitHub%7D">https://github.com/jinggqu/NextGen-UIA}{GitHub}</a>.</p>
</details>


<h3 id="95-Spatial-Transcriptomics-Expression-Prediction-from-Histopathology-Based-on-Cross-Modal-Mask-Reconstruction-and-Contrastive-Learning"><a href="#95-Spatial-Transcriptomics-Expression-Prediction-from-Histopathology-Based-on-Cross-Modal-Mask-Reconstruction-and-Contrastive-Learning" class="headerlink" title="[95] Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08854">Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning</a></h3><p><em>Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 该研究提出了一种基于对比学习的深度学习方法，从全切片图像预测空间转录组数据，显著提升了基因表达的预测准确性。</p>
<details>
  <summary>Details</summary>
Motivation: 由于空间转录组数据获取成本高，大规模数据难以获得，因此开发了一种高效预测方法。

<p>Method: 采用对比学习的深度学习框架，从全切片图像预测基因表达。</p>
<p>Result: 在六种疾病数据集上，预测高表达基因、高变异基因和标记基因的PCC分别提升了6.27%、6.11%和11.26%。</p>
<p>Conclusion: 该方法不仅保持了基因间相关性，还适用于小样本数据，并展示了在癌症组织定位中的潜力。</p>
<p>Abstract: Spatial transcriptomics is a technology that captures gene expression levels<br>at different spatial locations, widely used in tumor microenvironment analysis<br>and molecular profiling of histopathology, providing valuable insights into<br>resolving gene expression and clinical diagnosis of cancer. Due to the high<br>cost of data acquisition, large-scale spatial transcriptomics data remain<br>challenging to obtain. In this study, we develop a contrastive learning-based<br>deep learning method to predict spatially resolved gene expression from<br>whole-slide images. Evaluation across six different disease datasets<br>demonstrates that, compared to existing studies, our method improves Pearson<br>Correlation Coefficient (PCC) in the prediction of highly expressed genes,<br>highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%<br>respectively. Further analysis indicates that our method preserves gene-gene<br>correlations and applies to datasets with limited samples. Additionally, our<br>method exhibits potential in cancer tissue localization based on biomarker<br>expression.</p>
</details>


<h3 id="96-StreamSplat-Towards-Online-Dynamic-3D-Reconstruction-from-Uncalibrated-Video-Streams"><a href="#96-StreamSplat-Towards-Online-Dynamic-3D-Reconstruction-from-Uncalibrated-Video-Streams" class="headerlink" title="[96] StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08862">StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</a></h3><p><em>Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: StreamSplat是一个实时重建动态3D场景的框架，通过静态编码器和动态解码器的创新技术，解决了未校准输入、动态场景建模和长期稳定性的挑战。</p>
<details>
  <summary>Details</summary>
Motivation: 现有的方法难以同时处理未校准输入、动态场景建模和长期稳定性三大挑战，因此提出了StreamSplat。

<p>Method: 采用静态编码器中的概率采样机制和动态解码器中的双向变形场技术，实现在线重建。</p>
<p>Result: 在静态和动态基准测试中，StreamSplat表现优于现有方法，支持任意长度视频流的重建。</p>
<p>Conclusion: StreamSplat在重建质量和动态建模方面展现优越性能，是首个支持在线重建的框架。</p>
<p>Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams<br>is crucial for numerous real-world applications. However, existing methods<br>struggle to jointly address three key challenges: 1) processing uncalibrated<br>inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)<br>maintaining long-term stability and computational efficiency. To this end, we<br>introduce StreamSplat, the first fully feed-forward framework that transforms<br>uncalibrated video streams of arbitrary length into dynamic 3D Gaussian<br>Splatting (3DGS) representations in an online manner, capable of recovering<br>scene dynamics from temporally local observations. We propose two key technical<br>innovations: a probabilistic sampling mechanism in the static encoder for 3DGS<br>position prediction, and a bidirectional deformation field in the dynamic<br>decoder that enables robust and efficient dynamic modeling. Extensive<br>experiments on static and dynamic benchmarks demonstrate that StreamSplat<br>consistently outperforms prior works in both reconstruction quality and dynamic<br>scene modeling, while uniquely supporting online reconstruction of arbitrarily<br>long video streams. Code and models are available at<br><a target="_blank" rel="noopener" href="https://github.com/nickwzk/StreamSplat">https://github.com/nickwzk/StreamSplat</a>.</p>
</details>


<h3 id="97-DiscoVLA-Discrepancy-Reduction-in-Vision-Language-and-Alignment-for-Parameter-Efficient-Video-Text-Retrieval"><a href="#97-DiscoVLA-Discrepancy-Reduction-in-Vision-Language-and-Alignment-for-Parameter-Efficient-Video-Text-Retrieval" class="headerlink" title="[97] DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08887">DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval</a></h3><p><em>Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 本文提出了DiscoVLA方法，通过同时解决视觉、语言和对齐三个方面的差异，显著提升了视频-文本检索的性能。</p>
<details>
  <summary>Details</summary>
Motivation: 现有方法在将图像级预训练模型CLIP迁移到视频-文本检索时，主要关注视觉差异，而忽略了语言和对齐差异，导致性能不足。

<p>Method: 提出DiscoVLA方法，通过图像-视频特征融合减少视觉和语言差异，生成伪图像标题学习图像级对齐，并通过图像-视频对齐蒸馏增强视频级对齐。</p>
<p>Result: 在MSRVTT数据集上，DiscoVLA的R@1达到50.5%，优于之前方法1.5%。</p>
<p>Conclusion: DiscoVLA通过综合解决视觉、语言和对齐差异，显著提升了视频-文本检索性能，为相关研究提供了新思路。</p>
<p>Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP<br>for video-text retrieval is a prominent area of research. While CLIP is focused<br>on image-level vision-language matching, video-text retrieval demands<br>comprehensive understanding at the video level. Three key discrepancies emerge<br>in the transfer from image-level to video-level: vision, language, and<br>alignment. However, existing methods mainly focus on vision while neglecting<br>language and alignment. In this paper, we propose Discrepancy Reduction in<br>Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all<br>three discrepancies. Specifically, we introduce Image-Video Features Fusion to<br>integrate image-level and video-level features, effectively tackling both<br>vision and language discrepancies. Additionally, we generate pseudo image<br>captions to learn fine-grained image-level alignment. To mitigate alignment<br>discrepancies, we propose Image-to-Video Alignment Distillation, which<br>leverages image-level alignment knowledge to enhance video-level alignment.<br>Extensive experiments demonstrate the superiority of our DiscoVLA. In<br>particular, on MSRVTT with CLIP (ViT-B&#x2F;16), DiscoVLA outperforms previous<br>methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is<br>available at <a target="_blank" rel="noopener" href="https://github.com/LunarShen/DsicoVLA">https://github.com/LunarShen/DsicoVLA</a>.</p>
</details>


<h3 id="98-Product-of-Experts-for-Visual-Generation"><a href="#98-Product-of-Experts-for-Visual-Generation" class="headerlink" title="[98] Product of Experts for Visual Generation"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08894">Product of Experts for Visual Generation</a></h3><p><em>Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一个专家乘积（PoE）框架，通过训练自由的方法从异构模型中组合知识，并在图像和视频合成任务中显示出优于单一方法的可控性。</p>
<details>
  <summary>Details</summary>
Motivation: 整合来自不同来源（如视觉生成模型、视觉语言模型和人类知识）的多样化知识，以增强视觉生成任务的表现。

<p>Method: 使用专家乘积框架和退火重要性采样（AIS）从异质模型中组合知识。</p>
<p>Result: 在图像和视频合成任务中表现出更好的可控性，并提供灵活的用户界面。</p>
<p>Conclusion: 该框架在视觉生成任务中具有潜力，能够灵活整合多源知识。</p>
<p>Abstract: Modern neural models capture rich priors and have complementary knowledge<br>over shared data domains, e.g., images and videos. Integrating diverse<br>knowledge from multiple sources – including visual generative models, visual<br>language models, and sources with human-crafted knowledge such as graphics<br>engines and physics simulators – remains under-explored. We propose a Product<br>of Experts (PoE) framework that performs inference-time knowledge composition<br>from heterogeneous models. This training-free approach samples from the product<br>distribution across experts via Annealed Importance Sampling (AIS). Our<br>framework shows practical benefits in image and video synthesis tasks, yielding<br>better controllability than monolithic methods and additionally providing<br>flexible user interfaces for specifying visual generation goals.</p>
</details>


<h3 id="99-WetCat-Automating-Skill-Assessment-in-Wetlab-Cataract-Surgery-Videos"><a href="#99-WetCat-Automating-Skill-Assessment-in-Wetlab-Cataract-Surgery-Videos" class="headerlink" title="[99] WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08896">WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos</a></h3><p><em>Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: WetCat是首个专门用于自动化技能评估的湿实验室白内障手术视频数据集，旨在解决传统手动评估的低效和主观性问题。</p>
<details>
  <summary>Details</summary>
Motivation: 传统湿实验室培训依赖手动评估，效率低且主观性强，亟需自动化工具提升效率和客观性。

<p>Method: 引入WetCat数据集，包含高分辨率手术视频、详细阶段标注和关键结构语义分割，支持标准化技能评估。</p>
<p>Result: WetCat为开发可解释的AI评估工具奠定基础，推动客观化、可扩展的手术培训。</p>
<p>Conclusion: WetCat填补了湿实验室自动化技能评估的空白，为眼科培训设立了新标准。</p>
<p>Abstract: To meet the growing demand for systematic surgical training, wetlab<br>environments have become indispensable platforms for hands-on practice in<br>ophthalmology. Yet, traditional wetlab training depends heavily on manual<br>performance evaluations, which are labor-intensive, time-consuming, and often<br>subject to variability. Recent advances in computer vision offer promising<br>avenues for automated skill assessment, enhancing both the efficiency and<br>objectivity of surgical education. Despite notable progress in ophthalmic<br>surgical datasets, existing resources predominantly focus on real surgeries or<br>isolated tasks, falling short of supporting comprehensive skill evaluation in<br>controlled wetlab settings. To address these limitations, we introduce WetCat,<br>the first dataset of wetlab cataract surgery videos specifically curated for<br>automated skill assessment. WetCat comprises high-resolution recordings of<br>surgeries performed by trainees on artificial eyes, featuring comprehensive<br>phase annotations and semantic segmentations of key anatomical structures.<br>These annotations are meticulously designed to facilitate skill assessment<br>during the critical capsulorhexis and phacoemulsification phases, adhering to<br>standardized surgical skill assessment frameworks. By focusing on these<br>essential phases, WetCat enables the development of interpretable, AI-driven<br>evaluation tools aligned with established clinical metrics. This dataset lays a<br>strong foundation for advancing objective, scalable surgical education and sets<br>a new benchmark for automated workflow analysis and skill assessment in<br>ophthalmology training. The dataset and annotations are publicly available in<br>Synapse <a target="_blank" rel="noopener" href="https://www.synapse.org/Synapse:syn66401174/files">https://www.synapse.org/Synapse:syn66401174/files</a>.</p>
</details>


<h3 id="100-MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis"><a href="#100-MIRAGE-Multimodal-foundation-model-and-benchmark-for-comprehensive-retinal-OCT-image-analysis" class="headerlink" title="[100] MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08900">MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis</a></h3><p><em>José Morano,Botond Fazekas,Emese Sükei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunović</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出了一种多模态基础模型MIRAGE，用于分析OCT和SLO图像，并建立了一个新的评估基准，验证其优于现有方法。</p>
<details>
  <summary>Details</summary>
Motivation: 现有眼科AI模型依赖大量标注数据且在独立数据上表现不佳，MIRAGE旨在解决这些问题。

<p>Method: 提出了多模态基础模型MIRAGE，同时设计了OCT&#x2F;SLO分类与分割任务的评估基准。</p>
<p>Result: MIRAGE在分类和分割任务中均优于通用和专用基础模型。</p>
<p>Conclusion: MIRAGE为眼科图像AI分析提供了更稳健的基础，代码和基准已开源。</p>
<p>Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting<br>clinicians in analyzing ophthalmic images, such as optical coherence tomography<br>(OCT). However, developing AI models often requires extensive annotation, and<br>existing models tend to underperform on independent, unseen data. Foundation<br>models (FMs), large AI models trained on vast unlabeled datasets, have shown<br>promise in overcoming these challenges. Nonetheless, available FMs for<br>ophthalmology lack extensive validation, especially for segmentation tasks, and<br>focus on a single imaging modality. In this context, we propose MIRAGE, a novel<br>multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)<br>images. Additionally, we propose a new evaluation benchmark with OCT&#x2F;SLO<br>classification and segmentation tasks. The comparison with general and<br>specialized FMs and segmentation methods shows the superiority of MIRAGE in<br>both types of tasks, highlighting its suitability as a basis for the<br>development of robust AI systems for retinal OCT image analysis. Both MIRAGE<br>and the evaluation benchmark are publicly available:<br><a target="_blank" rel="noopener" href="https://github.com/j-morano/MIRAGE">https://github.com/j-morano/MIRAGE</a>.</p>
</details>


<h3 id="101-Inherently-Faithful-Attention-Maps-for-Vision-Transformers"><a href="#101-Inherently-Faithful-Attention-Maps-for-Vision-Transformers" class="headerlink" title="[101] Inherently Faithful Attention Maps for Vision Transformers"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08915">Inherently Faithful Attention Maps for Vision Transformers</a></h3><p><em>Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 提出一种基于注意力机制的方法，通过二进制注意力掩码确保只有关注的图像区域影响预测，解决上下文偏差问题。</p>
<details>
  <summary>Details</summary>
Motivation: 上下文可能强烈影响物体感知，导致偏差表示，尤其是在物体出现在分布外背景时。

<p>Method: 提出两阶段框架：第一阶段处理全图以发现物体部分和任务相关区域；第二阶段利用注意力掩码限制感受野到这些区域，进行集中分析并过滤潜在噪声。</p>
<p>Result: 在多个基准测试中，该方法显著提升了对虚假相关性和分布外背景的鲁棒性。</p>
<p>Conclusion: 两阶段联合训练框架能够有效解决上下文偏差问题，提升模型表现。</p>
<p>Abstract: We introduce an attention-based method that uses learned binary attention<br>masks to ensure that only attended image regions influence the prediction.<br>Context can strongly affect object perception, sometimes leading to biased<br>representations, particularly when objects appear in out-of-distribution<br>backgrounds. At the same time, many image-level object-centric tasks require<br>identifying relevant regions, often requiring context. To address this<br>conundrum, we propose a two-stage framework: stage 1 processes the full image<br>to discover object parts and identify task-relevant regions, while stage 2<br>leverages input attention masking to restrict its receptive field to these<br>regions, enabling a focused analysis while filtering out potentially spurious<br>information. Both stages are trained jointly, allowing stage 2 to refine stage</p>
<ol>
<li>Extensive experiments across diverse benchmarks demonstrate that our<br>approach significantly improves robustness against spurious correlations and<br>out-of-distribution backgrounds.</li>
</ol>
</details>


<h3 id="102-Socratic-MCTS-Test-Time-Visual-Reasoning-by-Asking-the-Right-Questions"><a href="#102-Socratic-MCTS-Test-Time-Visual-Reasoning-by-Asking-the-Right-Questions" class="headerlink" title="[102] Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08927">Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions</a></h3><p><em>David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi</em></p>
<p>Main category: cs.CV</p>
<p>TL;DR: 论文探讨了一种在视觉语言模型中通过搜索机制诱导隐藏知识和长推理链的方法，无需额外训练。</p>
<details>
  <summary>Details</summary>
Motivation: 研究旨在探索是否可以从已部署的非推理模型中引出隐藏知识，而无需重新训练或监督。

<p>Method: 采用蒙特卡洛树搜索（MCTS）启发式算法，通过注入子问题-子答案对来引导模型推理。</p>
<p>Result: 在三个基准测试中表现一致提升，尤其在MMMU-PRO上总体提升2%，其中文科领域提升9%。</p>
<p>Conclusion: 将推理视为搜索过程，可以有效帮助非推理模型连接碎片化知识并生成长推理链。</p>
<p>Abstract: Recent research in vision-language models (VLMs) has centered around the<br>possibility of equipping them with implicit long-form chain-of-thought<br>reasoning – akin to the success observed in language models – via<br>distillation and reinforcement learning. But what about the non-reasoning<br>models already trained and deployed across the internet? Should we simply<br>abandon them, or is there hope for a search mechanism that can elicit hidden<br>knowledge and induce long reasoning traces – without any additional training<br>or supervision? In this paper, we explore this possibility using a Monte Carlo<br>Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer<br>pairs into the model’s output stream. We show that framing reasoning as a<br>search process – where subquestions act as latent decisions within a broader<br>inference trajectory – helps the model “connect the dots” between fragmented<br>knowledge and produce extended reasoning traces in non-reasoning models. We<br>evaluate our method across three benchmarks and observe consistent<br>improvements. Notably, our approach yields a 2% overall improvement on<br>MMMU-PRO, including a significant 9% gain in Liberal Arts.</p>
</details>


<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="103-Hierarchical-Lexical-Graph-for-Enhanced-Multi-Hop-Retrieval"><a href="#103-Hierarchical-Lexical-Graph-for-Enhanced-Multi-Hop-Retrieval" class="headerlink" title="[103] Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08074">Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval</a></h3><p><em>Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala</em></p>
<p>Main category: cs.IR</p>
<p>TL;DR: 论文提出了一种分层词汇图（HLG）索引方法，改进了基于检索增强生成（RAG）的模型在多文档语义分散情况下的表现，并通过两种检索器和合成数据集验证了其有效性。</p>
<details>
  <summary>Details</summary>
Motivation: 传统的RAG方法在多文档跨语义检索时表现不佳，论文旨在解决这一问题。

<p>Method: 设计了HLG索引结构，并构建了两种检索器（StatementGraphRAG和TopicGraphRAG），同时提出了合成数据集生成方法。</p>
<p>Result: 在五个数据集上的实验显示，方法比传统RAG在召回率和正确率上平均提升了23.1%。</p>
<p>Conclusion: HLG索引和配套检索器显著提升了RAG在多文档检索中的表现，且开源工具可供使用。</p>
<p>Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in<br>external evidence, yet it still falters when answers must be pieced together<br>across semantically distant documents. We close this gap with the Hierarchical<br>Lexical Graph (HLG), a three-tier index that (i) traces every atomic<br>proposition to its source, (ii) clusters propositions into latent topics, and<br>(iii) links entities and relations to expose cross-document paths. On top of<br>HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,<br>which performs fine-grained entity-aware beam search over propositions for<br>high-precision factoid questions, and TopicGraphRAG, which selects coarse<br>topics before expanding along entity links to supply broad yet relevant context<br>for exploratory queries. Additionally, existing benchmarks lack the complexity<br>required to rigorously evaluate multi-hop summarization systems, often focusing<br>on single-document queries or limited datasets. To address this, we introduce a<br>synthetic dataset generation pipeline that curates realistic, multi-document<br>question-answer pairs, enabling robust evaluation of multi-hop retrieval<br>systems. Extensive experiments across five datasets demonstrate that our<br>methods outperform naive chunk-based RAG achieving an average relative<br>improvement of 23.1% in retrieval recall and correctness. Open-source Python<br>library is available at <a target="_blank" rel="noopener" href="https://github.com/awslabs/graphrag-toolkit">https://github.com/awslabs/graphrag-toolkit</a>.</p>
</details>


<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="104-Modality-Balancing-Preference-Optimization-of-Large-Multimodal-Models-by-Adversarial-Negative-Mining"><a href="#104-Modality-Balancing-Preference-Optimization-of-Large-Multimodal-Models-by-Adversarial-Negative-Mining" class="headerlink" title="[104] Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08022">Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining</a></h3><p><em>Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文提出了一种名为MBPO的新偏好学习框架，通过生成硬负样本和结合在线数据来解决大型多模态模型（LMMs）中的模态不平衡问题。</p>
<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法未能有效抑制LLM骨干的内部偏差，且依赖离线数据，无法适应动态分布变化。

<p>Method: MBPO通过对抗性扰动生成硬负样本，并结合在线数据与GRPO方法进行训练。</p>
<p>Result: 实验表明MBPO能提升LMM在视觉语言任务中的表现并减少幻觉。</p>
<p>Conclusion: MBPO通过平衡模态输入有效解决了LMM的模态不平衡问题。</p>
<p>Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been<br>significantly advanced by instruction tuning and further strengthened by recent<br>preference optimization. Yet, most LMMs still suffer from severe modality<br>imbalance during reasoning, i.e., outweighing language prior biases over visual<br>inputs, which bottlenecks their generalization to downstream tasks and causes<br>hallucinations. However, existing preference optimization approaches for LMMs<br>do not focus on restraining the internal biases of their Large Language Model<br>(LLM) backbones when curating the training data. Moreover, they heavily rely on<br>offline data and lack the capacity to explore diverse responses adaptive to<br>dynamic distributional shifts during training. Meanwhile, Group Relative Policy<br>Optimization (GRPO), a recent method using online-generated data and verified<br>rewards to improve reasoning capabilities, remains largely underexplored in LMM<br>alignment. In this paper, we propose a novel preference learning framework,<br>Modality-Balancing Preference Optimization (MBPO), to address the modality<br>imbalance in LMMs. MBPO constructs a more effective offline preference dataset<br>by generating hard negatives, i.e., rejected responses misled by LLM biases due<br>to limited usage of visual information, through adversarial perturbation of<br>input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended<br>tasks to generate online responses with verified rewards. GRPO is then employed<br>to train the model with offline-online hybrid data. Extensive experiments<br>demonstrate that MBPO can enhance LMM performance on challenging<br>vision-language tasks and effectively reduce hallucinations.</p>
</details>


<h3 id="105-Bingo-Boosting-Efficient-Reasoning-of-LLMs-via-Dynamic-and-Significance-based-Reinforcement-Learning"><a href="#105-Bingo-Boosting-Efficient-Reasoning-of-LLMs-via-Dynamic-and-Significance-based-Reinforcement-Learning" class="headerlink" title="[105] Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.08125">Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning</a></h3><p><em>Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang</em></p>
<p>Main category: cs.LG</p>
<p>TL;DR: 论文提出Bingo框架，通过改进基于长度的奖励设计来提升语言模型的高效推理能力，兼顾准确性和效率。</p>
<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型展现强大推理能力，但输出冗长且冗余，现有强化学习方法多关注准确性，忽视推理效率。Bingo框架旨在解决这一问题。

<p>Method: Bingo引入两种机制：1)显著性感知长度奖励，逐步减少不重要标记；2)动态长度奖励，初期鼓励详细推理，后期提高效率。</p>
<p>Result: 在多个推理基准测试中，Bingo在准确性和效率上均优于基准方法，实现两者的良好平衡。</p>
<p>Conclusion: Bingo的成功证明通过显式训练提升语言模型高效推理的潜力，为未来研究提供方向。</p>
<p>Abstract: Large language models have demonstrated impressive reasoning capabilities,<br>yet they often suffer from inefficiencies due to unnecessarily verbose or<br>redundant outputs. While many works have explored reinforcement learning (RL)<br>to enhance reasoning abilities, most primarily focus on improving accuracy,<br>with limited attention to reasoning efficiency. Some existing approaches<br>introduce direct length-based rewards to encourage brevity, but this often<br>leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL<br>framework that advances length-based reward design to boost efficient<br>reasoning. Bingo incorporates two key mechanisms: a significance-aware length<br>reward, which gradually guides the model to reduce only insignificant tokens,<br>and a dynamic length reward, which initially encourages elaborate reasoning for<br>hard questions but decays over time to improve overall efficiency. Experiments<br>across multiple reasoning benchmarks show that Bingo improves both accuracy and<br>efficiency. It outperforms the vanilla reward and several other length-based<br>reward baselines in RL, achieving a favorable trade-off between accuracy and<br>efficiency. These results underscore the potential of training LLMs explicitly<br>for efficient reasoning.</p>
</details>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-06-12/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
