<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Byter">







<title>2025-10-10 | Daily arXiv</title>



    <link rel="icon" href="/icon.png">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    






<script src='https://unpkg.com/valine@1.4.16/dist/Valine.min.js'></script>



  <meta name="generator" content="Hexo 7.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Daily arXiv.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Archive</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                2025-10-10
            
            
        </div>
        <span class="post-date">
            Oct 10, 2025
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <div id=toc></div>

<h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1><ul>
<li><a href="#cs.CV">cs.CV</a> [Total: 60]</li>
<li><a href="#cs.CL">cs.CL</a> [Total: 65]</li>
<li><a href="#cs.CY">cs.CY</a> [Total: 2]</li>
<li><a href="#cs.LG">cs.LG</a> [Total: 5]</li>
<li><a href="#cs.CR">cs.CR</a> [Total: 2]</li>
<li><a href="#cs.AI">cs.AI</a> [Total: 4]</li>
<li><a href="#cs.IR">cs.IR</a> [Total: 1]</li>
<li><a href="#cs.RO">cs.RO</a> [Total: 3]</li>
<li><a href="#cs.SD">cs.SD</a> [Total: 2]</li>
<li><a href="#cs.HC">cs.HC</a> [Total: 1]</li>
<li><a href="#cs.GR">cs.GR</a> [Total: 1]</li>
<li><a href="#eess.IV">eess.IV</a> [Total: 4]</li>
</ul>
<div id='cs.CV'></div>

<h1 id="cs-CV-Back"><a href="#cs-CV-Back" class="headerlink" title="cs.CV [Back]"></a>cs.CV <a href="#toc">[Back]</a></h1><h3 id="1-Milestone-Determination-for-Autonomous-Railway-Operation-cs-CV-cs-LGPDF"><a href="#1-Milestone-Determination-for-Autonomous-Railway-Operation-cs-CV-cs-LGPDF" class="headerlink" title="[1] Milestone Determination for Autonomous Railway Operation cs.CV | cs.LGPDF"></a>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06229">Milestone Determination for Autonomous Railway Operation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06229" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Josh Hunter, John McDermid, Simon Burton, Poppy Fynes, Mia Dempster</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种基于里程碑确定的计算机视觉方法，用于铁路自动化，通过生成上下文相关的序列数据，简化动态组件识别，专注于关键决策点。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 铁路自动化领域的计算机视觉系统面临高质量序列数据稀缺的问题，传统数据集缺乏时空上下文，而替代方案又存在真实性和适用性问题。</p>
<p><strong>Result:</strong> 该方法为铁路自动化提供了一个实用的框架，能够在可控环境中训练视觉代理，优化实时决策。</p>
<p><strong>Insight:</strong> 专注于关键决策点而非动态内容的泛化识别，可以显著简化计算机视觉系统的训练过程，同时提高其在铁路自动化中的实用性和可靠性。</p>
<p><strong>Abstract:</strong> In the field of railway automation, one of the key challenges has been the development of effective computer vision systems due to the limited availability of high-quality, sequential data. Traditional datasets are restricted in scope, lacking the spatio temporal context necessary for real-time decision-making, while alternative solutions introduce issues related to realism and applicability. By focusing on route-specific, contextually relevant cues, we can generate rich, sequential datasets that align more closely with real-world operational logic. The concept of milestone determination allows for the development of targeted, rule-based models that simplify the learning process by eliminating the need for generalized recognition of dynamic components, focusing instead on the critical decision points along a route. We argue that this approach provides a practical framework for training vision agents in controlled, predictable environments, facilitating safer and more efficient machine learning systems for railway automation.</p>
  </div>
</details>

<hr>
<h3 id="2-CML-Bench-A-Framework-for-Evaluating-and-Enhancing-LLM-Powered-Movie-Scripts-Generation-cs-CV-cs-CLPDF"><a href="#2-CML-Bench-A-Framework-for-Evaluating-and-Enhancing-LLM-Powered-Movie-Scripts-Generation-cs-CV-cs-CLPDF" class="headerlink" title="[2] CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation cs.CV | cs.CLPDF"></a>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06231">CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06231" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mingzhe Zheng, Dingjie Song, Guanyu Zhou, Jun You, Jiahao Zhan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了CML-Bench框架，用于评估和改进LLM生成的电影剧本质量，重点关注对话连贯性、角色一致性和情节合理性三个维度，并通过CML-Instruction提示策略提升了LLM生成剧本的效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLM在生成结构化文本方面表现出色，但电影剧本需要更复杂的故事性和情感深度，这是LLM目前难以捕捉的。</p>
<p><strong>Result:</strong> 实验表明，CML-Bench能有效区分高质量人类剧本和LLM生成剧本的弱点，而CML-Instruction显著提升了LLM生成剧本的质量和人类偏好。</p>
<p><strong>Insight:</strong> 1) 电影剧本生成需关注故事性和情感深度；2) 结构化提示策略（如CML-Instruction）能显著改善LLM生成效果；3) 定量评估框架有助于系统性优化生成内容。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable proficiency in generating highly structured texts. However, while exhibiting a high degree of structural organization, movie scripts demand an additional layer of nuanced storytelling and emotional depth-the ‘soul’ of compelling cinema-that LLMs often fail to capture. To investigate this deficiency, we first curated CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup Language (CML), where ‘content’ consists of segments from esteemed, high-quality movie scripts and ‘summary’ is a concise description of the content. Through an in-depth analysis of the intrinsic multi-shot continuity and narrative structures within these authentic scripts, we identified three pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we propose the CML-Bench, featuring quantitative metrics across these dimensions. CML-Bench effectively assigns high scores to well-crafted, human-written scripts while concurrently pinpointing the weaknesses in screenplays generated by LLMs. To further validate our benchmark, we introduce CML-Instruction, a prompting strategy with detailed instructions on character dialogue and event logic, to guide LLMs to generate more structured and cinematically sound scripts. Extensive experiments validate the effectiveness of our benchmark and demonstrate that LLMs guided by CML-Instruction generate higher-quality screenplays, with results aligned with human preferences.</p>
  </div>
</details>

<hr>
<h3 id="3-User-to-Video-A-Model-for-Spammer-Detection-Inspired-by-Video-Classification-Technology-cs-CVPDF"><a href="#3-User-to-Video-A-Model-for-Spammer-Detection-Inspired-by-Video-Classification-Technology-cs-CVPDF" class="headerlink" title="[3] User to Video: A Model for Spammer Detection Inspired by Video Classification Technology cs.CVPDF"></a>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06233">User to Video: A Model for Spammer Detection Inspired by Video Classification Technology</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06233" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haoyang Zhang, Zhou Yang, Yucai Pang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于视频分类技术的垃圾用户检测模型UVSD，通过将用户行为子空间视为帧图像并构建用户行为视频，结合视频分类算法进行检测。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 受视频分类技术的启发，将用户行为序列类比为视频帧，从而设计一种新的垃圾用户检测方法。</p>
<p><strong>Result:</strong> 在WEIBO和TWITTER数据集上，UVSD模型优于现有方法。</p>
<p><strong>Insight:</strong> 将用户行为序列建模为视频帧是一种新颖且有效的垃圾用户检测方法。</p>
<p><strong>Abstract:</strong> This article is inspired by video classification technology. If the user behavior subspace is viewed as a frame image, consecutive frame images are viewed as a video. Following this novel idea, a model for spammer detection based on user videoization, called UVSD, is proposed. Firstly, a user2piexl algorithm for user pixelization is proposed. Considering the adversarial behavior of user stances, the user is viewed as a pixel, and the stance is quantified as the pixel’s RGB. Secondly, a behavior2image algorithm is proposed for transforming user behavior subspace into frame images. Low-rank dense vectorization of subspace user relations is performed using representation learning, while cutting and diffusion algorithms are introduced to complete the frame imageization. Finally, user behavior videos are constructed based on temporal features. Subsequently, a video classification algorithm is combined to identify the spammers. Experiments using publicly available datasets, i.e., WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art methods.</p>
  </div>
</details>

<hr>
<h3 id="4-Uncertainty-Quantification-In-Surface-Landmines-and-UXO-Classification-Using-MC-Dropout-cs-CV-cs-AI-cs-LG-stat-OTPDF"><a href="#4-Uncertainty-Quantification-In-Surface-Landmines-and-UXO-Classification-Using-MC-Dropout-cs-CV-cs-AI-cs-LG-stat-OTPDF" class="headerlink" title="[4] Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout cs.CV | cs.AI | cs.LG | stat.OTPDF"></a>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06238">Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG | stat.OT</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06238" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sagar Lekhak, Emmett J. Ientilucci, Dimah Dera, Susmita Ghosh</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于MC Dropout的深度学习模型，用于量化地表地雷和未爆弹药（UXO）分类中的不确定性。通过将MC Dropout集成到经过微调的ResNet-50架构中，研究展示了在对抗性扰动和噪声条件下的预测可靠性评估。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统确定性神经网络在地雷和UXO分类中易受噪声和对抗性攻击影响，可能导致漏检或误分类。因此，需要量化不确定性以提高模型的可靠性和鲁棒性。</p>
<p><strong>Result:</strong> 实验结果表明，MC Dropout能够有效量化不确定性，并在对抗性和噪声条件下标记不可靠预测，为扫雷操作提供了额外的决策依据。</p>
<p><strong>Insight:</strong> 该研究强调了不确定性量化在扫雷任务中的重要性，并为开发更鲁棒的模型奠定了基础。同时，揭示了对抗性攻击对现有模型的威胁，呼唤更多研究提升实用场景中的可靠性。</p>
<p><strong>Abstract:</strong> Detecting surface landmines and unexploded ordnances (UXOs) using deep learning has shown promise in humanitarian demining. However, deterministic neural networks can be vulnerable to noisy conditions and adversarial attacks, leading to missed detection or misclassification. This study introduces the idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated into a fine-tuned ResNet-50 architecture for surface landmine and UXO classification, which was tested on a simulated dataset. Integrating the MC Dropout approach helps quantify epistemic uncertainty, providing an additional metric for prediction reliability, which could be helpful to make more informed decisions in demining operations. Experimental results on clean, adversarially perturbed, and noisy test images demonstrate the model’s ability to flag unreliable predictions under challenging conditions. This proof-of-concept study highlights the need for uncertainty quantification in demining, raises awareness about the vulnerability of existing neural networks in demining to adversarial threats, and emphasizes the importance of developing more robust and reliable models for practical applications.</p>
  </div>
</details>

<hr>
<h3 id="5-multimodars-A-Rust-powered-toolkit-for-multi-modality-cardiac-image-fusion-and-registration-cs-CV-physics-med-phPDF"><a href="#5-multimodars-A-Rust-powered-toolkit-for-multi-modality-cardiac-image-fusion-and-registration-cs-CV-physics-med-phPDF" class="headerlink" title="[5] multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration cs.CV | physics.med-phPDF"></a>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06241">multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | physics.med-ph</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06241" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Anselm W. Stark, Marc Ilic, Ali Mokhtari, Pooya Mohammadi Kazaj, Christoph Graeni</span></p>
<p><strong>TL;DR:</strong> multimodars是一个基于Rust的工具包，专注于多模态心脏图像的融合和配准，旨在解决现有工具在确定性、性能和灵活性上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 心脏成像中，不同模态的图像具有互补性（如高分辨率的血管内影像和提供整体几何结构的CCTA），但缺乏一个开源、灵活的工具包来支持多状态分析并提供高性能和确定性行为。</p>
<p><strong>Result:</strong> multimodars实现了高效的多模态图像融合和配准，并为用户提供了易于集成的管道工具。</p>
<p><strong>Insight:</strong> 通过结合高性能的Rust实现和灵活的NumPy数据模型，multimodars展示了在多模态医学图像处理中平衡性能与易用性的潜力。</p>
<p><strong>Abstract:</strong> Combining complementary imaging modalities is critical to build reliable 3D coronary models: intravascular imaging gives sub-millimetre resolution but limited whole-vessel context, while CCTA supplies 3D geometry but suffers from limited spatial resolution and artefacts (e.g., blooming). Prior work demonstrated intravascular&#x2F;CCTA fusion, yet no open, flexible toolkit is tailored for multi-state analysis (rest&#x2F;stress, pre-&#x2F;post-stenting) while offering deterministic behaviour, high performance, and easy pipeline integration. multimodars addresses this gap with deterministic alignment algorithms, a compact NumPy-centred data model, and an optimised Rust backend suitable for scalable, reproducible experiments. The package accepts CSV&#x2F;NumPy inputs including data formats produced by the AIVUS-CAA software</p>
  </div>
</details>

<hr>
<h3 id="6-Does-Physics-Knowledge-Emerge-in-Frontier-Models-cs-CVPDF"><a href="#6-Does-Physics-Knowledge-Emerge-in-Frontier-Models-cs-CVPDF" class="headerlink" title="[6] Does Physics Knowledge Emerge in Frontier Models? cs.CVPDF"></a>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06251">Does Physics Knowledge Emerge in Frontier Models?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06251" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ieva Bagdonaviciute, Vibhav Vineet</span></p>
<p><strong>TL;DR:</strong> 前沿视觉语言模型（VLMs）在视觉感知和通用推理方面表现优异，但其物理动力学理解能力尚不明确。本文通过三个物理模拟数据集（CLEVRER、Physion和Physion++）对六种前沿VLM进行了评测，发现感知能力与物理推理能力的相关性较弱，揭示了当前模型的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探究前沿视觉语言模型是否具备物理动力学理解能力，以及感知能力与物理推理能力之间的关系。</p>
<p><strong>Result:</strong> 当前VLMs在感知和物理推理任务中表现各异，但两类能力的相关性较弱，模型未能将二者紧密结合为因果理解能力。</p>
<p><strong>Insight:</strong> 前沿VLMs在物理动力学理解上存在局限性，感知与推理能力未紧密结合，亟需设计更紧密耦合的架构。</p>
<p><strong>Abstract:</strong> Leading Vision-Language Models (VLMs) show strong results in visual perception and general reasoning, but their ability to understand and predict physical dynamics remains unclear. We benchmark six frontier VLMs on three physical simulation datasets - CLEVRER, Physion, and Physion++ - where the evaluation tasks test whether a model can predict outcomes or hypothesize about alternative situations. To probe deeper, we design diagnostic subtests that isolate perception (objects, colors, occluders) from physics reasoning (motion prediction, spatial relations). Intuitively, stronger diagnostic performance should support higher evaluation accuracy. Yet our analysis reveals weak correlations: models that excel at perception or physics reasoning do not consistently perform better on predictive or counterfactual evaluation. This counterintuitive gap exposes a central limitation of current VLMs: perceptual and physics skills remain fragmented and fail to combine into causal understanding, underscoring the need for architectures that bind perception and reasoning more tightly.</p>
  </div>
</details>

<hr>
<h3 id="7-Enhanced-Self-Distillation-Framework-for-Efficient-Spiking-Neural-Network-Training-cs-CVPDF"><a href="#7-Enhanced-Self-Distillation-Framework-for-Efficient-Spiking-Neural-Network-Training-cs-CVPDF" class="headerlink" title="[7] Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training cs.CVPDF"></a>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06254">Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06254" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiaochen Zhao, Chengting Yu, Kairong Yu, Lei Liu, Aili Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种增强的自蒸馏框架，用于高效训练脉冲神经网络（SNN），通过联合优化速率反向传播和自蒸馏，减少了训练复杂度并提升了性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的SNN训练方法（如代理梯度和BPTT）在性能和计算开销上均落后于人工神经网络（ANN），尤其是在时间维度上计算和内存开销线性增长。论文旨在解决这一问题，提升SNN的训练效率和性能。</p>
<p><strong>Result:</strong> 在CIFAR-10、CIFAR-100、CIFAR10-DVS和ImageNet等数据集上的实验表明，该方法在减少训练复杂度的同时实现了高性能SNN训练。</p>
<p><strong>Insight:</strong> 低质量的自生成知识可能阻碍收敛，因此分解教师信号并仅使用可靠部分能有效提升训练效率。</p>
<p><strong>Abstract:</strong> Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on neuromorphic hardware due to their sparse activation patterns. However, conventional training methods based on surrogate gradients and Backpropagation Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in performance, but also incur significant computational and memory overheads that grow linearly with the temporal dimension. To enable high-performance SNN training under limited computational resources, we propose an enhanced self-distillation framework, jointly optimized with rate-based backpropagation. Specifically, the firing rates of intermediate SNN layers are projected onto lightweight ANN branches, and high-quality knowledge generated by the model itself is used to optimize substructures through the ANN pathways. Unlike traditional self-distillation paradigms, we observe that low-quality self-generated knowledge may hinder convergence. To address this, we decouple the teacher signal into reliable and unreliable components, ensuring that only reliable knowledge is used to guide the optimization of the model. Extensive experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that our method reduces training complexity while achieving high-performance SNN training. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn">https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn</a>.</p>
  </div>
</details>

<hr>
<h3 id="8-Ensemble-Deep-Learning-and-LLM-Assisted-Reporting-for-Automated-Skin-Lesion-Diagnosis-cs-CV-cs-AI-cs-LGPDF"><a href="#8-Ensemble-Deep-Learning-and-LLM-Assisted-Reporting-for-Automated-Skin-Lesion-Diagnosis-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[8] Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis cs.CV | cs.AI | cs.LGPDF"></a>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06260">Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06260" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sher Khan, Raz Muhammad, Adil Hussain, Muhammad Sajjad, Muhammad Rashid</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种统一的AI框架，通过集成异构卷积神经网络和大型语言模型，实现皮肤病变的自动诊断和临床报告生成，提高诊断可靠性和患者沟通效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前皮肤病诊断存在观察者间变异性和肤色数据偏见等问题，且现有系统多将自然语言处理作为事后解释而非临床决策的一部分。</p>
<p><strong>Result:</strong> 该方法提升了诊断精确性，同时支持从检测到患者教育的全流程，改善了皮肤病变的早期干预。</p>
<p><strong>Insight:</strong> 融合多模态AI和临床工作流，不仅能提高诊断质量，还能通过患者教育增强早期干预效果。</p>
<p><strong>Abstract:</strong> Cutaneous malignancies demand early detection for favorable outcomes, yet current diagnostics suffer from inter-observer variability and access disparities. While AI shows promise, existing dermatological systems are limited by homogeneous architectures, dataset biases across skin tones, and fragmented approaches that treat natural language processing as separate post-hoc explanations rather than integral to clinical decision-making. We introduce a unified framework that fundamentally reimagines AI integration for dermatological diagnostics through two synergistic innovations. First, a purposefully heterogeneous ensemble of architecturally diverse convolutional neural networks provides complementary diagnostic perspectives, with an intrinsic uncertainty mechanism flagging discordant cases for specialist review – mimicking clinical best practices. Second, we embed large language model capabilities directly into the diagnostic workflow, transforming classification outputs into clinically meaningful assessments that simultaneously fulfill medical documentation requirements and deliver patient-centered education. This seamless integration generates structured reports featuring precise lesion characterization, accessible diagnostic reasoning, and actionable monitoring guidance – empowering patients to recognize early warning signs between visits. By addressing both diagnostic reliability and communication barriers within a single cohesive system, our approach bridges the critical translational gap that has prevented previous AI implementations from achieving clinical impact. The framework represents a significant advancement toward deployable dermatological AI that enhances diagnostic precision while actively supporting the continuum of care from initial detection through patient education, ultimately improving early intervention rates for skin lesions.</p>
  </div>
</details>

<hr>
<h3 id="9-Vision-Transformer-for-Transient-Noise-Classification-cs-CV-astro-ph-IM-cs-LG-gr-qcPDF"><a href="#9-Vision-Transformer-for-Transient-Noise-Classification-cs-CV-astro-ph-IM-cs-LG-gr-qcPDF" class="headerlink" title="[9] Vision Transformer for Transient Noise Classification cs.CV | astro-ph.IM | cs.LG | gr-qcPDF"></a>[9] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06273">Vision Transformer for Transient Noise Classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | astro-ph.IM | cs.LG | gr-qc</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06273" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Divyansh Srivastava, Andrzej Niedzielski</span></p>
<p><strong>TL;DR:</strong> 使用Vision Transformer (ViT)模型对LIGO数据中的瞬态噪声（glitches）进行分类，结合Gravity Spy数据集和O3a运行中的两个新增类别，实现了92.26%的分类效率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> LIGO数据中的瞬态噪声（glitches）干扰了引力波的探测。随着O3运行的开展，引入了两个新的噪声类别，需要训练新模型以提高分类效果。</p>
<p><strong>Result:</strong> 实现了92.26%的分类效率，展示了ViT在区分瞬态噪声方面的潜力。</p>
<p><strong>Insight:</strong> Vision Transformer在引力波探测噪声分类中表现出色，有望进一步提升引力波探测的准确性。</p>
<p><strong>Abstract:</strong> Transient noise (glitches) in LIGO data hinders the detection of gravitational waves (GW). The Gravity Spy project has categorized these noise events into various classes. With the O3 run, there is the inclusion of two additional noise classes and thus a need to train new models for effective classification. We aim to classify glitches in LIGO data into 22 existing classes from the first run plus 2 additional noise classes from O3a using the Vision Transformer (ViT) model. We train a pre-trained Vision Transformer (ViT-B&#x2F;32) model on a combined dataset consisting of the Gravity Spy dataset with the additional two classes from the LIGO O3a run. We achieve a classification efficiency of 92.26%, demonstrating the potential of Vision Transformer to improve the accuracy of gravitational wave detection by effectively distinguishing transient noise.   Key words: gravitational waves –vision transformer –machine learning</p>
  </div>
</details>

<hr>
<h3 id="10-General-and-Efficient-Visual-Goal-Conditioned-Reinforcement-Learning-using-Object-Agnostic-Masks-cs-CV-cs-LGPDF"><a href="#10-General-and-Efficient-Visual-Goal-Conditioned-Reinforcement-Learning-using-Object-Agnostic-Masks-cs-CV-cs-LGPDF" class="headerlink" title="[10] General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks cs.CV | cs.LGPDF"></a>[10] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06277">General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06277" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fahim Shahriar, Cheryl Wang, Alireza Azimi, Gautham Vasan, Hany Hamed Elanwar</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于掩码的目标表示方法，用于目标条件强化学习（GCRL），通过对象无关的视觉提示实现高效学习与泛化。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有目标表示方法（如目标状态图像、3D坐标或独热向量）存在泛化性差、收敛慢或需要特殊设备的问题。掩码表示可以避免这些局限性。</p>
<p><strong>Result:</strong> 在仿真中达到99.9%的训练与未见测试对象的到达准确率，并成功应用于真实机器人任务。</p>
<p><strong>Insight:</strong> 掩码表示可以简化目标条件强化学习的复杂性，同时提升泛化能力和任务完成效率。</p>
<p><strong>Abstract:</strong> Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse objectives using a unified policy. The success of GCRL, however, is contingent on the choice of goal representation. In this work, we propose a mask-based goal representation system that provides object-agnostic visual cues to the agent, enabling efficient learning and superior generalization. In contrast, existing goal representation methods, such as target state images, 3D coordinates, and one-hot vectors, face issues of poor generalization to unseen objects, slow convergence, and the need for special cameras. Masks can be processed to generate dense rewards without requiring error-prone distance calculations. Learning with ground truth masks in simulation, we achieved 99.9% reaching accuracy on training and unseen test objects. Our proposed method can be utilized to perform pick-up tasks with high accuracy, without using any positional information of the target. Moreover, we demonstrate learning from scratch and sim-to-real transfer applications using two different physical robots, utilizing pretrained open vocabulary object detection models for mask generation.</p>
  </div>
</details>

<hr>
<h3 id="11-Improving-the-Spatial-Resolution-of-GONG-Solar-Images-to-GST-Quality-Using-Deep-Learning-cs-CV-cs-AIPDF"><a href="#11-Improving-the-Spatial-Resolution-of-GONG-Solar-Images-to-GST-Quality-Using-Deep-Learning-cs-CV-cs-AIPDF" class="headerlink" title="[11] Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning cs.CV | cs.AIPDF"></a>[11] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06281">Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06281" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenyang Li, Qin Li, Haimin Wang, Bo Shen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于GAN的超分辨率方法，用于提升GONG低分辨率太阳图像的质量，使其接近BBSO&#x2F;GST的高分辨率观测水平。通过Real-ESRGAN模型，显著恢复了太阳黑子和细丝等精细结构。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 高分辨率太阳成像对小规模动态特征（如细丝和纤维）的捕获至关重要。然而，GONG的全盘Hα图像分辨率不足，无法清晰呈现这些结构，因此需要一种有效的超分辨率方法。</p>
<p><strong>Result:</strong> 模型有效恢复了太阳黑子半影和细丝的精细细节，平均MSE为467.15，RMSE为21.59，交叉相关度为0.7794。图像对的轻微错位限制了定量表现。</p>
<p><strong>Insight:</strong> GAN技术在超分辨率任务中表现出色，但对图像对齐要求较高。未来可通过扩大数据集和优化对齐进一步提升重建质量。</p>
<p><strong>Abstract:</strong> High-resolution (HR) solar imaging is crucial for capturing fine-scale dynamic features such as filaments and fibrils. However, the spatial resolution of the full-disk H$\alpha$ images is limited and insufficient to resolve these small-scale structures. To address this, we propose a GAN-based superresolution approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the Global Oscillation Network Group (GONG) to a quality comparable with HR observations from the Big Bear Solar Observatory&#x2F;Goode Solar Telescope (BBSO&#x2F;GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a relativistic discriminator. We carefully aligned GONG-GST pairs. The model effectively recovers fine details within sunspot penumbrae and resolves fine details in filaments and fibrils, achieving an average mean squared error (MSE) of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC) of 0.7794. Slight misalignments between image pairs limit quantitative performance, which we plan to address in future work alongside dataset expansion to further improve reconstruction quality.</p>
  </div>
</details>

<hr>
<h3 id="12-ChainMPQ-Interleaved-Text-Image-Reasoning-Chains-for-Mitigating-Relation-Hallucinations-cs-CV-cs-AIPDF"><a href="#12-ChainMPQ-Interleaved-Text-Image-Reasoning-Chains-for-Mitigating-Relation-Hallucinations-cs-CV-cs-AIPDF" class="headerlink" title="[12] ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations cs.CV | cs.AIPDF"></a>[12] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06292">ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06292" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yike Wu, Yiwei Wang, Yujun Cai</span></p>
<p><strong>TL;DR:</strong> ChainMPQ是一种无需训练的方法，通过多视角问题和交错的图像-文本链，减少大型视觉语言模型中的关系幻觉问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 关系幻觉在大型视觉语言模型中占比最大但研究最少，影响了模型的可靠性。</p>
<p><strong>Result:</strong> 实验表明ChainMPQ在多模型和基准测试中显著减少关系幻觉，消融研究验证了其核心模块的有效性。</p>
<p><strong>Insight:</strong> 通过多视角问题和逐步推理的交错链，可以有效提升模型对关系的理解能力。</p>
<p><strong>Abstract:</strong> While Large Vision-Language Models (LVLMs) achieve strong performance in multimodal tasks, hallucinations continue to hinder their reliability. Among the three categories of hallucinations, which include object, attribute, and relation, relation hallucinations account for the largest proportion but have received the least attention. To address this issue, we propose ChainMPQ (Multi-Perspective Questions guided Interleaved Chain of Image and Text), a training-free method that improves relational inference in LVLMs by utilizing accumulated textual and visual memories. ChainMPQ first extracts subject and object keywords from the question to enhance the corresponding image regions. It then constructs multi-perspective questions that focus on the three core components of a relationship: the subject, the object, and the relation that links them. These questions are sequentially input to the model, with textual and visual memories from earlier steps providing supporting context for subsequent ones, thereby forming an interleaved chain of images and text that guides progressive relational reasoning. Experiments on multiple LVLMs and benchmarks show that ChainMPQ substantially reduces relation hallucinations, while ablation studies further validate the effectiveness of its three core modules.</p>
  </div>
</details>

<hr>
<h3 id="13-Scalable-deep-fusion-of-spaceborne-lidar-and-synthetic-aperture-radar-for-global-forest-structural-complexity-mapping-cs-CV-cs-LG-stat-APPDF"><a href="#13-Scalable-deep-fusion-of-spaceborne-lidar-and-synthetic-aperture-radar-for-global-forest-structural-complexity-mapping-cs-CV-cs-LG-stat-APPDF" class="headerlink" title="[13] Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping cs.CV | cs.LG | stat.APPDF"></a>[13] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06299">Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG | stat.AP</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06299" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tiago de Conto, John Armston, Ralph Dubayah</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种可扩展的深度学习方法，通过融合GEDI星载激光雷达和多模态SAR数据，生成了全球高分辨率（25米）的森林结构复杂度地图，实现了高效、准确的预测和不确定性估计。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统星载激光雷达（GEDI）采样稀疏，无法实现连续高分辨率森林结构复杂度制图。为了解决这一问题，需要结合SAR数据以实现全球范围的连续监测。</p>
<p><strong>Result:</strong> 模型全局R2达到0.82，能够生成高分辨率（25米）全球森林结构复杂度地图，并支持多时相监测。预测结果在不同生物群落和时间段均表现良好。</p>
<p><strong>Insight:</strong> 1. 多模态数据融合（激光雷达+ SAR）显著提升了森林结构复杂度的制图能力； 2. 轻量化模型设计使其具备可扩展性和计算效率，适合全球范围应用； 3. 不确定性估计和迁移学习为生态系统监测提供了灵活工具。</p>
<p><strong>Abstract:</strong> Forest structural complexity metrics integrate multiple canopy attributes into a single value that reflects habitat quality and ecosystem function. Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has enabled mapping of structural complexity in temperate and tropical forests, but its sparse sampling limits continuous high-resolution mapping. We present a scalable, deep learning framework fusing GEDI observations with multimodal Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25 m) wall-to-wall maps of forest structural complexity. Our adapted EfficientNetV2 architecture, trained on over 130 million GEDI footprints, achieves high performance (global R2 &#x3D; 0.82) with fewer than 400,000 parameters, making it an accessible tool that enables researchers to process datasets at any scale without requiring specialized computing infrastructure. The model produces accurate predictions with calibrated uncertainty estimates across biomes and time periods, preserving fine-scale spatial patterns. It has been used to generate a global, multi-temporal dataset of forest structural complexity from 2015 to 2022. Through transfer learning, this framework can be extended to predict additional forest structural variables with minimal computational cost. This approach supports continuous, multi-temporal monitoring of global forest structural dynamics and provides tools for biodiversity conservation and ecosystem management efforts in a changing climate.</p>
  </div>
</details>

<hr>
<h3 id="14-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding-cs-CVPDF"><a href="#14-Lumina-DiMOO-An-Omni-Diffusion-Large-Language-Model-for-Multi-Modal-Generation-and-Understanding-cs-CVPDF" class="headerlink" title="[14] Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding cs.CVPDF"></a>[14] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06308">Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06308" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan</span></p>
<p><strong>TL;DR:</strong> Lumina-DiMOO是一种基于全离散扩散模型的开源多模态生成与理解基础模型，通过高效的采样和多任务支持，超越现有开源统一多模态模型的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有统一多模态模型在采样效率和任务多样性上存在局限，Lumina-DiMOO旨在通过离散扩散模型解决这些问题。</p>
<p><strong>Result:</strong> 在多项任务中超越现有开源统一多模态模型，展现了更高的采样效率和性能。</p>
<p><strong>Insight:</strong> 离散扩散模型在多模态任务中具有潜力，为未来研究方向提供了新思路。</p>
<p><strong>Abstract:</strong> We introduce Lumina-DiMOO, an open-source foundational model for seamless multi-modal generation and understanding. Lumina-DiMOO sets itself apart from prior unified models by utilizing a fully discrete diffusion modeling to handle inputs and outputs across various modalities. This innovative approach allows Lumina-DiMOO to achieve higher sampling efficiency compared to previous autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a broad spectrum of multi-modal tasks, including text-to-image generation, image-to-image generation (e.g., image editing, subject-driven generation, and image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves state-of-the-art performance on multiple benchmarks, surpassing existing open-source unified multi-modal models. To foster further advancements in multi-modal and discrete diffusion model research, we release our code and checkpoints to the community. Project Page: <a target="_blank" rel="noopener" href="https://synbol.github.io/Lumina-DiMOO">https://synbol.github.io/Lumina-DiMOO</a>.</p>
  </div>
</details>

<hr>
<h3 id="15-TransFIRA-Transfer-Learning-for-Face-Image-Recognizability-Assessment-cs-CV-cs-AI-cs-LGPDF"><a href="#15-TransFIRA-Transfer-Learning-for-Face-Image-Recognizability-Assessment-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[15] TransFIRA: Transfer Learning for Face Image Recognizability Assessment cs.CV | cs.AI | cs.LGPDF"></a>[15] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06353">TransFIRA: Transfer Learning for Face Image Recognizability Assessment</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06353" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Allen Tu, Kartik Narayan, Joshua Gleason, Jennifer Xu, Matthew Meyn</span></p>
<p><strong>TL;DR:</strong> TransFIRA提出了一种基于迁移学习的轻量级、无需标注的人脸图像可识别性评估框架，通过嵌入空间的几何特性定义可识别性，实现了在验证任务上的SOTA性能，并拓展至其他模态。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统人脸图像质量评估方法依赖视觉启发式或标注数据，无法直接反映编码器的决策几何特性。</p>
<p><strong>Result:</strong> 在BRIAR和IJB-C数据集上实现了SOTA验证精度，并在跨数据集测试中表现出鲁棒性。</p>
<p><strong>Insight:</strong> 嵌入空间的几何特性为评估可识别性提供了自然且高效的依据，并可推广至其他识别任务。</p>
<p><strong>Abstract:</strong> Face recognition in unconstrained environments such as surveillance, video, and web imagery must contend with extreme variation in pose, blur, illumination, and occlusion, where conventional visual quality metrics fail to predict whether inputs are truly recognizable to the deployed encoder. Existing FIQA methods typically rely on visual heuristics, curated annotations, or computationally intensive generative pipelines, leaving their predictions detached from the encoder’s decision geometry. We introduce TransFIRA (Transfer Learning for Face Image Recognizability Assessment), a lightweight and annotation-free framework that grounds recognizability directly in embedding space. TransFIRA delivers three advances: (i) a definition of recognizability via class-center similarity (CCS) and class-center angular separation (CCAS), yielding the first natural, decision-boundary–aligned criterion for filtering and weighting; (ii) a recognizability-informed aggregation strategy that achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly doubling correlation with true recognizability, all without external labels, heuristics, or backbone-specific training; and (iii) new extensions beyond faces, including encoder-grounded explainability that reveals how degradations and subject-specific factors affect recognizability, and the first recognizability-aware body recognition assessment. Experiments confirm state-of-the-art results on faces, strong performance on body recognition, and robustness under cross-dataset shifts. Together, these contributions establish TransFIRA as a unified, geometry-driven framework for recognizability assessment – encoder-specific, accurate, interpretable, and extensible across modalities – significantly advancing FIQA in accuracy, explainability, and scope.</p>
  </div>
</details>

<hr>
<h3 id="16-Road-Surface-Condition-Detection-with-Machine-Learning-using-New-York-State-Department-of-Transportation-Camera-Images-and-Weather-Forecast-Data-cs-CV-cs-LGPDF"><a href="#16-Road-Surface-Condition-Detection-with-Machine-Learning-using-New-York-State-Department-of-Transportation-Camera-Images-and-Weather-Forecast-Data-cs-CV-cs-LGPDF" class="headerlink" title="[16] Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data cs.CV | cs.LGPDF"></a>[16] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06440">Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06440" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Carly Sutter, Kara J. Sulia, Nick P. Bassill, Christopher D. Wirz, Christopher D. Thorncroft</span></p>
<p><strong>TL;DR:</strong> 该研究利用机器学习（卷积神经网络和随机森林）结合纽约州交通部摄像头图像和天气预报数据，自动分类道路表面状况，准确率达81.5%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 纽约州交通部目前依赖人工观察摄像头和实地巡查评估道路状况，耗时耗力。机器学习可提供自动化支持，提升决策效率。</p>
<p><strong>Result:</strong> 模型在未见过的摄像头数据上达到81.5%的准确率，满足实际需求。</p>
<p><strong>Insight:</strong> 结合图像与天气数据能显著提升道路状况分类的泛化性能，适用于大规模交通管理。</p>
<p><strong>Abstract:</strong> The New York State Department of Transportation (NYSDOT) has a network of roadside traffic cameras that are used by both the NYSDOT and the public to observe road conditions. The NYSDOT evaluates road conditions by driving on roads and observing live cameras, tasks which are labor-intensive but necessary for making critical operational decisions during winter weather events. However, machine learning models can provide additional support for the NYSDOT by automatically classifying current road conditions across the state. In this study, convolutional neural networks and random forests are trained on camera images and weather data to predict road surface conditions. Models are trained on a hand-labeled dataset of ~22,000 camera images, each classified by human labelers into one of six road surface conditions: severe snow, snow, wet, dry, poor visibility, or obstructed. Model generalizability is prioritized to meet the operational needs of the NYSDOT decision makers, and the weather-related road surface condition model in this study achieves an accuracy of 81.5% on completely unseen cameras.</p>
  </div>
</details>

<hr>
<h3 id="17-From-Captions-to-Keyframes-Efficient-Video-Summarization-via-Caption-and-Context-Aware-Frame-Scoring-cs-CVPDF"><a href="#17-From-Captions-to-Keyframes-Efficient-Video-Summarization-via-Caption-and-Context-Aware-Frame-Scoring-cs-CVPDF" class="headerlink" title="[17] From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring cs.CVPDF"></a>[17] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06509">From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06509" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shih-Yao Lin, Sibendu Paul, Caren Chen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为KeyScore的多模态帧评分框架，结合字幕和视觉上下文来评估帧的重要性，并引入STACFP生成紧凑且多样的候选帧，实现了高效的视频摘要。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长视频的高效语言理解需要选取少量保留语义和上下文信息的帧。现有方法通常依赖固定数量的帧或显式视频摘要，忽略了多模态对齐的效率问题。</p>
<p><strong>Result:</strong> 在MSRVTT、MSVD和DiDeMo数据集上，该方法显著优于标准的8帧编码器，实现了高效且可扩展的视频理解。</p>
<p><strong>Insight:</strong> 强调视觉和文本信号的多模态对齐是实现高效视频理解的关键，无需显式的视频摘要步骤。</p>
<p><strong>Abstract:</strong> Efficient video-language understanding requires selecting a small set of frames that retain semantic and contextual information from long videos. We propose KeyScore, a multimodal frame scoring framework that jointly leverages captions and visual context to estimate frame-level importance. By combining semantic similarity, temporal diversity, and contextual drop impact, KeyScore identifies the most informative frames for downstream tasks such as retrieval, captioning, and video-language reasoning. To complement KeyScore, we introduce STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which generates compact and diverse frame candidates for long-form videos. Together, these modules achieve up to 99% frame reduction compared to full-frame inference and substantially outperform standard 8-frame encoders on MSRVTT, MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment between visual and textual signals enables scalable, efficient, and caption-grounded video understanding – without explicit video summarization.</p>
  </div>
</details>

<hr>
<h3 id="18-LogSTOP-Temporal-Scores-over-Prediction-Sequences-for-Matching-and-Retrieval-cs-CV-cs-AIPDF"><a href="#18-LogSTOP-Temporal-Scores-over-Prediction-Sequences-for-Matching-and-Retrieval-cs-CV-cs-AIPDF" class="headerlink" title="[18] LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval cs.CV | cs.AIPDF"></a>[18] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06512">LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06512" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Avishree Khare, Hideki Okamoto, Bardh Hoxha, Georgios Fainekos, Rajeev Alur</span></p>
<p><strong>TL;DR:</strong> LogSTOP是一种用于计算时间属性分数的评分函数，基于局部属性的预测序列，适用于视频和音频的查询匹配与检索任务，性能优于大型视觉&#x2F;音频语言模型和其他时间逻辑基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有神经模型（如YOLO和HuBERT）可以检测视频帧或音频片段中的局部属性（如物体或情感），但其输出是单帧&#x2F;片段的分数。为了支持时间属性（如“说话者最终是否快乐”）的查询匹配和检索任务，需要将这些局部分数扩展到时间序列上。</p>
<p><strong>Result:</strong> 实验显示，LogSTOP在视频中的物体检测和音频中的情感分析任务中，分别比大型视觉&#x2F;音频语言模型和其他时间逻辑基线性能提升至少16%。在视频检索任务中，其平均精度和召回率也显著优于零样本文本到视频检索基线。</p>
<p><strong>Insight:</strong> 1）局部属性的时间扩展可以显著提升复杂时间逻辑查询的性能；2）LogSTOP的高效性使其适用于实际应用；3）时间逻辑与神经模型的结合为多媒体检索提供了新思路。</p>
<p><strong>Abstract:</strong> Neural models such as YOLO and HuBERT can be used to detect local properties such as objects (“car”) and emotions (“angry”) in individual frames of videos and audio clips respectively. The likelihood of these detections is indicated by scores in [0, 1]. Lifting these scores to temporal properties over sequences can be useful for several downstream applications such as query matching (e.g., “does the speaker eventually sound happy in this audio clip?”), and ranked retrieval (e.g., “retrieve top 5 videos with a 10 second scene where a car is detected until a pedestrian is detected”). In this work, we formalize this problem of assigning Scores for TempOral Properties (STOPs) over sequences, given potentially noisy score predictors for local properties. We then propose a scoring function called LogSTOP that can efficiently compute these scores for temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP, with YOLO and HuBERT, outperforms Large Vision &#x2F; Audio Language Models and other Temporal Logic-based baselines by at least 16% on query matching with temporal properties over objects-in-videos and emotions-in-speech respectively. Similarly, on ranked retrieval with temporal properties over objects and actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a 19% and 16% increase in mean average precision and recall over zero-shot text-to-video retrieval baselines respectively.</p>
  </div>
</details>

<hr>
<h3 id="19-VUGEN-Visual-Understanding-priors-for-GENeration-cs-CVPDF"><a href="#19-VUGEN-Visual-Understanding-priors-for-GENeration-cs-CVPDF" class="headerlink" title="[19] VUGEN: Visual Understanding priors for GENeration cs.CVPDF"></a>[19] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06529">VUGEN: Visual Understanding priors for GENeration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06529" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xiangyi Chen, Théophane Vallaeys, Maha Elbayad, John Nguyen, Jakob Verbeek</span></p>
<p><strong>TL;DR:</strong> VUGEN提出了一种新框架，利用预训练的视觉语言模型的视觉理解先验，实现高效高质量的图像生成，避免了传统方法的表示不匹配问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视觉语言模型在图像理解上表现优异，但在图像生成方面仍面临挑战，如生成与理解表示之间的不匹配或架构复杂性问题。</p>
<p><strong>Result:</strong> VUGEN在COCO数据集上显著提升生成性能（DPG Bench从71.17到74.32，FID从11.86到9.06），并保持理解能力不变。</p>
<p><strong>Insight:</strong> 结果表明，无需依赖复杂潜在扩散或VAE的解码器，直接利用视觉理解先验能实现高质量的图像生成。</p>
<p><strong>Abstract:</strong> Recent advances in Vision-Language Models (VLMs) have enabled unified understanding across text and images, yet equipping these models with robust image generation capabilities remains challenging. Existing approaches often rely on reconstruction-oriented autoencoders or complex bridging mechanisms, leading to misalignment between understanding and generation representations, or architectural complexity. In this work, we propose VUGEN, a novel framework that explicitly leverages VLM’s pretrained visual understanding priors for efficient and high-quality image generation. Our approach first transforms the high-dimensional latent space of the VLM’s native vision encoder into a lower-dimensional, tractable distribution that maximally preserves visual information. The VLM is then trained to sample within this reduced latent space, ensuring alignment with its visual understanding capabilities. Finally, a dedicated pixel decoder maps these generated latents back to the image space. We find that a VAE-free pixel diffusion decoder to be on par or better than commonly used complex latent diffusion decoders that internally rely on VAE latents. Extensive experiments demonstrate that VUGEN achieves superior image generation performance, improving DPG Bench from 71.17 to 74.32 and FID from 11.86 to 9.06 on COCO, while fully preserving the VLM’s original understanding capabilities.</p>
  </div>
</details>

<hr>
<h3 id="20-Cluster-Paths-Navigating-Interpretability-in-Neural-Networks-cs-CV-cs-LGPDF"><a href="#20-Cluster-Paths-Navigating-Interpretability-in-Neural-Networks-cs-CV-cs-LGPDF" class="headerlink" title="[20] Cluster Paths: Navigating Interpretability in Neural Networks cs.CV | cs.LGPDF"></a>[20] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06541">Cluster Paths: Navigating Interpretability in Neural Networks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06541" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nicholas M. Kroeger, Vincent Bindschaedler</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种称为簇路径（cluster paths）的后解释性方法，通过聚类神经网络中间层的激活来表示输入的序列。该方法通过四项指标评估簇路径的有效性，并在多个任务中验证了其识别虚假特征、保持高保真度和稳定性，以及检测分布外样本的能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 深度神经网络在视觉任务中表现出色，但其决策过程不透明，可能导致误信、未察觉的偏见和意外失败。为了解决这一可解释性问题，作者提出了簇路径方法。</p>
<p><strong>Result:</strong> 在CIFAR-10虚假特征实验中识别了颜色捷径，CelebA任务中达到90%忠实度和96%稳定性，并能有效检测分布外样本。</p>
<p><strong>Insight:</strong> 簇路径不仅能提供可解释性，还能揭示网络的潜在视觉概念（如颜色、纹理），并扩展到大规模视觉模型（如ViT）。</p>
<p><strong>Abstract:</strong> While modern deep neural networks achieve impressive performance in vision tasks, they remain opaque in their decision processes, risking unwarranted trust, undetected biases and unexpected failures. We propose cluster paths, a post-hoc interpretability method that clusters activations at selected layers and represents each input as its sequence of cluster IDs. To assess these cluster paths, we introduce four metrics: path complexity (cognitive load), weighted-path purity (class alignment), decision-alignment faithfulness (predictive fidelity), and path agreement (stability under perturbations). In a spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts and collapse when the cue is removed. On a five-class CelebA hair-color task, they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise without sacrificing accuracy. Scaling to a Vision Transformer pretrained on ImageNet, we extend cluster paths to concept paths derived from prompting a large language model on minimal path divergences. Finally, we show that cluster paths can serve as an effective out-of-distribution (OOD) detector, reliably flagging anomalous samples before the model generates over-confident predictions. Cluster paths uncover visual concepts, such as color palettes, textures, or object contexts, at multiple network depths, demonstrating that cluster paths scale to large vision models while generating concise and human-readable explanations.</p>
  </div>
</details>

<hr>
<h3 id="21-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer-cs-CVPDF"><a href="#21-Ming-UniVision-Joint-Image-Understanding-and-Generation-with-a-Unified-Continuous-Tokenizer-cs-CVPDF" class="headerlink" title="[21] Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer cs.CVPDF"></a>[21] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06590">Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06590" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为MingTok的连续潜空间视觉分词器，通过统一的自动回归范式实现图像理解与生成任务。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法采用离散潜空间的分词器与大型语言模型的令牌对齐，量化误差限制了语义表达能力和视觉语言理解能力。</p>
<p><strong>Result:</strong> 在理解和生成任务上均达到先进水平。</p>
<p><strong>Insight:</strong> 统一连续视觉表示可以调和理解与生成任务对分词器的竞争需求。</p>
<p><strong>Abstract:</strong> Visual tokenization remains a core challenge in unifying visual understanding and generation within the autoregressive paradigm. Existing methods typically employ tokenizers in discrete latent spaces to align with the tokens from large language models, where the quantization errors can limit semantic expressiveness and degrade the capability of vision-language understanding. To address this, we introduce MingTok, a new family of visual tokenizers with a continuous latent space, for unified autoregressive generation and understanding. While understanding tasks favor discriminative high-dimensional features, generation tasks prefer compact low-level codes. Thus, to reconcile these competing demands, MingTok adopts a three-stage sequential architecture involving low-level encoding, semantic expansion, and visual reconstruction. Built on top of it, Ming-UniVision eliminates the need for task-specific visual representations, and unifies diverse vision-language tasks under a single autoregrsssive prediction paradigm. By formulating both understanding and generation as next-token prediction in a shared continuous space, it seamlessly supports multi-round, in-context tasks such as iterative understanding, generation and editing. Empirically, we find that using a unified continuous visual representation reconciles the competing requirements on the tokenizers by the understanding and generation tasks, thereby leading to state-of-the-art level performance across both domains. We hope our findings will facilitate unified visual tokenization in the continuous domain. Inference code and model weights are released to benefit community.</p>
  </div>
</details>

<hr>
<h3 id="22-A-Bridge-from-Audio-to-Video-Phoneme-Viseme-Alignment-Allows-Every-Face-to-Speak-Multiple-Languages-cs-CVPDF"><a href="#22-A-Bridge-from-Audio-to-Video-Phoneme-Viseme-Alignment-Allows-Every-Face-to-Speak-Multiple-Languages-cs-CVPDF" class="headerlink" title="[22] A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages cs.CVPDF"></a>[22] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06612">A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06612" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zibo Su, Kun Wei, Jiahua Li, Xu Yang, Cheng Deng</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了MuEx框架，通过音素-视素对齐技术解决了多语言驱动的人脸动画生成问题，并在多语言数据集上展示了卓越性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前语音驱动人脸动画合成（TFS）模型在英语上表现良好，但在非英语语言中效果不佳，主要因为训练数据以英语为主且缺乏跨语言泛化能力。</p>
<p><strong>Result:</strong> MuEx在多语言数据集上表现优异，并能零样本泛化到未见过的语言。</p>
<p><strong>Insight:</strong> 音素和视素作为通用中介可以有效解决跨语言人脸动画生成的挑战。</p>
<p><strong>Abstract:</strong> Speech-driven talking face synthesis (TFS) focuses on generating lifelike facial animations from audio input. Current TFS models perform well in English but unsatisfactorily in non-English languages, producing wrong mouth shapes and rigid facial expressions. The terrible performance is caused by the English-dominated training datasets and the lack of cross-language generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture that employs phonemes and visemes as universal intermediaries to bridge audio and video modalities, achieving lifelike multilingual TFS. To alleviate the influence of linguistic differences and dataset bias, we extract audio and video features as phonemes and visemes respectively, which are the basic units of speech sounds and mouth movements. To address audiovisual synchronization issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which establishes robust cross-modal correspondences between phonemes and visemes. In addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12 diverse languages with 95.04 hours of high-quality videos for training and evaluating multilingual TFS performance. Extensive experiments demonstrate that MuEx achieves superior performance across all languages in MTFB and exhibits effective zero-shot generalization to unseen languages without additional training.</p>
  </div>
</details>

<hr>
<h3 id="23-MSITrack-A-Challenging-Benchmark-for-Multispectral-Single-Object-Tracking-cs-CVPDF"><a href="#23-MSITrack-A-Challenging-Benchmark-for-Multispectral-Single-Object-Tracking-cs-CVPDF" class="headerlink" title="[23] MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking cs.CVPDF"></a>[23] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06619">MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06619" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Tao Feng, Tingfa Xu, Haolin Qin, Tianhao Li, Shuaihao Han</span></p>
<p><strong>TL;DR:</strong> 论文介绍了MSITrack，一个大规模、多样化的多光谱单目标跟踪数据集，旨在解决RGB跟踪器在复杂场景中的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> RGB跟踪器在遮挡、相似物体干扰和复杂背景等真实场景中表现受限，多光谱数据因其像素级光谱反射能力能提升目标辨识度，但当前多光谱跟踪数据集稀缺。</p>
<p><strong>Result:</strong> 实验表明多光谱数据显著提升了跟踪性能，优于RGB基线。</p>
<p><strong>Insight:</strong> 多光谱数据在复杂场景中具有显著优势，未来可推动多光谱跟踪算法的进一步发展。</p>
<p><strong>Abstract:</strong> Visual object tracking in real-world scenarios presents numerous challenges including occlusion, interference from similar objects and complex backgrounds-all of which limit the effectiveness of RGB-based trackers. Multispectral imagery, which captures pixel-level spectral reflectance, enhances target discriminability. However, the availability of multispectral tracking datasets remains limited. To bridge this gap, we introduce MSITrack, the largest and most diverse multispectral single object tracking dataset to date. MSITrack offers the following key features: (i) More Challenging Attributes-including interference from similar objects and similarity in color and texture between targets and backgrounds in natural scenarios, along with a wide range of real-world tracking challenges; (ii) Richer and More Natural Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack far exceeds the scope of existing benchmarks. Many of these scenes and categories are introduced to the multispectral tracking domain for the first time; (iii) Larger Scale-300 videos comprising over 129k frames of multispectral imagery. To ensure annotation precision, each frame has undergone meticulous processing, manual labeling and multi-stage verification. Extensive evaluations using representative trackers demonstrate that the multispectral data in MSITrack significantly improves performance over RGB-only baselines, highlighting its potential to drive future advancements in the field. The MSITrack dataset is publicly available at: <a target="_blank" rel="noopener" href="https://github.com/Fengtao191/MSITrack">https://github.com/Fengtao191/MSITrack</a>.</p>
  </div>
</details>

<hr>
<h3 id="24-StaR-KVQA-Structured-Reasoning-Traces-for-Implicit-Knowledge-Visual-Question-Answering-cs-CV-cs-AIPDF"><a href="#24-StaR-KVQA-Structured-Reasoning-Traces-for-Implicit-Knowledge-Visual-Question-Answering-cs-CV-cs-AIPDF" class="headerlink" title="[24] StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering cs.CV | cs.AIPDF"></a>[24] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06638">StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06638" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhihao Wen, Wenkang Wei, Yuan Fang, Xingtong Yu, Hui Zhang</span></p>
<p><strong>TL;DR:</strong> StaR-KVQA提出了一个结构化推理方法，通过监督双重符号关系路径和自然语言解释，提升隐式知识视觉问答（IK-KVQA）的准确性和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有MLLM在IK-KVQA中缺乏显式推理监督，生成的理由不一致，且标准监督微调后泛化能力差。</p>
<p><strong>Result:</strong> 在OK-VQA基准上，StaR-KVQA比最强基线提高了11.3%的准确率，并展示了强大的跨域泛化能力。</p>
<p><strong>Insight:</strong> 结构化推理痕迹可以有效提升模型的透明性和泛化性，减少对外部资源的依赖。</p>
<p><strong>Abstract:</strong> Knowledge-based Visual Question Answering (KVQA) requires models to ground entities in images and reason over factual knowledge. We study its implicit-knowledge variant, IK-KVQA, where a multimodal large language model (MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs lack explicit reasoning supervision and produce inconsistent justifications, and generalize poorly after standard supervised fine-tuning (SFT). We present StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises structured traces - dual symbolic relation paths plus path-grounded natural-language explanations - so that reasoning becomes transparent and verifiable. With one open-source MLLM, StaR-KVQA constructs and selects path-grounded reasoning traces to form a trace-enriched dataset, then fine-tunes via structured self-distillation to align generation with supervision; no external retrievers, verifiers, or curated knowledge bases (KBs) are used, traces are built offline, and inference is a single autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over the strongest baseline while exhibiting robust cross-domain generalization.</p>
  </div>
</details>

<hr>
<h3 id="25-Automated-Neural-Architecture-Design-for-Industrial-Defect-Detection-cs-CV-cs-AIPDF"><a href="#25-Automated-Neural-Architecture-Design-for-Industrial-Defect-Detection-cs-CV-cs-AIPDF" class="headerlink" title="[25] Automated Neural Architecture Design for Industrial Defect Detection cs.CV | cs.AIPDF"></a>[25] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06669">Automated Neural Architecture Design for Industrial Defect Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06669" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang</span></p>
<p><strong>TL;DR:</strong> AutoNAD是一个自动化神经网络架构设计框架，用于工业表面缺陷检测（SDD），通过联合搜索卷积、Transformer和多层感知机，解决了类内差异和类间相似性两大挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 工业SDD面临类内差异和类间相似性的挑战，传统人工设计模型效率低且效果不佳，因此需要一种自动化方法提升检测性能和效率。</p>
<p><strong>Result:</strong> 在三个工业缺陷数据集上验证了AutoNAD的有效性，并将其整合到缺陷成像与检测平台中。</p>
<p><strong>Insight:</strong> 自动化设计框架可显著减少人工设计成本，同时提升工业SDD的性能和效率。</p>
<p><strong>Abstract:</strong> Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at <a target="_blank" rel="noopener" href="https://github.com/Yuxi104/AutoNAD">https://github.com/Yuxi104/AutoNAD</a>.</p>
  </div>
</details>

<hr>
<h3 id="26-Heptapod-Language-Modeling-on-Visual-Signals-cs-CV-cs-AIPDF"><a href="#26-Heptapod-Language-Modeling-on-Visual-Signals-cs-CV-cs-AIPDF" class="headerlink" title="[26] Heptapod: Language Modeling on Visual Signals cs.CV | cs.AIPDF"></a>[26] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06673">Heptapod: Language Modeling on Visual Signals</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06673" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yongxin Zhu, Jiawei Chen, Yuanzhe Chen, Zhuo Chen, Dongya Jia</span></p>
<p><strong>TL;DR:</strong> Heptapod是一个基于视觉信号的自回归语言模型，通过因果注意力和二维分布预测实现图像生成，显著优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的视觉自回归模型依赖语义分词器和CFG，缺乏统一的生成和监督学习目标。Heptapod旨在通过新的学习框架弥补这一缺陷。</p>
<p><strong>Result:</strong> 在ImageNet生成基准上，FID达到2.70，优于现有自回归方法。</p>
<p><strong>Insight:</strong> 通过统一的生成和监督学习目标，模型能够更全面地捕捉图像语义，为视觉信号的语言建模提供了新思路。</p>
<p><strong>Abstract:</strong> We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \textbf{causal attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend of semantic tokenizers}. Our key innovation is \textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond.</p>
  </div>
</details>

<hr>
<h3 id="27-DreamOmni2-Multimodal-Instruction-based-Editing-and-Generation-cs-CVPDF"><a href="#27-DreamOmni2-Multimodal-Instruction-based-Editing-and-Generation-cs-CVPDF" class="headerlink" title="[27] DreamOmni2: Multimodal Instruction-based Editing and Generation cs.CVPDF"></a>[27] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06679">DreamOmni2: Multimodal Instruction-based Editing and Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06679" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu</span></p>
<p><strong>TL;DR:</strong> DreamOmni2提出了基于多模态指令的编辑和生成任务，解决了传统方法的局限性，并通过创新的数据合成和模型框架实现了高效的多图像输入处理和复杂指令解析。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的基于指令的图像编辑仅依赖语言指令，无法捕捉细节，而基于主题的生成局限于具体对象，忽略了抽象概念。这两种方法在实际应用中存在显著不足。</p>
<p><strong>Result:</strong> 实验表明DreamOmni2在多模态指令编辑和生成任务上表现优异，验证了方法的有效性。</p>
<p><strong>Insight:</strong> 多模态指令支持（文本+图像）扩展了任务的适用范围，结合具体和抽象概念的处理能力，显著提升了实用性。创新数据合成和模型设计是关键。</p>
<p><strong>Abstract:</strong> Recent advancements in instruction-based image editing and subject-driven generation have garnered significant attention, yet both tasks still face limitations in meeting practical user needs. Instruction-based editing relies solely on language instructions, which often fail to capture specific editing details, making reference images necessary. Meanwhile, subject-driven generation is limited to combining concrete objects or people, overlooking broader, abstract concepts. To address these challenges, we propose two novel tasks: multimodal instruction-based editing and generation. These tasks support both text and image instructions and extend the scope to include both concrete and abstract concepts, greatly enhancing their practical applications. We introduce DreamOmni2, tackling two primary challenges: data creation and model framework design. Our data synthesis pipeline consists of three steps: (1) using a feature mixing method to create extraction data for both abstract and concrete concepts, (2) generating multimodal instruction-based editing training data using the editing and extraction models, and (3) further applying the extraction model to create training data for multimodal instruction-based editing. For the framework, to handle multi-image input, we propose an index encoding and position encoding shift scheme, which helps the model distinguish images and avoid pixel confusion. Additionally, we introduce joint training with the VLM and our generation&#x2F;editing model to better process complex instructions. In addition, we have proposed comprehensive benchmarks for these two new tasks to drive their development. Experiments show that DreamOmni2 has achieved impressive results. Models and codes will be released.</p>
  </div>
</details>

<hr>
<h3 id="28-Semantic-Segmentation-Algorithm-Based-on-Light-Field-and-LiDAR-Fusion-cs-CV-cs-AIPDF"><a href="#28-Semantic-Segmentation-Algorithm-Based-on-Light-Field-and-LiDAR-Fusion-cs-CV-cs-AIPDF" class="headerlink" title="[28] Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion cs.CV | cs.AIPDF"></a>[28] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06687">Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06687" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jie Luo, Yuxuan Jiang, Xin Jin, Mingyu Liu, Yihui Fan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于光场和LiDAR融合的多模态语义分割算法(Mlpfseg)，通过特征补全和深度感知模块提升了复杂场景（如遮挡）下的分割效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自动驾驶中的语义分割在遮挡等复杂场景下面临挑战，光场和LiDAR提供了互补的视觉与空间信息，但二者的有效融合因视角局限和模态差异受到阻碍。</p>
<p><strong>Result:</strong> 相比纯图像分割和纯点云分割，Mlpfseg在mIoU指标上分别提升1.71和2.38。</p>
<p><strong>Insight:</strong> 多模态融合（光场+LiDAR）能有效解决复杂场景中的分割问题，特征补全和深度感知是实现模态互补的关键技术。</p>
<p><strong>Abstract:</strong> Semantic segmentation serves as a cornerstone of scene understanding in autonomous driving but continues to face significant challenges under complex conditions such as occlusion. Light field and LiDAR modalities provide complementary visual and spatial cues that are beneficial for robust perception; however, their effective integration is hindered by limited viewpoint diversity and inherent modality discrepancies. To address these challenges, the first multimodal semantic segmentation dataset integrating light field data and point cloud data is proposed. Based on this dataset, we proposed a multi-modal light field point-cloud fusion segmentation network(Mlpfseg), incorporating feature completion and depth perception to segment both camera images and LiDAR point clouds simultaneously. The feature completion module addresses the density mismatch between point clouds and image pixels by performing differential reconstruction of point-cloud feature maps, enhancing the fusion of these modalities. The depth perception module improves the segmentation of occluded objects by reinforcing attention scores for better occlusion awareness. Our method outperforms image-only segmentation by 1.71 Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38 mIoU, demonstrating its effectiveness.</p>
  </div>
</details>

<hr>
<h3 id="29-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis-cs-CVPDF"><a href="#29-SCas4D-Structural-Cascaded-Optimization-for-Boosting-Persistent-4D-Novel-View-Synthesis-cs-CVPDF" class="headerlink" title="[29] SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis cs.CVPDF"></a>[29] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06694">SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06694" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jipeng Lyu, Jiahua Dong, Yu-Xiong Wang</span></p>
<p><strong>TL;DR:</strong> SCas4D通过级联优化框架利用3D高斯泼溅中的结构性模式，高效建模动态场景，仅需100次迭代即可达到与现有方法相当的效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动态场景建模在保持计算效率的同时捕捉精确变形存在挑战。SCas4D旨在通过利用真实世界中变形的层次模式来解决这一问题。</p>
<p><strong>Result:</strong> 在每帧100次迭代内实现收敛，效果与现有方法相当，并在自监督关节对象分割和新视角合成等任务中表现优异。</p>
<p><strong>Insight:</strong> 真实世界变形具有层次性，通过结构化的级联优化可以大幅提升计算效率和建模精度。</p>
<p><strong>Abstract:</strong> Persistent dynamic scene modeling for tracking and novel-view synthesis remains challenging due to the difficulty of capturing accurate deformations while maintaining computational efficiency. We propose SCas4D, a cascaded optimization framework that leverages structural patterns in 3D Gaussian Splatting for dynamic scenes. The key idea is that real-world deformations often exhibit hierarchical patterns, where groups of Gaussians share similar transformations. By progressively refining deformations from coarse part-level to fine point-level, SCas4D achieves convergence within 100 iterations per time frame and produces results comparable to existing methods with only one-twentieth of the training iterations. The approach also demonstrates effectiveness in self-supervised articulated object segmentation, novel view synthesis, and dense point tracking tasks.</p>
  </div>
</details>

<hr>
<h3 id="30-Evaluating-LLMs-for-Historical-Document-OCR-A-Methodological-Framework-for-Digital-Humanities-cs-CV-cs-AI-cs-CL-68T50PDF"><a href="#30-Evaluating-LLMs-for-Historical-Document-OCR-A-Methodological-Framework-for-Digital-Humanities-cs-CV-cs-AI-cs-CL-68T50PDF" class="headerlink" title="[30] Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities cs.CV | cs.AI | cs.CL | 68T50PDF"></a>[30] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06743">Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.CL | 68T50</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06743" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Maria Levchenko</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种评估大型语言模型（LLM）在历史文档OCR中的方法论框架，解决了传统指标无法捕捉的时空偏差和时代特定错误问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 数字人文学者越来越多地使用LLM进行历史文档数字化，但缺乏针对LLM的OCR评估框架。传统指标无法有效衡量历史语料库创建中的关键问题，如时空偏差和时代特定错误。</p>
<p><strong>Result:</strong> Gemini和Qwen模型表现优于传统OCR，但存在过度历史化问题；后处理OCR校正反而降低了性能。</p>
<p><strong>Insight:</strong> 论文为数字人文学者提供了LLM选择和语料库质量评估的实用指南，揭示了后处理校正的局限性。</p>
<p><strong>Abstract:</strong> Digital humanities scholars increasingly use Large Language Models for historical document digitization, yet lack appropriate evaluation frameworks for LLM-based OCR. Traditional metrics fail to capture temporal biases and period-specific errors crucial for historical corpus creation. We present an evaluation methodology for LLM-based historical OCR, addressing contamination risks and systematic biases in diplomatic transcription. Using 18th-century Russian Civil font texts, we introduce novel metrics including Historical Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside protocols for contamination control and stability testing. We evaluate 12 multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR while exhibiting over-historicization: inserting archaic characters from incorrect historical periods. Post-OCR correction degrades rather than improves performance. Our methodology provides digital humanities practitioners with guidelines for model selection and quality assessment in historical corpus digitization.</p>
  </div>
</details>

<hr>
<h3 id="31-DeRainMamba-A-Frequency-Aware-State-Space-Model-with-Detail-Enhancement-for-Image-Deraining-cs-CVPDF"><a href="#31-DeRainMamba-A-Frequency-Aware-State-Space-Model-with-Detail-Enhancement-for-Image-Deraining-cs-CVPDF" class="headerlink" title="[31] DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining cs.CVPDF"></a>[31] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06746">DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06746" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhiliang Zhu, Tao Zeng, Tao Yang, Guoliang Luo, Jiyong Zeng</span></p>
<p><strong>TL;DR:</strong> 论文提出DeRainMamba，通过结合频率感知状态空间模块（FASSM）和多方向感知卷积（MDPConv），在去雨任务中平衡雨线去除与细节保留，并在多个公开数据集上表现优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于Mamba的模型在去雨任务中因难以捕捉细粒度细节和缺乏频率域感知而受限，因此需要一种更有效的方法来平衡雨线移除与图像细节保留。</p>
<p><strong>Result:</strong> 在四个公开数据集上的实验显示，DeRainMamba在PSNR和SSIM指标上优于现有方法，且参数量和计算成本更低。</p>
<p><strong>Insight:</strong> 结合频率域建模与空间细节增强的框架为单图像去雨任务提供了新思路，高效且性能优越。</p>
<p><strong>Abstract:</strong> Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.</p>
  </div>
</details>

<hr>
<h3 id="32-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot-cs-CVPDF"><a href="#32-OBS-Diff-Accurate-Pruning-For-Diffusion-Models-in-One-Shot-cs-CVPDF" class="headerlink" title="[32] OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot cs.CVPDF"></a>[32] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06751">OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06751" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Junhan Zhu, Hesong Wang, Mingluo Su, Zefang Wang, Huan Wang</span></p>
<p><strong>TL;DR:</strong> OBS-Diff提出了一种用于大规模文本到图像扩散模型的一次性剪枝框架，通过改进OBS方法并结合时间感知的Hessian构建，实现了高效的训练无关压缩。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模文本到图像扩散模型的计算成本过高，现有的一次性剪枝方法无法直接应用，因其迭代去噪特性与普通网络不同。OBS-Diff旨在填补这一空白。</p>
<p><strong>Result:</strong> OBS-Diff在一次性剪枝任务中表现优异，显著加速推理同时保持视觉质量。</p>
<p><strong>Insight:</strong> 通过优化剪枝标准以减少误差累积，特别是早期时间步的加权处理，是提升扩散模型剪枝效果的关键。</p>
<p><strong>Abstract:</strong> Large-scale text-to-image diffusion models, while powerful, suffer from prohibitive computational cost. Existing one-shot network pruning methods can hardly be directly applied to them due to the iterative denoising nature of diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel one-shot pruning framework that enables accurate and training-free compression of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex architectures of modern diffusion models and supporting diverse pruning granularity, including unstructured, N:M semi-structured, and structured (MHA heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the iterative dynamics of the diffusion process, by examining the problem from an error-accumulation perspective, we propose a novel timestep-aware Hessian construction that incorporates a logarithmic-decrease weighting scheme, assigning greater importance to earlier timesteps to mitigate potential error accumulation; (iii) Furthermore, a computationally efficient group-wise sequential pruning strategy is proposed to amortize the expensive calibration process. Extensive experiments show that OBS-Diff achieves state-of-the-art one-shot pruning for diffusion models, delivering inference acceleration with minimal degradation in visual quality.</p>
  </div>
</details>

<hr>
<h3 id="33-A-deep-multiple-instance-learning-approach-based-on-coarse-labels-for-high-resolution-land-cover-mapping-cs-CVPDF"><a href="#33-A-deep-multiple-instance-learning-approach-based-on-coarse-labels-for-high-resolution-land-cover-mapping-cs-CVPDF" class="headerlink" title="[33] A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping cs.CVPDF"></a>[33] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06769">A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06769" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gianmarco Perantoni, Lorenzo Bruzzone</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于深度多实例学习（DMIL）的方法，利用低分辨率标签训练高分辨率土地覆盖分类器，通过灵活的池化层隐式学习高分辨率标签，实验证明了其有效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 高分辨率土地覆盖映射中，训练标签的数量和质量是关键问题。现有低分辨率或过时的产品可以提供大量弱标签，但需要有效利用这些标签训练高分辨率分类器。</p>
<p><strong>Result:</strong> 在IEEE GRSS Data Fusion Contest数据集上，提出的方法优于标准的训练策略。</p>
<p><strong>Insight:</strong> 通过弱监督学习方法（如DMIL和PUL），可以有效利用低分辨率标签训练高分辨率分类器，为土地覆盖映射提供了新的解决方案。</p>
<p><strong>Abstract:</strong> The quantity and the quality of the training labels are central problems in high-resolution land-cover mapping with machine-learning-based solutions. In this context, weak labels can be gathered in large quantities by leveraging on existing low-resolution or obsolete products. In this paper, we address the problem of training land-cover classifiers using high-resolution imagery (e.g., Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived land-cover maps). Inspired by recent works in Deep Multiple Instance Learning (DMIL), we propose a method that trains pixel-level multi-class classifiers and predicts low-resolution labels (i.e., patch-level classification), where the actual high-resolution labels are learned implicitly without direct supervision. This is achieved with flexible pooling layers that are able to link the semantics of the pixels in the high-resolution imagery to the low-resolution reference labels. Then, the Multiple Instance Learning (MIL) problem is re-framed in a multi-class and in a multi-label setting. In the former, the low-resolution annotation represents the majority of the pixels in the patch. In the latter, the annotation only provides us information on the presence of one of the land-cover classes in the patch and thus multiple labels can be considered valid for a patch at a time, whereas the low-resolution labels provide us only one label. Therefore, the classifier is trained with a Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020 IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed framework compared to standard training strategies.</p>
  </div>
</details>

<hr>
<h3 id="34-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models-cs-CVPDF"><a href="#34-TTRV-Test-Time-Reinforcement-Learning-for-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[34] TTRV: Test-Time Reinforcement Learning for Vision Language Models cs.CVPDF"></a>[34] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06783">TTRV: Test-Time Reinforcement Learning for Vision Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06783" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Akshit Singh, Shyam Marjit, Wei Lin, Paul Gavrikov, Serena Yeung-Levy</span></p>
<p><strong>TL;DR:</strong> 论文提出TTRV方法，通过测试时强化学习增强视觉语言模型的能力，无需标注数据即可在推理时动态调整模型，显著提升了目标识别和视觉问答任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有强化学习方法通常依赖标注数据和独立训练集，这与人类直接从环境中学习的方式不同。TTRV旨在通过测试时自适应提升视觉语言模型的表现，无需额外标注数据。</p>
<p><strong>Result:</strong> TTRV在目标识别和VQA任务中分别实现了最高52.4%和29.8%的提升，平均提升为24.6%和10.0%。在图像识别中，TTRV在8个基准测试中平均超越GPT-4o 2.3%。</p>
<p><strong>Insight:</strong> 1. 测试时强化学习可显著提升视觉语言模型性能；2. 即使在单样本极端数据约束下，TTRV仍能带来明显改进；3. 该方法在与私有模型的竞争中表现出色。</p>
<p><strong>Abstract:</strong> Existing methods for extracting reward signals in Reinforcement Learning typically rely on labeled data and dedicated training splits, a setup that contrasts with how humans learn directly from their environment. In this work, we propose TTRV to enhance vision language understanding by adapting the model on the fly at inference time, without the need for any labeled data. Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework by designing rewards based on the frequency of the base model’s output, while inferring on each test sample multiple times. Further, we also propose to control the diversity of the model’s output by simultaneously rewarding the model for obtaining low entropy of the output empirical distribution. Our approach delivers consistent gains across both object recognition and visual question answering (VQA), with improvements of up to 52.4% and 29.8%, respectively, and average boosts of 24.6% and 10.0% across 16 datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining highly competitive on VQA, demonstrating that test-time reinforcement learning can match or exceed the strongest proprietary models. Finally, we find many interesting properties of test-time RL for VLMs: for example, even in extremely data-constrained scenarios, where adaptation is performed on a single randomly chosen unlabeled test example, TTRV still yields non-trivial improvements of up to 5.5% in recognition tasks.</p>
  </div>
</details>

<hr>
<h3 id="35-VA-Adapter-Adapting-Ultrasound-Foundation-Model-to-Echocardiography-Probe-Guidance-cs-CVPDF"><a href="#35-VA-Adapter-Adapting-Ultrasound-Foundation-Model-to-Echocardiography-Probe-Guidance-cs-CVPDF" class="headerlink" title="[35] VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance cs.CVPDF"></a>[35] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06809">VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06809" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Teng Wang, Haojun Jiang, Yuxuan Wang, Zhenguo Sun, Shiji Song</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为VA-Adapter的高效参数适配器，用于将预训练的超声基础模型适应于心脏超声探针引导任务，帮助初级超声医师实时获取高质量图像。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 心脏超声操作难度高，专业人员短缺，导致患者难以获得及时检查服务。本研究旨在利用基础模型从大数据中学到的医学知识，为探针引导任务提供实时操作建议。</p>
<p><strong>Result:</strong> 大量实验表明，VA-Adapter在探针引导任务中表现优于现有强基线模型。</p>
<p><strong>Insight:</strong> 通过适配器结构，可以高效地将大规模预训练模型的知识迁移到特定医疗任务，同时大幅减少微调成本。</p>
<p><strong>Abstract:</strong> Echocardiography is a critical tool for detecting heart diseases. Recently, ultrasound foundation models have demonstrated remarkable capabilities in cardiac ultrasound image analysis. However, obtaining high-quality ultrasound images is a prerequisite for accurate diagnosis. Due to the exceptionally high operational difficulty of cardiac ultrasound, there is a shortage of highly skilled personnel, which hinders patients from receiving timely examination services. In this paper, we aim to adapt the medical knowledge learned by foundation models from vast datasets to the probe guidance task, which is designed to provide real-time operational recommendations for junior sonographers to acquire high-quality ultrasound images. Moreover, inspired by the practice where experts optimize action decisions based on past explorations, we meticulously design a parameter-efficient Vision-Action Adapter (VA-Adapter) to enable foundation model’s image encoder to encode vision-action sequences, thereby enhancing guidance performance. With built-in sequential reasoning capabilities in a compact design, the VA-Adapter enables a pre-trained ultrasound foundation model to learn precise probe adjustment strategies by fine-tuning only a small subset of parameters. Extensive experiments demonstrate that the VA-Adapter can surpass strong probe guidance models. Our code will be released after acceptance.</p>
  </div>
</details>

<hr>
<h3 id="36-Efficient-Discriminative-Joint-Encoders-for-Large-Scale-Vision-Language-Reranking-cs-CV-cs-LGPDF"><a href="#36-Efficient-Discriminative-Joint-Encoders-for-Large-Scale-Vision-Language-Reranking-cs-CV-cs-LGPDF" class="headerlink" title="[36] Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking cs.CV | cs.LGPDF"></a>[36] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06820">Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06820" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mitchell Keren Taraday, Shahaf Wagner, Chaim Baskin</span></p>
<p><strong>TL;DR:</strong> EDJE是一种高效的多模态联合编码器，通过预处理视觉token并压缩存储，大幅降低了在线计算和存储需求，同时保持了检索性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态检索方法在视觉特征提取阶段计算成本高，难以大规模部署。EDJE旨在解决这一瓶颈。</p>
<p><strong>Result:</strong> EDJE在处理速度和存储效率上显著优于现有方法，50k对图像-文本&#x2F;秒，每图像仅需49kB存储。</p>
<p><strong>Insight:</strong> 通过分离预处理和在线计算，可以实现高效的多模态检索，为大规模部署提供了新思路。</p>
<p><strong>Abstract:</strong> Multimodal retrieval still leans on embedding-based models like CLIP for fast vector search over pre-computed image embeddings. Yet, unlike text retrieval, where joint-encoder rerankers are standard, comparable vision–language rerankers are largely absent. We find that seminal joint encoders such as BLIP are severely bottlenecked by an expensive visual feature-extraction stage, preventing practical deployment at scale. Motivated by this bottleneck, we introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes vision tokens offline and compresses them via a lightweight attention-based adapter, so online inference runs only a compact joint encoder over a small set of visual tokens plus the text. EDJE preserves strong retrieval performance while drastically reducing storage and online compute, enabling high-throughput inference. Specifically, EDJE processes 50k image–text pairs&#x2F;second while requiring 49kB of disk storage per image, matching prior art on Flickr (zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints will be made publicly available shortly.</p>
  </div>
</details>

<hr>
<h3 id="37-Continual-Action-Quality-Assessment-via-Adaptive-Manifold-Aligned-Graph-Regularization-cs-CVPDF"><a href="#37-Continual-Action-Quality-Assessment-via-Adaptive-Manifold-Aligned-Graph-Regularization-cs-CVPDF" class="headerlink" title="[37] Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization cs.CVPDF"></a>[37] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06842">Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06842" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kanglei Zhou, Qingyi Pan, Xingxing Zhang, Hubert P. H. Shum, Frederick W. B. Li</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个名为MAGR++的方法，用于解决动作质量评估中的持续学习问题，通过自适应流形对齐图正则化来稳定特征表示并减少遗忘。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动作质量评估（AQA）在实际应用中面临质量分布动态变化的挑战，传统的静态方法难以适应这种变化。持续学习（CL）可以帮助解决这一问题，但现有的参数高效微调方法在AQA中表现不足。</p>
<p><strong>Result:</strong> 实验表明MAGR++在离线（3.6%）和在线（12.2%）评估中均优于基线方法。</p>
<p><strong>Insight:</strong> 全参数微调对特征学习至关重要，但需结合正则化以避免过拟合和特征流形偏移。</p>
<p><strong>Abstract:</strong> Action Quality Assessment (AQA) quantifies human actions in videos, supporting applications in sports scoring, rehabilitation, and skill evaluation. A major challenge lies in the non-stationary nature of quality distributions in real-world scenarios, which limits the generalization ability of conventional methods. We introduce Continual AQA (CAQA), which equips AQA with Continual Learning (CL) capabilities to handle evolving distributions while mitigating catastrophic forgetting. Although parameter-efficient fine-tuning of pretrained models has shown promise in CL for image classification, we find it insufficient for CAQA. Our empirical and theoretical analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is necessary for effective representation learning; yet (ii) uncontrolled FPFT induces overfitting and feature manifold shift, thereby aggravating forgetting. To address this, we propose Adaptive Manifold-Aligned Graph Regularization (MAGR++), which couples backbone fine-tuning that stabilizes shallow layers while adapting deeper ones with a two-step feature rectification pipeline: a manifold projector to translate deviated historical features into the current representation space, and a graph regularizer to align local and global distributions. We construct four CAQA benchmarks from three datasets with tailored evaluation protocols and strong baselines, enabling systematic cross-dataset comparison. Extensive experiments show that MAGR++ achieves state-of-the-art performance, with average correlation gains of 3.6% offline and 12.2% online over the strongest baseline, confirming its robustness and effectiveness. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/ZhouKanglei/MAGRPP">https://github.com/ZhouKanglei/MAGRPP</a>.</p>
  </div>
</details>

<hr>
<h3 id="38-Online-Generic-Event-Boundary-Detection-cs-CV-eess-IVPDF"><a href="#38-Online-Generic-Event-Boundary-Detection-cs-CV-eess-IVPDF" class="headerlink" title="[38] Online Generic Event Boundary Detection cs.CV | eess.IVPDF"></a>[38] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06855">Online Generic Event Boundary Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | eess.IV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06855" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Hyungrok Jung, Daneul Kim, Seunggyun Lim, Jeany Son, Jonghyun Choi</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个在线通用事件边界检测（On-GEBD）的新任务和框架Estimator，用于实时检测流媒体视频中的事件边界。Estimator包含两个核心组件——一致性事件预测器（CEA）和在线边界判别器（OBD），通过预测未来帧与实际帧的差异来检测事件边界，实验结果表明该方法优于基线，并与离线方法性能相当。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有通用事件边界检测（GEBD）方法需要完整视频帧才能预测，而人类可以实时在线处理数据。论文旨在弥合这一差距，提出实时检测流媒体视频中事件边界的任务。</p>
<p><strong>Result:</strong> 实验表明，Estimator优于所有基线方法，性能与离线GEBD方法相当。</p>
<p><strong>Insight:</strong> 实时事件边界的检测需要动态预测和误差分析，人类认知理论（EST）可有效指导算法设计。</p>
<p><strong>Abstract:</strong> Generic Event Boundary Detection (GEBD) aims to interpret long-form videos through the lens of human perception. However, current GEBD methods require processing complete video frames to make predictions, unlike humans processing data online and in real-time. To bridge this gap, we introduce a new task, Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries of generic events immediately in streaming videos. This task faces unique challenges of identifying subtle, taxonomy-free event changes in real-time, without the access to future frames. To tackle these challenges, we propose a novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST) which explains how humans segment ongoing activity into events by leveraging the discrepancies between predicted and actual information. Our framework consists of two key components: the Consistent Event Anticipator (CEA), and the Online Boundary Discriminator (OBD). Specifically, the CEA generates a prediction of the future frame reflecting current event dynamics based solely on prior frames. Then, the OBD measures the prediction error and adaptively adjusts the threshold using statistical tests on past errors to capture diverse, subtle event transitions. Experimental results demonstrate that Estimator outperforms all baselines adapted from recent online video understanding models and achieves performance comparable to prior offline-GEBD methods on the Kinetics-GEBD and TAPOS datasets.</p>
  </div>
</details>

<hr>
<h3 id="39-Explaining-raw-data-complexity-to-improve-satellite-onboard-processing-cs-CV-cs-AIPDF"><a href="#39-Explaining-raw-data-complexity-to-improve-satellite-onboard-processing-cs-CV-cs-AIPDF" class="headerlink" title="[39] Explaining raw data complexity to improve satellite onboard processing cs.CV | cs.AIPDF"></a>[39] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06858">Explaining raw data complexity to improve satellite onboard processing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06858" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Adrien Dorise, Marjorie Bellizzi, Adrien Girard, Benjamin Francesconi, Stéphane May</span></p>
<p><strong>TL;DR:</strong> 论文探讨了在卫星上直接使用原始数据（raw data）进行AI处理的可行性，并通过模拟实验比较了在原始数据和预处理数据上训练的物体检测模型的性能差异，发现原始数据在高置信度下对物体边界识别较差，提出了改进轮廓方法的建议。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着处理器能力的提升，将AI模型直接部署在卫星上进行遥感数据处理成为可能。然而，使用原始传感器数据而非预处理的地面产品带来了新的挑战。目前的研究主要依赖预处理数据，而直接利用原始数据的研究较少。</p>
<p><strong>Result:</strong> 实验结果表明，在低到中等置信度阈值下，两种模型的性能相似；但在高置信度下，原始数据训练的模型在物体边界识别上表现较差。</p>
<p><strong>Insight:</strong> 改进AI模型的轮廓识别方法可能是提高原始数据上物体检测性能的关键，从而推动卫星上AI处理的进一步发展。</p>
<p><strong>Abstract:</strong> With increasing processing power, deploying AI models for remote sensing directly onboard satellites is becoming feasible. However, new constraints arise, mainly when using raw, unprocessed sensor data instead of preprocessed ground-based products. While current solutions primarily rely on preprocessed sensor images, few approaches directly leverage raw data. This study investigates the effects of utilising raw data on deep learning models for object detection and classification tasks. We introduce a simulation workflow to generate raw-like products from high-resolution L1 imagery, enabling systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are trained on both raw and L1 datasets, and their performance is compared using standard detection metrics and explainability tools. Results indicate that while both models perform similarly at low to medium confidence thresholds, the model trained on raw data struggles with object boundary identification at high confidence levels. It suggests that adapting AI architectures with improved contouring methods can enhance object detection on raw images, improving onboard AI for remote sensing.</p>
  </div>
</details>

<hr>
<h3 id="40-HARP-NeXt-High-Speed-and-Accurate-Range-Point-Fusion-Network-for-3D-LiDAR-Semantic-Segmentation-cs-CV-cs-ROPDF"><a href="#40-HARP-NeXt-High-Speed-and-Accurate-Range-Point-Fusion-Network-for-3D-LiDAR-Semantic-Segmentation-cs-CV-cs-ROPDF" class="headerlink" title="[40] HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation cs.CV | cs.ROPDF"></a>[40] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06876">HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06876" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Samir Abou Haidar, Alexandre Chariot, Mehdi Darouich, Cyril Joly, Jean-Emmanuel Deschaud</span></p>
<p><strong>TL;DR:</strong> HARP-NeXt提出了一种高速且精确的激光雷达语义分割网络，通过新颖的预处理方法和多尺度范围点融合主干，显著提升了速度和精度，无需依赖测试时增强或集成模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有激光雷达语义分割方法在速度和精度之间存在权衡，点基和稀疏卷积方法准确但速度慢，投影方法快速但丢失几何信息。此外，预处理阶段和测试时增强增加了计算负担。</p>
<p><strong>Result:</strong> 在nuScenes和SemanticKITTI基准测试中，HARP-NeXt在速度-精度权衡上优于所有现有方法，无需集成模型或TTA，速度比PTv3快24倍。</p>
<p><strong>Insight:</strong> 预处理优化和多尺度信息融合是实现高速且精确激光雷达语义分割的关键，避免深层网络和TTA提升了效率。</p>
<p><strong>Abstract:</strong> LiDAR semantic segmentation is crucial for autonomous vehicles and mobile robots, requiring high accuracy and real-time processing, especially on resource-constrained embedded systems. Previous state-of-the-art methods often face a trade-off between accuracy and speed. Point-based and sparse convolution-based methods are accurate but slow due to the complexity of neighbor searching and 3D convolutions. Projection-based methods are faster but lose critical geometric information during the 2D projection. Additionally, many recent methods rely on test-time augmentation (TTA) to improve performance, which further slows the inference. Moreover, the pre-processing phase across all methods increases execution time and is demanding on embedded platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR semantic segmentation network. We first propose a novel pre-processing methodology that significantly reduces computational overhead. Then, we design the Conv-SE-NeXt feature extraction block to efficiently capture representations without deep layer stacking per network stage. We also employ a multi-scale range-point fusion backbone that leverages information at multiple abstraction levels to preserve essential geometric details, thereby enhancing accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that HARP-NeXt achieves a superior speed-accuracy trade-off compared to all state-of-the-art methods, and, without relying on ensemble models or TTA, is comparable to the top-ranked PTv3, while running 24$\times$ faster. The code is available at <a target="_blank" rel="noopener" href="https://github.com/SamirAbouHaidar/HARP-NeXt">https://github.com/SamirAbouHaidar/HARP-NeXt</a></p>
  </div>
</details>

<hr>
<h3 id="41-Addressing-the-ID-Matching-Challenge-in-Long-Video-Captioning-cs-CVPDF"><a href="#41-Addressing-the-ID-Matching-Challenge-in-Long-Video-Captioning-cs-CVPDF" class="headerlink" title="[41] Addressing the ID-Matching Challenge in Long Video Captioning cs.CVPDF"></a>[41] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06973">Addressing the ID-Matching Challenge in Long Video Captioning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06973" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhantao Yang, Huangji Wang, Ruili Feng, Han Zhang, Yuting Hu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为RICE的新方法，利用大型视觉语言模型（LVLM）解决长视频字幕生成中的ID匹配问题，显著提升了精度和召回率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长视频字幕生成中的ID匹配问题至关重要，但现有方法泛化能力有限且依赖逐点匹配，效果不佳。论文旨在利用LVLM的先验知识解决这一问题。</p>
<p><strong>Result:</strong> RICE将ID匹配的精度从50%提升至90%，召回率从15%提升至80%，实现了对长视频中个体的持续跟踪。</p>
<p><strong>Insight:</strong> LVLM的ID匹配能力可以通过优化图像信息和个体描述来显著提升，为解决复杂视频字幕生成问题提供了新思路。</p>
<p><strong>Abstract:</strong> Generating captions for long and complex videos is both critical and challenging, with significant implications for the growing fields of text-to-video generation and multi-modal understanding. One key challenge in long video captioning is accurately recognizing the same individuals who appear in different frames, which we refer to as the ID-Matching problem. Few prior works have focused on this important issue. Those that have, usually suffer from limited generalization and depend on point-wise matching, which limits their overall effectiveness. In this paper, unlike previous approaches, we build upon LVLMs to leverage their powerful priors. We aim to unlock the inherent ID-Matching capabilities within LVLMs themselves to enhance the ID-Matching performance of captions. Specifically, we first introduce a new benchmark for assessing the ID-Matching capabilities of video captions. Using this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights that the performance of ID-Matching can be improved through two methods: 1) enhancing the usage of image information and 2) increasing the quantity of information of individual descriptions. Based on these insights, we propose a novel video captioning method called Recognizing Identities for Captioning Effectively (RICE). Extensive experiments including assessments of caption quality and ID-Matching performance, demonstrate the superiority of our approach. Notably, when implemented on GPT-4o, our RICE improves the precision of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15% to 80% compared to baseline. RICE makes it possible to continuously track different individuals in the captions of long videos.</p>
  </div>
</details>

<hr>
<h3 id="42-No-MoCap-Needed-Post-Training-Motion-Diffusion-Models-with-Reinforcement-Learning-using-Only-Textual-Prompts-cs-CVPDF"><a href="#42-No-MoCap-Needed-Post-Training-Motion-Diffusion-Models-with-Reinforcement-Learning-using-Only-Textual-Prompts-cs-CVPDF" class="headerlink" title="[42] No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts cs.CVPDF"></a>[42] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06988">No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06988" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Girolamo Macaluso, Lorenzo Mandelli, Mirko Bicchierai, Stefano Berretti, Andrew D. Bagdanov</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于强化学习的后训练框架，通过仅使用文本提示微调预训练的运动扩散模型，无需额外的动作捕捉数据，实现了对未见动作或风格的适应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的运动扩散模型在适应新动作或风格时需要额外的动作捕捉数据和完整重训练，成本高昂且难以扩展。本文旨在解决这一问题，提出一种低成本、高效且隐私友好的方法。</p>
<p><strong>Result:</strong> 实验结果表明，该方法在HumanML3D和KIT-ML数据集上显著提高了生成运动的多样性和质量，同时保持了对原始分布的生成性能。用户研究和定量指标均支持这一结论。</p>
<p><strong>Insight:</strong> 本文的洞察在于展示了强化学习可以有效地用于扩散模型的领域适应，同时强调了无需配对运动数据的方法在隐私保护和数据效率方面的优势。</p>
<p><strong>Abstract:</strong> Diffusion models have recently advanced human motion generation, producing realistic and diverse animations from textual prompts. However, adapting these models to unseen actions or styles typically requires additional motion capture data and full retraining, which is costly and difficult to scale. We propose a post-training framework based on Reinforcement Learning that fine-tunes pretrained motion diffusion models using only textual prompts, without requiring any motion ground truth. Our approach employs a pretrained text-motion retrieval network as a reward signal and optimizes the diffusion policy with Denoising Diffusion Policy Optimization, effectively shifting the model’s generative distribution toward the target domain without relying on paired motion data. We evaluate our method on cross-dataset adaptation and leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across both latent- and joint-space diffusion architectures. Results from quantitative metrics and user studies show that our approach consistently improves the quality and diversity of generated motions, while preserving performance on the original distribution. Our approach is a flexible, data-efficient, and privacy-preserving solution for motion adaptation.</p>
  </div>
</details>

<hr>
<h3 id="43-Bayesian-Modelling-of-Multi-Year-Crop-Type-Classification-Using-Deep-Neural-Networks-and-Hidden-Markov-Models-cs-CVPDF"><a href="#43-Bayesian-Modelling-of-Multi-Year-Crop-Type-Classification-Using-Deep-Neural-Networks-and-Hidden-Markov-Models-cs-CVPDF" class="headerlink" title="[43] Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models cs.CVPDF"></a>[43] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07008">Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07008" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gianmarco Perantoni, Giulio Weikmann, Lorenzo Bruzzone</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合深度学习和贝叶斯建模的新方法，用于分类年度卫星图像时间序列（SITS）。方法整合了Transformer Encoder（TE）和隐马尔可夫模型（HMM），旨在捕捉时间相关性和多年作物类型模式。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 年度土地覆盖图的时间一致性对建模多年土地覆盖变化至关重要。现有方法常忽略时间一致性，导致预测结果不稳定。</p>
<p><strong>Result:</strong> 在47种作物类型和6年Sentinel-2数据的验证中，HMM显著提升了分类性能和F1分数。</p>
<p><strong>Insight:</strong> 建模时间一致性对多年度作物分类至关重要，HMM可作为提升深度学习方法时间鲁棒性的有效工具。</p>
<p><strong>Abstract:</strong> The temporal consistency of yearly land-cover maps is of great importance to model the evolution and change of the land cover over the years. In this paper, we focus the attention on a novel approach to classification of yearly satellite image time series (SITS) that combines deep learning with Bayesian modelling, using Hidden Markov Models (HMMs) integrated with Transformer Encoder (TE) based DNNs. The proposed approach aims to capture both i) intricate temporal correlations in yearly SITS and ii) specific patterns in multiyear crop type sequences. It leverages the cascade classification of an HMM layer built on top of the TE, discerning consistent yearly crop-type sequences. Validation on a multiyear crop type classification dataset spanning 47 crop types and six years of Sentinel-2 acquisitions demonstrates the importance of modelling temporal consistency in the predicted labels. HMMs enhance the overall performance and F1 scores, emphasising the effectiveness of the proposed approach.</p>
  </div>
</details>

<hr>
<h3 id="44-DADO-A-Depth-Attention-framework-for-Object-Discovery-cs-CVPDF"><a href="#44-DADO-A-Depth-Attention-framework-for-Object-Discovery-cs-CVPDF" class="headerlink" title="[44] DADO: A Depth-Attention framework for Object Discovery cs.CVPDF"></a>[44] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07089">DADO: A Depth-Attention framework for Object Discovery</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07089" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Federico Gonzalez, Estefania Talavera, Petia Radeva</span></p>
<p><strong>TL;DR:</strong> DADO提出了一种结合注意力机制和深度模型的框架，用于无监督对象发现，通过动态加权来解决噪声注意力或复杂场景问题，并在标准基准测试中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 无监督对象发现是计算机视觉中的重要挑战，传统方法常受限于噪声注意力或复杂场景。DADO旨在结合深度和注意力信息，提升对象发现的准确性和鲁棒性。</p>
<p><strong>Result:</strong> 在标准基准测试中，DADO在对象发现准确性和鲁棒性上优于现有方法，且无需微调。</p>
<p><strong>Insight:</strong> 结合深度信息和注意力机制可以有效解决无监督对象发现中的复杂性和噪声问题，动态加权方法是一个值得进一步探索的方向。</p>
<p><strong>Abstract:</strong> Unsupervised object discovery, the task of identifying and localizing objects in images without human-annotated labels, remains a significant challenge and a growing focus in computer vision. In this work, we introduce a novel model, DADO (Depth-Attention self-supervised technique for Discovering unseen Objects), which combines an attention mechanism and a depth model to identify potential objects in images. To address challenges such as noisy attention maps or complex scenes with varying depth planes, DADO employs dynamic weighting to adaptively emphasize attention or depth features based on the global characteristics of each image. We evaluated DADO on standard benchmarks, where it outperforms state-of-the-art methods in object discovery accuracy and robustness without the need for fine-tuning.</p>
  </div>
</details>

<hr>
<h3 id="45-Enhancing-Concept-Localization-in-CLIP-based-Concept-Bottleneck-Models-cs-CVPDF"><a href="#45-Enhancing-Concept-Localization-in-CLIP-based-Concept-Bottleneck-Models-cs-CVPDF" class="headerlink" title="[45] Enhancing Concept Localization in CLIP-based Concept Bottleneck Models cs.CVPDF"></a>[45] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07115">Enhancing Concept Localization in CLIP-based Concept Bottleneck Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07115" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Rémi Kazmierczak, Steve Azzolin, Eloïse Berthier, Goran Frehse, Gianni Franchi</span></p>
<p><strong>TL;DR:</strong> 论文提出CHILI方法，解决CLIP在概念瓶颈模型（CBMs）中因概念幻觉导致的解释不忠实问题，通过解耦图像嵌入并局部化目标概念像素，提升解释的可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基于CLIP的概念瓶颈模型（CBMs）在零样本场景下提取概念时容易产生概念幻觉（错误预测概念的存在或缺失），从而影响解释的忠实性。</p>
<p><strong>Result:</strong> CHILI显著减少了概念幻觉现象，并生成了更忠实且可解释的显著性解释。</p>
<p><strong>Insight:</strong> 1. CLIP在CBMs中的概念幻觉是一个关键问题；2. 嵌入空间的解耦和局部化是改善解释忠实性的有效途径。</p>
<p><strong>Abstract:</strong> This paper addresses explainable AI (XAI) through the lens of Concept Bottleneck Models (CBMs) that do not require explicit concept annotations, relying instead on concepts extracted using CLIP in a zero-shot manner. We show that CLIP, which is central in these techniques, is prone to concept hallucination, incorrectly predicting the presence or absence of concepts within an image in scenarios used in numerous CBMs, hence undermining the faithfulness of explanations. To mitigate this issue, we introduce Concept Hallucination Inhibition via Localized Interpretability (CHILI), a technique that disentangles image embeddings and localizes pixels corresponding to target concepts. Furthermore, our approach supports the generation of saliency-based explanations that are more interpretable.</p>
  </div>
</details>

<hr>
<h3 id="46-MoRe-Monocular-Geometry-Refinement-via-Graph-Optimization-for-Cross-View-Consistency-cs-CVPDF"><a href="#46-MoRe-Monocular-Geometry-Refinement-via-Graph-Optimization-for-Cross-View-Consistency-cs-CVPDF" class="headerlink" title="[46] MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency cs.CVPDF"></a>[46] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07119">MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07119" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dongki Jung, Jaehoon Choi, Yonghan Lee, Sungmin Eum, Heesung Kwon</span></p>
<p><strong>TL;DR:</strong> MoRe是一种无需训练的几何优化方法，通过图优化提升跨视角一致性和尺度对齐，利用单目几何先验信息增强3D重建和新视角合成效果。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 单目3D基础模型在感知任务中潜力巨大，但存在跨视角一致性和尺度模糊问题。MoRe旨在通过训练无关的方法优化几何先验信息。</p>
<p><strong>Result:</strong> MoRe显著提升了跨视角一致性，优化了稀疏视角下的渲染效果，并在3D重建和新视角合成任务中表现出色。</p>
<p><strong>Insight:</strong> 通过几何约束和图优化结合的单目方法，能够在无需额外训练的情况下有效提升跨视角一致性，为单目3D感知提供了新思路。</p>
<p><strong>Abstract:</strong> Monocular 3D foundation models offer an extensible solution for perception tasks, making them attractive for broader 3D vision applications. In this paper, we propose MoRe, a training-free Monocular Geometry Refinement method designed to improve cross-view consistency and achieve scale alignment. To induce inter-frame relationships, our method employs feature matching between frames to establish correspondences. Rather than applying simple least squares optimization on these matched points, we formulate a graph-based optimization framework that performs local planar approximation using the estimated 3D points and surface normals estimated by monocular foundation models. This formulation addresses the scale ambiguity inherent in monocular geometric priors while preserving the underlying 3D structure. We further demonstrate that MoRe not only enhances 3D reconstruction but also improves novel view synthesis, particularly in sparse view rendering scenarios.</p>
  </div>
</details>

<hr>
<h3 id="47-Validation-of-Various-Normalization-Methods-for-Brain-Tumor-Segmentation-Can-Federated-Learning-Overcome-This-Heterogeneity-cs-CV-cs-DCPDF"><a href="#47-Validation-of-Various-Normalization-Methods-for-Brain-Tumor-Segmentation-Can-Federated-Learning-Overcome-This-Heterogeneity-cs-CV-cs-DCPDF" class="headerlink" title="[47] Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity? cs.CV | cs.DCPDF"></a>[47] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07126">Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.DC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07126" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jan Fiszer, Dominika Ciupek, Maciej Malawski</span></p>
<p><strong>TL;DR:</strong> 该论文研究了联邦学习（FL）在非独立同分布（non-IID）数据下对脑肿瘤分割的影响，通过不同MRI强度归一化方法模拟数据异质性。FL表现出对数据不一致的鲁棒性，性能与集中式模型相当。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管深度学习在医学影像中广泛应用，但数据隐私和异质性限制了其效果。FL是潜在解决方案，但其在non-IID数据下的表现需要验证。</p>
<p><strong>Result:</strong> FL在Dice分数上达到92%，与集中式模型相当，表现出对归一化不一致的鲁棒性。</p>
<p><strong>Insight:</strong> FL能有效解决医学数据隐私和异质性问题，适用于高要求的医疗应用。</p>
<p><strong>Abstract:</strong> Deep learning (DL) has been increasingly applied in medical imaging, however, it requires large amounts of data, which raises many challenges related to data privacy, storage, and transfer. Federated learning (FL) is a training paradigm that overcomes these issues, though its effectiveness may be reduced when dealing with non-independent and identically distributed (non-IID) data. This study simulates non-IID conditions by applying different MRI intensity normalization techniques to separate data subsets, reflecting a common cause of heterogeneity. These subsets are then used for training and testing models for brain tumor segmentation. The findings provide insights into the influence of the MRI intensity normalization methods on segmentation models, both training and inference. Notably, the FL methods demonstrated resilience to inconsistently normalized data across clients, achieving the 3D Dice score of 92%, which is comparable to a centralized model (trained using all data). These results indicate that FL is a solution to effectively train high-performing models without violating data privacy, a crucial concern in medical applications. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/SanoScience/fl-varying-normalization">https://github.com/SanoScience/fl-varying-normalization</a>.</p>
  </div>
</details>

<hr>
<h3 id="48-Graph-Conditioned-Diffusion-for-Controllable-Histopathology-Image-Generation-cs-CV-cs-AIPDF"><a href="#48-Graph-Conditioned-Diffusion-for-Controllable-Histopathology-Image-Generation-cs-CV-cs-AIPDF" class="headerlink" title="[48] Graph Conditioned Diffusion for Controllable Histopathology Image Generation cs.CV | cs.AIPDF"></a>[48] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07129">Graph Conditioned Diffusion for Controllable Histopathology Image Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07129" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sarah Cechnicka, Matthew Baugh, Weitong Zhang, Mischa Dombrowski, Zhe Li</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于图的扩散模型（Graph-Conditioned-Diffusion），用于可控的医学图像生成，通过引入图节点表征图像中的主要结构及其关系，实现了对生成内容的精细化控制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 医学图像（如病理图像）具有固有的结构和纹理特征，现有扩散模型在噪声潜在空间中缺乏语义结构和强先验，难以实现有意义的可控生成。</p>
<p><strong>Result:</strong> 在真实世界的病理图像用例中，生成的数据可以可靠地替代标注的患者数据用于下游分割任务。</p>
<p><strong>Insight:</strong> 通过引入结构化图表征，扩散模型可以在医学图像生成中实现更高水平的控制，解决了现有方法在语义结构上的不足。</p>
<p><strong>Abstract:</strong> Recent advances in Diffusion Probabilistic Models (DPMs) have set new standards in high-quality image synthesis. Yet, controlled generation remains challenging, particularly in sensitive areas such as medical imaging. Medical images feature inherent structure such as consistent spatial arrangement, shape or texture, all of which are critical for diagnosis. However, existing DPMs operate in noisy latent spaces that lack semantic structure and strong priors, making it difficult to ensure meaningful control over generated content. To address this, we propose graph-based object-level representations for Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding to each major structure in the image, encapsulating their individual features and relationships. These graph representations are processed by a transformer module and integrated into a diffusion model via the text-conditioning mechanism, enabling fine-grained control over generation. We evaluate this approach using a real-world histopathology use case, demonstrating that our generated data can reliably substitute for annotated patient data in downstream segmentation tasks. The code is available here.</p>
  </div>
</details>

<hr>
<h3 id="49-Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models-cs-CVPDF"><a href="#49-Few-Shot-Adaptation-Benchmark-for-Remote-Sensing-Vision-Language-Models-cs-CVPDF" class="headerlink" title="[49] Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models cs.CVPDF"></a>[49] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07135">Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07135" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Karim El Khoury, Maxime Zanella, Christophe De Vleeschouwer, Benoit Macq</span></p>
<p><strong>TL;DR:</strong> 该论文首次提出了一个结构化基准，用于评估遥感视觉语言模型（RSVLMs）在少样本学习中的适应性，揭示了不同模型在少样本条件下的表现差异，并呼吁开发更稳健的方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管RSVLMs在大规模预训练后表现出色，但它们在少样本学习等低数据环境下的泛化能力尚未充分研究，因此需要系统地评估和比较其适应性。</p>
<p><strong>Result:</strong> 发现零样本表现相似的模型在少样本适应中表现差异显著，且现有方法中无明确最优方法。</p>
<p><strong>Insight:</strong> RSVLMs的少样本适应性与其骨干网络结构和预训练策略密切相关，未来研究需针对遥感任务设计更有效的适应方法。</p>
<p><strong>Abstract:</strong> Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable potential thanks to large-scale pretraining, achieving strong zero-shot performance on various tasks. However, their ability to generalize in low-data regimes, such as few-shot learning, remains insufficiently explored. In this work, we present the first structured benchmark for evaluating few-shot adaptation methods on RSVLMs. We conduct comprehensive experiments across ten remote sensing scene classification datasets, applying five widely used few-shot adaptation strategies to three state-of-the-art RSVLMs with varying backbones. Our findings reveal that models with similar zero-shot performance can exhibit markedly different behavior under few-shot adaptation, with some RSVLMs being inherently more amenable to such adaptation than others. The variability of performance and the absence of a clear winner among existing methods highlight the need for the development of more robust methods for few-shot adaptation tailored to RS. To facilitate future research, we provide a reproducible benchmarking framework and open-source code to systematically evaluate RSVLMs under few-shot conditions. The source code is publicly available on Github: <a target="_blank" rel="noopener" href="https://github.com/elkhouryk/fewshot_RSVLMs">https://github.com/elkhouryk/fewshot_RSVLMs</a></p>
  </div>
</details>

<hr>
<h3 id="50-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods-cs-CVPDF"><a href="#50-Are-We-Using-the-Right-Benchmark-An-Evaluation-Framework-for-Visual-Token-Compression-Methods-cs-CVPDF" class="headerlink" title="[50] Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods cs.CVPDF"></a>[50] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07143">Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07143" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenfei Liao, Wensong Wang, Zichen Wen, Xu Zheng, Yiyu Wang</span></p>
<p><strong>TL;DR:</strong> 该论文指出当前多模态大语言模型（MLLMs）中视觉标记压缩方法的评估存在任务不匹配问题，并提出新的评估框架VTC-Bench，通过数据过滤机制提升评估的公平性和准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有基准测试原本是为评估MLLMs的感知和推理能力设计的，而非视觉标记压缩方法。简单下采样的表现优于许多先进方法，表明当前基准存在噪声，亟需更合适的评估框架。</p>
<p><strong>Result:</strong> 实验表明简单下采样在现有基准中表现优于许多先进方法，验证了基准的噪声问题；VTC-Bench提供了更可靠的评估结果。</p>
<p><strong>Insight:</strong> 仅依赖现有基准评估视觉标记压缩方法可能误导研究；数据过滤是提升评估质量的有效手段。</p>
<p><strong>Abstract:</strong> Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at <a target="_blank" rel="noopener" href="https://github.com/Chenfei-Liao/VTC-Bench">https://github.com/Chenfei-Liao/VTC-Bench</a>.</p>
  </div>
</details>

<hr>
<h3 id="51-MV-Performer-Taming-Video-Diffusion-Model-for-Faithful-and-Synchronized-Multi-view-Performer-Synthesis-cs-CVPDF"><a href="#51-MV-Performer-Taming-Video-Diffusion-Model-for-Faithful-and-Synchronized-Multi-view-Performer-Synthesis-cs-CVPDF" class="headerlink" title="[51] MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis cs.CVPDF"></a>[51] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07190">MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07190" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yihao Zhi, Chenghong Li, Hongjie Liao, Xihe Yang, Zhengwentai Sun</span></p>
<p><strong>TL;DR:</strong> MV-Performer是一种创新的视频扩散模型框架，专注于从单目全身捕捉生成同步的多视角视频，解决了现有方法在360度视角变化上的局限性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视频生成方法主要集中于前视角的相机轨迹重定向，而难以生成360度的视角变化。本文旨在解决这一局限性，特别是在人类为中心的领域中。</p>
<p><strong>Result:</strong> 在三个数据集上的实验表明，MV-Performer在人类为中心的4D新视角合成任务中表现出色，具有高效的鲁棒性。</p>
<p><strong>Insight:</strong> 通过结合多视角信息和条件信号，MV-Performer在360度视角变化和视频同步性方面取得了显著进展，为人类为中心的新视角合成提供了有力工具。</p>
<p><strong>Abstract:</strong> Recent breakthroughs in video generation, powered by large-scale datasets and diffusion techniques, have shown that video diffusion models can function as implicit 4D novel view synthesizers. Nevertheless, current methods primarily concentrate on redirecting camera trajectory within the front view while struggling to generate 360-degree viewpoint changes. In this paper, we focus on human-centric subdomain and present MV-Performer, an innovative framework for creating synchronized novel view videos from monocular full-body captures. To achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset and incorporate an informative condition signal. Specifically, we use the camera-dependent normal maps rendered from oriented partial point clouds, which effectively alleviate the ambiguity between seen and unseen observations. To maintain synchronization in the generated videos, we propose a multi-view human-centric video diffusion model that fuses information from the reference video, partial rendering, and different viewpoints. Additionally, we provide a robust inference procedure for in-the-wild video cases, which greatly mitigates the artifacts induced by imperfect monocular depth estimation. Extensive experiments on three datasets demonstrate our MV-Performer’s state-of-the-art effectiveness and robustness, setting a strong model for human-centric 4D novel view synthesis.</p>
  </div>
</details>

<hr>
<h3 id="52-Resolution-scaling-governs-DINOv3-transfer-performance-in-chest-radiograph-classification-cs-CV-cs-AI-cs-LGPDF"><a href="#52-Resolution-scaling-governs-DINOv3-transfer-performance-in-chest-radiograph-classification-cs-CV-cs-AI-cs-LGPDF" class="headerlink" title="[52] Resolution scaling governs DINOv3 transfer performance in chest radiograph classification cs.CV | cs.AI | cs.LGPDF"></a>[52] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07191">Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07191" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung</span></p>
<p><strong>TL;DR:</strong> 论文探讨了DINOv3在胸部X光分类中的迁移性能，发现更高的输入分辨率（512x512）能显著提升模型表现，而更大的分辨率（1024x1024）则未带来进一步增益，ConvNeXt-B在多数情况下优于ViT-B&#x2F;16。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 自监督学习（SSL）在视觉表示学习中取得进展，但其在胸部X光这种高分辨率、细粒度任务中的价值尚不明确。研究目的是评估DINOv3在该领域的性能及其设计选择的有效性。</p>
<p><strong>Result:</strong> 1. 512x512分辨率下，DINOv3表现优于DINOv2和ImageNet；2. ConvNeXt-B普遍优于ViT-B&#x2F;16；3. 更大的分辨率（1024x1024）未带来显著提升；4. 冻结特征表现较差，凸显微调的重要性。</p>
<p><strong>Insight:</strong> 1. 胸部X光任务中，分辨率提升（512x512）是关键优化方向；2. ConvNeXt-B更适合此类细粒度任务；3. 临床应用中，512x512的DINOv3初始化ConvNeXt-B是实用高效选择。</p>
<p><strong>Abstract:</strong> Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta’s DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n&gt;814,000). Two representative backbones were evaluated: ViT-B&#x2F;16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B&#x2F;16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.</p>
  </div>
</details>

<hr>
<h3 id="53-TalkCuts-A-Large-Scale-Dataset-for-Multi-Shot-Human-Speech-Video-Generation-cs-CVPDF"><a href="#53-TalkCuts-A-Large-Scale-Dataset-for-Multi-Shot-Human-Speech-Video-Generation-cs-CVPDF" class="headerlink" title="[53] TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation cs.CVPDF"></a>[53] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07249">TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07249" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiaben Chen, Zixin Wang, Ailing Zeng, Yang Fu, Xueyang Yu</span></p>
<p><strong>TL;DR:</strong> TalkCuts 是一个专注于多镜头人类语音视频生成的大规模数据集，提供多样化的镜头视角和丰富的标注信息。Orator 是一个基于 LLM 的多模态生成框架，展示了数据集的价值。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有数据集多聚焦于单镜头静态视角，而 TalkCuts 填补了多镜头语音视频生成领域的空白，支持更丰富的多模态学习和可控视频生成研究。</p>
<p><strong>Result:</strong> 实验表明，TalkCuts 显著提升了生成视频的电影连贯性和视觉吸引力，适用于姿态引导和音频驱动的场景。</p>
<p><strong>Insight:</strong> TalkCuts 为可控多镜头语音视频生成和多模态学习提供了重要基础，展示了语言模型在多模态视频生成中的潜力。</p>
<p><strong>Abstract:</strong> In this work, we present TalkCuts, a large-scale dataset designed to facilitate the study of multi-shot human speech video generation. Unlike existing datasets that focus on single-shot, static viewpoints, TalkCuts offers 164k clips totaling over 500 hours of high-quality human speech videos with diverse camera shots, including close-up, half-body, and full-body views. The dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X motion annotations, covering over 10k identities, enabling multimodal learning and evaluation. As a first attempt to showcase the value of the dataset, we present Orator, an LLM-guided multi-modal generation framework as a simple baseline, where the language model functions as a multi-faceted director, orchestrating detailed specifications for camera transitions, speaker gesticulations, and vocal modulation. This architecture enables the synthesis of coherent long-form videos through our integrated multi-modal video generation module. Extensive experiments in both pose-guided and audio-driven settings show that training on TalkCuts significantly enhances the cinematographic coherence and visual appeal of generated multi-shot speech videos. We believe TalkCuts provides a strong foundation for future work in controllable, multi-shot speech video generation and broader multimodal learning.</p>
  </div>
</details>

<hr>
<h3 id="54-Evaluating-Fundus-Specific-Foundation-Models-for-Diabetic-Macular-Edema-Detection-cs-CVPDF"><a href="#54-Evaluating-Fundus-Specific-Foundation-Models-for-Diabetic-Macular-Edema-Detection-cs-CVPDF" class="headerlink" title="[54] Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection cs.CVPDF"></a>[54] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07277">Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07277" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Franco Javier Arellano, José Ignacio Orlando</span></p>
<p><strong>TL;DR:</strong> 该论文比较了Fundus-Specific Foundation Models (FM) 与标准迁移学习方法在糖尿病黄斑水肿(DME)检测任务中的表现，发现FM在大多数情况下并未显著优于轻量级CNN模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 糖尿病黄斑水肿(DME)是导致糖尿病患者视力丧失的主要原因之一，但深度学习应用于该任务的挑战在于标注数据的稀缺性。Foundation Models (FM) 被视为一种潜在解决方案，但尚未明确其在DME检测中的实际效果。</p>
<p><strong>Result:</strong> EfficientNet-B0在大多数评估设置中表现最优，FM仅在特定数据集(如OEFI)中显示出潜力。FLAIR在零样本设置中表现竞争性，但总体而言FM未显著超越轻量级CNN。</p>
<p><strong>Insight:</strong> 在某些数据稀缺的场景中，轻量级CNN仍可能是更高效的基线方法，而FM的应用可能需要更细致的设计或更适合的任务。</p>
<p><strong>Abstract:</strong> Diabetic Macular Edema (DME) is a leading cause of vision loss among patients with Diabetic Retinopathy (DR). While deep learning has shown promising results for automatically detecting this condition from fundus images, its application remains challenging due the limited availability of annotated data. Foundation Models (FM) have emerged as an alternative solution. However, it is unclear if they can cope with DME detection in particular. In this paper, we systematically compare different FM and standard transfer learning approaches for this task. Specifically, we compare the two most popular FM for retinal images–RETFound and FLAIR–and an EfficientNet-B0 backbone, across different training regimes and evaluation settings in IDRiD, MESSIDOR-2 and OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do not consistently outperform fine-tuned CNNs in this task. In particular, an EfficientNet-B0 ranked first or second in terms of area under the ROC and precision&#x2F;recall curves in most evaluation settings, with RETFound only showing promising results in OEFI. FLAIR, on the other hand, demonstrated competitive zero-shot performance, achieving notable AUC-PR scores when prompted appropriately. These findings reveal that FM might not be a good tool for fine-grained ophthalmic tasks such as DME detection even after fine-tuning, suggesting that lightweight CNNs remain strong baselines in data-scarce environments.</p>
  </div>
</details>

<hr>
<h3 id="55-SpecGuard-Spectral-Projection-based-Advanced-Invisible-Watermarking-cs-CVPDF"><a href="#55-SpecGuard-Spectral-Projection-based-Advanced-Invisible-Watermarking-cs-CVPDF" class="headerlink" title="[55] SpecGuard: Spectral Projection-based Advanced Invisible Watermarking cs.CVPDF"></a>[55] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07302">SpecGuard: Spectral Projection-based Advanced Invisible Watermarking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07302" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Inzamamul Alam, Md Tanvir Islam, Khan Muhammad, Simon S. Woo</span></p>
<p><strong>TL;DR:</strong> SpecGuard提出了一种基于频谱投影的先进隐形水印方法，通过在频域中嵌入信息，提高了对抗多种攻击的鲁棒性，同时保持了水印的不可见性和容量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有水印方法在面对各种图像变换（如畸变、对抗扰动和图像再生）时缺乏鲁棒性，难以在实际场景中可靠地保护版权信息。因此，作者提出了SpecGuard以解决这一问题。</p>
<p><strong>Result:</strong> 实验表明SpecGuard在不可见性、容量和鲁棒性上均优于现有方法，尤其是在对抗几何畸变和对抗扰动时表现突出。</p>
<p><strong>Insight:</strong> 频域水印嵌入方法结合小波分解和Parseval定理的使用，为提升水印的鲁棒性和不可见性提供了新思路。</p>
<p><strong>Abstract:</strong> Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primarily including distortions, image regeneration, and adversarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking approach for robust and invisible image watermarking. Unlike prior approaches, we embed the message inside hidden convolution layers by converting from the spatial domain to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet projection. Spectral projection employs Fast Fourier Transform approximation to transform spatial data into the frequency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, including adversarial, geometric, and regeneration-based distortions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval’s theorem to effectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark’s invisibility, capacity, and robustness. Comprehensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models. To ensure reproducibility, the full code is released on \href{<a target="_blank" rel="noopener" href="https://github.com/inzamamulDU/SpecGuard_ICCV_2025%7D%7B/textcolor%7Bblue%7D%7B/textbf%7BGitHub%7D%7D%7D">https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}</a>.</p>
  </div>
</details>

<hr>
<h3 id="56-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation-cs-CVPDF"><a href="#56-MATRIX-Mask-Track-Alignment-for-Interaction-aware-Video-Generation-cs-CVPDF" class="headerlink" title="[56] MATRIX: Mask Track Alignment for Interaction-aware Video Generation cs.CVPDF"></a>[56] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07310">MATRIX: Mask Track Alignment for Interaction-aware Video Generation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07310" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi</span></p>
<p><strong>TL;DR:</strong> 该论文提出MATRIX方法，通过对齐视频DiTs的注意力与多实例掩码轨迹，提升多实例及主客体交互的视频生成效果，并提出了InterGenEval评估协议。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前视频DiTs在多实例或主客体交互建模方面表现不佳，论文旨在探究它们如何内部表征交互，并提出改进方法。</p>
<p><strong>Result:</strong> MATRIX提升了交互保真度和语义对齐，减少了漂移和幻觉现象。</p>
<p><strong>Insight:</strong> 交互主导的注意力集中在少数层，针对性对齐这些层能显著提升交互建模能力。</p>
<p><strong>Abstract:</strong> Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.</p>
  </div>
</details>

<hr>
<h3 id="57-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation-cs-CV-cs-ROPDF"><a href="#57-WristWorld-Generating-Wrist-Views-via-4D-World-Models-for-Robotic-Manipulation-cs-CV-cs-ROPDF" class="headerlink" title="[57] WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation cs.CV | cs.ROPDF"></a>[57] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07313">WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV | cs.RO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07313" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin</span></p>
<p><strong>TL;DR:</strong> WristWorld是一个4D世界模型，首次实现仅从锚点视图生成腕视图视频，通过几何一致性和时空一致性提升机器人操作性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大规模数据集中腕视图记录稀缺，导致锚点视图和腕视图之间存在巨大差距，现有世界模型无法解决这一问题。</p>
<p><strong>Result:</strong> 在Droid、Calvin和Franka Panda数据集上实现SOTA生成效果，提升VLA性能，任务完成长度平均提高3.81%。</p>
<p><strong>Insight:</strong> 几何先验和跨视图先验能够有效解决极端视角偏移问题，为机器人操作的视觉感知提供新思路。</p>
<p><strong>Abstract:</strong> Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.</p>
  </div>
</details>

<hr>
<h3 id="58-Pixel-Perfect-Depth-with-Semantics-Prompted-Diffusion-Transformers-cs-CVPDF"><a href="#58-Pixel-Perfect-Depth-with-Semantics-Prompted-Diffusion-Transformers-cs-CVPDF" class="headerlink" title="[58] Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers cs.CVPDF"></a>[58] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07316">Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07316" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了Pixel-Perfect Depth，一种基于像素空间扩散生成的单目深度估计模型，通过避免VAE压缩引入的飞像素问题，生成高质量的点云。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前基于生成模型的深度估计方法因使用VAE压缩深度图到隐空间，导致边缘和细节出现飞像素问题。本文旨在直接通过像素空间生成解决这一问题。</p>
<p><strong>Result:</strong> 在五个基准测试中取得了最优性能，显著优于其他模型，尤其在边缘感知点云评估中表现突出。</p>
<p><strong>Insight:</strong> 像素空间生成能有效避免隐空间压缩带来的伪影，但需高效设计；语义信息的融入可显著提升深度图的全局一致性。</p>
<p><strong>Abstract:</strong> This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.</p>
  </div>
</details>

<hr>
<h3 id="59-Quantum-enhanced-Computer-Vision-Going-Beyond-Classical-Algorithms-cs-CVPDF"><a href="#59-Quantum-enhanced-Computer-Vision-Going-Beyond-Classical-Algorithms-cs-CVPDF" class="headerlink" title="[59] Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms cs.CVPDF"></a>[59] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07317">Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07317" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin</span></p>
<p><strong>TL;DR:</strong> 这篇论文探讨了量子增强计算机视觉（QeCV）这一新兴领域，综述了量子计算方法在计算机视觉中的潜力和应用。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统非量子方法在某些场景下无法找到合理的解或只能提供近似解，而量子计算可能通过利用量子力学效应在这些领域提供更好的时间可扩展性或解决方案。</p>
<p><strong>Result:</strong> 论文总结了现有的量子计算工具和学习资源，并讨论了QeCV的发表、评审以及对社会的影响。</p>
<p><strong>Insight:</strong> 量子计算在计算机视觉中的应用潜力巨大，但需要开发全新的算法以适应量子硬件，并释放量子计算范式的潜力。</p>
<p><strong>Abstract:</strong> Quantum-enhanced Computer Vision (QeCV) is a new research field at the intersection of computer vision, optimisation theory, machine learning and quantum computing. It has high potential to transform how visual signals are processed and interpreted with the help of quantum computing that leverages quantum-mechanical effects in computations inaccessible to classical (i.e. non-quantum) computers. In scenarios where existing non-quantum methods cannot find a solution in a reasonable time or compute only approximate solutions, quantum computers can provide, among others, advantages in terms of better time scalability for multiple problem classes. Parametrised quantum circuits can also become, in the long term, a considerable alternative to classical neural networks in computer vision. However, specialised and fundamentally new algorithms must be developed to enable compatibility with quantum hardware and unveil the potential of quantum computational paradigms in computer vision. This survey contributes to the existing literature on QeCV with a holistic review of this research field. It is designed as a quantum computing reference for the computer vision community, targeting computer vision students, scientists and readers with related backgrounds who want to familiarise themselves with QeCV. We provide a comprehensive introduction to QeCV, its specifics, and methodologies for formulations compatible with quantum hardware and QeCV methods, leveraging two main quantum computational paradigms, i.e. gate-based quantum computing and quantum annealing. We elaborate on the operational principles of quantum computers and the available tools to access, program and simulate them in the context of QeCV. Finally, we review existing quantum computing tools and learning materials and discuss aspects related to publishing and reviewing QeCV papers, open challenges and potential social implications.</p>
  </div>
</details>

<hr>
<h3 id="60-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation-cs-CVPDF"><a href="#60-Temporal-Prompting-Matters-Rethinking-Referring-Video-Object-Segmentation-cs-CVPDF" class="headerlink" title="[60] Temporal Prompting Matters: Rethinking Referring Video Object Segmentation cs.CVPDF"></a>[60] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07319">Temporal Prompting Matters: Rethinking Referring Video Object Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07319" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种通过解构RVOS任务并使用时序提示生成与选择（Tenet）框架来解决RVOS问题的方法，利用基础分割模型和外部检测器&#x2F;跟踪器生成高质量提示，并通过提示偏好学习评估其质量，实现高效适应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的RVOS方法通常需要密集标注的端到端训练，计算成本高且扩展性差。作者希望通过重新思考任务的关键因素，利用现有基础模型和外部工具实现高效解决方案。</p>
<p><strong>Result:</strong> 在RVOS基准测试中验证了Tenet框架的有效性，展示了无需端到端训练的适应性。</p>
<p><strong>Insight:</strong> 时序提示的质量和选择对RVOS任务至关重要，通过解构任务并利用外部工具可以显著降低计算成本，提升模型适应效率。</p>
<p><strong>Abstract:</strong> Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.</p>
  </div>
</details>

<hr>
<div id='cs.CL'></div>

<h1 id="cs-CL-Back"><a href="#cs-CL-Back" class="headerlink" title="cs.CL [Back]"></a>cs.CL <a href="#toc">[Back]</a></h1><h3 id="61-OpenStaxQA-A-multilingual-dataset-based-on-open-source-college-textbooks-cs-CLPDF"><a href="#61-OpenStaxQA-A-multilingual-dataset-based-on-open-source-college-textbooks-cs-CLPDF" class="headerlink" title="[61] OpenStaxQA: A multilingual dataset based on open-source college textbooks cs.CLPDF"></a>[61] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06239">OpenStaxQA: A multilingual dataset based on open-source college textbooks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06239" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pranav Gupta</span></p>
<p><strong>TL;DR:</strong> OpenStaxQA是一个基于43本开放式大学教材的多语言评估数据集，支持英语、西班牙语和波兰语。作者使用量化低秩适配器（QLoRa）对大语言模型（LLM）进行了微调和评估，并通过在AI2推理挑战开发数据集上的零样本评估验证其潜在泛化能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 动机在于为大学教育应用提供一个多语言的评估基准，同时探索开放式教育资源在推动大语言模型性能提升方面的潜力。</p>
<p><strong>Result:</strong> 结果表明，OpenStaxQA数据集可用于微调大语言模型，并在其他任务（如AI2推理挑战）上展现出潜在的泛化能力。</p>
<p><strong>Insight:</strong> 开放式教育资源可以成为构建高质量数据集的重要来源，并且多语言数据集的设计有助于推动大语言模型的通用性。</p>
<p><strong>Abstract:</strong> We present OpenStaxQA, an evaluation benchmark specific to college-level educational applications based on 43 open-source college textbooks in English, Spanish, and Polish, available under a permissive Creative Commons license. We finetune and evaluate large language models (LLMs) with approximately 7 billion parameters on this dataset using quantized low rank adapters (QLoRa). Additionally we also perform a zero-shot evaluation on the AI2 reasoning challenge dev dataset in order to check if OpenStaxQA can lead to an improved performance on other tasks. We also discuss broader impacts relevant to datasets such as OpenStaxQA.</p>
  </div>
</details>

<hr>
<h3 id="62-Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets-cs-CL-cs-AI-cs-DBPDF"><a href="#62-Knowledge-Graph-Guided-Multi-Agent-Distillation-for-Reliable-Industrial-Question-Answering-with-Datasets-cs-CL-cs-AI-cs-DBPDF" class="headerlink" title="[62] Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets cs.CL | cs.AI | cs.DBPDF"></a>[62] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06240">Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.DB</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06240" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了知识图谱引导的多智能体蒸馏方法（KG-MASD），旨在提升工业问答系统的安全性和可靠性，通过结合知识图谱和多智能体协作推理，生成高质量的指令调优数据，并将推理能力和验证能力同时蒸馏到轻量级学生模型中。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 工业问答系统在高风险场景中需具备更高的安全性和可靠性，但现有方法在多智能体协作推理中面临迭代不可控和输出不可验证的问题，同时传统蒸馏方法难以将协作推理能力转移到轻量级模型中。</p>
<p><strong>Result:</strong> 在工业QA数据集上的实验表明，KG-MASD的准确性较基线方法提升2.4%-20.1%，并显著增强了可靠性。</p>
<p><strong>Insight:</strong> 知识图谱的引入不仅增强了状态表示的丰富性，还确保了推理的可验证性，为高风险工业场景中的可信AI部署提供了有效解决方案。</p>
<p><strong>Abstract:</strong> Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at <a target="_blank" rel="noopener" href="https://github.com/erwinmsmith/KG-MAD/">https://github.com/erwinmsmith/KG-MAD/</a>.</p>
  </div>
</details>

<hr>
<h3 id="63-CoT-Referring-Improving-Referring-Expression-Tasks-with-Grounded-Reasoning-cs-CL-cs-AIPDF"><a href="#63-CoT-Referring-Improving-Referring-Expression-Tasks-with-Grounded-Reasoning-cs-CL-cs-AIPDF" class="headerlink" title="[63] CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning cs.CL | cs.AIPDF"></a>[63] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06243">CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qihua Dong, Luis Figueroa, Handong Zhao, Kushal Kafle, Jason Kuen</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种名为CoT Referring的新策略，通过链式思维的数据结构增强多模态模型在指称表达任务中的推理能力，显著提升了复杂查询场景下的准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 指称表达理解和分割任务是评估多模态大语言模型（MLLMs）能力的关键任务。现有的方法在处理复杂查询时表现不佳，因此需要一种更系统的方法来提升模型的推理能力。</p>
<p><strong>Result:</strong> 在RefCOCO&#x2F;+&#x2F;g数据集及新构建的评测基准上，CoT Referring方法比基线模型提升了2.5%以上的性能。</p>
<p><strong>Insight:</strong> 通过结构化链式思维推理和多模态数据的系统性对齐，可以有效提升模型在复杂指称表达任务中的表现。统一的MLLM框架结合自适应损失函数为多模态任务提供了新的优化方向。</p>
<p><strong>Abstract:</strong> Referring Expression Comprehension and Segmentation are critical tasks for assessing the integration of language understanding and image comprehension, serving as benchmarks for Multimodal Large Language Models (MLLMs) capabilities. To address these challenges, we propose a new strategy, CoT Referring, which enhances model reasoning across modalities through a structured, chain-of-thought training data structure. Our approach systematically parses textual structures to a sequential referring step, where in each step it identifies relationships and ensures consistent reference alignment, thereby improving accuracy in complex query scenarios. We restructure the training data to enforce a new output form, providing new annotations for existing datasets and compiling an evaluation benchmark from existing resources. This benchmark is designed explicitly for complex referring cases. We also integrate detection and segmentation capabilities into a unified MLLM framework, training it with a novel adaptive weighted loss to optimize performance. Experimental results on our curated benchmark and RefCOCO&#x2F;+&#x2F;g demonstrate the effectiveness of our approach, with a notable increase of 2.5%+ over baseline models.</p>
  </div>
</details>

<hr>
<h3 id="64-TRepLiNa-Layer-wise-CKA-REPINA-Alignment-Improves-Low-Resource-Machine-Translation-in-Aya-23-8B-cs-CL-cs-AIPDF"><a href="#64-TRepLiNa-Layer-wise-CKA-REPINA-Alignment-Improves-Low-Resource-Machine-Translation-in-Aya-23-8B-cs-CL-cs-AIPDF" class="headerlink" title="[64] TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B cs.CL | cs.AIPDF"></a>[64] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06249">TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06249" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Toshiki Nakai, Ravi Kiran Chikkala, Lena Sophie Oberkircher, Nicholas Jennings, Natalia Skachkova</span></p>
<p><strong>TL;DR:</strong> TRepLiNa是一种结合CKA和REPINA的层对齐方法，用于提升低资源语言（LRL）到高资源语言（HRL）的翻译质量，尤其在数据稀缺环境下效果显著。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 解决印度多样低资源语言因资源匮乏导致的翻译质量差问题，探索在LLM特定层强制跨语言相似性是否有效。</p>
<p><strong>Result:</strong> TRepLiNa在中层对齐中表现最佳，显著提升了低资源语言的翻译质量，是一种低成本实用方法。</p>
<p><strong>Insight:</strong> 中层表征对齐对跨语言翻译任务尤为关键，尤其是在数据稀缺时，TRepLiNa提供了一种有效的解决方案。</p>
<p><strong>Abstract:</strong> The 2025 Multimodal Models for Low-Resource Contexts and Social Impact (MMLoSo) Language Challenge addresses one of India’s most pressing linguistic gaps: the lack of resources for its diverse low-resource languages (LRLs). In this study, we investigate whether enforcing cross-lingual similarity in specific internal layers of a decoder-only multilingual large language model (LLM) can improve translation quality from LRL to high-resource language (HRL). Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric that encourages representations of different languages to align, with REPINA, a regularization method that constrains parameter updates to remain close to the pretrained model, into a joint method we call TRepLiNa. In this research project, we experiment with zero-shot, few-shot, and fine-tuning settings using Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari, Santali, Bhili) with Hindi&#x2F;English pivots. Our results show that aligning mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach to improving LRL translation, especially in data-scarce settings.</p>
  </div>
</details>

<hr>
<h3 id="65-EverydayMMQA-A-Multilingual-and-Multimodal-Framework-for-Culturally-Grounded-Spoken-Visual-QA-cs-CL-cs-AI-68T50-F-2-2-I-2-7PDF"><a href="#65-EverydayMMQA-A-Multilingual-and-Multimodal-Framework-for-Culturally-Grounded-Spoken-Visual-QA-cs-CL-cs-AI-68T50-F-2-2-I-2-7PDF" class="headerlink" title="[65] EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA cs.CL | cs.AI | 68T50 | F.2.2; I.2.7PDF"></a>[65] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06371">EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | 68T50 | F.2.2; I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06371" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Firoj Alam, Ali Ezzat Shahroor, Md. Arid Hasan, Zien Sheikh Ali, Hunzalah Hassan Bhatti</span></p>
<p><strong>TL;DR:</strong> 论文提出了EverydayMMQA框架和OASIS数据集，专注于多语言和多模态的文化语境视觉问答，填补了现有模型在低资源和少数民族语言中文化背景知识不足的空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大规模多模态模型在视觉问答（VQA）等任务中表现优异，但在需要文化背景和日常生活知识的查询中效果不佳，尤其是在低资源和少数民族语言中。</p>
<p><strong>Result:</strong> 评测了4个闭源模型、3个开源模型和1个微调模型，OASIS数据集测试了模型在涉及实用推理、常识和文化感知的任务上的能力。</p>
<p><strong>Insight:</strong> 文化背景和多语言支持是提升多模态模型泛化能力的关键，OASIS为构建具备文化意识的模型提供了重要基准。</p>
<p><strong>Abstract:</strong> Large-scale multimodal models achieve strong results on tasks like Visual Question Answering (VQA), but they often fail when queries require culturally grounded, everyday knowledge, particularly in low-resource and underrepresented languages. To bridge this gap, we introduce Everyday Multimodal and Multilingual QA (EverydayMMQA), a framework for creating large-scale, culturally-grounded datasets for spoken and visual question answering (SVQA). Using this framework, we developed OASIS, a multimodal dataset integrating speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS contains 3.7M spoken questions, enabling four unique input combinations: speech-only, text-only, speech+image, and text+image. Focused on English and Arabic varieties, 18 countries, the dataset content is curated to reflect diverse, real-world situations. OASIS tests models on tasks beyond object recognition that involve pragmatic, commonsense, and culturally aware reasoning. We benchmarked four closed-source models, three open-source models, and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark and training dataset for building multimodal LLMs for a comprehensive set of everyday tasks within cultural contexts. The framework and dataset will be made publicly available to the community.</p>
  </div>
</details>

<hr>
<h3 id="66-Semantic-Regexes-Auto-Interpreting-LLM-Features-with-a-Structured-Language-cs-CLPDF"><a href="#66-Semantic-Regexes-Auto-Interpreting-LLM-Features-with-a-Structured-Language-cs-CLPDF" class="headerlink" title="[66] Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language cs.CLPDF"></a>[66] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06378">Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06378" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Angie Boggust, Donghao Ren, Yannick Assogba, Dominik Moritz, Arvind Satyanarayan</span></p>
<p><strong>TL;DR:</strong> 论文提出了‘语义正则表达式’（semantic regexes）作为一种结构化语言，用于精确描述大型语言模型（LLM）的特征，解决了自然语言描述模糊和不一致的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大型语言模型特征解释方法通常使用自然语言描述，但这些描述往往模糊、不一致，且需要人工重新标注。为了解决这些问题，研究提出了语义正则表达式。</p>
<p><strong>Result:</strong> 实验表明，语义正则表达式在准确性上与自然语言描述相当，但更简洁一致，同时支持量化特征复杂度等新分析方式。用户研究发现其有助于建立准确的LLM特征心理模型。</p>
<p><strong>Insight:</strong> 结构化语言可以显著提升特征解释的精确性和一致性，同时为模型级别的分析提供了新工具。</p>
<p><strong>Abstract:</strong> Automated interpretability aims to translate large language model (LLM) features into human understandable descriptions. However, these natural language feature descriptions are often vague, inconsistent, and require manual relabeling. In response, we introduce semantic regexes, structured language descriptions of LLM features. By combining primitives that capture linguistic and semantic feature patterns with modifiers for contextualization, composition, and quantification, semantic regexes produce precise and expressive feature descriptions. Across quantitative benchmarks and qualitative analyses, we find that semantic regexes match the accuracy of natural language while yielding more concise and consistent feature descriptions. Moreover, their inherent structure affords new types of analyses, including quantifying feature complexity across layers, scaling automated interpretability from insights into individual features to model-wide patterns. Finally, in user studies, we find that semantic regex descriptions help people build accurate mental models of LLM feature activations.</p>
  </div>
</details>

<hr>
<h3 id="67-Protecting-De-identified-Documents-from-Search-based-Linkage-Attacks-cs-CL-cs-AIPDF"><a href="#67-Protecting-De-identified-Documents-from-Search-based-Linkage-Attacks-cs-CL-cs-AIPDF" class="headerlink" title="[67] Protecting De-identified Documents from Search-based Linkage Attacks cs.CL | cs.AIPDF"></a>[67] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06383">Protecting De-identified Documents from Search-based Linkage Attacks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06383" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pierre Lison, Mark Anderson</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种方法，通过构建N-gram倒排索引和使用LLM重写技术，有效防止文档的去标识化后仍能被搜索链接回原始数据的风险，同时保持文本的语义完整性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的去标识化模型虽然能隐藏文档中的个人身份信息，但无法解决文本仍可能被搜索链接回原始数据集的风险，容易导致隐私泄露。</p>
<p><strong>Result:</strong> 在法院案例数据集上的实验表明，该方法能有效防止搜索式链接攻击，同时保持文本内容的语义一致性。</p>
<p><strong>Insight:</strong> LLM技术在隐私保护中的应用潜力显著，通过语义重写可以在保护隐私的同时不影响文本的实用性和可读性。</p>
<p><strong>Abstract:</strong> While de-identification models can help conceal the identity of the individual(s) mentioned in a document, they fail to address linkage risks, defined as the potential to map the de-identified text back to its source. One straightforward way to perform such linkages is to extract phrases from the de-identified document and then check their presence in the original dataset. This paper presents a method to counter search-based linkage attacks while preserving the semantic integrity of the text. The method proceeds in two steps. We first construct an inverted index of the N-grams occurring in the document collection, making it possible to efficiently determine which N-grams appear in less than $k$ documents (either alone or in combination with other N-grams). An LLM-based rewriter is then iteratively queried to reformulate those spans until linkage is no longer possible. Experimental results on a collection of court cases show that the method is able to effectively prevent search-based linkages while remaining faithful to the original content.</p>
  </div>
</details>

<hr>
<h3 id="68-Controllable-Stylistic-Text-Generation-with-Train-Time-Attribute-Regularized-Diffusion-cs-CLPDF"><a href="#68-Controllable-Stylistic-Text-Generation-with-Train-Time-Attribute-Regularized-Diffusion-cs-CLPDF" class="headerlink" title="[68] Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion cs.CLPDF"></a>[68] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06386">Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06386" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Fan Zhou, Chang Tian, Tim Van de Cruys</span></p>
<p><strong>TL;DR:</strong> 本文提出了RegDiff框架，通过训练时属性正则化的扩散模型实现可控风格文本生成，避免了采样时的高计算成本。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的CFG和CG方法在可控文本生成中存在语义保留不足或计算成本高的问题，需一种更高效且精准的方法。</p>
<p><strong>Result:</strong> 在五个数据集上的实验表明，RegDiff在多风格属性生成任务中优于基线方法。</p>
<p><strong>Insight:</strong> 训练时属性正则化可显著降低扩散模型的计算成本，同时保持高质量的属性控制能力。</p>
<p><strong>Abstract:</strong> Generating stylistic text with specific attributes is a key problem in controllable text generation. Recently, diffusion models have emerged as a powerful paradigm for both visual and textual generation. Existing approaches can be broadly categorized into classifier-free guidance (CFG) and classifier guidance (CG) methods. While CFG effectively preserves semantic content, it often fails to provide effective attribute control. In contrast, CG modifies the denoising trajectory using classifier gradients, enabling better attribute alignment but incurring high computational costs during sampling and suffering from classifier generalization issues. In this work, we propose RegDiff, a regularized diffusion framework that leverages attribute features without requiring a pretrained classifier during sampling, thereby achieving controllable generation with reduced computational costs. Specifically, RegDiff employs a VAE-based encoder–decoder architecture to ensure reconstruction fidelity and a latent diffusion model trained with attribute supervision to enable controllable text generation. Attribute information is injected only during training. Experiments on five datasets spanning multiple stylistic attributes demonstrate that RegDiff outperforms strong baselines in generating stylistic texts. These results validate the effectiveness of RegDiff as an efficient solution for attribute-controllable text diffusion. Our code, datasets, and resources will be released upon publication at <a target="_blank" rel="noopener" href="https://github.com/xxxx">https://github.com/xxxx</a>.</p>
  </div>
</details>

<hr>
<h3 id="69-FinLFQA-Evaluating-Attributed-Text-Generation-of-LLMs-in-Financial-Long-Form-Question-Answering-cs-CLPDF"><a href="#69-FinLFQA-Evaluating-Attributed-Text-Generation-of-LLMs-in-Financial-Long-Form-Question-Answering-cs-CLPDF" class="headerlink" title="[69] FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering cs.CLPDF"></a>[69] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06426">FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06426" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yitao Long, Tiansheng Hu, Yilun Zhao, Arman Cohan, Chen Zhao</span></p>
<p><strong>TL;DR:</strong> FinLFQA是一个新的评测基准，旨在评估LLM在复杂金融领域长形式问题回答中的属真文本生成能力，强调多层次的属真性评估，并提出了自动评估框架。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有评测基准大多关注简单的文本证据检索属真方法，而忽略了金融等真实场景中需要多层次属真（如数值推理、领域知识）的需求，因此亟需更全面的评测工具。</p>
<p><strong>Result:</strong> 实验表明，细粒度评测指标能区分模型能力；端到端生成与后处理性能相当；外部反馈下迭代细化才有效。</p>
<p><strong>Insight:</strong> 属真性需多层次评估；端到端方法在多任务场景中表现优异；外部反馈是迭代优化的关键。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) frequently hallucinate to long-form questions, producing plausible yet factually incorrect answers. A common mitigation strategy is to provide attribution to LLM outputs. However, existing benchmarks primarily focus on simple attribution that retrieves supporting textual evidence as references. We argue that in real-world scenarios such as financial applications, attribution goes beyond reference retrieval. We introduce FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate long-form answers to complex financial questions with reliable and nuanced attributions. FinLFQA evaluates three critical aspects of attribution through human annotations: (1) supporting evidence extracted from financial reports, (2) intermediate numerical reasoning steps, and (3) domain-specific financial knowledge that informs the reasoning process. We further provide an automatic evaluation framework covering both answer quality and attribution quality. Through extensive experiments on eight LLMs across multiple attribution-generation paradigms, we find that fine-grained metrics are important to distinguish model capabilities, that end-to-end generation achieves comparable performance to post-hoc approaches, and that iterative refinement only helps when guided by external feedback.</p>
  </div>
</details>

<hr>
<h3 id="70-Bridging-Discourse-Treebanks-with-a-Unified-Rhetorical-Structure-Parser-cs-CLPDF"><a href="#70-Bridging-Discourse-Treebanks-with-a-Unified-Rhetorical-Structure-Parser-cs-CLPDF" class="headerlink" title="[70] Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser cs.CLPDF"></a>[70] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06427">Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06427" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Elena Chistova</span></p>
<p><strong>TL;DR:</strong> UniRST是一个统一的RST风格话语解析器，能够处理11种语言的18个树库，无需修改其关系库。通过两种训练策略（Multi-Head和Masked-Union），解决了关系库不兼容的问题，并在多语言端到端话语解析中表现优于大多数单树库基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是解决多语言话语解析中关系库不兼容的问题，同时提出一个统一的解析器，能够在多语言和多树库环境下高效工作。</p>
<p><strong>Result:</strong> 结果显示，（1）Masked-Union策略在参数效率上表现最佳；（2）UniRST在18个单树库基线中优于16个，证明了其在多语言端到端话语解析中的优势。</p>
<p><strong>Insight:</strong> 该研究的核心洞察是：通过选择性标签掩码和共享参数训练，可以在多语言和多树库环境下实现高效且统一的RST风格话语解析，同时显著提升低资源语言的性能。</p>
<p><strong>Abstract:</strong> We introduce UniRST, the first unified RST-style discourse parser capable of handling 18 treebanks in 11 languages without modifying their relation inventories. To overcome inventory incompatibilities, we propose and evaluate two training strategies: Multi-Head, which assigns separate relation classification layer per inventory, and Masked-Union, which enables shared parameter training through selective label masking. We first benchmark monotreebank parsing with a simple yet effective augmentation technique for low-resource settings. We then train a unified model and show that (1) the parameter efficient Masked-Union approach is also the strongest, and (2) UniRST outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a single-model, multilingual end-to-end discourse parsing across diverse resources.</p>
  </div>
</details>

<hr>
<h3 id="71-MathRobust-LV-Evaluation-of-Large-Language-Models’-Robustness-to-Linguistic-Variations-in-Mathematical-Reasoning-cs-CLPDF"><a href="#71-MathRobust-LV-Evaluation-of-Large-Language-Models’-Robustness-to-Linguistic-Variations-in-Mathematical-Reasoning-cs-CLPDF" class="headerlink" title="[71] MathRobust-LV: Evaluation of Large Language Models’ Robustness to Linguistic Variations in Mathematical Reasoning cs.CLPDF"></a>[71] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06430">MathRobust-LV: Evaluation of Large Language Models’ Robustness to Linguistic Variations in Mathematical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06430" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Neeraja Kirtane, Yuvraj Khanna, Peter Relan</span></p>
<p><strong>TL;DR:</strong> 该论文提出了MathRobust-LV，用于评估大语言模型在数学推理中对语言变化的鲁棒性。研究发现，尽管大语言模型在数学基准测试中表现优异，但在语言变化下其准确性会下降，尤其是较小的模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的数学推理评估主要集中在高难度竞赛（如IMO），而忽视了高中水平数学问题在真实教育场景中的应用。教师在评估中会重述问题但保持难度不变，因此需要评估模型对这种语言变化的鲁棒性。</p>
<p><strong>Result:</strong> 实验显示，模型在语言变化下准确性下降，小型模型下降9-11%，前沿模型（如GPT-5、Gemini-2.5pro）相对稳定。</p>
<p><strong>Insight:</strong> 语言变化的鲁棒性是当前大语言模型在数学推理中的一个重要挑战，即使是前沿模型也存在可测量的性能下降。</p>
<p><strong>Abstract:</strong> Large language models excel on math benchmarks, but their math reasoning robustness to linguistic variation is underexplored. While recent work increasingly treats high-difficulty competitions like the IMO as the gold standard for evaluating reasoning, we believe in comprehensive benchmarking of high school-level math problems in real educational settings. We introduce MathRobust-LV, a test set and evaluation methodology that mirrors how instructors rephrase problems across assessments while keeping difficulty constant: we change surface details (names, contexts, variables) while preserving numerical structure and answers. In contrast to prior efforts that alter problem content or emphasize IMO-level tasks, we focus on high-school-level dataset problems at the difficulty level where models are currently deployed in educational settings: tutoring and assessment systems. In these applications, instructors rephrase identical concepts in varied ways, making linguistic robustness essential for reliable deployment. Although MATH data benchmarking is often regarded as saturated, our experiment on 34 models reveals that accuracy declines when moving from the baseline to the variants. These drops are severe for smaller models (9-11%) while stronger models also show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain comparatively stable. Our results highlight that robustness to linguistic variation is a fundamental challenge, exposing reasoning vulnerabilities in models.</p>
  </div>
</details>

<hr>
<h3 id="72-A-Survey-on-Agentic-Security-Applications-Threats-and-Defenses-cs-CL-cs-AI-cs-CRPDF"><a href="#72-A-Survey-on-Agentic-Security-Applications-Threats-and-Defenses-cs-CL-cs-AI-cs-CRPDF" class="headerlink" title="[72] A Survey on Agentic Security: Applications, Threats and Defenses cs.CL | cs.AI | cs.CRPDF"></a>[72] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06445">A Survey on Agentic Security: Applications, Threats and Defenses</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.CR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06445" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Asif Shahriar, Md Nafiu Rahman, Sadif Ahmed, Farig Sadeque, Md Rizwan Parvez</span></p>
<p><strong>TL;DR:</strong> 本文是第一篇全面调查自主LLM代理在网络安全领域应用的论文，围绕应用、威胁和防御三大支柱展开，分析了150多篇论文，揭示了新兴趋势和研究空白。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着被动LLM向自主LLM代理的快速转变，网络安全面临新的风险和挑战，需要系统梳理其应用、威胁和防御措施。</p>
<p><strong>Result:</strong> 总结了代理安全的应用场景、潜在威胁和防御措施，指出了模型和模态覆盖方面的研究空白。</p>
<p><strong>Insight:</strong> 代理安全领域亟需更多关注模型和模态的多样性，同时需要开发针对新兴威胁的有效防御策略。</p>
<p><strong>Abstract:</strong> The rapid shift from passive LLMs to autonomous LLM-agents marks a new paradigm in cybersecurity. While these agents can act as powerful tools for both offensive and defensive operations, the very agentic context introduces a new class of inherent security risks. In this work we present the first holistic survey of the agentic security landscape, structuring the field around three interdependent pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 150 papers, explaining how agents are used, the vulnerabilities they possess, and the countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage.</p>
  </div>
</details>

<hr>
<h3 id="73-Linguistically-Informed-Tokenization-Improves-ASR-for-Underresourced-Languages-cs-CLPDF"><a href="#73-Linguistically-Informed-Tokenization-Improves-ASR-for-Underresourced-Languages-cs-CLPDF" class="headerlink" title="[73] Linguistically Informed Tokenization Improves ASR for Underresourced Languages cs.CLPDF"></a>[73] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06461">Linguistically Informed Tokenization Improves ASR for Underresourced Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06461" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Massimo Daul, Alessio Tosolini, Claire Bowern</span></p>
<p><strong>TL;DR:</strong> 论文研究了针对资源匮乏语言的自动语音识别（ASR）系统，通过改进分词策略（采用音位分词而非拼写分词），显著提升了Yan-nhangu语言的识别效果，并验证了ASR在语言文档任务中的实用性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现代ASR系统依赖大数据和复杂的transformer架构，难以应用于资源匮乏的语言。研究旨在探索如何通过改进分词策略提升ASR在这些语言中的性能，并验证其在语言文档任务中的实际价值。</p>
<p><strong>Result:</strong> 音位分词显著优于拼写分词，降低了WER和CER。手动修正ASR输出比传统转录快得多。</p>
<p><strong>Insight:</strong> 语言学知识驱动的分词策略能有效提升资源匮乏语言的ASR性能，ASR工具在语言文档中具有实际应用潜力。</p>
<p><strong>Abstract:</strong> Automatic speech recognition (ASR) is a crucial tool for linguists aiming to perform a variety of language documentation tasks. However, modern ASR systems use data-hungry transformer architectures, rendering them generally unusable for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu, a dormant Indigenous Australian language, comparing the effects of phonemic and orthographic tokenization strategies on performance. In parallel, we explore ASR’s viability as a tool in a language documentation pipeline. We find that a linguistically informed phonemic tokenization system substantially improves WER and CER compared to a baseline orthographic tokenization scheme. Finally, we show that hand-correcting the output of an ASR model is much faster than hand-transcribing audio from scratch, demonstrating that ASR can work for underresourced languages.</p>
  </div>
</details>

<hr>
<h3 id="74-Test-Time-Scaling-of-Reasoning-Models-for-Machine-Translation-cs-CL-I-2-7PDF"><a href="#74-Test-Time-Scaling-of-Reasoning-Models-for-Machine-Translation-cs-CL-I-2-7PDF" class="headerlink" title="[74] Test-Time Scaling of Reasoning Models for Machine Translation cs.CL | I.2.7PDF"></a>[74] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06471">Test-Time Scaling of Reasoning Models for Machine Translation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06471" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zihao Li, Shaoxiong Ji, Jörg Tiedemann</span></p>
<p><strong>TL;DR:</strong> 论文研究了测试时扩展（TTS）在机器翻译（MT）中的作用，发现通用推理模型（RMs）的直接翻译效果有限，但通过领域微调或多步自我校正可以显著提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 测试时扩展在数学和编码任务中表现优异，但对其在机器翻译中的应用效果尚不清楚。本研究旨在探索这一方法的实际价值。</p>
<p><strong>Result:</strong> 通用模型的TTS效果有限且不稳定，领域特定模型则能显著受益；强制过度推理会损害性能，而后编辑场景下TTS效果显著。</p>
<p><strong>Insight:</strong> TTS的价值不在单次通用翻译，而在于任务专用模型或多步工作流（如自我校正）。</p>
<p><strong>Abstract:</strong> Test-time scaling (TTS) has enhanced the performance of Reasoning Models (RMs) on various tasks such as math and coding, yet its efficacy in machine translation (MT) remains underexplored. This paper investigates whether increased inference-time computation improves translation quality. We evaluate 12 RMs across a diverse suite of MT benchmarks spanning multiple domains, examining three scenarios: direct translation, forced-reasoning extrapolation, and post-editing. Our findings show that for general-purpose RMs, TTS provides limited and inconsistent benefits for direct translation, with performance quickly plateauing. However, the effectiveness of TTS is unlocked by domain-specific fine-tuning, which aligns a model’s reasoning process with task requirements, leading to consistent improvements up to an optimal, self-determined reasoning depth. We also find that forcing a model to reason beyond its natural stopping point consistently degrades translation quality. In contrast, TTS proves highly effective in a post-editing context, reliably turning self-correction into a beneficial process. These results indicate that the value of inference-time computation in MT lies not in enhancing single-pass translation with general models, but in targeted applications like multi-step, self-correction workflows and in conjunction with task-specialized models.</p>
  </div>
</details>

<hr>
<h3 id="75-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels-cs-CL-cs-AIPDF"><a href="#75-Webscale-RL-Automated-Data-Pipeline-for-Scaling-RL-Data-to-Pretraining-Levels-cs-CL-cs-AIPDF" class="headerlink" title="[75] Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels cs.CL | cs.AIPDF"></a>[75] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06499">Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06499" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhepeng Cen, Haolin Chen, Shiyu Wang, Zuxin Liu, Zhiwei Liu</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为Webscale-RL的数据管道，用于将大规模预训练文档转化为多样化的问答对，以解决强化学习中的数据瓶颈问题。构建的数据集包含120万个样本，实验表明其显著优于持续预训练和基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLM）在文本数据上表现优异，但存在训练与生成的差距，且推理能力受限。强化学习（RL）能弥补这一差距，但面临数据规模不足的瓶颈。</p>
<p><strong>Result:</strong> 实验显示，基于Webscale-RL数据集训练的模型在多个基准测试中表现显著优于持续预训练和其他基线方法，且数据效率更高（最高可达100倍）。</p>
<p><strong>Insight:</strong> 该工作为强化学习扩展到预训练规模提供了可行路径，可能推动更高效、更强大的语言模型发展。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.</p>
  </div>
</details>

<hr>
<h3 id="76-From-Acceleration-to-Saturation-Scaling-Behavior-of-Bootstrapped-Language-Model-Pretraining-cs-CL-cs-LGPDF"><a href="#76-From-Acceleration-to-Saturation-Scaling-Behavior-of-Bootstrapped-Language-Model-Pretraining-cs-CL-cs-LGPDF" class="headerlink" title="[76] From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining cs.CL | cs.LGPDF"></a>[76] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06548">From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06548" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Seng Pei Liew, Takuya Kato</span></p>
<p><strong>TL;DR:</strong> 研究了基于预训练模型的二次预训练（bootstrapped pretraining）的效率，发现其扩展效率随着基础模型预训练程度的增加而下降，表现为对数下降的标度定律。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探索bootstrapped pretraining（如持续预训练或模型增长）的效率，尤其是在基础模型过度预训练的情况下，以降低从头训练语言模型的成本。</p>
<p><strong>Result:</strong> 结果表明，基础模型预训练越彻底，二次预训练的附加收益越少，揭示了多阶段预训练策略中的固有权衡。</p>
<p><strong>Insight:</strong> 研究为高效训练语言模型提供了实用见解，并指出过度预训练模型的再利用需要慎重考虑。</p>
<p><strong>Abstract:</strong> Bootstrapped pretraining, i.e., the reuse of a pretrained base model for further pretraining, such as continual pretraining or model growth, is promising at reducing the cost of training language models from scratch. However, its effectiveness remains unclear, especially when applied to overtrained base models. In this work, we empirically study the scaling behavior of bootstrapped pretraining and find that its scaling efficiency diminishes in a predictable manner: The scaling exponent with respect to second-stage pretraining tokens decreases logarithmically with the number of tokens used to pretrain the base model. The joint dependence on first- and second-stage tokens is accurately modeled by a simple scaling law. Such saturation effect reveals a fundamental trade-off in multi-stage pretraining strategies: the more extensively a model is pretrained, the less additional benefit bootstrapping provides. Our findings provide practical insights for efficient language model training and raise important considerations for the reuse of overtrained models.</p>
  </div>
</details>

<hr>
<h3 id="77-The-Algebra-of-Meaning-Why-Machines-Need-Montague-More-Than-Moore’s-Law-cs-CL-cs-AI-cs-LOPDF"><a href="#77-The-Algebra-of-Meaning-Why-Machines-Need-Montague-More-Than-Moore’s-Law-cs-CL-cs-AI-cs-LOPDF" class="headerlink" title="[77] The Algebra of Meaning: Why Machines Need Montague More Than Moore’s Law cs.CL | cs.AI | cs.LOPDF"></a>[77] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06559">The Algebra of Meaning: Why Machines Need Montague More Than Moore’s Law</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LO</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06559" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cheonkam Jeong, Sungdo Kim, Jewoo Park</span></p>
<p><strong>TL;DR:</strong> 论文指出当前语言模型在语义处理上存在类型错误，提出通过蒙塔古类型的逻辑形式编译自然语言输入，结合神经符号架构实现合规性导向的决策。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有语言模型虽然流畅，但在处理语义类型（如描述性、规范性和法律性）时表现不佳，导致幻觉和不透明的合规性问题。论文认为这些问题源于语义类型理论的缺失。</p>
<p><strong>Result:</strong> 论文提出了评估计划，包括法律推理基准和多法域合成数据集，旨在验证系统在统一语义代数中的表现。</p>
<p><strong>Insight:</strong> 可信的自主系统需要对语义进行组合性类型化，使其能够在一个统一的框架中区分描述性、规范性和法律责任。</p>
<p><strong>Abstract:</strong> Contemporary language models are fluent yet routinely mis-handle the types of meaning their outputs entail. We argue that hallucination, brittle moderation, and opaque compliance outcomes are symptoms of missing type-theoretic semantics rather than data or scale limitations. Building on Montague’s view of language as typed, compositional algebra, we recast alignment as a parsing problem: natural-language inputs must be compiled into structures that make explicit their descriptive, normative, and legal dimensions under context.   We present Savassan, a neuro-symbolic architecture that compiles utterances into Montague-style logical forms and maps them to typed ontologies extended with deontic operators and jurisdictional contexts. Neural components extract candidate structures from unstructured inputs; symbolic components perform type checking, constraint reasoning, and cross-jurisdiction mapping to produce compliance-aware guidance rather than binary censorship. In cross-border scenarios, the system “parses once” (e.g., defect claim(product x, company y)) and projects the result into multiple legal ontologies (e.g., defamation risk in KR&#x2F;JP, protected opinion in US, GDPR checks in EU), composing outcomes into a single, explainable decision.   This paper contributes: (i) a diagnosis of hallucination as a type error; (ii) a formal Montague-ontology bridge for business&#x2F;legal reasoning; and (iii) a production-oriented design that embeds typed interfaces across the pipeline. We outline an evaluation plan using legal reasoning benchmarks and synthetic multi-jurisdiction suites. Our position is that trustworthy autonomy requires compositional typing of meaning, enabling systems to reason about what is described, what is prescribed, and what incurs liability within a unified algebra of meaning.</p>
  </div>
</details>

<hr>
<h3 id="78-TinyScientist-An-Interactive-Extensible-and-Controllable-Framework-for-Building-Research-Agents-cs-CLPDF"><a href="#78-TinyScientist-An-Interactive-Extensible-and-Controllable-Framework-for-Building-Research-Agents-cs-CLPDF" class="headerlink" title="[78] TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents cs.CLPDF"></a>[78] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06579">TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06579" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haofei Yu, Keyang Xuan, Fenghai Li, Kunlun Zhu, Zijie Lei</span></p>
<p><strong>TL;DR:</strong> TinyScientist提出了一个交互式、可扩展且可控的框架，用于构建研究型智能代理，通过开源代码、交互式网页演示和PyPI包简化自动研究流程的开发与维护。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着LLM在自动研究中应用的增加，多代理系统、规划和工具使用的复杂性使得扩展和维护研究流程变得困难。TinyScientist旨在解决这一问题，提供一个灵活且易于扩展的框架。</p>
<p><strong>Result:</strong> 通过开源工具和包，框架能够轻松支持最新的自动研究流程，并广泛适用于研究和开发场景。</p>
<p><strong>Insight:</strong> 模块化和可扩展性是简化复杂研究流程的关键，交互设计可增强用户与代理的协作效率。</p>
<p><strong>Abstract:</strong> Automatic research with Large Language Models (LLMs) is rapidly gaining importance, driving the development of increasingly complex workflows involving multi-agent systems, planning, tool usage, code execution, and human-agent interaction to accelerate research processes. However, as more researchers and developers begin to use and build upon these tools and platforms, the complexity and difficulty of extending and maintaining such agentic workflows have become a significant challenge, particularly as algorithms and architectures continue to advance. To address this growing complexity, TinyScientist identifies the essential components of the automatic research workflow and proposes an interactive, extensible, and controllable framework that easily adapts to new tools and supports iterative growth. We provide an open-source codebase, an interactive web demonstration, and a PyPI Python package to make state-of-the-art auto-research pipelines broadly accessible to every researcher and developer.</p>
  </div>
</details>

<hr>
<h3 id="79-Do-Internal-Layers-of-LLMs-Reveal-Patterns-for-Jailbreak-Detection-cs-CLPDF"><a href="#79-Do-Internal-Layers-of-LLMs-Reveal-Patterns-for-Jailbreak-Detection-cs-CLPDF" class="headerlink" title="[79] Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection? cs.CLPDF"></a>[79] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06594">Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06594" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis</span></p>
<p><strong>TL;DR:</strong> 该论文研究了大型语言模型（LLMs）的内部层是否能为越狱攻击检测提供模式，通过分析GPT-J和Mamba2的内部表示，发现越狱提示与良性提示在隐藏层中存在差异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着对话式LLMs的普及，越狱攻击（通过精心设计的提示诱导模型输出受限内容）成为迫切问题。现有防御机制无法完全抵抗新型攻击，因此研究内部层动态可能提供新思路。</p>
<p><strong>Result:</strong> 初步结果表明，越狱提示在特定层中表现出与良性提示显著不同的行为，暗示内部层动态可用于检测越狱攻击。</p>
<p><strong>Insight:</strong> LLMs内部层的激活模式可能隐含越狱行为的特征，这为开发无需依赖外部规则的新型防御机制提供了可能。</p>
<p><strong>Abstract:</strong> Jailbreaking large language models (LLMs) has emerged as a pressing concern with the increasing prevalence and accessibility of conversational LLMs. Adversarial users often exploit these models through carefully engineered prompts to elicit restricted or sensitive outputs, a strategy widely referred to as jailbreaking. While numerous defense mechanisms have been proposed, attackers continuously develop novel prompting techniques, and no existing model can be considered fully resistant. In this study, we investigate the jailbreak phenomenon by examining the internal representations of LLMs, with a focus on how hidden layers respond to jailbreak versus benign prompts. Specifically, we analyze the open-source LLM GPT-J and the state-space model Mamba2, presenting preliminary findings that highlight distinct layer-wise behaviors. Our results suggest promising directions for further research on leveraging internal model dynamics for robust jailbreak detection and defense.</p>
  </div>
</details>

<hr>
<h3 id="80-A-Comparative-Analysis-of-Contextual-Representation-Flow-in-State-Space-and-Transformer-Architectures-cs-CL-cs-LGPDF"><a href="#80-A-Comparative-Analysis-of-Contextual-Representation-Flow-in-State-Space-and-Transformer-Architectures-cs-CL-cs-LGPDF" class="headerlink" title="[80] A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures cs.CL | cs.LGPDF"></a>[80] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06640">A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06640" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Nhat M. Hoang, Do Xuan Long, Cong-Duy Nguyen, Min-Yen Kan, Luu Anh Tuan</span></p>
<p><strong>TL;DR:</strong> 本文首次对状态空间模型（SSMs）和Transformer模型（TBMs）中的上下文信息流动进行了统一的层级和标记级别的分析，揭示了它们在表示传播中的关键差异和底层机制。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管SSMs作为TBMs的高效替代方案在处理长序列时表现出线性扩展和低内存消耗的优势，但它们在上下文信息流动方面的差异尚未深入研究。本文旨在填补这一空白。</p>
<p><strong>Result:</strong> 研究发现TBMs早期迅速均匀化标记表示，后期才重新引入多样性；而SSMs早期保留标记独特性，深层时才趋向均匀化。差异源于TBMs的结构设计和SSMs的训练动态。</p>
<p><strong>Insight:</strong> 这些发现揭示了两种架构的归纳偏置，为未来长上下文推理模型的设计和训练提供了理论依据。</p>
<p><strong>Abstract:</strong> State Space Models (SSMs) have recently emerged as efficient alternatives to Transformer-Based Models (TBMs) for long-sequence processing, offering linear scaling and lower memory use. Yet, how contextual information flows across layers and tokens in these architectures remains understudied. We present the first unified, token- and layer-level analysis of representation propagation in SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing, we characterize how representations evolve within and across layers. We find a key divergence: TBMs rapidly homogenize token representations, with diversity reemerging only in later layers, while SSMs preserve token uniqueness early but converge to homogenization deeper. Theoretical analysis and parameter randomization further reveal that oversmoothing in TBMs stems from architectural design, whereas in SSMs it arises mainly from training dynamics. These insights clarify the inductive biases of both architectures and inform future model and training designs for long-context reasoning.</p>
  </div>
</details>

<hr>
<h3 id="81-Aligning-Large-Language-Models-via-Fully-Self-Synthetic-Data-cs-CLPDF"><a href="#81-Aligning-Large-Language-Models-via-Fully-Self-Synthetic-Data-cs-CLPDF" class="headerlink" title="[81] Aligning Large Language Models via Fully Self-Synthetic Data cs.CLPDF"></a>[81] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06652">Aligning Large Language Models via Fully Self-Synthetic Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06652" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为SAO的全自生成数据框架，用于大型语言模型的自我对齐，无需依赖昂贵的人类标注或外部奖励模型，通过模型自身生成数据并优化偏好，显著提升了聊天能力和下游任务表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统RLHF和RLAIF方法依赖昂贵的人工或外部模型标注，限制了大规模应用。论文试图通过完全自生成数据的方式降低成本，同时保持模型性能。</p>
<p><strong>Result:</strong> 实验表明SAO在AlpacaEval~2.0基准上显著提升了模型的聊天能力，同时在下游任务（如问答、数学推理）中保持了强性能。</p>
<p><strong>Insight:</strong> 完全自生成数据的对齐方法不仅降低了成本，还为LLM的自我改进提供了一种可行的解决方案。</p>
<p><strong>Abstract:</strong> Traditional reinforcement learning from human feedback (RLHF) for large language models (LLMs) relies on expensive human-annotated datasets, while Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs, requiring the collection of diverse prompts and corresponding responses, often necessitating external reward models or proprietary models like GPT-4 to annotate preference pairs. In this work, we introduce Self-Alignment Optimization (SAO), a fully self-synthetic framework for LLM alignment, where all training data, including prompts (i.e., user queries), responses, and preferences, are generated by the model itself. Specifically, SAO first instructs the LLM to engage in persona role-play and generate diverse prompts and responses, which are then self-evaluated for preference optimization. Extensive experiments demonstrate that SAO effectively enhances the model’s chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining strong performance on downstream objective tasks (e.g., question-answering, math reasoning). Our work provides a practical solution for self-improvement in aligning LLMs, and the code for reproducing our results is available at: <a target="_blank" rel="noopener" href="https://github.com/SJY8460/SAO">https://github.com/SJY8460/SAO</a>.</p>
  </div>
</details>

<hr>
<h3 id="82-ToolMem-Enhancing-Multimodal-Agents-with-Learnable-Tool-Capability-Memory-cs-CLPDF"><a href="#82-ToolMem-Enhancing-Multimodal-Agents-with-Learnable-Tool-Capability-Memory-cs-CLPDF" class="headerlink" title="[82] ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory cs.CLPDF"></a>[82] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06664">ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06664" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yunzhong Xiao, Yangmin Li, Hewei Wang, Yunlong Tang, Zora Zhiruo Wang</span></p>
<p><strong>TL;DR:</strong> ToolMem是一种通过学习工具能力记忆来增强多模态代理的方法，通过总结工具的优缺点并存储在记忆中，代理能够在推理时选择最适合的工具，从而提高任务准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统代理通常依赖固定工具，无法灵活选择最适合的工具。受人类通过交互学习工具能力的启发，本文提出ToolMem，旨在让代理通过学习工具能力记忆来优化工具选择。</p>
<p><strong>Result:</strong> ToolMem在文本和多模态生成场景中分别提升了14.8%和28.7%的性能预测准确性，并在多工具选择中分别提高了21%和24%的绝对性能。</p>
<p><strong>Insight:</strong> 通过学习工具的能力记忆，代理能够更灵活地适应不同任务需求，这种动态工具选择机制为实现更智能的多模态代理提供了新思路。</p>
<p><strong>Abstract:</strong> Agents utilizing tools powered by large language models (LLMs) or vision-language models (VLMs) have demonstrated remarkable progress in diverse tasks across text and visual modalities. Unlike traditional tools such as calculators, which give deterministic outputs, neural tools perform uncertainly across task scenarios. While different tools for a task may excel in varied scenarios, existing agents typically rely on fixed tools, thus limiting the flexibility in selecting the most suitable tool for specific tasks. In contrast, humans snowball their understanding of the capabilities of different tools by interacting with them, and apply this knowledge to select the optimal tool when solving a future task. To build agents that similarly benefit from this process, we propose ToolMem that enables agents to develop memories of tool capabilities from previous interactions, by summarizing their strengths and weaknesses and storing them in memory; at inference, the agent can retrieve relevant entries from ToolMem, and select the best tool to solve individual tasks more accurately. We evaluate ToolMem on learning varied text generation and text-to-image generation neural tools. Compared to no-memory, generic agents, we find ToolMem-augmented agents predict tool performance 14.8% and 28.7% more accurately across text and multimodal generation scenarios. Moreover, ToolMem facilitates optimal tool selection among multiple choices by 21% and 24% absolute increases in respective scenarios.</p>
  </div>
</details>

<hr>
<h3 id="83-PIKA-Expert-Level-Synthetic-Datasets-for-Post-Training-Alignment-from-Scratch-cs-CLPDF"><a href="#83-PIKA-Expert-Level-Synthetic-Datasets-for-Post-Training-Alignment-from-Scratch-cs-CLPDF" class="headerlink" title="[83] PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch cs.CLPDF"></a>[83] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06670">PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06670" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shangjian Yin, Shining Liang, Wenbiao Ding, Yuli Qian, Zhouxing Shi</span></p>
<p><strong>TL;DR:</strong> PiKa提出了一种高效的专家级对齐数据集，仅需30k SFT示例即可超越需要大量数据的现有方法，显著降低了对齐开源LLM的成本和门槛。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的大多数对齐数据集要么私有，要么需要昂贵的人工标注，限制了可复现性和扩展性。即使是RLAIF，数据质量问题仍存在，且不清楚需多少数据才能微调出强指令模型。</p>
<p><strong>Result:</strong> PiKa-SFT在AlpacaEval 2.0和Arena-Hard基准上甚至超越了官方使用1000万专有数据训练的Llama-3-8B-Instruct模型。</p>
<p><strong>Insight:</strong> 高质量的对齐可以通过小规模数据集实现，为开源LLM对齐提供了可扩展的解决方案。</p>
<p><strong>Abstract:</strong> Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs). However, its effectiveness depends on high-quality instruction data. Most existing alignment datasets are either private or require costly human annotation, which limits reproducibility and scalability. Even with Reinforcement Learning from AI Feedback (RLAIF), concerns about data quality remain. Moreover, it is unclear how much data is actually required to fine-tune a base model into a strong instruction-following model. Current approaches often rely on over 300k examples even at the supervised fine-tuning (SFT) stage, yet they still underperform compared to proprietary models, creating barriers for academic and resource-limited communities. To address this gap, we introduce PiKa, a data-efficient family of expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only 30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets, we show that PiKa-SFT outperforms models trained on much larger data. On AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses the official Llama-3-8B-Instruct model trained on over 10 million proprietary examples. We further extend our study by training the Qwen2.5 series (0.5B to 7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that high-quality alignment can be achieved with significantly less data, offering a scalable path for open-source LLM alignment. Code and data: <a target="_blank" rel="noopener" href="https://github.com/SJY8460/PiKa">https://github.com/SJY8460/PiKa</a>.</p>
  </div>
</details>

<hr>
<h3 id="84-Incremental-Summarization-for-Customer-Support-via-Progressive-Note-Taking-and-Agent-Feedback-cs-CL-cs-AI-cs-LGPDF"><a href="#84-Incremental-Summarization-for-Customer-Support-via-Progressive-Note-Taking-and-Agent-Feedback-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[84] Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback cs.CL | cs.AI | cs.LGPDF"></a>[84] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06677">Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06677" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yisha Wu, Cen, Zhao, Yuanpei Cao, Xiaoqing Su</span></p>
<p><strong>TL;DR:</strong> 这篇论文提出了一种增量式摘要系统，用于客服对话中智能生成简洁的笔记，减少客服人员的上下文切换和重复检查。该系统结合了Mixtral-8x7B模型和DeBERTa分类器，并通过客服编辑反馈优化模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 客服人员在处理复杂对话时需要频繁切换上下文，传统的批量摘要方法效率低下且冗余。因此，需要一种增量式摘要系统，能够实时生成简洁笔记并优化摘要质量。</p>
<p><strong>Result:</strong> 实验结果表明，系统相比批量摘要减少了3%的处理时间（复杂案例中减少高达9%），同时客服满意度较高。</p>
<p><strong>Insight:</strong> 增量式摘要结合实时反馈能够显著提升客服效率和质量，尤其是在复杂对话场景中。</p>
<p><strong>Abstract:</strong> We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents’ context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.</p>
  </div>
</details>

<hr>
<h3 id="85-Learning-to-Rewrite-Prompts-for-Bootstrapping-LLMs-on-Downstream-Tasks-cs-CL-cs-AI-cs-LG-eess-ASPDF"><a href="#85-Learning-to-Rewrite-Prompts-for-Bootstrapping-LLMs-on-Downstream-Tasks-cs-CL-cs-AI-cs-LG-eess-ASPDF" class="headerlink" title="[85] Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks cs.CL | cs.AI | cs.LG | eess.ASPDF"></a>[85] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06695">Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06695" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Qinhao Zhou, Xiang Xiang, Kun He, John E. Hopcroft</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种针对机器翻译任务的提示优化方法，通过小参数模型和基于反向翻译的策略，降低训练开销并提升性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有提示工程方法主要优化指令部分，但对输入部分关键的机器翻译任务适用性有限，因此需要专门设计优化方法。</p>
<p><strong>Result:</strong> 该方法在降低训练开销的同时，实现了机器翻译任务的高效性能。</p>
<p><strong>Insight:</strong> 提示工程不仅关注指令部分，输入部分的优化同样重要，尤其是对机器翻译等任务；小参数模型结合反向翻译策略是一种高效解决方案。</p>
<p><strong>Abstract:</strong> In recent years, the growing interest in Large Language Models (LLMs) has significantly advanced prompt engineering, transitioning from manual design to model-based optimization. Prompts for LLMs generally comprise two components: the \textit{instruction}, which defines the task or objective, and the \textit{input}, which is tailored to the instruction type. In natural language generation (NLG) tasks such as machine translation, the \textit{input} component is particularly critical, while the \textit{instruction} component tends to be concise. Existing prompt engineering methods primarily focus on optimizing the \textit{instruction} component for general tasks, often requiring large-parameter LLMs as auxiliary tools. However, these approaches exhibit limited applicability for tasks like machine translation, where the \textit{input} component plays a more pivotal role. To address this limitation, this paper introduces a novel prompt optimization method specifically designed for machine translation tasks. The proposed approach employs a small-parameter model trained using a back-translation-based strategy, significantly reducing training overhead for single-task optimization while delivering highly effective performance. With certain adaptations, this method can also be extended to other downstream tasks.</p>
  </div>
</details>

<hr>
<h3 id="86-How-Language-Models-Conflate-Logical-Validity-with-Plausibility-A-Representational-Analysis-of-Content-Effects-cs-CLPDF"><a href="#86-How-Language-Models-Conflate-Logical-Validity-with-Plausibility-A-Representational-Analysis-of-Content-Effects-cs-CLPDF" class="headerlink" title="[86] How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects cs.CLPDF"></a>[86] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06700">How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06700" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Leonardo Bertolazzi, Sandro Pezzelle, Raffaelle Bernardi</span></p>
<p><strong>TL;DR:</strong> 本文探讨了大型语言模型（LLMs）如何将逻辑有效性（validity）与语义合理性（plausibility）混淆，揭示了其内部表征中两者的线性关系和几何对齐特征。研究表明，这种对齐导致模型在判断时将合理性误认为有效性，并通过干预表征实现了去偏。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 人类和LLMs都会因语义内容的合理性而影响对逻辑有效性的判断，即内容效应（content effects）。人类的行为可用双加工理论解释，但LLMs的机制尚不明确，因此需要研究其内部表征如何编码这两种概念。</p>
<p><strong>Result:</strong> 研究发现：1）LLMs内部有效性与合理性紧密对齐；2）对齐程度预测了行为内容效应的强度；3）去偏向量能显著减少内容效应，提高逻辑推理准确性。</p>
<p><strong>Insight:</strong> LLMs对逻辑概念的表征受到语义信息的干扰，可能导致推理偏差。通过表征干预可以改善这一问题，为开发更逻辑化的系统提供了新思路。</p>
<p><strong>Abstract:</strong> Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.</p>
  </div>
</details>

<hr>
<h3 id="87-Scaling-LLM-Multi-turn-RL-with-End-to-end-Summarization-based-Context-Management-cs-CL-cs-AI-cs-LGPDF"><a href="#87-Scaling-LLM-Multi-turn-RL-with-End-to-end-Summarization-based-Context-Management-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[87] Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management cs.CL | cs.AI | cs.LGPDF"></a>[87] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06727">Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06727" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于摘要的上下文管理方法（SUPO），通过LLM生成的摘要压缩工具使用历史，突破了固定上下文窗口的限制，并在多回合工具使用任务中显著提高了成功率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有RL方法在处理长序列多回合工具使用时，会受到上下文长度限制的影响，导致指令跟随性能下降和计算成本高昂。</p>
<p><strong>Result:</strong> 在交互式函数调用和搜索任务中，SUPO显著提高了成功率，同时保持了较低的工作上下文长度。</p>
<p><strong>Insight:</strong> 摘要机制为突破固定上下文限制提供了一种可扩展的解决方案，尤其在复杂任务中，测试时进一步扩展摘要轮次可以带来额外性能提升。</p>
<p><strong>Abstract:</strong> We study reinforcement learning (RL) fine-tuning of large language model (LLM) agents for long-horizon multi-turn tool use, where context length quickly becomes a fundamental bottleneck. Existing RL pipelines can suffer from degraded instruction following, excessive rollout costs, and most importantly, strict context limits. To address these challenges, we introduce summarization-based context management to training. In specific, it periodically compresses the tool using history by LLM-generated summaries that retain task-relevant information to keep a compact context while enabling the agent to scale beyond the fixed context window. Building on this formulation, we derive a policy gradient representation that seamlessly enables standard LLM RL infrastructures to optimize both tool-use behaviors as well as summarization strategies in an end-to-end fashion. We instantiate this framework with \underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization (\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond a fixed context limit. Experiments on interactive function calling and searching tasks demonstrate that \texttt{SUPO} significantly improves the success rate while maintaining the same or even lower working context length compared to baselines. We also demonstrate that for complex searching tasks, \texttt{SUPO} can further improve the evaluation performance when scaling test-time maximum round of summarization beyond that of training time. Our results establish summarization-based context management as a principled and scalable approach for training RL agents beyond a fixed context length limit.</p>
  </div>
</details>

<hr>
<h3 id="88-AWM-Accurate-Weight-Matrix-Fingerprint-for-Large-Language-Models-cs-CLPDF"><a href="#88-AWM-Accurate-Weight-Matrix-Fingerprint-for-Large-Language-Models-cs-CLPDF" class="headerlink" title="[88] AWM: Accurate Weight-Matrix Fingerprint for Large Language Models cs.CLPDF"></a>[88] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06738">AWM: Accurate Weight-Matrix Fingerprint for Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06738" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于权重矩阵的训练无关指纹方法（AWM），用于检测大型语言模型（LLM）是否衍生自现有基础模型，解决了后训练操作对模型识别的挑战。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型的训练成本高昂，保护其知识产权至关重要。然而，常见的后训练操作（如微调、剪枝等）对模型识别带来了巨大挑战。</p>
<p><strong>Result:</strong> 在150个正负模型对上实现了完美分类指标，计算时间小于30秒（NVIDIA 3090 GPU）。</p>
<p><strong>Insight:</strong> 训练无关的指纹方法为模型知识产权保护提供了可靠工具，尤其在应对复杂后训练操作时表现出色。</p>
<p><strong>Abstract:</strong> Protecting the intellectual property of large language models (LLMs) is crucial, given the substantial resources required for their training. Consequently, there is an urgent need for both model owners and third parties to determine whether a suspect LLM is trained from scratch or derived from an existing base model. However, the intensive post-training processes that models typically undergo-such as supervised fine-tuning, extensive continued pretraining, reinforcement learning, multi-modal extension, pruning, and upcycling-pose significant challenges to reliable identification. In this work, we propose a training-free fingerprinting method based on weight matrices. We leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel Alignment (CKA) similarity to neutralize the effects of parameter manipulations, yielding a highly robust and high-fidelity similarity metric. On a comprehensive testbed of 60 positive and 90 negative model pairs, our method demonstrates exceptional robustness against all six aforementioned post-training categories while exhibiting a near-zero risk of false positives. By achieving perfect scores on all classification metrics, our approach establishes a strong basis for reliable model lineage verification. Moreover, the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LUMIA-Group/AWM">https://github.com/LUMIA-Group/AWM</a>.</p>
  </div>
</details>

<hr>
<h3 id="89-Gold-Switch-Training-Free-Superposition-of-Slow-and-Fast-Thinking-LLMs-cs-CLPDF"><a href="#89-Gold-Switch-Training-Free-Superposition-of-Slow-and-Fast-Thinking-LLMs-cs-CLPDF" class="headerlink" title="[89] Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs cs.CLPDF"></a>[89] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06750">Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06750" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jaeseong Lee, Dayoung Kwon, seung-won hwang</span></p>
<p><strong>TL;DR:</strong> Gold-Switch提出了一种无需训练的叠加部署策略，通过轻量级调节选择性关闭LRM的推理，避免过度推理浪费资源。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型推理模型（LRM）在结构化任务中表现优秀，但容易因过度推理导致性能下降和资源浪费。传统方法需要部署多个模型，成本高昂且不切实际。</p>
<p><strong>Result:</strong> 该方法减少了过度推理的开销，同时保持了模型的推理能力。</p>
<p><strong>Insight:</strong> 选择性关闭模型的推理部分是一种高效优化计算资源的方式，同时不影响任务性能。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs) excel in structured tasks by emulating deliberate human reasoning but often suffer from overthinking, degrading performance and wasting resources. One possible baseline is to deploy both LLM and LRM, then route input by predicting whether it requires reasoning and may cause overthinking. However, deploying multiple models can be costly or impractical. We propose a superposed deployment strategy with a lightweight, training-free regulation to optimize inference by switching one model on and off. Instead of routing, we selectively unlearn from LRM at inference, scaling down computation while preserving reasoning. By analyzing the cumulative energy of singular values, we identify optimal low-rank projections to adjust reasoning just right.</p>
  </div>
</details>

<hr>
<h3 id="90-Adaptive-LLM-Symbolic-Reasoning-via-Dynamic-Logical-Solver-Composition-cs-CLPDF"><a href="#90-Adaptive-LLM-Symbolic-Reasoning-via-Dynamic-Logical-Solver-Composition-cs-CLPDF" class="headerlink" title="[90] Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition cs.CLPDF"></a>[90] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06774">Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06774" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lei Xu, Pierre Beckmann, Marco Valentino, André Freitas</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种动态逻辑求解器组合的自适应LLM-符号推理框架，通过自动识别自然语言问题中的形式推理策略并动态选择专用逻辑求解器，显著提升推理性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有神经符号NLP方法多为静态设计，限制了多样形式推理策略的使用，因此需要一种自适应框架以动态整合语言模型和逻辑求解器。</p>
<p><strong>Result:</strong> 框架在推理任务中优于基线模型（如GPT-4o和DeepSeek-V3.1），且对纯LLM方法的推理能力也有提升。</p>
<p><strong>Insight:</strong> 自适应推理不仅能提升神经符号方法的性能，还能增强纯LLM方法的表现，同时小型模型可通过后续训练优化改进。</p>
<p><strong>Abstract:</strong> Neuro-symbolic NLP methods aim to leverage the complementary strengths of large language models and formal logical solvers. However, current approaches are mostly static in nature, i.e., the integration of a target solver is predetermined at design time, hindering the ability to employ diverse formal inference strategies. To address this, we introduce an adaptive, multi-paradigm, neuro-symbolic inference framework that: (1) automatically identifies formal reasoning strategies from problems expressed in natural language; and (2) dynamically selects and applies specialized formal logical solvers via autoformalization interfaces. Extensive experiments on individual and multi-paradigm reasoning tasks support the following conclusions: LLMs are effective at predicting the necessary formal reasoning strategies with an accuracy above 90 percent. This enables flexible integration with formal logical solvers, resulting in our framework outperforming competing baselines by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively. Moreover, adaptive reasoning can even positively impact pure LLM methods, yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT settings with GPT-4o. Finally, although smaller models struggle with adaptive neuro-symbolic reasoning, post-training offers a viable path to improvement. Overall, this work establishes the foundations for adaptive LLM-symbolic reasoning, offering a path forward for unifying material and formal inferences on heterogeneous reasoning challenges.</p>
  </div>
</details>

<hr>
<h3 id="91-FURINA-A-Fully-Customizable-Role-Playing-Benchmark-via-Scalable-Multi-Agent-Collaboration-Pipeline-cs-CL-cs-AI-cs-HC-cs-MAPDF"><a href="#91-FURINA-A-Fully-Customizable-Role-Playing-Benchmark-via-Scalable-Multi-Agent-Collaboration-Pipeline-cs-CL-cs-AI-cs-HC-cs-MAPDF" class="headerlink" title="[91] FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline cs.CL | cs.AI | cs.HC | cs.MAPDF"></a>[91] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06800">FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.HC | cs.MA</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06800" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Haotian Wu, Shufan Jiang, Chios Chen, Yiyang Feng, Hehai Lin</span></p>
<p><strong>TL;DR:</strong> FURINA-Builder是一个可扩展的多智能体协作管道，用于自动构建完全可定制的角色扮演（RP）基准测试，解决了现有基准测试范围窄、交互范式过时和适应性有限的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大语言模型（LLMs）在角色扮演任务中的进步，现有基准测试因范围狭窄、交互范式过时以及难以适应多样化应用场景而迅速过时。</p>
<p><strong>Result:</strong> 实验表明，o3和DeepSeek-R1分别在英文和中文角色扮演任务中表现最佳，推理能力强的模型在RP任务中表现更好但幻觉问题也更突出。</p>
<p><strong>Insight:</strong> 研究发现，模型规模并不单调减少幻觉；推理能力与RP性能之间存在新的权衡，推理能力提升RP效果但同时也增加幻觉问题。</p>
<p><strong>Abstract:</strong> As large language models (LLMs) advance in role-playing (RP) tasks, existing benchmarks quickly become obsolete due to their narrow scope, outdated interaction paradigms, and limited adaptability across diverse application scenarios. To address this gap, we introduce FURINA-Builder, a novel multi-agent collaboration pipeline that automatically constructs fully customizable RP benchmarks at any scale. It enables evaluation of arbitrary characters across diverse scenarios and prompt formats, as the first benchmark builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues between a test character and other characters drawn from a well-constructed character-scene pool, while an LLM judge selects fine-grained evaluation dimensions and adjusts the test character’s responses into final test utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive role-playing benchmark featuring both established and synthesized test characters, each assessed with dimension-specific evaluation criteria. Human evaluation and preliminary separability analysis justify our pipeline and benchmark design. We conduct extensive evaluations of cutting-edge LLMs and find that o3 and DeepSeek-R1 achieve the best performance on English and Chinese RP tasks, respectively. Across all models, established characters consistently outperform synthesized ones, with reasoning capabilities further amplifying this disparity. Interestingly, we observe that model scale does not monotonically reduce hallucinations. More critically, for reasoning LLMs, we uncover a novel trade-off: reasoning improves RP performance but simultaneously increases RP hallucinations. This trade-off extends to a broader Pareto frontier between RP performance and reliability for all LLMs. These findings demonstrate the effectiveness of FURINA-Builder and the challenge posed by FURINA-Bench.</p>
  </div>
</details>

<hr>
<h3 id="92-Overview-of-the-Plagiarism-Detection-Task-at-PAN-2025-cs-CL-cs-IRPDF"><a href="#92-Overview-of-the-Plagiarism-Detection-Task-at-PAN-2025-cs-CL-cs-IRPDF" class="headerlink" title="[92] Overview of the Plagiarism Detection Task at PAN 2025 cs.CL | cs.IRPDF"></a>[92] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06805">Overview of the Plagiarism Detection Task at PAN 2025</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.IR</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06805" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">André Greiner-Petter, Maik Fröbe, Jan Philip Wahle, Terry Ruas, Bela Gipp</span></p>
<p><strong>TL;DR:</strong> PAN 2025的抄袭检测任务聚焦于识别科学文章中自动生成的抄袭内容，并通过新构建的大规模数据集评估方法性能。发现当前基于语义相似度的方法表现较好，但泛化能力不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大型语言模型的普及，自动生成的抄袭内容成为新挑战。PAN 2025旨在开发能识别此类抄袭并追溯源头的方法。</p>
<p><strong>Result:</strong> 基于嵌入向量的方法在PAN 2025上表现良好（召回率0.8，精确率0.5），但在PAN 2015上表现显著下降，表明泛化能力有限。</p>
<p><strong>Insight:</strong> 当前方法对新型抄袭有效，但需提升泛化能力；未来研究需结合更多上下文或多模态信息以提高鲁棒性。</p>
<p><strong>Abstract:</strong> The generative plagiarism detection task at PAN 2025 aims at identifying automatically generated textual plagiarism in scientific articles and aligning them with their respective sources. We created a novel large-scale dataset of automatically generated plagiarism using three large language models: Llama, DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation of this dataset, summarize and compare the results of all participants and four baselines, and evaluate the results on the last plagiarism detection task from PAN 2015 in order to interpret the robustness of the proposed approaches. We found that the current iteration does not invite a large variety of approaches as naive semantic similarity approaches based on embedding vectors provide promising results of up to 0.8 recall and 0.5 precision. In contrast, most of these approaches underperform significantly on the 2015 dataset, indicating a lack in generalizability.</p>
  </div>
</details>

<hr>
<h3 id="93-Adaptive-Tool-Generation-with-Models-as-Tools-and-Reinforcement-Learning-cs-CLPDF"><a href="#93-Adaptive-Tool-Generation-with-Models-as-Tools-and-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[93] Adaptive Tool Generation with Models as Tools and Reinforcement Learning cs.CLPDF"></a>[93] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06825">Adaptive Tool Generation with Models as Tools and Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06825" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Chenpeng Wang, Xiaojie Cheng, Chunye Wang, Linfeng Yang, Lei Zhang</span></p>
<p><strong>TL;DR:</strong> MTR提出了一种基于模拟的训练框架，通过多智能体架构和强化学习，实现了无需依赖实时API的工具增强推理，并在多跳QA任务中表现出色。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有工具增强语言模型依赖实时API，导致训练和部署时的可扩展性和可靠性问题，MTR旨在通过模拟方法解决这些问题。</p>
<p><strong>Result:</strong> 在四个多跳QA基准测试中，MTR的Exact Match分数与依赖实时API的系统相当，尤其在复杂推理任务中表现更优。</p>
<p><strong>Insight:</strong> 表明通过结构化轨迹学习工具推理是可行的，无需实时交互，为语言模型的工具增强提供了新思路。</p>
<p><strong>Abstract:</strong> Tool-augmented language models have demonstrated strong capabilities, but their reliance on live API access creates scalability and reliability challenges during training and deployment. We propose MTR, a simulation-first training framework for tool-augmented reasoning. Instead of relying on live APIs, MTR learns from complete ReAct traces with schema-validated, simulated observations. Our approach operates through a multi-agent architecture where a ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an AutoAgent produces structured think-act-observe sequences, and a ToolActor simulates realistic responses. Training proceeds in two stages: Stage-1 Supervised Fine-Tuning (SFT) teaches ‘trace grammar’ from complete reasoning sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy with a composite trace reward that balances answer correctness and internal consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue, 2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to live-API systems and excels on reasoning-intensive tasks, suggesting that effective tool reasoning can be learned from structured traces without live interactions.</p>
  </div>
</details>

<hr>
<h3 id="94-Mid-Training-of-Large-Language-Models-A-Survey-cs-CLPDF"><a href="#94-Mid-Training-of-Large-Language-Models-A-Survey-cs-CLPDF" class="headerlink" title="[94] Mid-Training of Large Language Models: A Survey cs.CLPDF"></a>[94] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06826">Mid-Training of Large Language Models: A Survey</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06826" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaixiang Mo, Yuxin Shi, Weiwei Weng, Zhiqiang Zhou, Shuman Liu</span></p>
<p><strong>TL;DR:</strong> 这篇论文首次将大语言模型（LLM）的中期训练（mid-training）作为一个统一范式进行系统综述，提出了涵盖数据分布、学习率调度和长上下文扩展的分类法，并总结了实际见解、评估基准和性能提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 虽然中期训练在先进系统中广泛应用，但缺乏对其作为统一范式的系统研究。本文旨在填补这一空白，提供关于中期训练的理论和实践指导。</p>
<p><strong>Result:</strong> 研究表明，中期训练能够缓解噪声标记的收益递减、稳定模型收敛，并扩展模型在后期训练中的能力。</p>
<p><strong>Insight:</strong> 中期训练的成功可以通过梯度噪声尺度、信息瓶颈理论和课程学习来解释，这些理论共同促进了模型的泛化和抽象能力。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are typically developed through large-scale pre-training followed by task-specific fine-tuning. Recent advances highlight the importance of an intermediate mid-training stage, where models undergo multiple annealing-style phases that refine data quality, adapt optimization schedules, and extend context length. This stage mitigates diminishing returns from noisy tokens, stabilizes convergence, and expands model capability in late training. Its effectiveness can be explained through gradient noise scale, the information bottleneck, and curriculum learning, which together promote generalization and abstraction. Despite widespread use in state-of-the-art systems, there has been no prior survey of mid-training as a unified paradigm. We introduce the first taxonomy of LLM mid-training spanning data distribution, learning-rate scheduling, and long-context extension. We distill practical insights, compile evaluation benchmarks, and report gains to enable structured comparisons across models. We also identify open challenges and propose avenues for future research and practice.</p>
  </div>
</details>

<hr>
<h3 id="95-SID-Multi-LLM-Debate-Driven-by-Self-Signals-cs-CL-cs-AIPDF"><a href="#95-SID-Multi-LLM-Debate-Driven-by-Self-Signals-cs-CL-cs-AIPDF" class="headerlink" title="[95] SID: Multi-LLM Debate Driven by Self Signals cs.CL | cs.AIPDF"></a>[95] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06843">SID: Multi-LLM Debate Driven by Self Signals</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06843" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种基于自信号的多LLM辩论方法SID，通过利用模型级置信度和token级语义聚焦信号，动态优化辩论过程，减少冗余计算并提高性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多LLM代理辩论方法（MAD）主要依赖外部结构（如辩论图或LLM-as-a-Judge），忽略了生成过程中产生的自信号（如token logits和注意力）。这可能导致冗余计算和性能下降。</p>
<p><strong>Result:</strong> 在多LLM和多模态LLM的多个基准测试中，SID在准确性和token消耗上均优于现有MAD方法。</p>
<p><strong>Insight:</strong> 自信号是多LLM辩论中被忽略的关键因素，其动态利用可以显著提升辩论系统的性能和效率。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{<a target="_blank" rel="noopener" href="https://github.com/xuhang2019/SID%7D%7B/texttt%7Bhttps://github.com/xuhang2019/SID%7D%7D">https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}</a>.</p>
  </div>
</details>

<hr>
<h3 id="96-OpenJAI-v1-0-An-Open-Thai-Large-Language-Model-cs-CL-cs-AIPDF"><a href="#96-OpenJAI-v1-0-An-Open-Thai-Large-Language-Model-cs-CL-cs-AIPDF" class="headerlink" title="[96] OpenJAI-v1.0: An Open Thai Large Language Model cs.CL | cs.AIPDF"></a>[96] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06847">OpenJAI-v1.0: An Open Thai Large Language Model</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06847" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Pontakorn Trakuekul, Attapol T. Rutherford, Jullajak Karnjanaekarin, Narongkorn Panitsrisit, Sumana Sumanakul</span></p>
<p><strong>TL;DR:</strong> OpenJAI-v1.0是一个基于Qwen3-14B的开源泰语和英语大语言模型，专注于提升指令遵循、长上下文理解及工具使用三种任务的表现。其表现优于其他开源泰语模型，且避免了灾难性遗忘。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 针对泰语AI社区对高质量开源语言模型的需求，OpenJAI-v1.0旨在填补这一空白，并提供多样化的任务支持。</p>
<p><strong>Result:</strong> OpenJAI-v1.0在多样化评测中超越其他开源泰语模型，且未出现灾难性遗忘现象。</p>
<p><strong>Insight:</strong> 精选数据对任务性能提升至关重要，开源模型的发布能促进泰语AI社区的发展。</p>
<p><strong>Abstract:</strong> We introduce OpenJAI-v1.0, an open-source large language model for Thai and English, developed from the Qwen3-14B model. Our work focuses on boosting performance on practical tasks through carefully curated data across three key use cases: instruction following, long-context understanding, and tool use. Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its base model and outperforms other leading open-source Thai models on a diverse suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is publicly released as another alternative NLP resource for the Thai AI community.</p>
  </div>
</details>

<hr>
<h3 id="97-Unlocking-Latent-Discourse-Translation-in-LLMs-Through-Quality-Aware-Decoding-cs-CLPDF"><a href="#97-Unlocking-Latent-Discourse-Translation-in-LLMs-Through-Quality-Aware-Decoding-cs-CLPDF" class="headerlink" title="[97] Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding cs.CLPDF"></a>[97] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06866">Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06866" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Wafaa Mohammed, Vlad Niculae, Chrysoula Zerva</span></p>
<p><strong>TL;DR:</strong> 提出了quality-aware decoding（QAD），用于从LLMs中提取潜在的语篇知识，从而改进上下文感知的翻译效果，该方法在语义丰富度和人类偏好上表现优越。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLMs）在机器翻译任务中表现出色，但在处理语篇现象（如代词解析和词汇连贯性）时仍存在不足。作者希望通过解码方法挖掘LLMs中潜在的语篇知识。</p>
<p><strong>Result:</strong> QAD在语义丰富度和人类偏好方面表现优越，验证了其在提升翻译质量方面的有效性。</p>
<p><strong>Insight:</strong> LLMs中确实编码了语篇知识，但需要通过适当的解码方法（如QAD）才能有效提取和应用。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) have emerged as strong contenders in machine translation.Yet, they still struggle to adequately handle discourse phenomena, such as pronoun resolution and lexical cohesion at the document level. In this study, we thoroughly investigate the discourse phenomena performance of LLMs in context-aware translation. We demonstrate that discourse knowledge is encoded within LLMs and propose the use of quality-aware decoding (QAD) to effectively extract this knowledge, showcasing its superiority over other decoding approaches through comprehensive analysis. Furthermore, we illustrate that QAD enhances the semantic richness of translations and aligns them more closely with human preferences.</p>
  </div>
</details>

<hr>
<h3 id="98-λ-GRPO-Unifying-the-GRPO-Frameworks-with-Learnable-Token-Preferences-cs-CLPDF"><a href="#98-λ-GRPO-Unifying-the-GRPO-Frameworks-with-Learnable-Token-Preferences-cs-CLPDF" class="headerlink" title="[98] $λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences cs.CLPDF"></a>[98] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06870">$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06870" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yining Wang, Jinman Zhao, Chuangxin Zhao, Shuhao Guan, Gerald Penn</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为$λ$-GRPO的方法，通过引入可学习的参数$λ$，动态调整令牌级别的权重，以解决GRPO框架中的长度偏差问题，并在多个数学推理基准上取得了显著提升。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的RLHF方法在LLMs推理能力提升中存在长度偏差问题，即较长的响应会均匀分配奖励给所有令牌，导致梯度更新不合理。现有的GRPO变体（如DAPO和Dr. GRPO）虽尝试调整令牌级损失聚合方式，但仍缺乏解释性。论文旨在提供一个统一框架，并通过学习令牌偏好来优化性能。</p>
<p><strong>Result:</strong> 在Qwen2.5模型（1.5B、3B和7B参数）上，$λ$-GRPO相比GRPO平均准确率分别提升1.9%、1.0%和1.7%，且无需额外计算成本或训练数据修改。</p>
<p><strong>Insight:</strong> 通过学习令牌偏好，可以更有效地优化LLMs的推理能力，且这种改进方式具有实际应用价值，无需额外的资源投入。</p>
<p><strong>Abstract:</strong> Reinforcement Learning with Human Feedback (RLHF) has been the dominant approach for improving the reasoning capabilities of Large Language Models (LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has simplified this paradigm by replacing the reward and value models with rule-based verifiers. A prominent example is Group Relative Policy Optimization (GRPO). However, GRPO inherently suffers from a length bias, since the same advantage is uniformly assigned to all tokens of a response. As a result, longer responses distribute the reward over more tokens and thus contribute disproportionately to gradient updates. Several variants, such as DAPO and Dr. GRPO, modify the token-level aggregation of the loss, yet these methods remain heuristic and offer limited interpretability regarding their implicit token preferences. In this work, we explore the possibility of allowing the model to learn its own token preference during optimization. We unify existing frameworks under a single formulation and introduce a learnable parameter $\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO to denote our method, and we find that $\lambda$-GRPO achieves consistent improvements over vanilla GRPO and DAPO on multiple mathematical reasoning benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO improves average accuracy by $+1.9%$, $+1.0%$, and $+1.7%$ compared to GRPO, respectively. Importantly, these gains come without any modifications to the training data or additional computational cost, highlighting the effectiveness and practicality of learning token preferences.</p>
  </div>
</details>

<hr>
<h3 id="99-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models-cs-CL-eess-ASPDF"><a href="#99-SHANKS-Simultaneous-Hearing-and-Thinking-for-Spoken-Language-Models-cs-CL-eess-ASPDF" class="headerlink" title="[99] SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models cs.CL | eess.ASPDF"></a>[99] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06917">SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06917" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin</span></p>
<p><strong>TL;DR:</strong> SHANKS是一种通用推理框架，允许语音语言模型（SLM）在实时收听用户输入的同时生成未说出的思维链推理，从而降低交互延迟并提升响应准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的大型语言模型（LLM）和语音语言模型（SLM）仅在用户说完后才开始思考和行动，导致高响应延迟，不适合实时语音交互。人类在倾听时就能思考，SHANKS试图模拟这一行为。</p>
<p><strong>Result:</strong> 在数学问题场景中，SHANKS打断用户错误的准确率比基线高37.1%；在工具辅助对话中，56.9%的工具调用能在用户说完前完成。</p>
<p><strong>Insight:</strong> SHANKS展示了模型在整个对话过程中持续思考的重要性，而非仅在用户说完后响应，为实时语音交互提供了新方向。</p>
<p><strong>Abstract:</strong> Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user’s turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally “think while listening.” In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at <a target="_blank" rel="noopener" href="https://d223302.github.io/SHANKS/">https://d223302.github.io/SHANKS/</a></p>
  </div>
</details>

<hr>
<h3 id="100-Open-ASR-Leaderboard-Towards-Reproducible-and-Transparent-Multilingual-and-Long-Form-Speech-Recognition-Evaluation-cs-CL-cs-AI-cs-SD-eess-ASPDF"><a href="#100-Open-ASR-Leaderboard-Towards-Reproducible-and-Transparent-Multilingual-and-Long-Form-Speech-Recognition-Evaluation-cs-CL-cs-AI-cs-SD-eess-ASPDF" class="headerlink" title="[100] Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation cs.CL | cs.AI | cs.SD | eess.ASPDF"></a>[100] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06961">Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.SD | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06961" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri</span></p>
<p><strong>TL;DR:</strong> 该论文提出了Open ASR Leaderboard，一个完全可复现的多语言和长语音ASR评估基准，通过标准化文本归一化和报告WER与RTFx，实现了公平的准确率和效率比较。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前ASR评估主要集中在短英语语音，且效率指标报告不足，缺乏透明和可复现的评测标准。</p>
<p><strong>Result:</strong> 研究发现Conformer编码器+LLM解码器在英语转录中准确率最高但速度慢，而CTC和TDT解码器在长语音和离线场景中更具效率优势。</p>
<p><strong>Insight:</strong> 1) 准确率和效率需权衡；2) 特定场景优化（如长语音）需要选择不同架构；3) 开源基准推动了ASR研究的透明性。</p>
<p><strong>Abstract:</strong> Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.</p>
  </div>
</details>

<hr>
<h3 id="101-EDUMATH-Generating-Standards-aligned-Educational-Math-Word-Problems-cs-CL-cs-AIPDF"><a href="#101-EDUMATH-Generating-Standards-aligned-Educational-Math-Word-Problems-cs-CL-cs-AIPDF" class="headerlink" title="[101] EDUMATH: Generating Standards-aligned Educational Math Word Problems cs.CL | cs.AIPDF"></a>[101] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06965">EDUMATH: Generating Standards-aligned Educational Math Word Problems</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06965" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Bryan R. Christ, Penelope Molitz, Jonathan Kropko, Thomas Hartvigsen</span></p>
<p><strong>TL;DR:</strong> 论文提出利用大语言模型（LLM）生成符合教育标准的数学应用题（MWP），并通过专家和LLM联合评估的方法验证其效果。研究构建了首个教师标注的数据集，并训练了一个12B的开源模型，效果优于现有基线模型。此外，学生测试表明，生成的MWP与人写MWP表现相近，但学生更偏好定制化的MWP。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 数学应用题是K-12教育的关键工具，但因班级规模大和教师负担重，难以实现个性化定制。LLM的潜力可以支持数学教育，解决这一问题。</p>
<p><strong>Result:</strong> 12B开源模型性能与更大模型相当；30B模型通过分类器超越基线；生成的MWP更接近人写MWP；学生测试显示生成MWP与人写MWP表现相似，但更偏好定制化MWP。</p>
<p><strong>Insight:</strong> LLM可以高效生成符合教育标准的MWP，减轻教师负担；小规模模型通过高质量数据可以达到或超越更大模型的性能；学生偏好定制化内容，验证了LLM在教育中的实用性。</p>
<p><strong>Abstract:</strong> Math word problems (MWPs) are critical K-12 educational tools, and customizing them to students’ interests and ability levels can increase learning outcomes. However, teachers struggle to find time to customize MWPs for each student given large class sizes and increasing burnout. We propose that LLMs can support math education by generating MWPs customized to student interests and math education standards. To this end, we use a joint human expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and closed LLMs and develop the first teacher-annotated dataset for standards-aligned educational MWP generation. We show the value of our data by using it to train a 12B open model that matches the performance of larger and more capable open models. We also use our teacher-annotated data to train a text classifier that enables a 30B open LLM to outperform existing closed baselines without any training. Next, we show our models’ MWPs are more similar to human-written MWPs than those from existing models. We conclude by conducting the first study of customized LLM-generated MWPs with grade school students, finding they perform similarly on our models’ MWPs relative to human-written MWPs but consistently prefer our customized MWPs.</p>
  </div>
</details>

<hr>
<h3 id="102-Probing-Social-Identity-Bias-in-Chinese-LLMs-with-Gendered-Pronouns-and-Social-Groups-cs-CLPDF"><a href="#102-Probing-Social-Identity-Bias-in-Chinese-LLMs-with-Gendered-Pronouns-and-Social-Groups-cs-CLPDF" class="headerlink" title="[102] Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups cs.CLPDF"></a>[102] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06974">Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06974" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Geng Liu, Feng Li, Junjie Mu, Mengxiao Zhu, Francesco Pierri</span></p>
<p><strong>TL;DR:</strong> 研究通过汉语提示和中国社会群体，探究中文大语言模型中的社会身份偏见，发现模型在群内（‘我们’）和群外（‘他们’）表述中存在系统性偏见，且这种偏见在真实对话中更显著。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 随着大语言模型在用户场景中的广泛应用，其可能反映并放大社会偏见的问题引发关注。研究聚焦中文模型，填补跨语言偏见评估的空白。</p>
<p><strong>Result:</strong> 模型在群内表述中表现积极，群外表述中呈现消极倾向，且这种偏见于真实互动中更显著。</p>
<p><strong>Insight:</strong> 社会身份偏见具有跨语言普适性，用户交互可能加剧模型偏见，强调需在非英语语境中加强偏见评估。</p>
<p><strong>Abstract:</strong> Large language models (LLMs) are increasingly deployed in user-facing applications, raising concerns about their potential to reflect and amplify social biases. We investigate social identity framing in Chinese LLMs using Mandarin-specific prompts across ten representative Chinese LLMs, evaluating responses to ingroup (“We”) and outgroup (“They”) framings, and extending the setting to 240 social groups salient in the Chinese context. To complement controlled experiments, we further analyze Chinese-language conversations from a corpus of real interactions between users and chatbots. Across models, we observe systematic ingroup-positive and outgroup-negative tendencies, which are not confined to synthetic prompts but also appear in naturalistic dialogue, indicating that bias dynamics might strengthen in real interactions. Our study provides a language-aware evaluation framework for Chinese LLMs, demonstrating that social identity biases documented in English generalize cross-linguistically and intensify in user-facing contexts.</p>
  </div>
</details>

<hr>
<h3 id="103-Towards-Reliable-Retrieval-in-RAG-Systems-for-Large-Legal-Datasets-cs-CL-cs-IR-I-2-7-H-3-3-K-5-0PDF"><a href="#103-Towards-Reliable-Retrieval-in-RAG-Systems-for-Large-Legal-Datasets-cs-CL-cs-IR-I-2-7-H-3-3-K-5-0PDF" class="headerlink" title="[103] Towards Reliable Retrieval in RAG Systems for Large Legal Datasets cs.CL | cs.IR | I.2.7; H.3.3; K.5.0PDF"></a>[103] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06999">Towards Reliable Retrieval in RAG Systems for Large Legal Datasets</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.IR | I.2.7; H.3.3; K.5.0</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06999" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Markus Reuter, Tobias Lingenberg, Rūta Liepiņa, Francesca Lagioia, Marco Lippi</span></p>
<p><strong>TL;DR:</strong> 本文提出了一种名为Summary-Augmented Chunking (SAC)的方法，通过在文本块中注入文档级合成摘要，解决法律领域中大型数据集检索的可靠性问题，显著减少了文档级检索失配（DRM），并提升了检索精度和召回率。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在法律领域，大型数据库中结构相似的文档往往导致检索系统失效，尤其是文档级检索失配（DRM）问题严重影响了检索增强生成（RAG）系统的可靠性。</p>
<p><strong>Result:</strong> SAC显著减少了DRM现象，同时提高了文本级检索的精度和召回率。实验证明通用摘要策略在法律任务中表现更优。</p>
<p><strong>Insight:</strong> 在法律数据集的应用中，全局上下文（如文档级摘要）比特定领域知识更能有效提升检索系统的可靠性；SAC方法具有实用性和易集成性。</p>
<p><strong>Abstract:</strong> Retrieval-Augmented Generation (RAG) is a promising approach to mitigate hallucinations in Large Language Models (LLMs) for legal applications, but its reliability is critically dependent on the accuracy of the retrieval step. This is particularly challenging in the legal domain, where large databases of structurally similar documents often cause retrieval systems to fail. In this paper, we address this challenge by first identifying and quantifying a critical failure mode we term Document-Level Retrieval Mismatch (DRM), where the retriever selects information from entirely incorrect source documents. To mitigate DRM, we investigate a simple and computationally efficient technique which we refer to as Summary-Augmented Chunking (SAC). This method enhances each text chunk with a document-level synthetic summary, thereby injecting crucial global context that would otherwise be lost during a standard chunking process. Our experiments on a diverse set of legal information retrieval tasks show that SAC greatly reduces DRM and, consequently, also improves text-level retrieval precision and recall. Interestingly, we find that a generic summarization strategy outperforms an approach that incorporates legal expert domain knowledge to target specific legal elements. Our work provides evidence that this practical, scalable, and easily integrable technique enhances the reliability of RAG systems when applied to large-scale legal document datasets.</p>
  </div>
</details>

<hr>
<h3 id="104-Pragyaan-Designing-and-Curating-High-Quality-Cultural-Post-Training-Datasets-for-Indian-Languages-cs-CL-cs-AIPDF"><a href="#104-Pragyaan-Designing-and-Curating-High-Quality-Cultural-Post-Training-Datasets-for-Indian-Languages-cs-CL-cs-AIPDF" class="headerlink" title="[104] Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages cs.CL | cs.AIPDF"></a>[104] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07000">Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07000" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Neel Prabhanjan Rachamalla, Aravind Konakalla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri</span></p>
<p><strong>TL;DR:</strong> 论文介绍了一种高质量的文化后训练数据集Pragyaan，专门针对印度语言，通过人机协同流程结合翻译与合成扩展，解决了现有数据集在多样性和文化背景上的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有开源数据集在印度语言上的覆盖不足，缺乏文化相关性和任务多样性，限制了大规模语言模型（LLMs）的效果。</p>
<p><strong>Result:</strong> 生成了22.5K和100K规模的印度语言数据集，覆盖13大类56小类任务。</p>
<p><strong>Insight:</strong> 文化背景和任务多样性对LLMs的多语言效果至关重要，人机协同方法能显著提升数据质量。</p>
<p><strong>Abstract:</strong> The effectiveness of Large Language Models (LLMs) depends heavily on the availability of high-quality post-training data, particularly instruction-tuning and preference-based examples. Existing open-source datasets, however, often lack multilingual coverage, cultural grounding, and suffer from task diversity gaps that are especially pronounced for Indian languages. We introduce a human-in-the-loop pipeline that combines translations with synthetic expansion to produce reliable and diverse Indic post-training data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56 sub-categories, leveraging 57 diverse datasets. Our dataset protocol incorporates several often-overlooked dimensions and emphasize task diversity, multi-turn dialogue, instruction fidelity, safety alignment, and preservation of cultural nuance, providing a foundation for more inclusive and effective multilingual LLMs.</p>
  </div>
</details>

<hr>
<h3 id="105-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling-cs-CL-cs-AI-cs-LGPDF"><a href="#105-Native-Hybrid-Attention-for-Efficient-Sequence-Modeling-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[105] Native Hybrid Attention for Efficient Sequence Modeling cs.CL | cs.AI | cs.LGPDF"></a>[105] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07019">Native Hybrid Attention for Efficient Sequence Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07019" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng</span></p>
<p><strong>TL;DR:</strong> 论文提出了Native Hybrid Attention（NHA），一种结合线性注意力和全注意力的混合架构，通过统一的层设计实现高效序列建模，同时保持长上下文记忆能力。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> Transformer虽然擅长序列建模，但其二次复杂度问题限制了效率；线性注意力提高了效率，但牺牲了长上下文的召回精度。NHA旨在解决这一矛盾。</p>
<p><strong>Result:</strong> 实验表明，NHA在召回密集和常识推理任务上优于Transformer和其他混合基线，且预训练的LLM与NHA结合可实现高效且高精度表现。</p>
<p><strong>Insight:</strong> NHA展示了混合注意力架构在平衡效率和精度上的潜力，为长序列建模提供了一种灵活的设计思路。</p>
<p><strong>Abstract:</strong> Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra &amp; inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at <a target="_blank" rel="noopener" href="https://github.com/JusenD/NHA">https://github.com/JusenD/NHA</a>.</p>
  </div>
</details>

<hr>
<h3 id="106-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models-cs-CL-cs-AI-I-2-7PDF"><a href="#106-Search-R3-Unifying-Reasoning-and-Embedding-Generation-in-Large-Language-Models-cs-CL-cs-AI-I-2-7PDF" class="headerlink" title="[106] Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models cs.CL | cs.AI | I.2.7PDF"></a>[106] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07048">Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | I.2.7</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07048" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yuntao Gui, James Cheng</span></p>
<p><strong>TL;DR:</strong> Search-R3提出了一种新颖框架，通过结合大型语言模型（LLM）的推理能力与嵌入生成，增强了其在检索任务中的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 尽管LLM在自然语言理解方面表现卓越，但在检索任务中尚未充分发挥潜力，Search-R3旨在填补这一空白。</p>
<p><strong>Result:</strong> 在多个基准测试中，Search-R3显著优于现有方法，证明了其在复杂知识密集型任务中的有效性。</p>
<p><strong>Insight:</strong> 推理与嵌入生成的结合是提升LLM检索能力的关键，这种一体化方法为未来研究提供了新方向。</p>
<p><strong>Abstract:</strong> Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs’ chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model’s ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: <a target="_blank" rel="noopener" href="https://github.com/ytgui/Search-R3">https://github.com/ytgui/Search-R3</a></p>
  </div>
</details>

<hr>
<h3 id="107-Does-Local-News-Stay-Local-Online-Content-Shifts-in-Sinclair-Acquired-Stations-cs-CLPDF"><a href="#107-Does-Local-News-Stay-Local-Online-Content-Shifts-in-Sinclair-Acquired-Stations-cs-CLPDF" class="headerlink" title="[107] Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations cs.CLPDF"></a>[107] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07060">Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07060" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Miriam Wanner, Sophia Hager, Anjalie Field</span></p>
<p><strong>TL;DR:</strong> 该研究探讨了辛克莱广播集团收购地方新闻台后对其内容的影响，发现这些台在收购后更多地报道全国性新闻，减少了地方性话题，同时增加了对两极化全国话题的报道。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 地方新闻台通常被视为非政治化信息的可靠来源，尤其是涉及居民关注的本地话题。辛克莱集团的收购行为引发了对其内容变化的关注。</p>
<p><strong>Result:</strong> 结果显示，收购后的地方新闻台更多地报道全国性新闻，减少了地方话题的报道，且内容更具两极化倾向。</p>
<p><strong>Insight:</strong> 媒体所有权的集中可能导致地方新闻失去其地方性特点，转而迎合全国性议题，影响公众的信息获取和观点形成。</p>
<p><strong>Abstract:</strong> Local news stations are often considered to be reliable sources of non-politicized information, particularly local concerns that residents care about. Because these stations are trusted news sources, viewers are particularly susceptible to the information they report. The Sinclair Broadcast group is a broadcasting company that has acquired many local news stations in the last decade. We investigate the effects of local news stations being acquired by Sinclair: how does coverage change? We use computational methods to investigate changes in internet content put out by local news stations before and after being acquired by Sinclair and in comparison to national news outlets. We find that there is clear evidence that local news stations report more frequently on national news at the expense of local topics, and that their coverage of polarizing national topics increases.</p>
  </div>
</details>

<hr>
<h3 id="108-Revisiting-Metric-Reliability-for-Fine-grained-Evaluation-of-Machine-Translation-and-Summarization-in-Indian-Languages-cs-CLPDF"><a href="#108-Revisiting-Metric-Reliability-for-Fine-grained-Evaluation-of-Machine-Translation-and-Summarization-in-Indian-Languages-cs-CLPDF" class="headerlink" title="[108] Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages cs.CLPDF"></a>[108] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07061">Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07061" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Amir Hossein Yari, Kalmit Kulkarni, Ahmad Raza Khan, Fajri Koto</span></p>
<p><strong>TL;DR:</strong> 论文提出了ITEM基准，用于系统评估26种自动指标在六种印度语言中与人类判断的一致性，揭示了基于LLM的评估器表现最佳、离群值影响显著、翻译和摘要任务中指标侧重点不同等发现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有自动指标主要针对英语等高资源语言，缺乏对印度语言的验证，限制了评估的普适性和可靠性。</p>
<p><strong>Result:</strong> 发现基于LLM的评估器表现最佳，离群值影响显著，翻译任务中指标更关注流畅性，而摘要任务更关注内容保真度。</p>
<p><strong>Insight:</strong> 需要为印度语言设计更鲁棒的评估指标，任务类型（翻译或摘要）会影响指标的适用性。</p>
<p><strong>Abstract:</strong> While automatic metrics drive progress in Machine Translation (MT) and Text Summarization (TS), existing metrics have been developed and validated almost exclusively for English and other high-resource languages. This narrow focus leaves Indian languages, spoken by over 1.5 billion people, largely overlooked, casting doubt on the universality of current evaluation practices. To address this gap, we introduce ITEM, a large-scale benchmark that systematically evaluates the alignment of 26 automatic metrics with human judgments across six major Indian languages, enriched with fine-grained annotations. Our extensive evaluation, covering agreement with human judgments, sensitivity to outliers, language-specific reliability, inter-metric correlations, and resilience to controlled perturbations, reveals four central findings: (1) LLM-based evaluators show the strongest alignment with human judgments at both segment and system levels; (2) outliers exert a significant impact on metric-human agreement; (3) in TS, metrics are more effective at capturing content fidelity, whereas in MT, they better reflect fluency; and (4) metrics differ in their robustness and sensitivity when subjected to diverse perturbations. Collectively, these findings offer critical guidance for advancing metric design and evaluation in Indian languages.</p>
  </div>
</details>

<hr>
<h3 id="109-TALENT-Table-VQA-via-Augmented-Language-Enhanced-Natural-text-Transcription-cs-CLPDF"><a href="#109-TALENT-Table-VQA-via-Augmented-Language-Enhanced-Natural-text-Transcription-cs-CLPDF" class="headerlink" title="[109] TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription cs.CLPDF"></a>[109] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07098">TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07098" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guo Yutong, Wanying Wang, Yue Wu, Zichen Miao, Haoyu Wang</span></p>
<p><strong>TL;DR:</strong> TALENT提出了一种轻量化框架，通过结合OCR文本和自然语言叙述来解决Table VQA问题，避免了直接使用计算昂贵的大型视觉语言模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的Table VQA方法要么依赖大型视觉语言模型，计算成本高；要么使用结构化输出（如Markdown表格）引入错误。TALENT旨在通过轻量化解决方案解决这些问题。</p>
<p><strong>Result:</strong> 实验表明，TALENT在公共数据集和ReTabVQA上能以更低计算成本匹配或超越大型视觉语言模型的性能。</p>
<p><strong>Insight:</strong> 通过分割感知和推理任务，轻量化组合可以在Table VQA中高效工作；自然语言叙述可能更适合LLM推理。</p>
<p><strong>Abstract:</strong> Table Visual Question Answering (Table VQA) is typically addressed by large vision-language models (VLMs). While such models can answer directly from images, they often miss fine-grained details unless scaled to very large sizes, which are computationally prohibitive, especially for mobile deployment. A lighter alternative is to have a small VLM perform OCR and then use a large language model (LLM) to reason over structured outputs such as Markdown tables. However, these representations are not naturally optimized for LLMs and still introduce substantial errors. We propose TALENT (Table VQA via Augmented Language-Enhanced Natural-text Transcription), a lightweight framework that leverages dual representations of tables. TALENT prompts a small VLM to produce both OCR text and natural language narration, then combines them with the question for reasoning by an LLM. This reframes Table VQA as an LLM-centric multimodal reasoning task, where the VLM serves as a perception-narration module rather than a monolithic solver. Additionally, we construct ReTabVQA, a more challenging Table VQA dataset requiring multi-step quantitative reasoning over table images. Experiments show that TALENT enables a small VLM-LLM combination to match or surpass a single large VLM at significantly lower computational cost on both public datasets and ReTabVQA.</p>
  </div>
</details>

<hr>
<h3 id="110-Reasoning-for-Hierarchical-Text-Classification-The-Case-of-Patents-cs-CLPDF"><a href="#110-Reasoning-for-Hierarchical-Text-Classification-The-Case-of-Patents-cs-CLPDF" class="headerlink" title="[110] Reasoning for Hierarchical Text Classification: The Case of Patents cs.CLPDF"></a>[110] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07167">Reasoning for Hierarchical Text Classification: The Case of Patents</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07167" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Lekang Jiang, Wenjun Sun, Stephan Goetz</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种名为RHC的层次文本分类（HTC）框架，将HTC重新定义为逐步推理任务，利用大语言模型（LLMs）通过两阶段训练实现更高的分类性能和可解释性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 专利主题分类是HTC中最具挑战性的场景之一，传统方法仅输出平面标签集，缺乏预测背后的逻辑解释。</p>
<p><strong>Result:</strong> RHC在专利分类及其他HTC基准测试中优于基线模型，准确率和宏观F1提升约3%，并生成自然语言解释。</p>
<p><strong>Insight:</strong> RHC通过逐步推理和语言模型的两阶段训练，为HTC任务提供了兼具高性能和可解释性的解决方案。</p>
<p><strong>Abstract:</strong> Hierarchical text classification (HTC) assigns documents to multiple levels of a pre-defined taxonomy. Automated patent subject classification represents one of the hardest HTC scenarios because of domain knowledge difficulty and a huge number of labels. Prior approaches only output a flat label set, which offers little insight into the reason behind predictions. Therefore, we propose Reasoning for Hierarchical Classification (RHC), a novel framework that reformulates HTC as a step-by-step reasoning task to sequentially deduce hierarchical labels. RHC trains large language models (LLMs) in two stages: a cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning format and a reinforcement learning (RL) stage to enhance multi-step reasoning ability. RHC demonstrates four advantages in our experiments. (1) Effectiveness: RHC surpasses previous baselines and outperforms the supervised fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2) Explainability: RHC produces natural-language justifications before prediction to facilitate human inspection. (3) Scalability: RHC scales favorably with model size with larger gains compared to standard fine-tuning. (4) Applicability: Beyond patents, we further demonstrate that RHC achieves state-of-the-art performance on other widely used HTC benchmarks, which highlights its broad applicability.</p>
  </div>
</details>

<hr>
<h3 id="111-More-Data-or-Better-Data-A-Critical-Analysis-of-Data-Selection-and-Synthesis-for-Mathematical-Reasoning-cs-CLPDF"><a href="#111-More-Data-or-Better-Data-A-Critical-Analysis-of-Data-Selection-and-Synthesis-for-Mathematical-Reasoning-cs-CLPDF" class="headerlink" title="[111] More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning cs.CLPDF"></a>[111] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07169">More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07169" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yike Zhao, Simin Guo, Ziqing Yang, Shifan Han, Dahua Lin</span></p>
<p><strong>TL;DR:</strong> 该论文分析了开源数学推理数据集和数据合成技术的实用性，强调数据质量的提升（如更可解释的格式或从强模型中提炼）通常优于单纯增加数据量。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型（LLM）的推理能力在许多下游任务中至关重要，但高度依赖训练数据的质量。然而，现有数据构建方法在实际应用中的效果尚未充分探索。</p>
<p><strong>Result:</strong> 研究发现，结构化数据格式或从更强模型中提炼的知识，比单纯增加数据量更有效。这一结果为数据集成提供了可操作的指导。</p>
<p><strong>Insight:</strong> 研究强调了在现实世界推理任务中，平衡‘更多数据’与‘更好数据’的重要性，为未来的研究提供了方向。</p>
<p><strong>Abstract:</strong> The reasoning capabilities of Large Language Models (LLMs) play a critical role in many downstream tasks, yet depend strongly on the quality of training data. Despite various proposed data construction methods, their practical utility in real-world pipelines remains underexplored. In this work, we conduct a comprehensive analysis of open-source datasets and data synthesis techniques for mathematical reasoning, evaluating them under a unified pipeline designed to mirror training and deployment scenarios. We further distill effective data selection strategies and identify practical methods suitable for industrial applications. Our findings highlight that structuring data in more interpretable formats, or distilling from stronger models often outweighs simply scaling up data volume. This study provides actionable guidance for integrating training data to enhance LLM capabilities, supporting both cost-effective data curation and scalable model enhancement. We hope this work will inspire further research on how to balance “more data” versus “better data” for real-world reasoning tasks.</p>
  </div>
</details>

<hr>
<h3 id="112-NurseLLM-The-First-Specialized-Language-Model-for-Nursing-cs-CL-cs-LGPDF"><a href="#112-NurseLLM-The-First-Specialized-Language-Model-for-Nursing-cs-CL-cs-LGPDF" class="headerlink" title="[112] NurseLLM: The First Specialized Language Model for Nursing cs.CL | cs.LGPDF"></a>[112] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07173">NurseLLM: The First Specialized Language Model for Nursing</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07173" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Md Tawkat Islam Khondaker, Julia Harrington, Shady Shehata</span></p>
<p><strong>TL;DR:</strong> NurseLLM是首个专为护理领域设计的语言模型，专注于多项选择题任务，通过多阶段数据生成流程构建大规模护理MCQ数据集，并在多个基准测试中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前大型语言模型在医疗系统中应用广泛，但在护理等专业领域的潜力尚未充分挖掘，因此开发专为护理设计的语言模型具有重要意义。</p>
<p><strong>Result:</strong> NurseLLM在多个基准测试中表现优于同类通用和医学专用语言模型。</p>
<p><strong>Insight:</strong> 专业领域的专用语言模型在性能和实用性上优于通用模型，逻辑推理和多代理协作系统在护理领域具有潜在应用前景。</p>
<p><strong>Abstract:</strong> Recent advancements in large language models (LLMs) have significantly transformed medical systems. However, their potential within specialized domains such as nursing remains largely underexplored. In this work, we introduce NurseLLM, the first nursing-specialized LLM tailored for multiple choice question-answering (MCQ) tasks. We develop a multi-stage data generation pipeline to build the first large scale nursing MCQ dataset to train LLMs on a broad spectrum of nursing topics. We further introduce multiple nursing benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of comparable size on different benchmarks, underscoring the importance of a specialized LLM for the nursing domain. Finally, we explore the role of reasoning and multi-agent collaboration systems in nursing, highlighting their promise for future research and applications.</p>
  </div>
</details>

<hr>
<h3 id="113-Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping-cs-CLPDF"><a href="#113-Customer-R1-Personalized-Simulation-of-Human-Behaviors-via-RL-based-LLM-Agent-in-Online-Shopping-cs-CLPDF" class="headerlink" title="[113] Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping cs.CLPDF"></a>[113] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07230">Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07230" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ziyi Wang, Yuxuan Lu, Yimeng Zhang, Jing Huang, Dakuo Wang</span></p>
<p><strong>TL;DR:</strong> Customer-R1是一种基于强化学习的LLM代理方法，旨在模拟在线购物中个性化的用户行为。该方法通过显式人物设定优化下一步行为和动作生成，显著优于现有基线。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有方法主要学习群体层面的策略，缺乏对用户个性的建模，导致模拟行为过于通用。</p>
<p><strong>Result:</strong> 在OPeRA数据集上，Customer-R1在下一步动作预测任务中显著优于提示和监督微调基线，且行为分布更接近真实用户。</p>
<p><strong>Insight:</strong> 显式的人物设定和强化学习的结合能够有效提升个性化行为模拟的真实性和准确性。</p>
<p><strong>Abstract:</strong> Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user’s persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users’ action distribution, indicating higher fidelity in personalized behavior simulation.</p>
  </div>
</details>

<hr>
<h3 id="114-Benchmarking-LLM-Causal-Reasoning-with-Scientifically-Validated-Relationships-cs-CL-cs-AIPDF"><a href="#114-Benchmarking-LLM-Causal-Reasoning-with-Scientifically-Validated-Relationships-cs-CL-cs-AIPDF" class="headerlink" title="[114] Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships cs.CL | cs.AIPDF"></a>[114] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07231">Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07231" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Donggyu Lee, Sungwon Park, Yerin Hwang, Hyunwoo Oh, Hyoshin Kim</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一个基于科学验证因果关系的基准测试，用于评估大型语言模型（LLMs）的因果推理能力。通过从顶级经济学和金融学期刊中提取的因果关系数据，构建了一个包含40,379项任务的多样化测试集。实验发现，现有LLMs在因果推理任务中表现不佳，最优模型精度仅为57.6%。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的因果推理基准测试多依赖合成数据且覆盖领域狭窄，无法有效评估LLMs的真实能力。</p>
<p><strong>Result:</strong> LLMs在因果推理任务中表现不佳，最优模型精度仅为57.6%，且模型规模与性能无显著相关性。</p>
<p><strong>Insight:</strong> 当前LLMs在因果推理能力上存在显著不足，凸显了在高风险应用中可靠因果推理的需求与现有技术之间的差距。</p>
<p><strong>Abstract:</strong> Causal reasoning is fundamental for Large Language Models (LLMs) to understand genuine cause-and-effect relationships beyond pattern matching. Existing benchmarks suffer from critical limitations such as reliance on synthetic data and narrow domain coverage. We introduce a novel benchmark constructed from casually identified relationships extracted from top-tier economics and finance journals, drawing on rigorous methodologies including instrumental variables, difference-in-differences, and regression discontinuity designs. Our benchmark comprises 40,379 evaluation items covering five task types across domains such as health, environment, technology, law, and culture. Experimental results on eight state-of-the-art LLMs reveal substantial limitations, with the best model achieving only 57.6% accuracy. Moreover, model scale does not consistently translate to superior performance, and even advanced reasoning models struggle with fundamental causal relationship identification. These findings underscore a critical gap between current LLM capabilities and demands of reliable causal reasoning in high-stakes applications.</p>
  </div>
</details>

<hr>
<h3 id="115-LAD-RAG-Layout-aware-Dynamic-RAG-for-Visually-Rich-Document-Understanding-cs-CLPDF"><a href="#115-LAD-RAG-Layout-aware-Dynamic-RAG-for-Visually-Rich-Document-Understanding-cs-CLPDF" class="headerlink" title="[115] LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding cs.CLPDF"></a>[115] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07233">LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07233" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhivar Sourati, Zheng Wang, Marianne Menglin Liu, Yazhe Hu, Mengqing Guo</span></p>
<p><strong>TL;DR:</strong> LAD-RAG是一个创新的布局感知动态RAG框架，用于视觉丰富文档（VRD）的理解，通过结合符号化的文档图和动态检索机制，显著提升了多页文档推理任务的检索质量和问答准确性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 传统的RAG方法在视觉丰富文档问答中存在局限性，因为它们忽略了文档的结构和跨页依赖关系，且检索时使用固定数量的页面，导致证据检索不完整和答案质量下降。</p>
<p><strong>Result:</strong> 在多个基准测试中，LAD-RAG实现了超过90%的完美召回率，且在相同噪声水平下比基线方法高出20%的召回率，显著提升了问答准确性。</p>
<p><strong>Insight:</strong> LAD-RAG的核心创新在于将符号化结构信息与神经表示结合，并通过动态检索机制解决传统RAG在多页文档推理中的局限性。</p>
<p><strong>Abstract:</strong> Question answering over visually rich documents (VRDs) requires reasoning not only over isolated content but also over documents’ structural organization and cross-page dependencies. However, conventional retrieval-augmented generation (RAG) methods encode content in isolated chunks during ingestion, losing structural and cross-page dependencies, and retrieve a fixed number of pages at inference, regardless of the specific demands of the question or context. This often results in incomplete evidence retrieval and degraded answer quality for multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs a symbolic document graph that captures layout structure and cross-page dependencies, adding it alongside standard neural embeddings to yield a more holistic representation of the document. During inference, an LLM agent dynamically interacts with the neural and symbolic indices to adaptively retrieve the necessary evidence based on the query. Experiments on MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG improves retrieval, achieving over 90% perfect recall on average without any top-k tuning, and outperforming baseline retrievers by up to 20% in recall at comparable noise levels, yielding higher QA accuracy with minimal latency.</p>
  </div>
</details>

<hr>
<h3 id="116-Red-Bandit-Test-Time-Adaptation-for-LLM-Red-Teaming-via-Bandit-Guided-LoRA-Experts-cs-CLPDF"><a href="#116-Red-Bandit-Test-Time-Adaptation-for-LLM-Red-Teaming-via-Bandit-Guided-LoRA-Experts-cs-CLPDF" class="headerlink" title="[116] Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts cs.CLPDF"></a>[116] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07239">Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07239" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Christos Ziakas, Nicholas Loo, Nishita Jain, Alessandra Russo</span></p>
<p><strong>TL;DR:</strong> Red-Bandit是一个针对LLMs的红队测试框架，通过在线自适应机制动态选择攻击风格，以高效发现和利用目标模型的漏洞。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的LLMs红队测试方法缺乏在推理阶段高效适应特定模型漏洞的能力，Red-Bandit旨在填补这一空白。</p>
<p><strong>Result:</strong> 在AdvBench上实现最高攻击成功率（ASR@10），同时生成更低困惑度的提示。此外，老虎机策略可用于诊断模型特有的漏洞。</p>
<p><strong>Insight:</strong> Red-Bandit不仅提升了红队测试的效率和效果，还为模型安全性诊断提供了新工具。</p>
<p><strong>Abstract:</strong> Automated red-teaming has emerged as a scalable approach for auditing Large Language Models (LLMs) prior to deployment, yet existing approaches lack mechanisms to efficiently adapt to model-specific vulnerabilities at inference. We introduce Red-Bandit, a red-teaming framework that adapts online to identify and exploit model failure modes under distinct attack styles (e.g., manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA experts, each specialized for a particular attack style, using reinforcement learning that rewards the generation of unsafe prompts via a rule-based safety model. At inference, a multi-armed bandit policy dynamically selects among these attack-style experts based on the target model’s response safety, balancing exploration and exploitation. Red-Bandit achieves state-of-the-art results on AdvBench under sufficient exploration (ASR@10), while producing more human-readable prompts (lower perplexity). Moreover, Red-Bandit’s bandit policy serves as a diagnostic tool for uncovering model-specific vulnerabilities by indicating which attack styles most effectively elicit unsafe behaviors.</p>
  </div>
</details>

<hr>
<h3 id="117-Hybrid-Reinforcement-When-Reward-Is-Sparse-It’s-Better-to-Be-Dense-cs-CL-cs-LGPDF"><a href="#117-Hybrid-Reinforcement-When-Reward-Is-Sparse-It’s-Better-to-Be-Dense-cs-CL-cs-LGPDF" class="headerlink" title="[117] Hybrid Reinforcement: When Reward Is Sparse, It’s Better to Be Dense cs.CL | cs.LGPDF"></a>[117] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07242">Hybrid Reinforcement: When Reward Is Sparse, It’s Better to Be Dense</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07242" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Leitian Tao, Ilia Kulikov, Swarnadeep Saha, Tianlu Wang, Jing Xu</span></p>
<p><strong>TL;DR:</strong> HERO（混合集成奖励优化）是一个结合验证器和奖励模型信号的强化学习框架，通过分层归一化和方差感知加权，提高了语言模型在数学推理任务中的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的验证器反馈（0-1信号）过于简单，无法捕捉部分正确或替代答案的细微之处，限制了学习效果。奖励模型提供的连续反馈可以补足这一缺陷。</p>
<p><strong>Result:</strong> 在多样化的数学推理任务中，HERO的表现优于仅使用奖励模型或验证器的基线方法，尤其在难以验证的任务上表现突出。</p>
<p><strong>Insight:</strong> 混合奖励设计能够保留验证器的稳定性，同时利用奖励模型的细微反馈，提升模型的推理能力。</p>
<p><strong>Abstract:</strong> Post-training for reasoning of large language models (LLMs) increasingly relies on verifiable rewards: deterministic checkers that provide 0-1 correctness signals. While reliable, such binary feedback is brittle–many tasks admit partially correct or alternative answers that verifiers under-credit, and the resulting all-or-nothing supervision limits learning. Reward models offer richer, continuous feedback, which can serve as a complementary supervisory signal to verifiers. We introduce HERO (Hybrid Ensemble Reward Optimization), a reinforcement learning framework that integrates verifier signals with reward-model scores in a structured way. HERO employs stratified normalization to bound reward-model scores within verifier-defined groups, preserving correctness while refining quality distinctions, and variance-aware weighting to emphasize challenging prompts where dense signals matter most. Across diverse mathematical reasoning benchmarks, HERO consistently outperforms RM-only and verifier-only baselines, with strong gains on both verifiable and hard-to-verify tasks. Our results show that hybrid reward design retains the stability of verifiers while leveraging the nuance of reward models to advance reasoning.</p>
  </div>
</details>

<hr>
<h3 id="118-LeMAJ-Legal-LLM-as-a-Judge-Bridging-Legal-Reasoning-and-LLM-Evaluation-cs-CL-cs-AIPDF"><a href="#118-LeMAJ-Legal-LLM-as-a-Judge-Bridging-Legal-Reasoning-and-LLM-Evaluation-cs-CL-cs-AIPDF" class="headerlink" title="[118] LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation cs.CL | cs.AIPDF"></a>[118] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07243">LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07243" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Joseph Enguehard, Morgane Van Ermengem, Kate Atkinson, Sujeong Cha, Arijit Ghosh Chowdhury</span></p>
<p><strong>TL;DR:</strong> 论文提出了LeMAJ方法，通过将法律领域的LLM输出拆分为‘Legal Data Points’（LDPs），提出了一种无参考的新型评估方法，并在法律问答任务中优于基线方法，同时与人类专家的评估更一致。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 法律领域的LLM评估存在独特挑战，现有方法依赖参考数据或标准化评估，但这对法律应用有局限性，且可靠性不足。</p>
<p><strong>Result:</strong> 实验表明，该方法优于多个基线方法，且在人类专家评估一致性上表现更优。</p>
<p><strong>Insight:</strong> 法律领域的LLM评估需要更贴近专业律师的逻辑，LDPs的实现为法律问答评估提供了新思路。</p>
<p><strong>Abstract:</strong> Evaluating large language model (LLM) outputs in the legal domain presents unique challenges due to the complex and nuanced nature of legal analysis. Current evaluation approaches either depend on reference data, which is costly to produce, or use standardized assessment methods, both of which have significant limitations for legal applications.   Although LLM-as-a-Judge has emerged as a promising evaluation technique, its reliability and effectiveness in legal contexts depend heavily on evaluation processes unique to the legal industry and how trustworthy the evaluation appears to the human legal expert. This is where existing evaluation methods currently fail and exhibit considerable variability.   This paper aims to close the gap: a) we break down lengthy responses into ‘Legal Data Points’ (LDPs), self-contained units of information, and introduce a novel, reference-free evaluation methodology that reflects how lawyers evaluate legal answers; b) we demonstrate that our method outperforms a variety of baselines on both our proprietary dataset and an open-source dataset (LegalBench); c) we show how our method correlates more closely with human expert evaluations and helps improve inter-annotator agreement; and finally d) we open source our Legal Data Points for a subset of LegalBench used in our experiments, allowing the research community to replicate our results and advance research in this vital area of LLM evaluation on legal question-answering.</p>
  </div>
</details>

<hr>
<h3 id="119-Don’t-Adapt-Small-Language-Models-for-Tools-Adapt-Tool-Schemas-to-the-Models-cs-CLPDF"><a href="#119-Don’t-Adapt-Small-Language-Models-for-Tools-Adapt-Tool-Schemas-to-the-Models-cs-CLPDF" class="headerlink" title="[119] Don’t Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models cs.CLPDF"></a>[119] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07248">Don’t Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07248" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jonggeun Lee, Woojung Song, Jongwook Han, Haesung Pyun, Yohan Jo</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种无需训练的方法PA-Tool，通过调整工具模式的命名以对齐小型语言模型的预训练知识，显著提升了工具使用任务的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 小型语言模型（SLMs）在工具增强的AI系统中具有计算优势，但在工具使用任务中存在模式不对齐的问题（如虚构的工具名称）。直接让模型适应任意模式效果不佳，因此作者提出调整模式以匹配模型的预训练知识。</p>
<p><strong>Result:</strong> 在MetaTool和RoTBench上的实验显示，性能提升了17%，模式不对齐错误减少了80%，使小型模型接近SOTA性能。</p>
<p><strong>Insight:</strong> 通过对模式（schema）而非模型进行调整，可以高效释放资源受限模型的工具使用潜力，避免重新训练的成本。</p>
<p><strong>Abstract:</strong> Small language models (SLMs) offer significant computational advantages for tool-augmented AI systems, yet they struggle with tool-use tasks, particularly in selecting appropriate tools and identifying correct parameters. A common failure mode is schema misalignment: models hallucinate plausible but non-existent tool names that reflect naming conventions internalized during pretraining but absent from the provided tool schema. Rather than forcing models to adapt to arbitrary schemas, we propose adapting schemas to align with models’ pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool Schema Generation), a training-free method that leverages peakedness-a signal from contamination detection indicating pretraining familiarity-to automatically rename tool components. By generating multiple candidates and selecting those with highest output concentration across samples, PA-Tool identifies pretrain-aligned naming patterns. Experiments on MetaTool and RoTBench show improvements of up to 17% points, with schema misalignment errors reduced by 80%. PA-Tool enables small models to approach state-of-the-art performance while maintaining computational efficiency for adaptation to new tools without retraining. Our work demonstrates that schema-level interventions can unlock the tool-use potential of resource-efficient models by adapting schemas to models rather than models to schemas.</p>
  </div>
</details>

<hr>
<h3 id="120-Online-Rubrics-Elicitation-from-Pairwise-Comparisons-cs-CL-cs-AI-cs-LGPDF"><a href="#120-Online-Rubrics-Elicitation-from-Pairwise-Comparisons-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[120] Online Rubrics Elicitation from Pairwise Comparisons cs.CL | cs.AI | cs.LGPDF"></a>[120] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07284">Online Rubrics Elicitation from Pairwise Comparisons</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07284" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">MohammadHossein Rezaei, Robert Vacareanu, Zihao Wang, Clinton Wang, Yunzhong He</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种动态生成评测标准的在线方法（OnlineRubrics），通过动态对比当前和参考策略的响应，避免了静态标准在训练中的局限性，并提升了大型语言模型的表现。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 静态评测标准在训练过程中容易受到奖励攻击行为的影响，且无法捕捉训练中出现的需求变化。为解决这一问题，论文提出了一种动态生成评测标准的方法。</p>
<p><strong>Result:</strong> 在AlpacaEval、GPQA、ArenaHard等数据集上，相比静态标准，该方法带来了最高8%的性能提升。</p>
<p><strong>Insight:</strong> 动态标准能够更好地适应训练需求，提升模型表现；评测标准中的透明性和实用性等主题是关键因素。</p>
<p><strong>Abstract:</strong> Rubrics provide a flexible way to train LLMs on open-ended long-form answers where verifiable rewards are not applicable and human preferences provide coarse signals. Prior work shows that reinforcement learning with rubric-based rewards leads to consistent gains in LLM post-training. Most existing approaches rely on rubrics that remain static over the course of training. Such static rubrics, however, are vulnerable to reward-hacking type behaviors and fail to capture emergent desiderata that arise during training. We introduce Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates evaluation criteria in an online manner through pairwise comparisons of responses from current and reference policies. This online process enables continuous identification and mitigation of errors as training proceeds. Empirically, this approach yields consistent improvements of up to 8% over training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as well as the validation sets of expert questions and rubrics. We qualitatively analyze the elicited criteria and identify prominent themes such as transparency, practicality, organization, and reasoning.</p>
  </div>
</details>

<hr>
<h3 id="121-On-the-Convergence-of-Moral-Self-Correction-in-Large-Language-Models-cs-CL-cs-LGPDF"><a href="#121-On-the-Convergence-of-Moral-Self-Correction-in-Large-Language-Models-cs-CL-cs-LGPDF" class="headerlink" title="[121] On the Convergence of Moral Self-Correction in Large Language Models cs.CL | cs.LGPDF"></a>[121] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07290">On the Convergence of Moral Self-Correction in Large Language Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07290" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Guangliang Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, Xitong Zhang</span></p>
<p><strong>TL;DR:</strong> 大型语言模型（LLM）能够通过自我修正（self-correction）改善其响应，特别是道德自我修正（moral self-correction）表现出性能收敛的特性。本文揭示了多轮互动中这种收敛行为的机制：持续的自我修正指令激活了道德概念，从而减少模型不确定性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 探讨LLM在缺乏具体问题细节的情况下，如何通过内部知识自我修正（intrinsic self-correction）来提高响应质量，尤其是道德领域的自我修正机制。</p>
<p><strong>Result:</strong> 道德自我修正表现出性能收敛的特性，这是因为持续的指令输入稳定了激活的道德概念，减少了不确定性。</p>
<p><strong>Insight:</strong> 持续的自我修正指令能够有效引导LLM在道德领域的行为趋于稳定，这一机制可以扩展到其他领域的自我修正研究。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only a general and abstract goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. Focusing on moral self-correction in LLMs, we reveal a key characteristic of intrinsic self-correction: performance convergence through multi-round interactions; and provide a mechanistic analysis of this convergence behavior. Based on our experimental results and analysis, we uncover the underlying mechanism of convergence: consistently injected self-correction instructions activate moral concepts that reduce model uncertainty, leading to converged performance as the activated moral concepts stabilize over successive rounds. This paper demonstrates the strong potential of moral self-correction by showing that it exhibits a desirable property of converged performance.</p>
  </div>
</details>

<hr>
<h3 id="122-Think-Natively-Unlocking-Multilingual-Reasoning-with-Consistency-Enhanced-Reinforcement-Learning-cs-CLPDF"><a href="#122-Think-Natively-Unlocking-Multilingual-Reasoning-with-Consistency-Enhanced-Reinforcement-Learning-cs-CLPDF" class="headerlink" title="[122] Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning cs.CLPDF"></a>[122] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07300">Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07300" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Kaiyu Huang</span></p>
<p><strong>TL;DR:</strong> 本文提出了M-Thinker模型，通过GRPO算法结合语言一致性奖励和跨语言思维对齐奖励，解决了大型推理模型在多语言任务中的输入输出不一致和推理能力不足的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的大型推理模型在处理非英语语言时存在输入输出语言不一致以及推理能力较低的问题，影响了用户体验和全球化部署。</p>
<p><strong>Result:</strong> M-Thinker-1.5B&#x2F;7B模型在MMATH和PolyMath基准测试中表现出色，实现了近100%的语言一致性，并在域外语言上展现了优秀的泛化能力。</p>
<p><strong>Insight:</strong> 通过强化学习对语言一致性和跨语言推理能力进行优化，可以有效提升模型在多语言任务中的表现，为全球化部署提供了可能。</p>
<p><strong>Abstract:</strong> Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the “think-then-answer” paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the model’s non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B&#x2F;7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.</p>
  </div>
</details>

<hr>
<h3 id="123-Agent-Bain-vs-Agent-McKinsey-A-New-Text-to-SQL-Benchmark-for-the-Business-Domain-cs-CLPDF"><a href="#123-Agent-Bain-vs-Agent-McKinsey-A-New-Text-to-SQL-Benchmark-for-the-Business-Domain-cs-CLPDF" class="headerlink" title="[123] Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain cs.CLPDF"></a>[123] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07309">Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07309" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yue Li, Ran Tao, Derek Hommel, Yusuf Denizay Dönder, Sungyong Chang</span></p>
<p><strong>TL;DR:</strong> CORGI是一个新的文本到SQL基准测试，专注于真实业务场景，涵盖描述性、解释性、预测性和建议性问题，揭示了LLM在高阶业务查询中的不足。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有文本到SQL基准测试主要关注历史数据的检索，无法满足业务领域对复杂查询的需求，特别是涉及因果推理、时间预测和多步决策的场景。</p>
<p><strong>Result:</strong> LLM在高阶业务查询（如预测和建议）中表现不佳，CORGI比BIRD基准难21%。</p>
<p><strong>Insight:</strong> 现有LLM在真实业务场景中的多步推理和智能决策能力仍需提升，强调了业务智能的需求与LLM能力的差距。</p>
<p><strong>Abstract:</strong> In the business domain, where data-driven decision making is crucial, text-to-SQL is fundamental for easy natural language access to structured data. While recent LLMs have achieved strong performance in code generation, existing text-to-SQL benchmarks remain focused on factual retrieval of past records. We introduce CORGI, a new benchmark specifically designed for real-world business contexts. CORGI is composed of synthetic databases inspired by enterprises such as Doordash, Airbnb, and Lululemon. It provides questions across four increasingly complex categories of business queries: descriptive, explanatory, predictive, and recommendational. This challenge calls for causal reasoning, temporal forecasting, and strategic recommendation, reflecting multi-level and multi-step agentic intelligence. We find that LLM performance drops on high-level questions, struggling to make accurate predictions and offer actionable plans. Based on execution success rate, the CORGI benchmark is about 21% more difficult than the BIRD benchmark. This highlights the gap between popular LLMs and the need for real-world business intelligence. We release a public dataset and evaluation framework, and a website for public submissions.</p>
  </div>
</details>

<hr>
<h3 id="124-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference-cs-CL-cs-AI-cs-LG-cs-SEPDF"><a href="#124-Vibe-Checker-Aligning-Code-Evaluation-with-Human-Preference-cs-CL-cs-AI-cs-LG-cs-SEPDF" class="headerlink" title="[124] Vibe Checker: Aligning Code Evaluation with Human Preference cs.CL | cs.AI | cs.LG | cs.SEPDF"></a>[124] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07315">Vibe Checker: Aligning Code Evaluation with Human Preference</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG | cs.SE</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07315" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu</span></p>
<p><strong>TL;DR:</strong> 本文提出了Vibe Checker，通过结合功能正确性和代码指令遵循能力，量化LLMs与人类编程偏好的对齐程度，揭示了指令遵循是影响用户体验的关键因素。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前的代码评估（如pass@k）仅关注功能正确性，忽略了非功能性的人类偏好（如代码可读性、意图保留等）。本文旨在填补这一空白，研究如何量化LLMs在代码生成中对指令遵循的表现。</p>
<p><strong>Result:</strong> 评估显示，即使是顶级LLMs也难以同时满足多项指令，且功能表现常出现退化。综合评分（功能正确性+指令遵循）与人类偏好相关性最高，且指令遵循是主要区分因素。</p>
<p><strong>Insight:</strong> 1. 功能正确性不足以全面评估代码生成质量；2. 指令遵循是用户体验的核心组成部分；3. Vibe Checker为开发更符合人类偏好的模型提供了新方向。</p>
<p><strong>Abstract:</strong> Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models’ code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.</p>
  </div>
</details>

<hr>
<h3 id="125-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling-cs-CL-cs-AI-cs-LGPDF"><a href="#125-Artificial-Hippocampus-Networks-for-Efficient-Long-Context-Modeling-cs-CL-cs-AI-cs-LGPDF" class="headerlink" title="[125] Artificial Hippocampus Networks for Efficient Long-Context Modeling cs.CL | cs.AI | cs.LGPDF"></a>[125] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07318">Artificial Hippocampus Networks for Efficient Long-Context Modeling</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CL | cs.AI | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07318" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种结合RNN和Transformer优点的记忆框架，引入人工海马网络（AHN）来高效处理长序列建模任务，显著减少计算和内存需求。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 长序列建模面临RNN固定大小内存的高效性与Transformer无损增长内存的高保真性之间的权衡问题。</p>
<p><strong>Result:</strong> 实验表明，AHN增强的模型在LV-Eval和InfiniteBench基准上表现优于滑动窗口基线，并接近完整注意力模型的性能，同时计算和内存需求大幅降低。</p>
<p><strong>Insight:</strong> AHN框架为长序列建模提供了一种高效且轻量化的解决方案，尤其是在推理阶段显著减少了资源消耗。</p>
<p><strong>Abstract:</strong> Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer’s KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/ByteDance-Seed/AHN">https://github.com/ByteDance-Seed/AHN</a>.</p>
  </div>
</details>

<hr>
<div id='cs.CY'></div>

<h1 id="cs-CY-Back"><a href="#cs-CY-Back" class="headerlink" title="cs.CY [Back]"></a>cs.CY <a href="#toc">[Back]</a></h1><h3 id="126-Surgeons-Are-Indian-Males-and-Speech-Therapists-Are-White-Females-Auditing-Biases-in-Vision-Language-Models-for-Healthcare-Professionals-cs-CY-cs-AI-cs-CVPDF"><a href="#126-Surgeons-Are-Indian-Males-and-Speech-Therapists-Are-White-Females-Auditing-Biases-in-Vision-Language-Models-for-Healthcare-Professionals-cs-CY-cs-AI-cs-CVPDF" class="headerlink" title="[126] Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals cs.CY | cs.AI | cs.CVPDF"></a>[126] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06280">Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06280" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zohaib Hasan Siddiqui, Dayam Nadeem, Mohammad Masudur Rahman, Mohammad Nadeem, Shahab Saquib Sohail</span></p>
<p><strong>TL;DR:</strong> 该论文研究了视觉语言模型（VLMs）在医疗职业领域中反映的人口统计偏见，并提出了一种评估协议来衡量和评估这些偏见及其操作风险。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> AI模型（如CLIP和OpenCLIP）在医疗职业中可能存在偏见，这可能对公平性、合规性和患者信任产生负面影响。</p>
<p><strong>Result:</strong> 实验表明，多个VLMs在医疗职业中表现出一致的人口统计偏见。</p>
<p><strong>Insight:</strong> 研究强调了在医疗等关键领域中识别和解决AI模型偏见的重要性，以避免对公平性和患者信任的负面影响。</p>
<p><strong>Abstract:</strong> Vision language models (VLMs), such as CLIP and OpenCLIP, can encode and reflect stereotypical associations between medical professions and demographic attributes learned from web-scale data. We present an evaluation protocol for healthcare settings that quantifies associated biases and assesses their operational risk. Our methodology (i) defines a taxonomy spanning clinicians and allied healthcare roles (e.g., surgeon, cardiologist, dentist, nurse, pharmacist, technician), (ii) curates a profession-aware prompt suite to probe model behavior, and (iii) benchmarks demographic skew against a balanced face corpus. Empirically, we observe consistent demographic biases across multiple roles and vision models. Our work highlights the importance of bias identification in critical domains such as healthcare as AI-enabled hiring and workforce analytics can have downstream implications for equity, compliance, and patient trust.</p>
  </div>
</details>

<hr>
<h3 id="127-Asking-For-It-Question-Answering-for-Predicting-Rule-Infractions-in-Online-Content-Moderation-cs-CY-cs-AI-cs-CL-cs-HC-cs-LGPDF"><a href="#127-Asking-For-It-Question-Answering-for-Predicting-Rule-Infractions-in-Online-Content-Moderation-cs-CY-cs-AI-cs-CL-cs-HC-cs-LGPDF" class="headerlink" title="[127] Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation cs.CY | cs.AI | cs.CL | cs.HC | cs.LGPDF"></a>[127] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06350">Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CY | cs.AI | cs.CL | cs.HC | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06350" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mattia Samory, Diana Pamfile, Andrew To, Shruti Phadke</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新颖的问答框架ModQ，用于在线内容审核中规则违规的预测，通过将规则与评论关联，优于现有基线方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在线社区的规则多样且动态变化，传统审核方法难以应对。论文旨在通过问答框架提高审核的透明度和自动化能力。</p>
<p><strong>Result:</strong> ModQ在识别违规规则上优于现有基线，并能泛化到未见过的社区和规则。</p>
<p><strong>Insight:</strong> 问答框架为动态规则环境提供了一种灵活的审核方法，适用于低资源和多变的治理场景。</p>
<p><strong>Abstract:</strong> Online communities rely on a mix of platform policies and community-authored rules to define acceptable behavior and maintain order. However, these rules vary widely across communities, evolve over time, and are enforced inconsistently, posing challenges for transparency, governance, and automation. In this paper, we model the relationship between rules and their enforcement at scale, introducing ModQ, a novel question-answering framework for rule-sensitive content moderation. Unlike prior classification or generation-based approaches, ModQ conditions on the full set of community rules at inference time and identifies which rule best applies to a given comment. We implement two model variants - extractive and multiple-choice QA - and train them on large-scale datasets from Reddit and Lemmy, the latter of which we construct from publicly available moderation logs and rule descriptions. Both models outperform state-of-the-art baselines in identifying moderation-relevant rule violations, while remaining lightweight and interpretable. Notably, ModQ models generalize effectively to unseen communities and rules, supporting low-resource moderation settings and dynamic governance environments.</p>
  </div>
</details>

<hr>
<div id='cs.LG'></div>

<h1 id="cs-LG-Back"><a href="#cs-LG-Back" class="headerlink" title="cs.LG [Back]"></a>cs.LG <a href="#toc">[Back]</a></h1><h3 id="128-SaFeR-VLM-Toward-Safety-aware-Fine-grained-Reasoning-in-Multimodal-Models-cs-LG-cs-CVPDF"><a href="#128-SaFeR-VLM-Toward-Safety-aware-Fine-grained-Reasoning-in-Multimodal-Models-cs-LG-cs-CVPDF" class="headerlink" title="[128] SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models cs.LG | cs.CVPDF"></a>[128] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06871">SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06871" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Huahui Yi, Kun Wang, Qiankun Li, Miao Yu, Liang Lin</span></p>
<p><strong>TL;DR:</strong> SaFeR-VLM是一个安全感知的强化学习框架，旨在将安全性直接嵌入多模态推理过程中，通过数据集、安全感知的生成、结构化奖励模型和GRPO优化，显著提升了模型的安全性和实用性，超越了多个大规模模型。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的多模态大型推理模型（MLRMs）在推理过程中可能放大安全风险，现有防御措施主要作用于输出层面，未能约束推理过程，导致潜在风险。</p>
<p><strong>Result:</strong> SaFeR-VLM在安全性和实用性上超越了多个大规模模型，甚至在7B规模下优于GPT-5-mini和Gemini-2.5-Flash。</p>
<p><strong>Insight:</strong> 通过将安全性嵌入推理过程，解决了现有模型的潜在风险，证明了安全性和推理能力的协同提升是可行的。</p>
<p><strong>Abstract:</strong> Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal reasoning but often amplify safety risks under adversarial or unsafe prompts, a phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at the output level and do not constrain the reasoning process, leaving models exposed to implicit risks. In this paper, we propose SaFeR-VLM, a safety-aligned reinforcement learning framework that embeds safety directly into multimodal reasoning. The framework integrates four components: (I) QI-Safe-10K, a curated dataset emphasizing safety-critical and reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations undergo reflection and correction instead of being discarded; (III) structured reward modeling with multi-dimensional weighted criteria and explicit penalties for hallucinations and contradictions; and (IV) GRPO optimization, which reinforces both safe and corrected trajectories. This unified design shifts safety from a passive safeguard to an active driver of reasoning, enabling scalable and generalizable safety-aware reasoning. SaFeR-VLM further demonstrates robustness against both explicit and implicit risks, supporting dynamic and interpretable safety decisions beyond surface-level filtering. SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and helpfulness across six benchmarks, surpassing both same-scale and $&gt;10\times$ larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B. Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points respectively on safety metrics, achieving this improvement without any degradation in helpfulness performance. Our codes are available at <a target="_blank" rel="noopener" href="https://github.com/HarveyYi/SaFeR-VLM">https://github.com/HarveyYi/SaFeR-VLM</a>.</p>
  </div>
</details>

<hr>
<h3 id="129-The-Markovian-Thinker-cs-LG-cs-AI-cs-CLPDF"><a href="#129-The-Markovian-Thinker-cs-LG-cs-AI-cs-CLPDF" class="headerlink" title="[129] The Markovian Thinker cs.LG | cs.AI | cs.CLPDF"></a>[129] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06557">The Markovian Thinker</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06557" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Milad Aghajohari, Kamran Chitsaz, Amirhossein Kazemnejad, Sarath Chandar, Alessandro Sordoni</span></p>
<p><strong>TL;DR:</strong> 论文提出了Markovian Thinking范式，通过固定大小的状态解耦推理长度与上下文大小，实现了线性计算和恒定内存的使用。Delethink环境的实例化展示了该方法在长推理任务中的高效性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 标准强化学习（RL）的推理环境中，状态随推理长度增长，导致二次计算开销。作者希望通过重新设计推理环境，解决这一效率瓶颈。</p>
<p><strong>Result:</strong> Delethink在8K token的块中推理24K token，性能优于24K token的传统方法，计算开销显著降低。</p>
<p><strong>Insight:</strong> 重新设计推理环境是提升长推理效率的关键，现有预训练模型已具备Markovian特性的潜力。</p>
<p><strong>Abstract:</strong> Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL “thinking environment”, where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.</p>
  </div>
</details>

<hr>
<h3 id="130-Revisiting-Mixout-An-Overlooked-Path-to-Robust-Finetuning-cs-LG-cs-CVPDF"><a href="#130-Revisiting-Mixout-An-Overlooked-Path-to-Robust-Finetuning-cs-LG-cs-CVPDF" class="headerlink" title="[130] Revisiting Mixout: An Overlooked Path to Robust Finetuning cs.LG | cs.CVPDF"></a>[130] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06982">Revisiting Mixout: An Overlooked Path to Robust Finetuning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06982" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Masih Aminbeidokhti, Heitor Rapela Medeiros, Eric Granger, Marco Pedersoli</span></p>
<p><strong>TL;DR:</strong> 论文重新审视了Mixout方法，提出GMixout，通过动态锚点和显式采样频率提升模型微调的鲁棒性，实验证明其在域外泛化上优于现有方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的视觉基础模型微调通常在域内精度上表现良好，但在域外分布偏移时鲁棒性下降。Mixout作为一种正则化方法被重新审视，以解决这一问题。</p>
<p><strong>Result:</strong> 实验表明，GMixout在ImageNet、DomainNet等多个数据集上，不仅提升域内精度，且在域外分布偏移下超越Model Soups和其他参数高效微调基线。</p>
<p><strong>Insight:</strong> 研究揭示掩码锚点、采样频率和掩码稀疏性是控制模型鲁棒性的关键因素，动态调整这些参数能显著提升泛化性能。</p>
<p><strong>Abstract:</strong> Finetuning vision foundation models often improves in-domain accuracy but comes at the cost of robustness under distribution shift. We revisit Mixout, a stochastic regularizer that intermittently replaces finetuned weights with their pretrained reference, through the lens of a single-run, weight-sharing implicit ensemble. This perspective reveals three key levers that govern robustness: the \emph{masking anchor}, \emph{resampling frequency}, and \emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i) replaces the fixed anchor with an exponential moving-average snapshot that adapts during training, and (ii) regulates masking period via an explicit resampling-frequency hyperparameter. Our sparse-kernel implementation updates only a small fraction of parameters with no inference-time overhead, enabling training on consumer-grade GPUs. Experiments on benchmarks covering covariate shift, corruption, and class imbalance, ImageNet &#x2F; ImageNet-LT, DomainNet, iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy beyond zero-shot performance while surpassing both Model Soups and strong parameter-efficient finetuning baselines under distribution shift.</p>
  </div>
</details>

<hr>
<h3 id="131-Sharpness-Aware-Data-Generation-for-Zero-shot-Quantization-cs-LG-cs-CVPDF"><a href="#131-Sharpness-Aware-Data-Generation-for-Zero-shot-Quantization-cs-LG-cs-CVPDF" class="headerlink" title="[131] Sharpness-Aware Data Generation for Zero-shot Quantization cs.LG | cs.CVPDF"></a>[131] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07018">Sharpness-Aware Data Generation for Zero-shot Quantization</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07018" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Dung Hoang-Anh, Cuong Pham Trung Le, Jianfei Cai, Thanh-Toan Do</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新的零样本量化方法，通过在生成合成数据时考虑量化模型的锐度（sharpness）来提升模型的泛化能力。实验证明该方法在低比特量化设置下优于现有技术。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 零样本量化需要在不使用原始训练数据的情况下生成合成数据，但现有方法忽略了量化模型的锐度对泛化能力的影响。本文旨在通过优化锐度来改进合成数据的生成。</p>
<p><strong>Result:</strong> 在CIFAR-100和ImageNet数据集上的实验表明，该方法在低比特量化设置中优于现有技术。</p>
<p><strong>Insight:</strong> 量化模型的锐度对泛化能力有重要影响，通过梯度匹配优化锐度可以有效提升零样本量化的性能。</p>
<p><strong>Abstract:</strong> Zero-shot quantization aims to learn a quantized model from a pre-trained full-precision model with no access to original real training data. The common idea in zero-shot quantization approaches is to generate synthetic data for quantizing the full-precision model. While it is well-known that deep neural networks with low sharpness have better generalization ability, none of the previous zero-shot quantization works considers the sharpness of the quantized model as a criterion for generating training data. This paper introduces a novel methodology that takes into account quantized model sharpness in synthetic data generation to enhance generalization. Specifically, we first demonstrate that sharpness minimization can be attained by maximizing gradient matching between the reconstruction loss gradients computed on synthetic and real validation data, under certain assumptions. We then circumvent the problem of the gradient matching without real validation set by approximating it with the gradient matching between each generated sample and its neighbors. Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the superiority of the proposed method over the state-of-the-art techniques in low-bit quantization settings.</p>
  </div>
</details>

<hr>
<h3 id="132-A-Multi-Agent-Framework-for-Stateful-Inference-Time-Search-cs-LG-cs-AI-cs-CL-cs-MA-cs-SEPDF"><a href="#132-A-Multi-Agent-Framework-for-Stateful-Inference-Time-Search-cs-LG-cs-AI-cs-CL-cs-MA-cs-SEPDF" class="headerlink" title="[132] A Multi-Agent Framework for Stateful Inference-Time Search cs.LG | cs.AI | cs.CL | cs.MA | cs.SEPDF"></a>[132] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07147">A Multi-Agent Framework for Stateful Inference-Time Search</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.LG | cs.AI | cs.CL | cs.MA | cs.SE</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07147" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Arshika Lalan, Rajat Ghosh, Aditya Kolsur, Debojyoti Dutta</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种训练无关的多智能体状态推理框架，通过结合持久状态、对抗性变异和进化保留，显著提升了自动化单元测试生成的覆盖率和鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有无状态推理方法在多步任务中表现不佳，而任务特定微调或指令微调在需要深度推理和长时依赖的任务中效果有限。</p>
<p><strong>Result:</strong> 在HumanEval和TestGenEvalMini等基准测试中，该方法显著提升了单元测试生成的覆盖率，并适用于多种LLM家族（如Llama、Gemma和GPT）。</p>
<p><strong>Insight:</strong> 结合持久推理状态和进化搜索能有效提升单元测试生成的性能，尤其在复杂任务中表现突出。</p>
<p><strong>Abstract:</strong> Recent work explores agentic inference-time techniques to perform structured, multi-step reasoning. However, stateless inference often struggles on multi-step tasks due to the absence of persistent state. Moreover, task-specific fine-tuning or instruction-tuning often achieve surface-level code generation but remain brittle on tasks requiring deeper reasoning and long-horizon dependencies. To address these limitations, we propose stateful multi-agent evolutionary search, a training-free framework that departs from prior stateless approaches by combining (i) persistent inference-time state, (ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate its effectiveness in automated unit test generation through the generation of edge cases. We generate robust edge cases using an evolutionary search process, where specialized agents sequentially propose, mutate, and score candidates. A controller maintains persistent state across generations, while evolutionary preservation ensures diversity and exploration across all possible cases. This yields a generalist agent capable of discovering robust, high-coverage edge cases across unseen codebases. Experiments show our stateful multi-agent inference framework achieves substantial gains in coverage over stateless single-step baselines, evaluated on prevalent unit-testing benchmarks such as HumanEval and TestGenEvalMini and using three diverse LLM families - Llama, Gemma, and GPT. These results indicate that combining persistent inference-time state with evolutionary search materially improves unit-test generation.</p>
  </div>
</details>

<hr>
<div id='cs.CR'></div>

<h1 id="cs-CR-Back"><a href="#cs-CR-Back" class="headerlink" title="cs.CR [Back]"></a>cs.CR <a href="#toc">[Back]</a></h1><h3 id="133-Reading-Between-the-Lines-Towards-Reliable-Black-box-LLM-Fingerprinting-via-Zeroth-order-Gradient-Estimation-cs-CR-cs-AI-cs-CLPDF"><a href="#133-Reading-Between-the-Lines-Towards-Reliable-Black-box-LLM-Fingerprinting-via-Zeroth-order-Gradient-Estimation-cs-CR-cs-AI-cs-CLPDF" class="headerlink" title="[133] Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation cs.CR | cs.AI | cs.CLPDF"></a>[133] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06605">Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06605" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Shuo Shao, Yiming Li, Hongwei Yao, Yifei Chen, Yuchen Yang</span></p>
<p><strong>TL;DR:</strong> 该论文提出了ZeroPrint方法，通过零阶梯度估计在黑盒环境中生成LLM的指纹，解决了现有黑盒指纹方法因依赖非线性输出而导致信息损失的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> LLMs作为高价值知识产权需要可靠的版权保护手段，但现有黑盒指纹方法因依赖模型输出而难以生成独特指纹。</p>
<p><strong>Result:</strong> 在标准基准测试中，ZeroPrint显著优于现有黑盒方法，表现出优异的效果和鲁棒性。</p>
<p><strong>Insight:</strong> 输入梯度比输出更能反映模型的独特参数信息，是生成LLM指纹的更有效特征。</p>
<p><strong>Abstract:</strong> The substantial investment required to develop Large Language Models (LLMs) makes them valuable intellectual property, raising significant concerns about copyright protection. LLM fingerprinting has emerged as a key technique to address this, which aims to verify a model’s origin by extracting an intrinsic, unique signature (a “fingerprint”) and comparing it to that of a source model to identify illicit copies. However, existing black-box fingerprinting methods often fail to generate distinctive LLM fingerprints. This ineffectiveness arises because black-box methods typically rely on model outputs, which lose critical information about the model’s unique parameters due to the usage of non-linear functions. To address this, we first leverage Fisher Information Theory to formally demonstrate that the gradient of the model’s input is a more informative feature for fingerprinting than the output. Based on this insight, we propose ZeroPrint, a novel method that approximates these information-rich gradients in a black-box setting using zeroth-order estimation. ZeroPrint overcomes the challenge of applying this to discrete text by simulating input perturbations via semantic-preserving word substitutions. This operation allows ZeroPrint to estimate the model’s Jacobian matrix as a unique fingerprint. Experiments on the standard benchmark show ZeroPrint achieves a state-of-the-art effectiveness and robustness, significantly outperforming existing black-box methods.</p>
  </div>
</details>

<hr>
<h3 id="134-RedTWIZ-Diverse-LLM-Red-Teaming-via-Adaptive-Attack-Planning-cs-CR-cs-CLPDF"><a href="#134-RedTWIZ-Diverse-LLM-Red-Teaming-via-Adaptive-Attack-Planning-cs-CR-cs-CLPDF" class="headerlink" title="[134] RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning cs.CR | cs.CLPDF"></a>[134] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06994">RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.CR | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06994" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Artur Horal, Daniel Pina, Henrique Paz, Iago Paulo, João Soares</span></p>
<p><strong>TL;DR:</strong> 论文提出了RedTWIZ框架，用于通过自适应攻击规划对大型语言模型（LLM）进行多样性红队测试，评估其在AI辅助软件开发中的鲁棒性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LLM在对抗性攻击下的鲁棒性评估缺乏系统性和多样性，亟需一种能够生成多轮、目标导向的攻击策略的方法。</p>
<p><strong>Result:</strong> 实验表明，RedTWIZ的多轮对抗攻击策略成功诱导了前沿LLM产生不安全输出，揭示了LLM鲁棒性问题。</p>
<p><strong>Insight:</strong> 研究强调了需进一步探索提升LLM鲁棒性的方法，尤其是多轮对话中的对抗性防御。</p>
<p><strong>Abstract:</strong> This paper presents the vision, scientific contributions, and technical details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework, to audit the robustness of Large Language Models (LLMs) in AI-assisted software development. Our work is driven by three major research streams: (1) robust and systematic assessment of LLM conversational jailbreaks; (2) a diverse generative multi-turn attack suite, supporting compositional, realistic and goal-oriented jailbreak conversational strategies; and (3) a hierarchical attack planner, which adaptively plans, serializes, and triggers attacks tailored to specific LLM’s vulnerabilities. Together, these contributions form a unified framework – combining assessment, attack generation, and strategic planning – to comprehensively evaluate and expose weaknesses in LLMs’ robustness. Extensive evaluation is conducted to systematically assess and analyze the performance of the overall system and each component. Experimental results demonstrate that our multi-turn adversarial attack strategies can successfully lead state-of-the-art LLMs to produce unsafe generations, highlighting the pressing need for more research into enhancing LLM’s robustness.</p>
  </div>
</details>

<hr>
<div id='cs.AI'></div>

<h1 id="cs-AI-Back"><a href="#cs-AI-Back" class="headerlink" title="cs.AI [Back]"></a>cs.AI <a href="#toc">[Back]</a></h1><h3 id="135-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning-cs-AI-cs-CL-cs-LGPDF"><a href="#135-AlphaApollo-Orchestrating-Foundation-Models-and-Professional-Tools-into-a-Self-Evolving-System-for-Deep-Agentic-Reasoning-cs-AI-cs-CL-cs-LGPDF" class="headerlink" title="[135] AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning cs.AI | cs.CL | cs.LGPDF"></a>[135] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06261">AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06261" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhanke Zhou, Chentao Cao, Xiao Feng, Xuan Li, Zongze Li</span></p>
<p><strong>TL;DR:</strong> AlphaApollo是一个自演化的代理推理系统，通过结合基础模型和专业工具，解决了基础模型推理中容量有限和测试迭代不可靠的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 基础模型在推理时存在模型内部容量限制和测试迭代不可靠的问题，AlphaApollo通过整合专业工具和多轮多模型协作来提升推理能力。</p>
<p><strong>Result:</strong> 在AIME 2024&#x2F;2025评测中，AlphaApollo显著提升了Qwen2.5-14B-Instruct和Llama-3.3-70B-Instruct的性能，工具调用成功率超过80%。</p>
<p><strong>Insight:</strong> 通过工具整合和协作机制，AlphaApollo有效提升了基础模型的推理能力，展示了自演化系统的潜力。</p>
<p><strong>Abstract:</strong> We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024&#x2F;2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at <a target="_blank" rel="noopener" href="https://github.com/tmlr-group/AlphaApollo">https://github.com/tmlr-group/AlphaApollo</a>.</p>
  </div>
</details>

<hr>
<h3 id="136-PuzzlePlex-Benchmarking-Foundation-Models-on-Reasoning-and-Planning-with-Puzzles-cs-AI-cs-CLPDF"><a href="#136-PuzzlePlex-Benchmarking-Foundation-Models-on-Reasoning-and-Planning-with-Puzzles-cs-AI-cs-CLPDF" class="headerlink" title="[136] PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles cs.AI | cs.CLPDF"></a>[136] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06475">PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06475" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yitao Long, Yuru Jiang, Hongjun Liu, Yilun Zhao, Jingchen Sun</span></p>
<p><strong>TL;DR:</strong> 引入了PuzzlePlex基准，用于评估基础模型在复杂动态环境中的推理和规划能力，涵盖15种不同类型的谜题，并提出定制化评测指标和分析方法。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究背景是为了了解基础模型在复杂环境中的推理和规划能力，并通过多样化的谜题设计一个可扩展的评测基准。</p>
<p><strong>Result:</strong> 结果显示推理模型在指令设置中表现更优，而代码执行更具挑战性但可扩展性更强。</p>
<p><strong>Insight:</strong> PuzzlePlex为未来基础模型的推理和规划能力改进提供了指导，展示了评测基准的重要性。</p>
<p><strong>Abstract:</strong> This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models.</p>
  </div>
</details>

<hr>
<h3 id="137-Evolving-and-Executing-Research-Plans-via-Double-Loop-Multi-Agent-Collaboration-cs-AI-cs-CLPDF"><a href="#137-Evolving-and-Executing-Research-Plans-via-Double-Loop-Multi-Agent-Collaboration-cs-AI-cs-CLPDF" class="headerlink" title="[137] Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration cs.AI | cs.CLPDF"></a>[137] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06761">Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06761" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种双循环多智能体（DLMA）框架，用于自动生成和执行科研计划。上层循环（教授智能体）通过进化算法迭代优化研究提案，下层循环（博士生智能体）动态调整执行过程，确保计划的正确实施。实验表明，DLMA在研究论文自动评估中表现优异。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 端到端自动化科研过程面临高层计划生成与动态执行的双重挑战，传统的单层方法难以兼顾新颖性和正确性。</p>
<p><strong>Result:</strong> 在ACLAward和Laboratory等基准测试中，DLMA生成的论文在自动评估中达到SOTA分数，显著优于基线方法。</p>
<p><strong>Insight:</strong> 双循环设计将新颖性（进化）和正确性（执行）分离并协同优化，是自动化科研领域的重要突破。</p>
<p><strong>Abstract:</strong> Automating the end-to-end scientific research process poses a fundamental challenge: it requires both evolving high-level plans that are novel and sound, and executing these plans correctly amidst dynamic and uncertain conditions. To address this bilevel challenge, we propose a novel Double-Loop Multi-Agent (DLMA) framework to solve the given research problem automatically. The leader loop, composed of professor agents, is responsible for evolving research plans. It employs an evolutionary algorithm through involvement, improvement, and integration meetings to iteratively generate and refine a pool of research proposals, exploring the solution space effectively. The follower loop, composed of doctoral student agents, is responsible for executing the best-evolved plan. It dynamically adjusts the plan during implementation via pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is well-supported by contextual and external observations. Extensive experiments on benchmarks like ACLAward and Laboratory show that DLMA generates research papers that achieve state-of-the-art scores in automated evaluation, significantly outperforming strong baselines. Ablation studies confirm the critical roles of both loops, with evolution driving novelty and execution ensuring soundness.</p>
  </div>
</details>

<hr>
<h3 id="138-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces-cs-AI-cs-CLPDF"><a href="#138-Revisiting-the-Uniform-Information-Density-Hypothesis-in-LLM-Reasoning-Traces-cs-AI-cs-CLPDF" class="headerlink" title="[138] Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces cs.AI | cs.CLPDF"></a>[138] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06953">Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.AI | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06953" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Minju Gwak, Guijin Son, Jaehyung Kim</span></p>
<p><strong>TL;DR:</strong> 本文探索了大型语言模型（LLM）推理过程中信息密度的均匀性，提出了一种基于熵的逐步信息密度度量方法，并通过实验验证了信息密度均匀性与推理质量的正相关性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是验证均匀信息密度假说（UID）是否适用于LLM的推理过程，并探索信息密度的均匀性是否可以作为推理质量的指标。</p>
<p><strong>Result:</strong> 实验结果显示：1）信息密度均匀性与推理质量显著相关；2）正确的推理轨迹避免了信息密度的剧烈波动；3）基于信息密度的均匀性选择推理轨迹可提升准确性（相对提升10-32%）。</p>
<p><strong>Insight:</strong> 研究发现信息密度的均匀性是推理质量的重要指标，避免信息密度剧烈波动有助于生成更可靠的推理系统。</p>
<p><strong>Abstract:</strong> The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.</p>
  </div>
</details>

<hr>
<div id='cs.IR'></div>

<h1 id="cs-IR-Back"><a href="#cs-IR-Back" class="headerlink" title="cs.IR [Back]"></a>cs.IR <a href="#toc">[Back]</a></h1><h3 id="139-Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction-cs-IR-cs-CLPDF"><a href="#139-Crossing-Domains-without-Labels-Distant-Supervision-for-Term-Extraction-cs-IR-cs-CLPDF" class="headerlink" title="[139] Crossing Domains without Labels: Distant Supervision for Term Extraction cs.IR | cs.CLPDF"></a>[139] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06838">Crossing Domains without Labels: Distant Supervision for Term Extraction</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.IR | cs.CL</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06838" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种利用远程监督和大语言模型（LLM）的术语提取方法，解决了现有方法依赖昂贵人工标注和跨域迁移困难的问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有术语提取方法（ATE）依赖大量人工标注且跨域性能差，限制了实际应用。因此，需要一种更鲁棒、可扩展的解决方案和更现实的评估设置。</p>
<p><strong>Result:</strong> 在7个领域中，该方法在5个领域上优于先前方法，平均提升10个百分点。</p>
<p><strong>Insight:</strong> 1. 远程监督和伪标签生成可以有效减少对人工标注的依赖；2. LLM在术语提取任务中表现优越，尤其在跨域场景；3. 轻量级后处理方法对文档级一致性至关重要。</p>
<p><strong>Abstract:</strong> Automatic Term Extraction (ATE) is a critical component in downstream NLP tasks such as document tagging, ontology construction and patent analysis. Current state-of-the-art methods require expensive human annotation and struggle with domain transfer, limiting their practical deployment. This highlights the need for more robust, scalable solutions and realistic evaluation settings. To address this, we introduce a comprehensive benchmark spanning seven diverse domains, enabling performance evaluation at both the document- and corpus-levels. Furthermore, we propose a robust LLM-based model that outperforms both supervised cross-domain encoder models and few-shot learning baselines and performs competitively with its GPT-4o teacher on this benchmark. The first step of our approach is generating psuedo-labels with this black-box LLM on general and scientific domains to ensure generalizability. Building on this data, we fine-tune the first LLMs for ATE. To further enhance document-level consistency, oftentimes needed for downstream tasks, we introduce lightweight post-hoc heuristics. Our approach exceeds previous approaches on 5&#x2F;7 domains with an average improvement of 10 percentage points. We release our dataset and fine-tuned models to support future research in this area.</p>
  </div>
</details>

<hr>
<div id='cs.RO'></div>

<h1 id="cs-RO-Back"><a href="#cs-RO-Back" class="headerlink" title="cs.RO [Back]"></a>cs.RO <a href="#toc">[Back]</a></h1><h3 id="140-Vision-Language-Action-Models-for-Robotics-A-Review-Towards-Real-World-Applications-cs-RO-cs-AI-cs-CV-cs-LGPDF"><a href="#140-Vision-Language-Action-Models-for-Robotics-A-Review-Towards-Real-World-Applications-cs-RO-cs-AI-cs-CV-cs-LGPDF" class="headerlink" title="[140] Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications cs.RO | cs.AI | cs.CV | cs.LGPDF"></a>[140] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07077">Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07077" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kento Kawaharazuka, Jihoon Oh, Jun Yamada, Ingmar Posner, Yuke Zhu</span></p>
<p><strong>TL;DR:</strong> 这篇论文综述了Vision-Language-Action (VLA) 模型在机器人领域的应用，涵盖了从模型架构、学习范式到实际部署的全栈视角。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 大型语言模型(LLMs)和视觉语言模型(VLMs)在机器人领域的应用潜力引起了广泛关注。VLA模型试图统一视觉、语言和动作数据，以实现在多样化任务和环境中的泛化能力，从而推动机器人技术的灵活和规模化部署。</p>
<p><strong>Result:</strong> 论文总结了当前VLA模型的研究进展，并为机器人社区提供了在实际系统中应用VLA模型的实用指南和资源。</p>
<p><strong>Insight:</strong> VLA模型的泛化能力有望减少对任务特定数据的依赖，但其在实际部署中仍需解决数据收集、模态对齐和硬件集成的挑战。</p>
<p><strong>Abstract:</strong> Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: <a target="_blank" rel="noopener" href="https://vla-survey.github.io/">https://vla-survey.github.io</a> .</p>
  </div>
</details>

<hr>
<h3 id="141-TrackVLA-Unleashing-Reasoning-and-Memory-Capabilities-in-VLA-Models-for-Embodied-Visual-Tracking-cs-RO-cs-AI-cs-CVPDF"><a href="#141-TrackVLA-Unleashing-Reasoning-and-Memory-Capabilities-in-VLA-Models-for-Embodied-Visual-Tracking-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[141] TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking cs.RO | cs.AI | cs.CVPDF"></a>[141] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07134">TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07134" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Jiahang Liu, Yunpeng Qi, Jiazhao Zhang, Minghan Li, Shaoan Wang</span></p>
<p><strong>TL;DR:</strong> TrackVLA++ 是一种新的视觉-语言-动作（VLA）模型，通过引入空间推理机制和目标识别内存（TIM），显著提升了具身视觉跟踪的能力，并在公开基准上实现了最先进的性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 现有的具身视觉跟踪方法缺乏显式的空间推理和有效的时间记忆能力，导致在严重遮挡或相似干扰物存在的情况下失败。TrackVLA++ 旨在解决这些问题。</p>
<p><strong>Result:</strong> TrackVLA++ 在公开基准上表现优异，显着超越现有方法，并且在零样本泛化能力上表现出色。</p>
<p><strong>Insight:</strong> 显式空间推理和目标记忆机制是具身视觉跟踪中解决遮挡和干扰问题的关键。</p>
<p><strong>Abstract:</strong> Embodied Visual Tracking (EVT) is a fundamental ability that underpins practical applications, such as companion robots, guidance robots and service assistants, where continuously following moving targets is essential. Recent advances have enabled language-guided tracking in complex and unstructured scenes. However, existing approaches lack explicit spatial reasoning and effective temporal memory, causing failures under severe occlusions or in the presence of similar-looking distractors. To address these challenges, we present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances embodied visual tracking with two key modules, a spatial reasoning mechanism and a Target Identification Memory (TIM). The reasoning module introduces a Chain-of-Thought paradigm, termed Polar-CoT, which infers the target’s relative position and encodes it as a compact polar-coordinate token for action prediction. Guided by these spatial priors, the TIM employs a gated update strategy to preserve long-horizon target memory, ensuring spatiotemporal consistency and mitigating target loss during extended occlusions. Extensive experiments show that TrackVLA++ achieves state-of-the-art performance on public benchmarks across both egocentric and multi-camera settings. On the challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong zero-shot generalization, enabling robust real-world tracking in dynamic and occluded scenarios.</p>
  </div>
</details>

<hr>
<h3 id="142-TIGeR-Tool-Integrated-Geometric-Reasoning-in-Vision-Language-Models-for-Robotics-cs-RO-cs-AI-cs-CVPDF"><a href="#142-TIGeR-Tool-Integrated-Geometric-Reasoning-in-Vision-Language-Models-for-Robotics-cs-RO-cs-AI-cs-CVPDF" class="headerlink" title="[142] TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics cs.RO | cs.AI | cs.CVPDF"></a>[142] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07181">TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.RO | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07181" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An</span></p>
<p><strong>TL;DR:</strong> TIGeR提出了一种新框架，将视觉语言模型（VLMs）从感知估计工具转变为几何计算工具，通过外部工具生成和执行精确的几何计算，显著提升了机器人操作的厘米级精度。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前VLMs在空间推理中依赖定性精度，无法满足机器人操作所需的计算精度需求。作者希望通过工具集成几何推理来解决这一限制。</p>
<p><strong>Result:</strong> TIGeR在几何推理基准测试中实现了SOTA性能，并在真实机器人操作任务中展示了厘米级精度。</p>
<p><strong>Insight:</strong> 工具集成可以弥补神经网络在精确几何计算上的不足，同时保留了VLMs的高层次推理能力。</p>
<p><strong>Abstract:</strong> Vision-Language Models (VLMs) have shown remarkable capabilities in spatial reasoning, yet they remain fundamentally limited to qualitative precision and lack the computational precision required for real-world robotics. Current approaches fail to leverage metric cues from depth sensors and camera calibration, instead reducing geometric problems to pattern recognition tasks that cannot deliver the centimeter-level accuracy essential for robotic manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel framework that transforms VLMs from perceptual estimators to geometric computers by enabling them to generate and execute precise geometric computations through external tools. Rather than attempting to internalize complex geometric operations within neural networks, TIGeR empowers models to recognize geometric reasoning requirements, synthesize appropriate computational code, and invoke specialized libraries for exact calculations. To support this paradigm, we introduce TIGeR-300K, a comprehensive tool-invocation-oriented dataset covering point transformations, pose estimation, trajectory generation, and spatial compatibility verification, complete with tool invocation sequences and intermediate computations. Through a two-stage training pipeline combining supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT) with our proposed hierarchical reward design, TIGeR achieves SOTA performance on geometric reasoning benchmarks while demonstrating centimeter-level precision in real-world robotic manipulation tasks.</p>
  </div>
</details>

<hr>
<div id='cs.SD'></div>

<h1 id="cs-SD-Back"><a href="#cs-SD-Back" class="headerlink" title="cs.SD [Back]"></a>cs.SD <a href="#toc">[Back]</a></h1><h3 id="143-XLSR-Kanformer-A-KAN-Intergrated-model-for-Synthetic-Speech-Detection-cs-SD-cs-CL-eess-ASPDF"><a href="#143-XLSR-Kanformer-A-KAN-Intergrated-model-for-Synthetic-Speech-Detection-cs-SD-cs-CL-eess-ASPDF" class="headerlink" title="[143] XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection cs.SD | cs.CL | eess.ASPDF"></a>[143] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06706">XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SD | cs.CL | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06706" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Phuong Tuan Dat, Tran Huy Dat</span></p>
<p><strong>TL;DR:</strong> 论文提出了一种新型的XLSR-Kanformer模型，通过将XLSR-Conformer中的MLP替换为Kolmogorov-Arnold Network (KAN)，显著提升了合成语音检测性能。在ASVspoof2021数据集上，相对误等率(EER)提升了60.55%，并在21LA子集上实现了0.70%的EER。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 由于语音合成技术的进步，欺骗攻击变得越来越复杂，给自动说话人验证系统带来了巨大挑战。现有基于自监督学习(SSL)的XLSR-Conformer模型虽有优异表现，但仍需架构改进以进一步提升性能。</p>
<p><strong>Result:</strong> 实验结果显示，XLSR-Kanformer在LA和DF子集上的EER相对提升了60.55%，并在21LA子集上实现了0.70%的EER，验证了方法的有效性。</p>
<p><strong>Insight:</strong> KAN作为一种通用逼近器，可以有效提升SSL模型的性能，未来在合成语音检测领域有广阔的应用前景。</p>
<p><strong>Abstract:</strong> Recent advancements in speech synthesis technologies have led to increasingly sophisticated spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer architecture, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator based on the Kolmogorov-Arnold representation theorem. Our experimental results on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed replacement is also robust to various SSL architectures. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.</p>
  </div>
</details>

<hr>
<h3 id="144-AudioMarathon-A-Comprehensive-Benchmark-for-Long-Context-Audio-Understanding-and-Efficiency-in-Audio-LLMs-cs-SD-cs-AI-cs-CL-eess-ASPDF"><a href="#144-AudioMarathon-A-Comprehensive-Benchmark-for-Long-Context-Audio-Understanding-and-Efficiency-in-Audio-LLMs-cs-SD-cs-AI-cs-CL-eess-ASPDF" class="headerlink" title="[144] AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs cs.SD | cs.AI | cs.CL | eess.ASPDF"></a>[144] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.07293">AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.SD | cs.AI | cs.CL | eess.AS</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.07293" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu</span></p>
<p><strong>TL;DR:</strong> AudioMarathon是一个针对长上下文音频理解的综合基准测试，旨在解决当前大型音频语言模型（LALMs）在处理长音频时的效率和性能问题。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 当前LALMs在处理长音频时面临注意力的二次方开销和长范围时间依赖建模的挑战，而现有音频基准测试多基于短片段，无法评估长上下文场景下的模型表现。</p>
<p><strong>Result:</strong> 实验显示随着音频长度增加，模型性能显著下降，突出了当前模型的局限性，同时分析了令牌修剪和KV缓存淘汰的权衡。</p>
<p><strong>Insight:</strong> AudioMarathon揭示了现有LALMs在时间推理和内存效率上的不足，为未来模型改进提供了方向，推动了音频和多模态研究的进步。</p>
<p><strong>Abstract:</strong> Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.</p>
  </div>
</details>

<hr>
<div id='cs.HC'></div>

<h1 id="cs-HC-Back"><a href="#cs-HC-Back" class="headerlink" title="cs.HC [Back]"></a>cs.HC <a href="#toc">[Back]</a></h1><h3 id="145-GPT-5-Model-Corrected-GPT-4V’s-Chart-Reading-Errors-Not-Prompting-cs-HC-cs-CL-cs-CVPDF"><a href="#145-GPT-5-Model-Corrected-GPT-4V’s-Chart-Reading-Errors-Not-Prompting-cs-HC-cs-CL-cs-CVPDF" class="headerlink" title="[145] GPT-5 Model Corrected GPT-4V’s Chart Reading Errors, Not Prompting cs.HC | cs.CL | cs.CVPDF"></a>[145] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06782">GPT-5 Model Corrected GPT-4V’s Chart Reading Errors, Not Prompting</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.HC | cs.CL | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06782" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Kaichun Yang, Jian Chen</span></p>
<p><strong>TL;DR:</strong> 论文通过定量评估比较了零样本大型语言模型（LLMs）和多模态GPT-4V在图表阅读任务中的表现，发现GPT-5显著提升了准确性，而提示变体的影响较小。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究旨在理解零样本大型语言模型和多模态模型在图表阅读任务中的表现差异，尤其是对GPT-4V难以处理的困难图像实例。</p>
<p><strong>Result:</strong> GPT-5大幅提高了准确性，而提示变体的效果有限，表明模型架构是影响推理准确性的主要因素。</p>
<p><strong>Insight:</strong> 模型改进（如GPT-5）在处理复杂任务（如图表阅读）时比提示工程更为关键。</p>
<p><strong>Abstract:</strong> We present a quantitative evaluation to understand the effect of zero-shot large-language model (LLMs) and prompting uses on chart reading tasks. We asked LLMs to answer 107 visualization questions to compare inference accuracies between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances, where GPT-4V failed to produce correct answers. Our results show that model architecture dominates the inference accuracy: GPT5 largely improved accuracy, while prompt variants yielded only small effects. Pre-registration of this work is available here: <a target="_blank" rel="noopener" href="https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3">https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3</a>; the Google Drive materials are here:<a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view">https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view</a>.</p>
  </div>
</details>

<hr>
<div id='cs.GR'></div>

<h1 id="cs-GR-Back"><a href="#cs-GR-Back" class="headerlink" title="cs.GR [Back]"></a>cs.GR <a href="#toc">[Back]</a></h1><h3 id="146-Capture-and-Interact-Rapid-3D-Object-Acquisition-and-Rendering-with-Gaussian-Splatting-in-Unity-cs-GR-cs-CVPDF"><a href="#146-Capture-and-Interact-Rapid-3D-Object-Acquisition-and-Rendering-with-Gaussian-Splatting-in-Unity-cs-GR-cs-CVPDF" class="headerlink" title="[146] Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity cs.GR | cs.CVPDF"></a>[146] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06802">Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">cs.GR | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06802" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Islomjon Shukhratov, Sergey Gorinsky</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于3D高斯泼溅（3D GS）的端到端流水线，用于快速捕获和实时渲染3D对象，应用于AR、数字孪生等领域，实现了移动设备扫描、云端处理和Unity交互渲染，速度达150 fps。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 实时捕获和渲染3D对象在许多应用中具有巨大潜力，如增强现实和数字孪生，但现有方法在速度和交互性上存在挑战。</p>
<p><strong>Result:</strong> 实验表明显卡处理扫描约需10分钟，笔记本电脑上实时渲染达150 fps。</p>
<p><strong>Insight:</strong> 该研究表明，结合移动设备和云端计算，能够高效实现3D对象的捕获与交互式渲染，为AR和远程协作提供了新思路。</p>
<p><strong>Abstract:</strong> Capturing and rendering three-dimensional (3D) objects in real time remain a significant challenge, yet hold substantial potential for applications in augmented reality, digital twin systems, remote collaboration and prototyping. We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS) to enable rapid acquisition and interactive rendering of real-world objects using a mobile device, cloud processing and a local computer. Users scan an object with a smartphone video, upload it for automated 3D reconstruction, and visualize it interactively in Unity at an average of 150 frames per second (fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and Unity rendering to support real-time telepresence. Our experiments show that the pipeline processes scans in approximately 10 minutes on a graphics processing unit (GPU) achieving real-time rendering on the laptop.</p>
  </div>
</details>

<hr>
<div id='eess.IV'></div>

<h1 id="eess-IV-Back"><a href="#eess-IV-Back" class="headerlink" title="eess.IV [Back]"></a>eess.IV <a href="#toc">[Back]</a></h1><h3 id="147-Stacked-Regression-using-Off-the-shelf-Stimulus-tuned-and-Fine-tuned-Neural-Networks-for-Predicting-fMRI-Brain-Responses-to-Movies-Algonauts-2025-Report-eess-IV-cs-AI-cs-CV-q-bio-NCPDF"><a href="#147-Stacked-Regression-using-Off-the-shelf-Stimulus-tuned-and-Fine-tuned-Neural-Networks-for-Predicting-fMRI-Brain-Responses-to-Movies-Algonauts-2025-Report-eess-IV-cs-AI-cs-CV-q-bio-NCPDF" class="headerlink" title="[147] Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report) eess.IV | cs.AI | cs.CV | q-bio.NCPDF"></a>[147] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06235">Stacked Regression using Off-the-shelf, Stimulus-tuned and Fine-tuned Neural Networks for Predicting fMRI Brain Responses to Movies (Algonauts 2025 Report)</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV | q-bio.NC</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06235" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Robert Scholz, Kunal Bagga, Christine Ahrends, Carlo Alberto Barbano</span></p>
<p><strong>TL;DR:</strong> 本文介绍了在Algonauts 2025挑战赛中提出的方法，通过整合多模态表示（包括大型语言模型、视频编码器、音频模型和视觉-语言模型），并采用堆叠回归结合预测，成功预测了电影刺激下的fMRI脑响应。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 研究动机是开发一种能够准确预测电影刺激下大脑响应的多模态编码模型，结合现有技术和微调策略以提高预测性能。</p>
<p><strong>Result:</strong> 团队在挑战赛中排名第10，证明了方法的有效性。</p>
<p><strong>Insight:</strong> 研究表明，多模态表示的融合和模型调优策略能显著提升大脑响应的预测性能，为未来脑编码模型提供了新思路。</p>
<p><strong>Abstract:</strong> We present our submission to the Algonauts 2025 Challenge, where the goal is to predict fMRI brain responses to movie stimuli. Our approach integrates multimodal representations from large language models, video encoders, audio models, and vision-language models, combining both off-the-shelf and fine-tuned variants. To improve performance, we enhanced textual inputs with detailed transcripts and summaries, and we explored stimulus-tuning and fine-tuning strategies for language and vision models. Predictions from individual models were combined using stacked regression, yielding solid results. Our submission, under the team name Seinfeld, ranked 10th. We make all code and resources publicly available, contributing to ongoing efforts in developing multimodal encoding models for brain activity.</p>
  </div>
</details>

<hr>
<h3 id="148-A-Total-Variation-Regularized-Framework-for-Epilepsy-Related-MRI-Image-Segmentation-eess-IV-cs-AI-cs-CVPDF"><a href="#148-A-Total-Variation-Regularized-Framework-for-Epilepsy-Related-MRI-Image-Segmentation-eess-IV-cs-AI-cs-CVPDF" class="headerlink" title="[148] A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation eess.IV | cs.AI | cs.CVPDF"></a>[148] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06276">A Total Variation Regularized Framework for Epilepsy-Related MRI Image Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06276" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mehdi Rabiee, Sergio Greco, Reza Shahbazian, Irina Trubitsyna</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种新的框架，用于分割3D脑MRI图像中的FCD区域，结合了Transformer增强的编码器-解码器结构和各向异性TV损失函数，显著提升了分割精度和一致性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> FCD由于其病变的细微性和小规模性，在脑MRI中难以检测，且现有方法在处理3D多模态输入和输出平滑性方面存在不足。</p>
<p><strong>Result:</strong> 在85名癫痫患者的公共数据集上，Dice系数提高了11.9%，精度提升了13.3%，假阳性簇减少了61.6%。</p>
<p><strong>Insight:</strong> TV损失项的引入可以有效提升分割结果的空间平滑性，减少假阳性簇，无需后处理。</p>
<p><strong>Abstract:</strong> Focal Cortical Dysplasia (FCD) is a primary cause of drug-resistant epilepsy and is difficult to detect in brain {magnetic resonance imaging} (MRI) due to the subtle and small-scale nature of its lesions. Accurate segmentation of FCD regions in 3D multimodal brain MRI images is essential for effective surgical planning and treatment. However, this task remains highly challenging due to the limited availability of annotated FCD datasets, the extremely small size and weak contrast of FCD lesions, the complexity of handling 3D multimodal inputs, and the need for output smoothness and anatomical consistency, which is often not addressed by standard voxel-wise loss functions. This paper presents a new framework for segmenting FCD regions in 3D brain MRI images. We adopt state-of-the-art transformer-enhanced encoder-decoder architecture and introduce a novel loss function combining Dice loss with an anisotropic {Total Variation} (TV) term. This integration encourages spatial smoothness and reduces false positive clusters without relying on post-processing. The framework is evaluated on a public FCD dataset with 85 epilepsy patients and demonstrates superior segmentation accuracy and consistency compared to standard loss formulations. The model with the proposed TV loss shows an 11.9% improvement on the Dice coefficient and 13.3% higher precision over the baseline model. Moreover, the number of false positive clusters is reduced by 61.6%</p>
  </div>
</details>

<hr>
<h3 id="149-SER-Diff-Synthetic-Error-Replay-Diffusion-for-Incremental-Brain-Tumor-Segmentation-eess-IV-cs-AI-cs-CVPDF"><a href="#149-SER-Diff-Synthetic-Error-Replay-Diffusion-for-Incremental-Brain-Tumor-Segmentation-eess-IV-cs-AI-cs-CVPDF" class="headerlink" title="[149] SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation eess.IV | cs.AI | cs.CVPDF"></a>[149] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06283">SER-Diff: Synthetic Error Replay Diffusion for Incremental Brain Tumor Segmentation</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.AI | cs.CV</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06283" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Sashank Makanaboyina</span></p>
<p><strong>TL;DR:</strong> SER-Diff是一种结合扩散模型和增量学习的创新框架，通过合成错误重放解决脑肿瘤分割中的灾难性遗忘问题，并在多个数据集上取得最优性能。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> 在脑肿瘤分割任务中，增量学习对于适应不断变化的临床数据至关重要，但灾难性遗忘问题限制了模型的性能。现有的增量学习方法依赖生成重放或额外存储，而扩散模型虽有潜力但尚未用于增量学习。</p>
<p><strong>Result:</strong> 在BraTS2020、BraTS2021和BraTS2023数据集上，SER-Diff的Dice分数分别达到95.8%、94.9%和94.6%，HD95值最低（4.4 mm、4.7 mm和4.9 mm），显著优于现有方法。</p>
<p><strong>Insight:</strong> SER-Diff的创新在于将扩散模型的细粒度生成能力与增量学习的需求结合，不仅缓解了灾难性遗忘，还提升了分割的准确性和解剖连贯性。</p>
<p><strong>Abstract:</strong> Incremental brain tumor segmentation is critical for models that must adapt to evolving clinical datasets without retraining on all prior data. However, catastrophic forgetting, where models lose previously acquired knowledge, remains a major obstacle. Recent incremental learning frameworks with knowledge distillation partially mitigate forgetting but rely heavily on generative replay or auxiliary storage. Meanwhile, diffusion models have proven effective for refining tumor segmentations, but have not been explored in incremental learning contexts. We propose Synthetic Error Replay Diffusion (SER-Diff), the first framework that unifies diffusion-based refinement with incremental learning. SER-Diff leverages a frozen teacher diffusion model to generate synthetic error maps from past tasks, which are replayed during training on new tasks. A dual-loss formulation combining Dice loss for new data and knowledge distillation loss for replayed errors ensures both adaptability and retention. Experiments on BraTS2020, BraTS2021, and BraTS2023 demonstrate that SER-Diff consistently outperforms prior methods. It achieves the highest Dice scores of 95.8%, 94.9%, and 94.6%, along with the lowest HD95 values of 4.4 mm, 4.7 mm, and 4.9 mm, respectively. These results indicate that SER-Diff not only mitigates catastrophic forgetting but also delivers more accurate and anatomically coherent segmentations across evolving datasets.</p>
  </div>
</details>

<hr>
<h3 id="150-Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data-eess-IV-cs-CV-cs-LGPDF"><a href="#150-Conditional-Denoising-Diffusion-Model-Based-Robust-MR-Image-Reconstruction-from-Highly-Undersampled-Data-eess-IV-cs-CV-cs-LGPDF" class="headerlink" title="[150] Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data eess.IV | cs.CV | cs.LGPDF"></a>[150] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2510.06335">Conditional Denoising Diffusion Model-Based Robust MR Image Reconstruction from Highly Undersampled Data</a> <span style="background-color: #2980b9; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;">eess.IV | cs.CV | cs.LG</span><span style="background-color: #e74c3c; color: white; padding: 1.5px 8px; border-radius: 4px; font-size: 0.8em; font-weight: bold; vertical-align: middle; margin: 0 5px; user-select: none;"><a href="https://arxiv.org/pdf/2510.06335" target="_blank" style="color: white; text-decoration: none;"><svg xmlns="http://www.w3.org/2000/svg" height="14px" viewBox="0 0 24 24" width="14px" fill="white" style="vertical-align: middle; margin-right: 3px;"><path d="M0 0h24v24H0V0z" fill="none"/><path d="M20 2H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-8.5 7.5c0 .83-.67 1.5-1.5 1.5H9v2H7.5V7H10c.83 0 1.5.67 1.5 1.5v1zm5 2c0 .83-.67 1.5-1.5 1.5h-2.5V7H15c.83 0 1.5.67 1.5 1.5v3zm4-3H19v1h1.5V11H19v2h-1.5V7h3v1.5zM9 9.5h1v-1H9v1zM4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm10 5.5h1v-3h-1v3z"/></svg>PDF</a></span></h3><p><span style="color: #7f8c8d; font-style: italic;">Mohammed Alsubaie, Wenxi Liu, Linxia Gu, Ovidiu C. Andronesi, Sirani M. Perera</span></p>
<p><strong>TL;DR:</strong> 该论文提出了一种基于条件去噪扩散模型的鲁棒MRI重建方法，通过在每个反向扩散步骤中嵌入测量模型，并结合成对的欠采样-真实数据训练，显著提升了重建图像的像素级保真度和感知真实性。</p>
<details style="background-color: #f6f8fa; border: 1px solid #e1e4e8; border-radius: 6px; padding: 20px; margin-top: 10px; margin-bottom: 20px;">
  <summary style="background-color: #eaf2f8; color: #2980b9; padding: 6px 12px; border-radius: 6px; border: none; cursor: pointer; display: inline-block; font-weight: 600; font-size: 0.9em; user-select: none;">Details</summary>
  <div style="margin-top: 10px;">

<p><strong>Motivation:</strong> MRI的采集时间过长是其临床应用的主要限制，而现有的欠采样重建方法往往无法兼顾图像质量和重建速度。扩散模型虽然展现了潜力，但通常在无监督或后处理中使用数据一致性。</p>
<p><strong>Result:</strong> 在fastMRI数据集上，该方法在SSIM、PSNR和LPIPS等指标上优于现有方法，尤其是LPIPS更好地捕捉了感知质量的提升。</p>
<p><strong>Insight:</strong> 将条件监督与迭代一致性更新结合，可以为加速MRI重建提供一种更鲁棒和实用的方法，同时兼顾像素级和感知层面的质量。</p>
<p><strong>Abstract:</strong> Magnetic Resonance Imaging (MRI) is a critical tool in modern medical diagnostics, yet its prolonged acquisition time remains a critical limitation, especially in time-sensitive clinical scenarios. While undersampling strategies can accelerate image acquisition, they often result in image artifacts and degraded quality. Recent diffusion models have shown promise for reconstructing high-fidelity images from undersampled data by learning powerful image priors; however, most existing approaches either (i) rely on unsupervised score functions without paired supervision or (ii) apply data consistency only as a post-processing step. In this work, we introduce a conditional denoising diffusion framework with iterative data-consistency correction, which differs from prior methods by embedding the measurement model directly into every reverse diffusion step and training the model on paired undersampled-ground truth data. This hybrid design bridges generative flexibility with explicit enforcement of MRI physics. Experiments on the fastMRI dataset demonstrate that our framework consistently outperforms recent state-of-the-art deep learning and diffusion-based methods in SSIM, PSNR, and LPIPS, with LPIPS capturing perceptual improvements more faithfully. These results demonstrate that integrating conditional supervision with iterative consistency updates yields substantial improvements in both pixel-level fidelity and perceptual realism, establishing a principled and practical advance toward robust, accelerated MRI reconstruction.</p>
  </div>
</details>

<hr>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2025-10-11/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2025-10-09/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
            
                Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> & <a target="_blank" rel="noopener" href="https://github.com/zoeingwingkei/frame/">Frame</a>
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
